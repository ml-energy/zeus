{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Zeus","text":"Deep Learning Energy Measurement and Optimization <p>Project News \u26a1 </p> <ul> <li>[2025/12] With NVIDIA, Google, and Meta, we led a NeurIPS 25 tutorial on Energy and Power as First-Class ML Design Metrics!</li> <li>[2025/12] The ML.ENERGY leaderboard got a major upgrade to v3. Read our in-depth technical analysis blog post.</li> <li>[2025/09] We shared our experience and design philosophy for The ML.ENERGY Benchmark in our NeurIPS 25 D&amp;B Spotlight paper.</li> <li>[2025/05] Zeus now supports CPU, DRAM, AMD GPU, Apple Silicon, and NVIDIA Jetson platform energy measurement!</li> <li>[2024/11] Perseus, an optimizer for large model training, appeared at SOSP'24! Paper | Blog | Optimizer</li> <li>[2024/05] Zeus is now a PyTorch ecosystem project. Read the PyTorch blog post here!</li> <li>[2024/02] Zeus was selected as a 2024 Mozilla Technology Fund awardee!</li> </ul> <p>Zeus is a library for (1) measuring the energy consumption of Deep Learning workloads and (2) optimizing their energy consumption.</p> <p>Zeus is part of The ML.ENERGY Initiative.</p>"},{"location":"#documentation-organization","title":"Documentation Organization","text":"<ul> <li>Getting Started: Instructions on installation and setup.</li> <li>Measuring Energy: How to measure time and energy programmatically and on the command line.</li> <li>Optimizing Energy: How to optimize energy.</li> <li>Research Overview: Overview of the research papers Zeus is rooted on.</li> <li>Source Code Reference: Auto-generated source code reference for the entire codebase.</li> </ul> <p>We also provide usage examples in our GitHub repository.</p> <p>If you find Zeus relevant to your research, please consider citing:</p> <pre><code>@inproceedings{zeus-nsdi23,\n    title     = {Zeus: Understanding and Optimizing {GPU} Energy Consumption of {DNN} Training},\n    author    = {Jie You and Jae-Won Chung and Mosharaf Chowdhury},\n    booktitle = {USENIX NSDI},\n    year      = {2023}\n}\n</code></pre>  Stay updated on latest news about Zeus and the ML.ENERGY Initiative.  Subscribe"},{"location":"getting_started/","title":"Getting Started","text":"<p>Most of the common setup steps are described in this page. Some optimizers or examples may require some extra setup steps, which are described in the corresponding documentation.</p>"},{"location":"getting_started/#installing-the-python-package","title":"Installing the Python package","text":""},{"location":"getting_started/#from-pypi","title":"From PyPI","text":"<p>Install the Zeus Python package simply with:</p> <pre><code>pip install zeus\n</code></pre>"},{"location":"getting_started/#from-source-for-development","title":"From source for development","text":"<p>You can also install Zeus from source by cloning our GitHub repository. Specifically for development, you can do an editable installation with extra dev dependencies:</p> <pre><code>git clone https://github.com/ml-energy/zeus.git\ncd zeus\npip install -e '.[dev]'\n</code></pre>"},{"location":"getting_started/#using-docker","title":"Using Docker","text":"<p>Dependencies</p> <p>You should have the following already installed on your system:</p> <ul> <li>Docker</li> <li>NVIDIA Container Toolkit</li> </ul> <p>Our Docker image should suit most of the use cases for Zeus. On top of the <code>nvidia/cuda:11.8.0-base-ubuntu22.04</code> image, we add:</p> <ul> <li>Miniconda 3, PyTorch, and Torchvision</li> <li>A copy of the Zeus repo in <code>/workspace/zeus</code></li> </ul> docker/Dockerfile Dockerfile<pre><code># Build instructions\n#   If you're building this image locally, make sure you specify `TARGETARCH`.\n#   Currently, this image supports `amd64` and `arm64`. For instance:\n#     docker build -t mlenergy/zeus:master --build-arg TARGETARCH=amd64 .\n\nFROM nvidia/cuda:11.8.0-base-ubuntu22.04\n\n# Basic installs\nARG DEBIAN_FRONTEND=noninteractive\nENV TZ='America/Detroit'\nRUN apt-get update -qq \\\n    &amp;&amp; apt-get -y --no-install-recommends install \\\n       build-essential software-properties-common wget git tar rsync cmake \\\n    &amp;&amp; apt-get clean all \\\n    &amp;&amp; rm -r /var/lib/apt/lists/*\n\n# Install Miniconda3 23.3.1\nENV PATH=\"/root/.local/miniconda3/bin:$PATH\"\nARG TARGETARCH\nRUN if [ \"$TARGETARCH\" = \"amd64\" ]; then \\\n      export CONDA_INSTALLER_PATH=\"Miniconda3-py39_23.3.1-0-Linux-x86_64.sh\"; \\\n    elif [ \"$TARGETARCH\" = \"arm64\" ]; then \\\n      export CONDA_INSTALLER_PATH=\"Miniconda3-py39_23.3.1-0-Linux-aarch64.sh\"; \\\n    else \\\n      echo \"Unsupported architecture ${TARGETARCH}\" &amp;&amp; exit 1; \\\n    fi \\\n    &amp;&amp; mkdir -p /root/.local \\\n    &amp;&amp; wget \"https://repo.anaconda.com/miniconda/$CONDA_INSTALLER_PATH\" \\\n    &amp;&amp; mkdir /root/.conda \\\n    &amp;&amp; bash \"$CONDA_INSTALLER_PATH\" -b -p /root/.local/miniconda3 \\\n    &amp;&amp; rm -f \"$CONDA_INSTALLER_PATH\" \\\n    &amp;&amp; ln -sf /root/.local/miniconda3/etc/profile.d/conda.sh /etc/profile.d/conda.sh\n\n# Install PyTorch and CUDA Toolkit\nRUN pip install --no-cache-dir torch==2.0.1 torchvision==0.15.2 --index-url https://download.pytorch.org/whl/cu118\n\n# Place stuff under /workspace\nWORKDIR /workspace\n\n# Snapshot of Zeus\nADD . /workspace/zeus\n\n# When an outside zeus directory is mounted, have it apply immediately.\nRUN cd /workspace/zeus &amp;&amp; pip install --no-cache-dir -e .\n</code></pre> <p>The default command would be:</p> <pre><code>docker run -it \\\n    --gpus all \\              # (1)!\n    --cap-add SYS_ADMIN \\   # (2)!\n    --ipc host \\          # (3)!\n    -v /sys/class/powercap/intel-rapl:/zeus_sys/class/powercap/intel-rapl \\ # (4)!\n    mlenergy/zeus:latest \\\n    bash\n</code></pre> <ol> <li>Mounts all GPUs into the Docker container. See Docker docs for more about the <code>--gpus</code> argument.</li> <li>The <code>SYS_ADMIN</code> Linux security capability is needed to change the GPU's power limit or frequency. See here for details and alternatives.</li> <li>PyTorch DataLoader workers need enough shared memory for IPC. Without this, they may run out of shared memory and die.</li> <li>Zeus reads Intel RAPL metrics for CPU/DRAM energy measurement through the <code>sysfs</code> interface. Docker disables this by default, so we need to mount it into the container separately (under <code>/zeus_sys</code>).</li> </ol> <p>Especially, <code>--cap-add SYS_ADMIN</code> is to be able to change the GPU's power limit or frequency, and <code>-v /sys/class/powercap/intel-rapl:/zeus_sys/class/powercap/intel-rapl</code> is to be able to measure CPU/DRAM energy via Intel RAPL. See System privileges for details.</p>"},{"location":"getting_started/#pulling-from-docker-hub","title":"Pulling from Docker Hub","text":"<p>Pre-built images are hosted on Docker Hub. There are three types of images available:</p> <ul> <li><code>latest</code>: The latest versioned release.</li> <li><code>v*</code>: Each versioned release.</li> <li><code>master</code>: The <code>HEAD</code> commit of Zeus. Usually stable enough, and you will get all the new features.</li> </ul>"},{"location":"getting_started/#building-the-image-locally","title":"Building the image locally","text":"<p>You should specify <code>TARGETARCH</code> to be one of <code>amd64</code> or <code>arm64</code> based on your environment:</p> <pre><code>git clone https://github.com/ml-energy/zeus.git\ncd zeus\ndocker build -t mlenergy/zeus:master --build-arg TARGETARCH=amd64 -f docker/Dockerfile .\n</code></pre>"},{"location":"getting_started/#verifying-installation","title":"Verifying installation","text":"<p>After installing the Zeus package, you can run the following to see whether packages and hardware are properly detected by Zeus.</p> <pre><code>$ python -m zeus.show_env\n================================================================================\n\nPython version: 3.9.19\n\n================================================================================\n\n[2024-09-09 16:40:14,495] [zeus.utils.framework](framework.py:25) PyTorch with CUDA support is available.\n[2024-09-09 16:40:14,496] [zeus.utils.framework](framework.py:45) JAX is not available\n\nPackage availability and versions:\n  Zeus: 0.10.0\n  PyTorch: 2.4.1+cu121\n  JAX: not available\n\n================================================================================\n\n[2024-09-09 16:40:14,512] [zeus.device.gpu.nvidia](nvidia.py:46) pynvml is available and initialized.\n\nGPU availability:\n  GPU 0: NVIDIA A40\n\n================================================================================\n\n[2024-09-09 16:40:14,519] [zeus.device.cpu.rapl](rapl.py:136) RAPL is available.\n[2024-09-09 16:40:14,519] [RaplWraparoundTracker](rapl.py:82) Monitoring wrap around of /sys/class/powercap/intel-rapl/intel-rapl:0/energy_uj\n[2024-09-09 16:40:14,528] [RaplWraparoundTracker](rapl.py:82) Monitoring wrap around of /sys/class/powercap/intel-rapl/intel-rapl:0/intel-rapl:0:0/energy_uj\n[2024-09-09 16:40:14,533] [RaplWraparoundTracker](rapl.py:82) Monitoring wrap around of /sys/class/powercap/intel-rapl/intel-rapl:1/energy_uj\n[2024-09-09 16:40:14,535] [RaplWraparoundTracker](rapl.py:82) Monitoring wrap around of /sys/class/powercap/intel-rapl/intel-rapl:1/intel-rapl:1:0/energy_uj\n\nCPU availability:\n  CPU 0:\n    CPU measurements available (/sys/class/powercap/intel-rapl/intel-rapl:0)\n    DRAM measurements available (/sys/class/powercap/intel-rapl/intel-rapl:0/intel-rapl:0:0)\n  CPU 1:\n    CPU measurements available (/sys/class/powercap/intel-rapl/intel-rapl:1)\n    DRAM measurements available (/sys/class/powercap/intel-rapl/intel-rapl:1/intel-rapl:1:0)\n\n================================================================================\n</code></pre>"},{"location":"getting_started/#system-privileges","title":"System privileges","text":"<p>Tip</p> <p>If you just want to measure GPU energy, you can skip this section.</p>"},{"location":"getting_started/#when-are-extra-system-privileges-needed","title":"When are extra system privileges needed?","text":"<ol> <li>CPU energy measurement: <code>root</code> privileges are needed when measuring CPU energy through the Intel RAPL interface. This is due to a security issue. Specifically, this is needed if you want to measure CPU energy via <code>ZeusMonitor</code> with <code>cpu_indices</code>.</li> <li>GPU energy optimization: The Linux security capability <code>SYS_ADMIN</code> (<code>root</code> is fine as well as it's stronger) is required in order to change the GPU's power limit or frequency. Specifically, this is needed by the <code>GlobalPowerLimitOptimizer</code> and the <code>PipelineFrequencyOptimizer</code>.</li> </ol>"},{"location":"getting_started/#option-1-running-applications-in-a-docker-container","title":"Option 1: Running applications in a Docker container","text":"<p>For CPU energy measurement, you are <code>root</code> inside a Docker container. You will just need to mount the RAPL sysfs directory into the Docker container. See here for instructions.</p> <p>For GPU energy optimization, you can pass <code>--cap-add SYS_ADMIN</code> to <code>docker run</code>. Since this significantly simplifies running Zeus, we recommend users to consider this option first. This is also possible for Kubernetes Pods with <code>securityContext.capabilities.add</code> in container specs (docs).</p>"},{"location":"getting_started/#option-2-deploying-the-zeus-daemon-zeusd","title":"Option 2: Deploying the Zeus daemon (<code>zeusd</code>)","text":"<p>Granting <code>SYS_ADMIN</code> to the entire application just to be able to change the GPU's configuration is granting too much. Instead, Zeus provides the Zeus daemon or <code>zeusd</code>, which is a simple server/daemon process that is designed to run with admin privileges and exposes the minimal set of APIs wrapping NVML methods for changing the GPU's configuration. Then, an unprivileged (i.e., run normally by any user) application can ask <code>zeusd</code> via a Unix Domain Socket to change the local node's GPU configuration on its behalf. The Zeus daemon also supports CPU/DRAM power measurement via Intel RAPL, which normally requires <code>root</code> privileges.</p> <p>To deploy <code>zeusd</code>:</p> <pre><code># Install zeusd\ncargo install zeusd\n\n# Run zeusd with admin privileges\nsudo zeusd serve \\\n    --socket-path /var/run/zeusd.sock \\   # (1)!\n    --socket-permissions 666            # (2)!\n</code></pre> <ol> <li>Unix domain socket path that <code>zeusd</code> listens to.</li> <li>Applications need write access to the socket to be able to talk to <code>zeusd</code>. This string is interpreted as UNIX file permissions.</li> </ol> <p>After deploying <code>zeusd</code>, set the <code>ZEUSD_SOCK_PATH</code> environment variable to the socket path (e.g., <code>/var/run/zeusd.sock</code>) to allow Zeus imported in your application find and talk to the running Zeus daemon. You can test out whether it worked with:</p> <pre><code>$ ZEUSD_SOCK_PATH=/var/run/zeusd.sock python -m zeus.show_env\n--------------------------------------------------------------------------------\n...other fields\n--------------------------------------------------------------------------------\n## CPU availability\n\nLogging output:\n[2025-07-25 16:40:40,396] [zeus.device.cpu.rapl](rapl.py:137) RAPL directory (/sys/class/powercap/intel-rapl) is available.\n\nDetected:\n  CPU 0:\n    CPU measurements available (Zeusd at /var/run/zeusd.sock)\n    DRAM measurements available (Zeusd at /var/run/zeusd.sock)\n  CPU 1:\n    CPU measurements available (Zeusd at /var/run/zeusd.sock)\n    DRAM measurements available (Zeusd at /var/run/zeusd.sock)\n\n--------------------------------------------------------------------------------\n</code></pre>"},{"location":"getting_started/#option-3-running-applications-with-sudo","title":"Option 3: Running applications with <code>sudo</code>","text":"<p>This is probably the worst option. However, if none of the options above work, you can run your application with <code>sudo</code>, which is essentially <code>root</code> and automatically has <code>SYS_ADMIN</code>.</p>"},{"location":"getting_started/#next-steps","title":"Next Steps","text":"<ul> <li>Measuring energy with the <code>ZeusMonitor</code>, programmatically or in the command line.</li> <li>Optimizing energy with Zeus energy optimizers.</li> </ul>"},{"location":"measure/","title":"Measuring Energy","text":"<p>Important</p> <p>Please refer to the Getting Started guide to first install Zeus and set up your environment before proceeding with this section.</p> <p>Tip</p> <p>Once you've installed Zeus, you can use our environment validation script to see if devices are being detected by Zeus as expected.</p> <p>Zeus makes it very easy to measure time, power, and energy both programmatically in Python and also on the command line. Measuring power and energy is also very low overhead, typically taking less than 10 ms for each call.</p>"},{"location":"measure/#programmatic-measurement","title":"Programmatic Measurement","text":""},{"location":"measure/#time-and-energy-consumption-of-a-chunk-of-code","title":"Time and energy consumption of a chunk of code","text":"<p><code>ZeusMonitor</code> makes it very simple to measure the GPU time and energy consumption of arbitrary Python code blocks.</p> <p>A measurement window is defined by a code block wrapped with <code>begin_window</code> and <code>end_window</code>. <code>end_window</code> will return a <code>Measurement</code> object, which holds the time and energy consumption of the window. Users can specify and measure multiple measurement windows at the same time, and they can be arbitrarily nested or overlapping as long as they are given different names.</p> <pre><code>from zeus.monitor import ZeusMonitor\n\nif __name__ == \"__main__\":\n    # All GPUs are measured simultaneously if `gpu_indices` is not given.\n    monitor = ZeusMonitor(gpu_indices=[torch.cuda.current_device()])\n\n    for epoch in range(100):\n        monitor.begin_window(\"epoch\")\n\n        steps = []\n        for x, y in train_loader:\n            monitor.begin_window(\"step\")\n            train_one_step(x, y)\n            result = monitor.end_window(\"step\")\n            steps.append(result)\n\n        mes = monitor.end_window(\"epoch\")\n        print(f\"Epoch {epoch} consumed {mes.time} s and {mes.total_energy} J.\")\n\n        avg_time = sum(map(lambda m: m.time, steps)) / len(steps)\n        avg_energy = sum(map(lambda m: m.total_energy, steps)) / len(steps)\n        print(f\"One step took {avg_time} s and {avg_energy} J on average.\")\n</code></pre> <p><code>zeus.monitor.PowerMonitor</code></p> <p>This monitor spawns a process that polls the instantaneous GPU power consumption API and exposes two methods: <code>get_power</code> and <code>get_energy</code>. For GPUs older than Volta that do not support querying energy directly, <code>ZeusMonitor</code> automatically uses the <code>PowerMonitor</code> internally.</p> <p>Use of global variables on GPUs older than Volta</p> <p>On GPUs older than Volta, you should not instantiate <code>ZeusMonitor</code> as a global variable without protecting it with <code>if __name__ == \"__main__\"</code>. It's because the energy query API is only available on Volta or newer NVIDIA GPU microarchitectures, and for older GPUs, a separate process that polls the power API has to be spawned (i.e., <code>PowerMonitor</code>). In this case, global code that spawns the process should be guarded with <code>if __name__ == \"__main__\"</code>. More details in Python docs.</p> <p><code>gpu_indices</code> and <code>CUDA_VISIBLE_DEVICES</code></p> <p>Zeus always respects <code>CUDA_VISIBLE_DEVICES</code> if set. In other words, if <code>CUDA_VISIBLE_DEVICES=1,3</code> and <code>gpu_indices=[1]</code>, Zeus will understand that as GPU 3 in the system.</p> <p><code>gpu_indices</code> and optimization</p> <p>In general, energy optimizers measure the energy of the GPU through a <code>ZeusMonitor</code> instance that is passed to their constructor. Thus, only the GPUs specified by <code>gpu_indices</code> will be the target of optimization.</p>"},{"location":"measure/#power-consumption-over-time","title":"Power consumption over time","text":"<p>Apart from energy, you can also measure the power consumption of GPUs over time by directly using the <code>PowerMonitor</code>. It measures power in three power domains: - GPU average power: Windowed average power consumption of the GPU over a one-second interval. - GPU instantaneous power: Instantaneous power consumption of the GPU at the time of the query. - GPU memory average power (Hopper or newer): Windowed average power consumption of the GPU's memory.</p> <p>Important</p> <p>Not all GPUs support all power domains, and this is not really documented well. You'll have to check on your GPU by instantiating <code>PowerMonitor</code>, which will automatically detect supported power domains.</p> <p>When <code>PowerMonitor</code> is instantiated, it spawns separate processes that poll the device's power consumption API and collects deduplicated power samples in-memory. Then, you can call <code>get_all_power_timelines</code> or <code>get_power_timeline</code> for a specific power domain to retrieve the power samples collected either for the whole lifetime of the monitor, or for a specific time window.</p>"},{"location":"measure/#synchronizing-cpu-and-gpu-computations","title":"Synchronizing CPU and GPU computations","text":"<p>Deep learning frameworks typically run actual computation on GPUs in an asynchronous fashion. That is, the CPU (Python interpreter) asynchronously dispatches computations to run on the GPU and moves on to dispatch the next computation without waiting for the GPU to finish. This helps GPUs achieve higher utilization with less idle time.</p> <p>Due to this asynchronous nature of Deep Learning frameworks, we need to be careful when we want to take time and energy measurements of GPU execution. We want only and all the computations dispatched between <code>begin_window</code> and <code>end_window</code> to be captured by our time and energy measurement. That's what the <code>sync_execution_with</code> parameter in <code>ZeusMonitor</code> and <code>sync_execution</code> paramter in <code>begin_window</code> and <code>end_window</code> are for. Depending on the Deep Learning framework you're using (currently PyTorch and JAX are supported), <code>ZeusMonitor</code> will automatically synchronize CPU and GPU execution to make sure all and only the computations dispatched between the window are captured.</p> <p>Tip</p> <p>Zeus has one function used globally across the codebase for device synchronization: <code>sync_execution</code>.</p> <p>Warning</p> <p><code>ZeusMonitor</code> covers only the common and simple case of device synchronization, when GPU indices (<code>gpu_indices</code>) correspond to one whole physical device. This is usually what you want, except when using more advanced device partitioning (e.g., using <code>--xla_force_host_platform_device_count</code> in JAX to partition CPUs into more pieces). In such cases, you probably want to opt out from using this function and handle synchronization manually at the appropriate granularity.</p>"},{"location":"measure/#distributed-power-measurement-and-aggregation","title":"Distributed Power Measurement and Aggregation","text":"<p><code>ZeusMonitor</code> is local to a single machine, but sometimes, you may want to monitor power across multiple nodes in a cluster. In this case, you can run the Zeus daemon (zeusd) on each machine and stream power readings over SSE (Server-Sent Events) to a central client (<code>PowerStreamingClient</code>) for real-time monitoring and aggregation.</p> <p>Each Zeus daemon endpoint is described by a <code>ZeusdConfig</code>, constructed via <code>ZeusdConfig.tcp(...)</code> or <code>ZeusdConfig.uds(...)</code>. Both <code>gpu_indices</code> and <code>cpu_indices</code> follow the same convention as <code>ZeusMonitor</code>:</p> <ul> <li><code>None</code> (default): Stream all available devices.</li> <li>A list of indices (e.g., <code>[0, 1]</code>): Stream only those devices.</li> <li>An empty list (<code>[]</code>): Skip streaming for that device type entirely.</li> </ul> <pre><code>from zeus.utils.zeusd import ZeusdConfig\nfrom zeus.monitor.power_streaming import PowerStreamingClient\n\n# SSE connections start immediately on construction.\nclient = PowerStreamingClient(\n    servers=[\n        ZeusdConfig.tcp(\"node1\", 4938, gpu_indices=[0, 1, 2, 3], cpu_indices=[0]),\n        ZeusdConfig.tcp(\"node2\", 4938),        # all GPUs + all CPUs\n        ZeusdConfig.uds(\"/var/run/zeusd.sock\"), # local UDS\n    ],\n)\n\n# Snapshot: latest readings from all endpoints, keyed by endpoint identifier.\nreadings = client.get_power()\nfor endpoint, pr in readings.items():\n    print(f\"{endpoint}: timestamp={pr.timestamp_s:.3f}s\")\n    print(f\"  GPU power: {pr.gpu_power_w}\")\n    for cpu_idx, cpu_reading in pr.cpu_power_w.items():\n        print(f\"  CPU {cpu_idx}: {cpu_reading.cpu_w:.1f} W, DRAM: {cpu_reading.dram_w}\")\n\n# Blocking iterator: yields a snapshot each time new SSE data arrives.\n# Iteration stops when stop() is called.\nfor readings in client:\n    for endpoint, pr in readings.items():\n        print(f\"{endpoint}: GPU={pr.gpu_power_w}, CPU={pr.cpu_power_w}\")\n\n# Async iterator: same as above, without blocking the event loop.\nasync for readings in client:\n    for endpoint, pr in readings.items():\n        print(f\"{endpoint}: GPU={pr.gpu_power_w}, CPU={pr.cpu_power_w}\")\n\nclient.stop()\n</code></pre> <p>Authentication</p> <p>If <code>zeusd</code> is running with <code>--signing-key-path</code> (i.e., JWT authentication enabled), set the <code>ZEUSD_TOKEN</code> environment variable or pass <code>token=</code> directly in <code>ZeusdConfig</code>. The client automatically checks <code>/discover</code> to determine whether auth is required and raises a clear error if a token is needed but not provided.</p> <p><code>get_power</code> returns a dict mapping each endpoint identifier to a <code>PowerReadings</code> object containing a Unix timestamp, per-GPU power in watts, and per-CPU <code>CpuPowerReading</code> objects (with <code>cpu_w</code> and optional <code>dram_w</code> fields, both in watts).</p> <p>The client spawns one background thread per device type per zeusd endpoint on construction. Each thread holds an SSE connection and automatically reconnects on disconnection. Unless <code>cpu_indices</code> is set to <code>[]</code>, the client probes the zeusd one-shot CPU power endpoint on init; if RAPL is not available on that server, a warning is logged and CPU streaming is skipped. Call <code>stop()</code> when done to cleanly shut down background threads. zeusd uses demand-driven polling -- power is only read from the hardware while at least one client is connected, so idle endpoints consume no resources.</p>"},{"location":"measure/#hardware-support","title":"Hardware Support","text":"<p>For GPUs, we currently support both NVIDIA (via NVML) and AMD GPUs (via AMDSMI, with ROCm 6.1 or later).</p> <p>CPU measurement is supported for devices that have the RAPL interface built in. This includes the majority of Intel CPUs and most modern AMD CPUs. DRAM energy measurement are available on some CPUs as well.</p> <p>To check CPU/GPU/DRAM measurement support, refer to Verifying installation.</p> <p>Energy measurement for Apple Silicon and Jetson Platforms is supported as well. For more information, refer to Apple Silicon and Jetson Platforms.</p>"},{"location":"measure/#generic-device-interfaces","title":"Generic device interfaces","text":"<p>The <code>get_gpus</code> function returns a <code>GPUs</code> object, which can be either an <code>NVIDIAGPUs</code> or <code>AMDGPUs</code> object depending on the availability of <code>nvml</code> or <code>amdsmi</code>. Each <code>GPUs</code> object contains one or more <code>GPU</code> instances, which are specifically <code>NVIDIAGPU</code> or <code>AMDGPU</code> objects.</p> <p>These <code>GPU</code> objects directly call respective <code>nvml</code> or <code>amdsmi</code> methods, providing a one-to-one mapping of methods for seamless GPU abstraction and support for multiple GPU types. For example: - <code>NVIDIAGPU.get_name</code> calls <code>pynvml.nvmlDeviceGetName</code>. - <code>AMDGPU.get_name</code> calls <code>amdsmi.amdsmi_get_gpu_asic_info</code>.</p> <p><code>get_cpus</code> is similar to <code>get_gpus</code>, but rather abstracts over CPU vendors; <code>get_soc</code> abstracts over SoC platforms (currently Apple Silicon and Jetson).</p>"},{"location":"measure/#amd-gpu","title":"AMD GPU","text":""},{"location":"measure/#initialization","title":"Initialization","text":"<p><code>amdsmi.amdsmi_get_energy_count</code> sometimes returns invalid values on older GPU models or ROCm versions (e.g., MI100 on ROCm 6.2). See ROCm issue #38 for more details. During the <code>AMDGPUs</code> object initialization, we call <code>amdsmi.amdsmi_get_energy_count</code> twice for each GPU, with a 0.5-second delay between calls. This difference is compared to power measurements to determine if <code>amdsmi.amdsmi_get_energy_count</code> is stable and reliable. Initialization takes 0.5 seconds regardless of the number of AMD GPUs.</p> <p><code>amdsmi.amdsmi_get_power_info</code> provides \"average_socket_power\" and \"current_socket_power\" fields, but the \"current_socket_power\" field is sometimes not supported and returns \"N/A.\" During the <code>AMDGPUs</code> object initialization, this method is checked, and if \"N/A\" is returned, the <code>AMDGPU.get_instant_power_usage</code> method is disabled. Instead, <code>AMDGPU.get_average_power_usage</code> needs to be used.</p>"},{"location":"measure/#rocm-and-amdsmi-versions","title":"ROCm and AMDSMI Versions","text":"<p>Only ROCm &gt;= 6.1 is supported, as the AMDSMI APIs for power and energy return wrong values. For more information, see AMDSMI issue #22. Ensure your <code>amdsmi</code> and ROCm versions are up-to-date.</p>"},{"location":"measure/#numa-cpus","title":"NUMA CPUs","text":"<p>If you have more than one CPU sockets, for instance, running our environment validation script will show two RAPL devices. To only measure the energy consumption of the CPU used by the current Python process, you can use the <code>get_current_cpu_index</code> helper function to retrieve the CPU index where the specified process ID is running and pass in only that index to the <code>cpu_indices</code> argument.</p>"},{"location":"measure/#apple-silicon","title":"Apple Silicon","text":"<p>To enable Apple Silicon energy monitoring, you need the optional <code>zeus-apple-silicon</code> dependency installed in the <code>apple</code> extra. You can install it with:</p> <pre><code>pip install zeus[apple]\n</code></pre> <p>Once installed, you can conduct measurement as normal with the <code>ZeusMonitor</code> (Programmatic measurement), and metrics for Apple Silicon will be included in a field called <code>soc_energy</code> within the <code>Measurement</code> object reported by <code>end_window</code>. For example:</p> <pre><code># ...\nmes = monitor.end_window(\"epoch\")\napple_energy_metrics = mes.soc_energy\n</code></pre> <p>For Apple Silicon, the <code>soc_energy</code> field will include metrics for:</p> <ul> <li>On-chip CPU (<code>cpu_total_mj</code>)</li> <li>Every efficiency core (<code>efficiency_cores_mj</code>)</li> <li>Every performance core (<code>performance_cores_mj</code>)</li> <li>Efficiency core manager (<code>efficiency_core_manager_mj</code>)</li> <li>Performance core manager (<code>performance_core_manager_mj</code>)</li> <li>DRAM (<code>dram_mj</code>)</li> <li>On-chip GPU (<code>gpu_mj</code>)</li> <li>GPU SRAM (<code>gpu_sram_mj</code>)</li> <li>Apple Neural Engine (ANE) (<code>ane_mj</code>)</li> </ul> <p>Note that units are in mJ.</p> <p>Some metrics may be unavailable for monitoring depending on the specific processor (e.g., DRAM is sometimes unavailable on M1 macs). If a certain subsystem's energy could not be measured, its entry in the result object will simply hold <code>None</code>.</p>"},{"location":"measure/#jetson-platforms","title":"Jetson Platforms","text":"<p>Energy measurement is currently supported for NVIDIA Jetson platforms. Similarly to Apple Silicon, metrics can be retrieved as normal with the Zeus monitor (Programmatic measurement), which collects them in the <code>soc_energy</code> field of the returned <code>Measurement</code> object. Metrics are reported by <code>end_window</code>. For example:</p> <pre><code># ...\nmes = monitor.end_window(\"epoch\")\njetson_energy_metrics = mes.soc_energy\n</code></pre> <p>Note: if you do not have a Jetson processor or are not running Linux, the Zeus monitor will skip measuring energy for Jetson platforms.</p> <p>For Jetson, the <code>soc_energy</code> field will include energy metrics for:</p> <ul> <li>On-chip CPU (<code>cpu_energy_mj</code>)</li> <li>On-chip GPU (<code>gpu_energy_mj</code>)</li> <li>Total chip energy (<code>total_energy_mj</code>)</li> </ul> <p>Note that units are in mJ.</p> <p>Some metrics may be unavailable for monitoring depending on the specific Jetson device model or configuration (e.g. custom-configured boards), though this is rarely the case. For any unavailable metrics, its entry in the result object will hold <code>None</code>.</p>"},{"location":"measure/#metric-monitoring","title":"Metric Monitoring","text":"<p>You can export Zeus measurements as Prometheus metrics. Three metrics are currently supported:  </p> <ol> <li>Energy consumption of a fixed code range (Histogram)</li> <li>Power draw over time (Gauge)</li> <li>Cumulative energy consumption over time (Counter)</li> </ol> <p>Prerequisite</p> <p>As Zeus is a library integrated to applications that are not necessarily servers, Zeus uses the push model for metric collection. As such, the Prometheus Push Gateway must be deployed and accessible to the Zeus-integrated application. Example Prometheus configurations can be found in our docker examples.</p> <pre><code>docker run -d -p 9091:9091 prom/pushgateway\n</code></pre>"},{"location":"measure/#supported-metrics-and-naming","title":"Supported Metrics and Naming","text":"<p>Zeus organizes metrics using static metric names and dynamic labels for flexibility and ease of querying in Prometheus. Metric names are static and cannot be overridden, but users can customize the context of the metrics by naming the window when using <code>begin_window()</code> and <code>end_window()</code>.</p> <p>Metric Name (<code>component</code> is <code>gpu</code>, <code>cpu</code>, or <code>dram</code>)</p> <ul> <li>Energy histogram: <code>energy_monitor_{component}_energy_joules</code></li> <li>Cumulative energy counter: <code>energy_monitor_{component}_energy_joules</code></li> <li>Power gauge: <code>power_monitor_{component}_power_watts</code></li> </ul> <p>Note that the power gauge metric only supports the GPU component at the moment. Tracking issue: #128</p> <p>Labels</p> <ul> <li><code>window</code>: The user-defined window name provided to <code>begin_window()</code> and <code>end_window()</code> (e.g., <code>energy_histogram.begin_window(\"epoch_energy\")</code>).</li> <li><code>index</code>: The index of the device (e.g., <code>0</code> for GPU 0).</li> </ul>"},{"location":"measure/#energyhistogram","title":"<code>EnergyHistogram</code>","text":"<p>This metric records energy consumption for GPUs, CPUs, and DRAM as Prometheus Histograms. This is ideal for observing the energy consumption distribution of a fixed and repeated code range.</p> <pre><code>from zeus.metric import EnergyHistogram\n\nif __name__ == \"__main__\":\n    energy_histogram = EnergyHistogram(\n        cpu_indices=[0], \n        gpu_indices=[0], \n        prometheus_url='http://localhost:9091', \n        job='training_energy_histogram'\n    )\n\n    for epoch in range(100):\n        energy_histogram.begin_window(\"epoch_energy\")\n        train_one_epoch(train_loader, model, optimizer, criterion, epoch, args)\n        acc1 = validate(val_loader, model, criterion, args)\n        energy_histogram.end_window(\"epoch_energy\")\n        print(f\"Epoch {epoch} completed. Validation Accuracy: {acc1}%\")\n</code></pre> <p>Tip</p> <p>Bucket ranges for GPUs, CPUs, and DRAM can be set during initialization.</p> <pre><code>energy_histogram = EnergyHistogram(\n    cpu_indices=[0], \n    gpu_indices=[0], \n    prometheus_url=\"http://localhost:9091\", \n    job=\"training_energy_histogram\",\n    gpu_bucket_range=[10.0, 25.0, 50.0, 100.0],\n    cpu_bucket_range=[5.0, 15.0, 30.0, 50.0],\n    dram_bucket_range=[2.0, 8.0, 20.0, 40.0],\n)\n</code></pre>"},{"location":"measure/#energycumulativecounter","title":"<code>EnergyCumulativeCounter</code>","text":"<p>This metric monitors cumulative energy consumption over time.</p> <pre><code>from zeus.metric import EnergyCumulativeCounter\n\nif __name__ == \"__main__\":\n    cumulative_counter_metric = EnergyCumulativeCounter(\n        cpu_indices=[0], \n        gpu_indices=[0], \n        update_period=2,  \n        prometheus_url='http://localhost:9091',\n        job='energy_counter_job'\n    )\n    train_loader = range(10) \n    val_loader = range(5)  \n\n    cumulative_counter_metric.begin_window(\"training_energy\")\n\n    for epoch in range(100):  \n        print(f\"\\n--- Epoch {epoch} ---\")\n        train_one_epoch(train_loader, model, optimizer, criterion, epoch, args)\n        acc1 = validate(val_loader, model, criterion, args)\n        print(f\"Epoch {epoch} completed. Validation Accuracy: {acc1:.2f}%.\")\n\n    cumulative_counter_metric.end_window(\"training_energy\")\n</code></pre> <p>Metric observations are pushed to Prometheus every <code>update_period</code> seconds.</p>"},{"location":"measure/#powergauge","title":"<code>PowerGauge</code>","text":"<p>This metric tracks real-time power consumption using Prometheus Gauges.</p> <pre><code>from zeus.metric import PowerGauge\n\nif __name__ == \"__main__\":\n    power_gauge_metric = PowerGauge(\n        gpu_indices=[0], \n        update_period=2,  \n        prometheus_url='http://localhost:9091',\n        job='power_gauge_job'\n    )\n    train_loader = range(10) \n    val_loader = range(5)  \n\n    power_gauge_metric.begin_window(\"training_power\")\n\n    for epoch in range(100):  \n        print(f\"\\n--- Epoch {epoch} ---\")\n        train_one_epoch(train_loader, model, optimizer, criterion, epoch, args)\n        acc1 = validate(val_loader, model, criterion, args)\n        print(f\"Epoch {epoch} completed. Validation Accuracy: {acc1:.2f}%.\")\n\n    power_gauge_metric.end_window(\"training_power\")\n</code></pre> <p>Metric observations are pushed to Prometheus every <code>update_period</code> seconds.</p>"},{"location":"measure/#querying-metrics-in-prometheus","title":"Querying Metrics in Prometheus","text":"<p>Once metrics are pushed to Prometheus, you can use PromQL to run simple analytics.</p> <p>Energy for a specific window <pre><code>energy_monitor_gpu_energy_joules{window=\"epoch_energy\"}\n</code></pre></p> <p>Sum of energy for a specific window <pre><code>sum(energy_monitor_gpu_energy_joules) by (window)\n</code></pre></p> <p>Sum of energy for specific GPU across all windows <pre><code>sum(energy_monitor_gpu_energy_joules{index=\"0\"})\n</code></pre></p>"},{"location":"measure/#cli-power-and-energy-monitor","title":"CLI power and energy monitor","text":"<p>The energy monitor measures the total energy consumed by the GPU during the lifetime of the monitor process. It's a simple wrapper around <code>ZeusMonitor</code>.</p> <pre><code>$ python -m zeus.monitor energy\n[2023-08-22 22:44:45,106] [ZeusMonitor](energy.py:157) Monitoring GPU [0, 1, 2, 3].\n[2023-08-22 22:44:46,210] [zeus.utils.framework](framework.py:38) PyTorch with CUDA support is available.\n[2023-08-22 22:44:46,760] [ZeusMonitor](energy.py:329) Measurement window 'zeus.monitor.energy' started.\n^C[2023-08-22 22:44:50,205] [ZeusMonitor](energy.py:329) Measurement window 'zeus.monitor.energy' ended.\nTotal energy (J):\nMeasurement(time=3.4480526447296143, energy={0: 224.2969999909401, 1: 232.83799999952316, 2: 233.3100000023842, 3: 234.53700000047684})\n</code></pre> <p>The power monitor periodically prints out the GPU's power draw. It's a simple wrapper around <code>PowerMonitor</code>.</p> <pre><code>$ python -m zeus.monitor power\n[2023-08-22 22:39:59,787] [PowerMonitor](power.py:134) Monitoring power usage of GPUs [0, 1, 2, 3]\n2023-08-22 22:40:00.800576\n{'GPU0': 66.176, 'GPU1': 68.792, 'GPU2': 66.898, 'GPU3': 67.53}\n2023-08-22 22:40:01.842590\n{'GPU0': 66.078, 'GPU1': 68.595, 'GPU2': 66.996, 'GPU3': 67.138}\n2023-08-22 22:40:02.845734\n{'GPU0': 66.078, 'GPU1': 68.693, 'GPU2': 66.898, 'GPU3': 67.236}\n2023-08-22 22:40:03.848818\n{'GPU0': 66.177, 'GPU1': 68.675, 'GPU2': 67.094, 'GPU3': 66.926}\n^C\nTotal time (s): 4.421529293060303\nTotal energy (J):\n{'GPU0': 198.52566362297537, 'GPU1': 206.22215216255188, 'GPU2': 201.08565518283845, 'GPU3': 201.79834523367884}\n</code></pre>"},{"location":"measure/#troubleshooting","title":"Troubleshooting","text":""},{"location":"measure/#repeated-execution-of-the-main-script-module-after-using-zeus-monitors","title":"Repeated execution of the main script module after using Zeus monitors","text":"<p>Zeus monitors (e.g., <code>ZeusMonitor</code>, <code>PowerMonitor</code>, <code>TemperatureMonitor</code>, <code>CarbonEmissionMonitor</code>, <code>EnergyCostMonitor</code>) use the <code>spawn</code> start method for helper processes. Each spawned subprocess re-imports your <code>__main__</code> module (as <code>__mp_main__</code>), so any work done at import time, such as loading a model or instantiating a monitor, runs again in every process and can exhaust GPU/CPU memory. Keep heavy initialization under <code>if __name__ == \"__main__\":</code> or inside functions, so subprocess imports stay lightweight.</p>"},{"location":"measure/#cpu-energy-measurement-missing-or-permission-denied-intel-rapl","title":"CPU energy measurement missing or permission denied (Intel RAPL)","text":"<p>Reading CPU/DRAM energy via Intel RAPL requires root because of kernel restrictions. If you cannot run as root, disable CPU measurement with <code>cpu_indices=[]</code> or follow the steps in System privileges to grant access.</p>"},{"location":"optimize/","title":"Optimizing Energy","text":"<p>Important</p> <p>You will need to set up your environment to make energy measurement and optimization work. Please refer to the Getting Started guide.</p> <p>Zeus provides multiple optimizers that tune different knobs either in the Deep Learning workload-side or the GPU-side.</p>"},{"location":"optimize/#power-limit-optimizer","title":"Power limit optimizer","text":"<p>Finds the optimal GPU power limit for DNN training. Users can control what optimal means, including minimum energy, minimum energy given maximum training slowdown, and minimum cost (linear combination of time and energy).</p>"},{"location":"optimize/#batch-size-optimizer","title":"Batch size optimizer","text":"<p>Finds the optimal DNN training batch size for training jobs that recur over time. This would be especially useful for production training jobs where the underlying dataset is constantly updated, and the model is periodically re-trained to keep it up-to-date.</p>"},{"location":"optimize/#pipeline-frequency-optimizer","title":"Pipeline frequency optimizer","text":"<p>In large model training (e.g., pre-training Large Language Models), pipeline parallelism is almost essential today. The pipeline frequency optimizer plans the GPU SM frequency across time for an iteration of pipeline parallel training. It generates a set of frequency plans, including a plan that reduces energy the most, another that reduces energy with negligible slowdown, and plans in the middle.</p>"},{"location":"optimize/batch_size_optimizer/","title":"Batch Size Optimizer","text":"<p>The batch size optimizer (BSO) finds the optimal DNN training batch size that minimizes cost:</p> \\[ \\eta \\cdot \\textrm{ETA} + (1 - \\eta) \\cdot \\textrm{MaxPower} \\cdot \\textrm{TTA} \\] <p>where ETA and TTA stands for Energy-to-Accuracy and Time-to-Accuracy, respectively, and accuracy is the target validation metric of training. Users can trade-off between energy and time by setting the \\(\\eta\\) parameter to be between 0 and 1.</p>"},{"location":"optimize/batch_size_optimizer/#usage","title":"Usage","text":"<p>In production environments, it is common for a single DNN to be trained and re-trained many times over time. This is because the data distribution changes over time, and the model needs to be re-trained to adapt to the new data distribution. The batch size optimizer uses these recurring training jobs as opportunities to find the optimal batch size for the DNN training job. To do this, the batch size optimizer uses a Multi-Armed Bandit algorithm to explore the batch size space. For more details, please refer to the Zeus paper.</p> <p>Constraints</p> <p>Currently, the batch size optimizer only supports cases where the number and type of GPUs used for each recurrent training job is always the same.</p>"},{"location":"optimize/batch_size_optimizer/#high-level-architecture","title":"High-level architecture","text":"<pre><code>sequenceDiagram;\n    participant BSO server\n    participant BSO client\n    loop Every recurrent training\n        BSO client-&gt;&gt;BSO server: Register the training job and ask for the batch size to use\n        BSO server-&gt;&gt;BSO client: Return the batch size to use and an integer trial number\n        loop Every epoch\n            BSO client-&gt;&gt;BSO server: At the end of each epoch, report time and energy consumption\n            BSO server-&gt;&gt;BSO client: Compute cost and determine whether the client should terminate training\n        end\n        BSO client-&gt;&gt;BSO server: Notify the server that the trial finished\n    end</code></pre> <p>In order to persist the state of the optimizer across all recurring training runs, we need to have a server that outlives individual training jobs. Therefore, the batch size optimizer consists of two parts: the server and the client. The server is a FastAPI server that manages the Multi-Armed Bandit algorithm and the database, while the client (<code>BatchSizeOptimizer</code>) is integrated into the training script.</p>"},{"location":"optimize/batch_size_optimizer/#deploying-the-server","title":"Deploying the server","text":"<p>Largely three steps: (1) starting the database, (2) running migration on the database, and (3) starting the BSO server.</p>"},{"location":"optimize/batch_size_optimizer/#clone-the-zeus-repository","title":"Clone the Zeus repository","text":"<p>To get example docker files and database migration scripts, clone the Zeus repository.</p> <pre><code>git clone https://github.com/ml-energy/zeus.git\n</code></pre>"},{"location":"optimize/batch_size_optimizer/#decide-on-the-database","title":"Decide on the database","text":"<p>By default, our examples use MySQL. However, you can use any database supported by SQLAlchemy. Please make sure the database's corresponding async connection driver (e.g., <code>asyncmy</code> for MySQL, <code>aiosqlite</code> for SQLite) is installed. For instance, to adapt our examples, you can (1) add <code>pip install</code> to <code>migration.Dockerfile</code> and <code>server.Dockerfile</code>, and (2) change the <code>db</code> container specification in <code>server-docker-compose.yaml</code>.</p>"},{"location":"optimize/batch_size_optimizer/#server-configuration","title":"Server configuration","text":"<p>You can configure the server using enviornment variables or the <code>.env</code> file. Below are the complete list of environment variables you can set and example values.</p> <pre><code>ZEUS_BSO_DB_USER=\"me\" \nZEUS_BSO_DB_PASSWORD=\"secret\"\nZEUS_BSO_DATABASE_URL=\"mysql+asyncmy://me:secret@localhost:3306/Zeus\"\nZEUS_BSO_ROOT_PASSWORD=\"secret*\"\nZEUS_BSO_SERVER_PORT=8000\nZEUS_BSO_LOG_LEVEL=\"INFO\"\nZEUS_BSO_ECHO_SQL=\"True\"\n</code></pre>"},{"location":"optimize/batch_size_optimizer/#with-docker-compose","title":"With Docker Compose","text":"<pre><code>cd docker/batch_size_optimizer\ndocker-compose -f ./server-docker-compose.yaml up\n</code></pre> <p>Docker Compose will first build necessary images and spin up the containers.</p>"},{"location":"optimize/batch_size_optimizer/#with-kubernetes","title":"With Kubernetes","text":"<ol> <li> <p>Build the Docker image.</p> <pre><code># From the repository root\ndocker build -f ./docker/batch_size_optimizer/server.Dockerfile -t bso-server . \ndocker build -f ./docker/batch_size_optimizer/migration.Dockerfile -t bso-migration .\n</code></pre> <p>Make sure Kubernetes has access to these build images. If you are locally using <code>minikube</code>, then the images are already available. However, if you are using the cloud such as AWS EKS, you should push the image to the registry and modify the image path in <code>server-docker-compose.yaml</code> to allow Kubernetes to pull the image.</p> </li> <li> <p>Convert Docker Compose files to Kubernetes YAML files using Kompose.</p> <pre><code>cd docker/batch_size_optimizer\ndocker-compose -f server-docker-compose.yaml config &gt; server-docker-compose-resolved.yaml\nkompose convert -f server-docker-compose-resolved.yaml -o ./kube/\nrm server-docker-compose-resolved.yaml\n</code></pre> <p>This first resolves env files using <code>docker-compose</code>, then converts it into Kubernetes YAML files in <code>./kube/</code>.</p> </li> <li> <p>Apply the Kubernetes YAML files.</p> <pre><code>cd kube\nkubectl apply -f .\n</code></pre> </li> </ol>"},{"location":"optimize/batch_size_optimizer/#with-just-python","title":"With just Python","text":"<p>You can also run the server without Docker or Kubernetes.</p> <ol> <li>Spin up the database of your choice.</li> <li>Set server configuration environment variables described here.</li> <li> <p>Perform DB migration with Alembic.</p> <ol> <li> <p>Install dependencies</p> <pre><code># From the repository root\npip install asyncmy cryptography\npip install '.[migration]'\n</code></pre> </li> <li> <p>Create the migration script. This will create scripts in <code>./versions</code>.</p> <pre><code>cd zeus/optimizer/batch_size\nalembic revision --autogenerate -m \"Create tables\" \n</code></pre> </li> <li> <p>Apply migration, either online or offline.</p> <pre><code># Online migration (applied directly to the database)\nalembic upgrade head \n# Offline migration (just generate SQL)\nalembic upgrade head --sql\n</code></pre> </li> </ol> </li> <li> <p>Spin up the server using <code>uvicorn</code>.</p> <pre><code>uvicorn zeus.optimizer.batch_size.server.router:app\n</code></pre> </li> </ol>"},{"location":"optimize/batch_size_optimizer/#integrating-batchsizeoptimizer","title":"Integrating <code>BatchSizeOptimizer</code>","text":"<p>In order for your recurring training job to communicate with the BSO server, you need to integrate the <code>BatchSizeOptimizer</code> class into your training script.</p> <ol> <li> <p>Install the Zeus package, including dependencies needed for the batch size optimizer.</p> <pre><code>pip install zeus[bso]\n</code></pre> </li> <li> <p>Integrate <code>BatchSizeOptimizer</code> to your training script.</p> <pre><code>from zeus.monitor import ZeusMonitor\nfrom zeus.optimizer.batch_size import BatchSizeOptimizer, JobSpec\n\nmonitor = ZeusMonitor()\n\n# On instantiation, the BSO will register the job to the BSO server\nbso = BatchSizeOptimizer(\n    monitor=monitor,\n    server_url=\"http://bso-server:8000\",\n    job=JobSpec(\n        job_id=os.environ.get(\"ZEUS_JOB_ID\"),\n        job_id_prefix=\"mnist\",\n        default_batch_size=256,\n        batch_sizes=[32, 64, 256, 512, 1024, 4096, 2048],\n        max_epochs=100\n    ),\n)\n\n# Get batch size to use from the server\nbatch_size = bso.get_batch_size()\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\neval_dataloader = DataLoader(eval_dataset, batch_size=batch_size)\n\n# Start measuring the time and energy consumption of this training run\nbso.on_train_begin()\n\nfor epoch in range(100):\n    for batch in train_dataloader:\n        # Training loop\n        pass\n\n    # The BSO server needs to know whether training has converged\n    metric = evaluate(model, eval_dataloader)\n    bso.on_evaluate(metric)\n\n    # The BSO server will determine whether to stop training\n    if bso.training_finished:\n        break\n</code></pre> </li> </ol>"},{"location":"optimize/batch_size_optimizer/#when-does-training-stop","title":"When does training stop?","text":"<p>The BSO server will determine whether to stop training and this will be reflected in the <code>training_finished</code> attribute of the <code>BatchSizeOptimizer</code> instance.</p> <p>If the DNN reaches the target validation metric, training should stop. However, training fails if</p> <ol> <li>it failed to converge within the configured <code>JobSpec.max_epochs</code> (reference) epochs, or</li> <li>its cost exceeded the early stopping threshold configured by <code>JobSpec.beta_knob</code>(reference) .</li> </ol> <p>In such failure cases, the optimizer will raise a <code>ZeusBSOTrainFailError</code>. This means that the chosen batch size was not useful, and the BSO server will never try this batch size again. The user should re-launch the training run in this case, and the BSO server will try another batch size.</p>"},{"location":"optimize/batch_size_optimizer/#integration-examples","title":"Integration examples","text":"<p>Two full examples are given for the batch size optimizer:</p> <ul> <li>MNIST: Single-GPU and data parallel training, with integration examples with Kubeflow</li> <li>Sentiment Analysis: Full training example with HuggingFace transformers using the Capriccio dataset, a sentiment analysis dataset with data drift.</li> </ul>"},{"location":"optimize/pipeline_frequency_optimizer/","title":"Pipeline Frequency Optimizer","text":"<p>The pipeline frequency optimizer optimizes the energy consumption of large model training, e.g., LLM pretraining.</p> <p>The core observation is that in pipeline parallel training, it is very difficult to split pipeline stages in perfectly equal size. Even for models like GPT, the first stage has the embeddings and the last stage has the language model head, making perfect balance nearly impossible to achieve. The pipeline frequency optimizer is based on our research paper Perseus. For more details about Perseus, check out our blog post.</p> The pipeline frequency optimizer in action"},{"location":"optimize/pipeline_frequency_optimizer/#usage","title":"Usage","text":"<p>Currently, it's a three-step process:</p> <ol> <li>Profile: Profile the computation time and energy consumption of the forward and backward instructions in each stage and each GPU frequency and the P2P blocking power consumption of the GPU.</li> <li>Optimize: Use <code>lowtime</code> to generate all Pareto-optimal frequency plans.</li> <li>Choose and start training: Among all the frequency plans generated by <code>lowtime</code>, choose the one that suits your use case.</li> </ol> <p>We have a reference integration with the large model training framework Merak, which supports 3D parallelism and automatically tracing and partitioning Hugging Face models. We've smoothed out some rough edges, integrated Zeus, and added example training scripts for GPT-3, BERT, and Wide-ResNet (pretty much any <code>torchvision</code> model).</p> <p>You don't have to be tied to Merak. If you have your own training framework, and you can integrate the pipeline frequency optimizer following the integration guide.</p>"},{"location":"optimize/pipeline_frequency_optimizer/#profile","title":"Profile","text":"<p>In order to run our optimization algorithm, we need the time &amp; energy profiling information of the forward and backward instruction in each stage for every GPU frequency. The CSV file should look like this for a 4-stage pipeline:</p> <pre><code>stage,instruction,frequency,time,energy\n0,forward,1740,0.09373254776000976,28.4944\n0,forward,1725,0.09390360514322917,28.434366666666666\n0,forward,1710,0.09381131331125896,28.288966666666667\n...\n0,backward,1740,0.24533510557810465,69.5691\n0,backward,1725,0.24538559118906658,69.2552\n0,backward,1710,0.24548352559407552,68.89453333333334\n...\n3,backward,690,0.4184921979904175,68.12243333333333\n3,backward,675,0.42459266185760497,68.77603333333334\n3,backward,660,0.4306272824605306,69.39623333333334\n</code></pre> <p>Since different frameworks and model implementations will have different performance, it's best to obtain these profiling results on the framework and model you'll be using. That being said, you can obtain this profiling information in however way you want as long as they have all the columns in the reference CSV file above. But as a reference, we have implemented an automatic profiler in Merak. Please refer to the examples directory in Merak for profiling instructions.</p> <p>Finally, we also need to take into account the power consumption of the GPU while it is blocking on P2P communication, i.e., waiting for either the activation or gradient from its neighbor stage. You can use our profiling script for that.</p> <p>Tip</p> <p>As you profile the time and energy consumption of an instruction, you will scan down from the highest to the lowest frequency. However, as you lower the GPU's frequency, both time and energy will start to inflate after some point. In other words, those frequencies take more time and energy and are simply inefficient (i.e., Pareto-suboptimal), so we won't be running anything with those frequencies. Therefore, you actually don't need to profile time and energy for every frequency. A good heuristic is to scan from higher frequencies to lower ones, and once energy consumption increases more than five consecutive times, just stop there.</p>"},{"location":"optimize/pipeline_frequency_optimizer/#optimize","title":"Optimize","text":"<p>With the CSV file that holds profiling results, you can use <code>lowtime</code> to generate all Pareto-optimal frequency plans.</p> <p>See <code>examples/pipeline_frequency_optimizer</code> to find the script <code>run_optimization.py</code>.</p>"},{"location":"optimize/pipeline_frequency_optimizer/#choose-and-start-training","title":"Choose and start training","text":"<p>Running <code>lowtime</code> optimization will produce a set of frequency assignment files (<code>freqs_pipeline_%05d.py</code>). Each file is also annotated with estimates for time and cost. The larger the number, the shorter the expected iteration time.</p> <p>Then, start the PFO server and plug in the frequency plan you chose:</p> <pre><code>$ docker exec -it merak-zeus bash\n# pip install '.[pfo-server]'\n# ZEUS_PFO_SCHEDULER_ARGS='{\"solution_path\": \"path/to/freqs_pipeline_%05d.py\"}' uvicorn zeus.optimizer.pipeline_frequency.server.router:app --port 7787\n</code></pre> <p>When you run training (with the same <code>run.sh</code> but without <code>--profile true</code>), the <code>PipelineFrequencyOptimizer</code> integrated into your training framework will automatically talk with the PFO server to figure out the right GPU frequency to set for the upcoming pipeline instruction and transparently set the GPU's frequency.</p>"},{"location":"optimize/pipeline_frequency_optimizer/#integrating-with-training-frameworks","title":"Integrating with training frameworks","text":"<p>This page aims to walk you through the process of integrating the pipeline frequency optimizer with arbitrary training frameworks. We also have a reference integration with Merak. Especially take a look at <code>Merak.runtime.pipe_engine</code>.</p>"},{"location":"optimize/pipeline_frequency_optimizer/#assumptions","title":"Assumptions","text":"<p>We assume that there are concrete regions of the framework's code where the forward pass and the backward pass exclusively happens. For instance, in DeepSpeed, <code>PipelineEngine</code> has <code>_exec_forward_pass</code> and <code>_exec_backward_pass</code>. As another example, in Megatron-LM, users can pass in their custom <code>forward_step_func</code> to <code>pretrain</code>, and <code>forward_step</code> in the codebase calls it. The backward pass is done (roughly) in the <code>backward_step</code> function.</p>"},{"location":"optimize/pipeline_frequency_optimizer/#integrate-pipelinefrequencyoptimizer","title":"Integrate <code>PipelineFrequencyOptimizer</code>","text":"<ol> <li>Add <code>zeus[pfo]</code> to your dependencies.</li> <li>Instantiate the <code>PipelineFrequencyOptimizer</code> somewhere before actual training runs. Let's call the object <code>opt</code>.</li> <li>Surround one training step with <code>opt.on_step_begin()</code> and <code>opt.on_step_end()</code>.</li> <li>Wrap the forward pass region with <code>opt.on_instruction_begin(\"forward\")</code> and <code>opt.on_instruction_end(\"forward\")</code>.</li> <li>Wrap the backward pass region with <code>opt.on_instruction_begin(\"backward\")</code> and <code>opt.on_instruction_end(\"backward\")</code>.</li> </ol>"},{"location":"optimize/pipeline_frequency_optimizer/#profiling-instructions","title":"Profiling Instructions","text":"<p>It's important to optimize on top of accurate measurements of forward and backward instructions. For now, we're taking an offline approach, where we run each instruction under a given GPU frequency N times and average time and energy consumption. See Merak's <code>profile</code> function.</p> <p>We're on the process of implementing an online approach that is directly integrated into <code>PipelineFrequencyOptimizer</code> so that you don't need to implement a separate profiler inside your framework.</p>"},{"location":"optimize/power_limit_optimizer/","title":"Power Limit Optimizer","text":"<p>The power limit optimizer (<code>GlobalPowerLimitOptimizer</code>) finds the optimal GPU power limit for DNN training. Users can customize the power limit optimizer to choose the optimal power limit based on their own criteria using the <code>OptimumSelector</code> interface.</p>"},{"location":"optimize/power_limit_optimizer/#usage","title":"Usage","text":"<p>Use cases currently supported are single GPU training and data parallel training. For data parallel training, the power limit of all GPUs involved are changed together, since all GPUs have the same computation load.</p> <p>Upcoming</p> <p>Distributed data parallel training support is planned (tracking issue).</p> <p>Extra system privileges needed</p> <p>In order to optimize the GPU power limit, the power limit optimizer should be able to change the power limit. This requires extra system privileges. See here for details.</p>"},{"location":"optimize/power_limit_optimizer/#globalpowerlimitoptimizer","title":"<code>GlobalPowerLimitOptimizer</code>","text":"<p>You can use the power limit optimizer by integrating <code>GlobalPowerLimitOptimizer</code> into your training loop. In order to inform the optimizer of epoch and training step boundaries, a couple methods need to be called inside the training loop (highlighted):</p> <pre><code>from zeus.monitor import ZeusMonitor\nfrom zeus.optimizer.power_limit import GlobalPowerLimitOptimizer\n\n# Data parallel training with four GPUs.\nmonitor = ZeusMonitor(gpu_indices=[0,1,2,3])\nplo = GlobalPowerLimitOptimizer(monitor)\n\nfor epoch in range(100):\n    plo.on_epoch_begin()\n\n    for x, y in train_dataloader:\n        plo.on_step_begin()\n        # Learn from x and y\n        plo.on_step_end()\n\n    plo.on_epoch_end()\n</code></pre> <p>We provide integration examples for Torchvision &amp; ImageNet single-GPU and data parallel training.</p> <p>What is the optimal power limit?</p> <p><code>GlobalPowerLimitOptimizer</code> accepts an optional <code>OptimumSelector</code> in its constructor, which defines how to choose one power limit among all the profiled power limits. Built-in optimum selectors are <code>Energy</code>, <code>Time</code>, <code>ZeusCost</code> and <code>MaxSlowdownConstraint</code>. Users can inherit from <code>OptimumSelector</code> to implement their custom optimum selector.</p>"},{"location":"optimize/power_limit_optimizer/#hfglobalpowerlimitoptimizer","title":"<code>HFGlobalPowerLimitOptimizer</code>","text":"<p>For easy use with HuggingFace Transformers, <code>HFGlobalPowerLimitOptimizer</code> is implemented as a HuggingFace Trainer Callback by inheriting from <code>TrainerCallback</code>. When initializing a HuggingFace Trainer or a TFL SFTTrainer, initialize and pass in <code>HFGlobalPowerLimitOptimizer</code> as shown below:</p> <pre><code>from transformers import Trainer\nfrom zeus.monitor import ZeusMonitor\nfrom zeus.optimizer.power_limit import HFGlobalPowerLimitOptimizer\n\nmonitor = ZeusMonitor()\nplo = HFGlobalPowerLimitOptimizer(monitor)\n\n# Also works with trl.SFTTrainer.\ntrainer = Trainer(\n    ...,\n    callbacks=[plo],\n)\n</code></pre> <p>Refer to our HuggingFace integration examples for:</p> <ul> <li>Transformers <code>Trainer</code> integration for causal language modeling (i.e., pre-training)</li> <li>TRL <code>SFTTrainer</code> integration for Gemma 7B supervised fine-tuning with QLoRA</li> </ul>"},{"location":"reference/","title":"Source Code Reference","text":""},{"location":"reference/#zeus","title":"zeus","text":"<p>Zeus is a framework for deep learning energy measurement and optimization.</p> <ul> <li><code>device</code>: Abstraction layer over compute devices</li> <li><code>monitor</code>: Programmatic power and energy measurement tools</li> <li><code>optimizer</code>: A collection of optimizers for time and energy</li> <li><code>utils</code>: Utility functions and classes</li> <li><code>callback</code>: Callback definition</li> <li><code>exception</code>: Base exception class definition</li> <li><code>metric</code>: Tools for defining and tracking power and energy-related metrics</li> <li><code>show_env</code>: Command line tool for install verification and device detection</li> <li><code>_legacy</code>: Legacy code mostly to keep our papers reproducible</li> </ul>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Source Code Reference<ul> <li>_legacy<ul> <li>job</li> <li>policy<ul> <li>interface</li> <li>mab</li> <li>optimizer</li> </ul> </li> <li>simulate</li> </ul> </li> <li>callback</li> <li>device<ul> <li>common</li> <li>cpu<ul> <li>common</li> <li>rapl</li> </ul> </li> <li>exception</li> <li>gpu<ul> <li>amd</li> <li>common</li> <li>nvidia</li> </ul> </li> <li>soc<ul> <li>apple</li> <li>common</li> <li>jetson</li> </ul> </li> </ul> </li> <li>exception</li> <li>metric</li> <li>monitor<ul> <li>carbon</li> <li>energy</li> <li>power</li> <li>power_streaming</li> <li>price</li> <li>temperature</li> </ul> </li> <li>optimizer<ul> <li>batch_size<ul> <li>client</li> <li>common</li> <li>exceptions</li> <li>server<ul> <li>batch_size_state<ul> <li>commands</li> <li>models</li> <li>repository</li> </ul> </li> <li>config</li> <li>database<ul> <li>db_connection</li> <li>repository</li> <li>schema</li> </ul> </li> <li>exceptions</li> <li>explorer</li> <li>job<ul> <li>commands</li> <li>models</li> <li>repository</li> </ul> </li> <li>mab</li> <li>optimizer</li> <li>router</li> <li>services<ul> <li>commands</li> <li>service</li> </ul> </li> </ul> </li> </ul> </li> <li>pipeline_frequency<ul> <li>common</li> <li>frequency_controller</li> <li>optimizer</li> <li>server<ul> <li>job_manager</li> <li>router</li> <li>scheduler</li> </ul> </li> </ul> </li> <li>power_limit</li> </ul> </li> <li>show_env</li> <li>utils<ul> <li>async_utils</li> <li>env</li> <li>framework</li> <li>lat_lon</li> <li>logging</li> <li>lr_scaler</li> <li>metric</li> <li>multiprocessing</li> <li>pydantic_v1</li> <li>testing</li> <li>zeusd</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/callback/","title":"callback","text":""},{"location":"reference/callback/#zeus.callback","title":"zeus.callback","text":"<p>Infrastructure for calling callbacks.</p>"},{"location":"reference/callback/#zeus.callback.Callback","title":"Callback","text":"<p>Base class for callbacks.</p> Source code in <code>zeus/callback.py</code> <pre><code>class Callback:\n    \"\"\"Base class for callbacks.\"\"\"\n\n    def on_train_begin(self) -&gt; None:\n        \"\"\"Called at the beginning of training.\"\"\"\n\n    def on_train_end(self) -&gt; None:\n        \"\"\"Called at the end of training.\"\"\"\n\n    def on_epoch_begin(self) -&gt; None:\n        \"\"\"Called at the beginning of each epoch.\"\"\"\n\n    def on_epoch_end(self) -&gt; None:\n        \"\"\"Called at the end of each epoch.\"\"\"\n\n    def on_step_begin(self) -&gt; None:\n        \"\"\"Called at the beginning of each step.\"\"\"\n\n    def on_step_end(self) -&gt; None:\n        \"\"\"Called at the end of each step.\"\"\"\n\n    def on_evaluate(self, metric: float) -&gt; None:\n        \"\"\"Called after evaluating the model.\"\"\"\n\n    def on_instruction_begin(self, name: str) -&gt; None:\n        \"\"\"Called at the beginning of pipeline instructions like forward or backward.\"\"\"\n\n    def on_instruction_end(self, name: str) -&gt; None:\n        \"\"\"Called at the end of pipeline instructions like forward or backward.\"\"\"\n</code></pre>"},{"location":"reference/callback/#zeus.callback.Callback.on_train_begin","title":"on_train_begin","text":"<pre><code>on_train_begin()\n</code></pre> <p>Called at the beginning of training.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_train_begin(self) -&gt; None:\n    \"\"\"Called at the beginning of training.\"\"\"\n</code></pre>"},{"location":"reference/callback/#zeus.callback.Callback.on_train_end","title":"on_train_end","text":"<pre><code>on_train_end()\n</code></pre> <p>Called at the end of training.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_train_end(self) -&gt; None:\n    \"\"\"Called at the end of training.\"\"\"\n</code></pre>"},{"location":"reference/callback/#zeus.callback.Callback.on_epoch_begin","title":"on_epoch_begin","text":"<pre><code>on_epoch_begin()\n</code></pre> <p>Called at the beginning of each epoch.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_epoch_begin(self) -&gt; None:\n    \"\"\"Called at the beginning of each epoch.\"\"\"\n</code></pre>"},{"location":"reference/callback/#zeus.callback.Callback.on_epoch_end","title":"on_epoch_end","text":"<pre><code>on_epoch_end()\n</code></pre> <p>Called at the end of each epoch.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_epoch_end(self) -&gt; None:\n    \"\"\"Called at the end of each epoch.\"\"\"\n</code></pre>"},{"location":"reference/callback/#zeus.callback.Callback.on_step_begin","title":"on_step_begin","text":"<pre><code>on_step_begin()\n</code></pre> <p>Called at the beginning of each step.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_step_begin(self) -&gt; None:\n    \"\"\"Called at the beginning of each step.\"\"\"\n</code></pre>"},{"location":"reference/callback/#zeus.callback.Callback.on_step_end","title":"on_step_end","text":"<pre><code>on_step_end()\n</code></pre> <p>Called at the end of each step.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_step_end(self) -&gt; None:\n    \"\"\"Called at the end of each step.\"\"\"\n</code></pre>"},{"location":"reference/callback/#zeus.callback.Callback.on_evaluate","title":"on_evaluate","text":"<pre><code>on_evaluate(metric)\n</code></pre> <p>Called after evaluating the model.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_evaluate(self, metric: float) -&gt; None:\n    \"\"\"Called after evaluating the model.\"\"\"\n</code></pre>"},{"location":"reference/callback/#zeus.callback.Callback.on_instruction_begin","title":"on_instruction_begin","text":"<pre><code>on_instruction_begin(name)\n</code></pre> <p>Called at the beginning of pipeline instructions like forward or backward.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_instruction_begin(self, name: str) -&gt; None:\n    \"\"\"Called at the beginning of pipeline instructions like forward or backward.\"\"\"\n</code></pre>"},{"location":"reference/callback/#zeus.callback.Callback.on_instruction_end","title":"on_instruction_end","text":"<pre><code>on_instruction_end(name)\n</code></pre> <p>Called at the end of pipeline instructions like forward or backward.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_instruction_end(self, name: str) -&gt; None:\n    \"\"\"Called at the end of pipeline instructions like forward or backward.\"\"\"\n</code></pre>"},{"location":"reference/callback/#zeus.callback.CallbackSet","title":"CallbackSet","text":"<p>               Bases: <code>Callback</code></p> <p>A set of callbacks.</p> Source code in <code>zeus/callback.py</code> <pre><code>class CallbackSet(Callback):\n    \"\"\"A set of callbacks.\"\"\"\n\n    def __init__(self, callbacks: list[Callback]) -&gt; None:\n        \"\"\"Initialize the callback set.\"\"\"\n        self.callbacks = callbacks\n\n    def on_train_begin(self) -&gt; None:\n        \"\"\"Called at the beginning of training.\"\"\"\n        for callback in self.callbacks:\n            callback.on_train_begin()\n\n    def on_train_end(self) -&gt; None:\n        \"\"\"Called at the end of training.\"\"\"\n        for callback in self.callbacks:\n            callback.on_train_end()\n\n    def on_epoch_begin(self) -&gt; None:\n        \"\"\"Called at the beginning of each epoch.\"\"\"\n        for callback in self.callbacks:\n            callback.on_epoch_begin()\n\n    def on_epoch_end(self) -&gt; None:\n        \"\"\"Called at the end of each epoch.\"\"\"\n        for callback in self.callbacks:\n            callback.on_epoch_end()\n\n    def on_step_begin(self) -&gt; None:\n        \"\"\"Called at the beginning of each step.\"\"\"\n        for callback in self.callbacks:\n            callback.on_step_begin()\n\n    def on_step_end(self) -&gt; None:\n        \"\"\"Called at the end of each step.\"\"\"\n        for callback in self.callbacks:\n            callback.on_step_end()\n\n    def on_evaluate(self, metric: float) -&gt; None:\n        \"\"\"Called after evaluating the model.\"\"\"\n        for callback in self.callbacks:\n            callback.on_evaluate(metric)\n\n    def on_instruction_begin(self, name: str) -&gt; None:\n        \"\"\"Called at the beginning of pipeline instructions like forward or backward.\"\"\"\n        for callback in self.callbacks:\n            callback.on_instruction_begin(name)\n\n    def on_instruction_end(self, name: str) -&gt; None:\n        \"\"\"Called at the end of pipeline instructions like forward or backward.\"\"\"\n        for callback in self.callbacks:\n            callback.on_instruction_end(name)\n</code></pre>"},{"location":"reference/callback/#zeus.callback.CallbackSet.__init__","title":"__init__","text":"<pre><code>__init__(callbacks)\n</code></pre> Source code in <code>zeus/callback.py</code> <pre><code>def __init__(self, callbacks: list[Callback]) -&gt; None:\n    \"\"\"Initialize the callback set.\"\"\"\n    self.callbacks = callbacks\n</code></pre>"},{"location":"reference/callback/#zeus.callback.CallbackSet.on_train_begin","title":"on_train_begin","text":"<pre><code>on_train_begin()\n</code></pre> <p>Called at the beginning of training.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_train_begin(self) -&gt; None:\n    \"\"\"Called at the beginning of training.\"\"\"\n    for callback in self.callbacks:\n        callback.on_train_begin()\n</code></pre>"},{"location":"reference/callback/#zeus.callback.CallbackSet.on_train_end","title":"on_train_end","text":"<pre><code>on_train_end()\n</code></pre> <p>Called at the end of training.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_train_end(self) -&gt; None:\n    \"\"\"Called at the end of training.\"\"\"\n    for callback in self.callbacks:\n        callback.on_train_end()\n</code></pre>"},{"location":"reference/callback/#zeus.callback.CallbackSet.on_epoch_begin","title":"on_epoch_begin","text":"<pre><code>on_epoch_begin()\n</code></pre> <p>Called at the beginning of each epoch.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_epoch_begin(self) -&gt; None:\n    \"\"\"Called at the beginning of each epoch.\"\"\"\n    for callback in self.callbacks:\n        callback.on_epoch_begin()\n</code></pre>"},{"location":"reference/callback/#zeus.callback.CallbackSet.on_epoch_end","title":"on_epoch_end","text":"<pre><code>on_epoch_end()\n</code></pre> <p>Called at the end of each epoch.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_epoch_end(self) -&gt; None:\n    \"\"\"Called at the end of each epoch.\"\"\"\n    for callback in self.callbacks:\n        callback.on_epoch_end()\n</code></pre>"},{"location":"reference/callback/#zeus.callback.CallbackSet.on_step_begin","title":"on_step_begin","text":"<pre><code>on_step_begin()\n</code></pre> <p>Called at the beginning of each step.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_step_begin(self) -&gt; None:\n    \"\"\"Called at the beginning of each step.\"\"\"\n    for callback in self.callbacks:\n        callback.on_step_begin()\n</code></pre>"},{"location":"reference/callback/#zeus.callback.CallbackSet.on_step_end","title":"on_step_end","text":"<pre><code>on_step_end()\n</code></pre> <p>Called at the end of each step.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_step_end(self) -&gt; None:\n    \"\"\"Called at the end of each step.\"\"\"\n    for callback in self.callbacks:\n        callback.on_step_end()\n</code></pre>"},{"location":"reference/callback/#zeus.callback.CallbackSet.on_evaluate","title":"on_evaluate","text":"<pre><code>on_evaluate(metric)\n</code></pre> <p>Called after evaluating the model.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_evaluate(self, metric: float) -&gt; None:\n    \"\"\"Called after evaluating the model.\"\"\"\n    for callback in self.callbacks:\n        callback.on_evaluate(metric)\n</code></pre>"},{"location":"reference/callback/#zeus.callback.CallbackSet.on_instruction_begin","title":"on_instruction_begin","text":"<pre><code>on_instruction_begin(name)\n</code></pre> <p>Called at the beginning of pipeline instructions like forward or backward.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_instruction_begin(self, name: str) -&gt; None:\n    \"\"\"Called at the beginning of pipeline instructions like forward or backward.\"\"\"\n    for callback in self.callbacks:\n        callback.on_instruction_begin(name)\n</code></pre>"},{"location":"reference/callback/#zeus.callback.CallbackSet.on_instruction_end","title":"on_instruction_end","text":"<pre><code>on_instruction_end(name)\n</code></pre> <p>Called at the end of pipeline instructions like forward or backward.</p> Source code in <code>zeus/callback.py</code> <pre><code>def on_instruction_end(self, name: str) -&gt; None:\n    \"\"\"Called at the end of pipeline instructions like forward or backward.\"\"\"\n    for callback in self.callbacks:\n        callback.on_instruction_end(name)\n</code></pre>"},{"location":"reference/exception/","title":"exception","text":""},{"location":"reference/exception/#zeus.exception","title":"zeus.exception","text":"<p>Base Zeus Exception Class.</p>"},{"location":"reference/exception/#zeus.exception.ZeusBaseError","title":"ZeusBaseError","text":"<p>               Bases: <code>Exception</code></p> <p>Zeus base exception class.</p> Source code in <code>zeus/exception.py</code> <pre><code>class ZeusBaseError(Exception):\n    \"\"\"Zeus base exception class.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Base Zeus Exception.\"\"\"\n        self.message = message\n        super().__init__(message)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return message.\"\"\"\n        return self.message\n</code></pre>"},{"location":"reference/exception/#zeus.exception.ZeusBaseError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/exception.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Base Zeus Exception.\"\"\"\n    self.message = message\n    super().__init__(message)\n</code></pre>"},{"location":"reference/exception/#zeus.exception.ZeusBaseError.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>Return message.</p> Source code in <code>zeus/exception.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return message.\"\"\"\n    return self.message\n</code></pre>"},{"location":"reference/metric/","title":"metric","text":""},{"location":"reference/metric/#zeus.metric","title":"zeus.metric","text":"<p>Track and export energy and power metrics via Prometheus.</p>"},{"location":"reference/metric/#zeus.metric.MonitoringProcessState","title":"MonitoringProcessState  <code>dataclass</code>","text":"<p>Represents the state of a monitoring window.</p> Source code in <code>zeus/metric.py</code> <pre><code>@dataclass\nclass MonitoringProcessState:\n    \"\"\"Represents the state of a monitoring window.\"\"\"\n\n    queue: mp.Queue\n    proc: SpawnProcess\n</code></pre>"},{"location":"reference/metric/#zeus.metric.Metric","title":"Metric","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all metric types in Zeus.</p> <p>Defines a common interface for metrics, ensuring consistent behavior for <code>begin_window</code> and <code>end_window</code> operations.</p> Source code in <code>zeus/metric.py</code> <pre><code>class Metric(abc.ABC):\n    \"\"\"Abstract base class for all metric types in Zeus.\n\n    Defines a common interface for metrics, ensuring consistent behavior\n    for `begin_window` and `end_window` operations.\n    \"\"\"\n\n    @abc.abstractmethod\n    def begin_window(self, name: str, sync_execution: bool = True) -&gt; None:\n        \"\"\"Start a new measurement window.\n\n        Args:\n            name (str): Name of the measurement window.\n            sync_execution (bool): Whether to wait for asynchronously dispatched computations\n                to finish before starting the measurement window.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def end_window(self, name: str, sync_execution: bool = True) -&gt; None:\n        \"\"\"End the current measurement window and report metrics.\n\n        Args:\n            name (str): Name of the measurement window.\n            sync_execution (bool): Whether to wait for asynchronously dispatched computations\n                to finish before starting the measurement window.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/metric/#zeus.metric.Metric.begin_window","title":"begin_window  <code>abstractmethod</code>","text":"<pre><code>begin_window(name, sync_execution=True)\n</code></pre> <p>Start a new measurement window.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the measurement window.</p> required <code>sync_execution</code> <code>bool</code> <p>Whether to wait for asynchronously dispatched computations to finish before starting the measurement window.</p> <code>True</code> Source code in <code>zeus/metric.py</code> <pre><code>@abc.abstractmethod\ndef begin_window(self, name: str, sync_execution: bool = True) -&gt; None:\n    \"\"\"Start a new measurement window.\n\n    Args:\n        name (str): Name of the measurement window.\n        sync_execution (bool): Whether to wait for asynchronously dispatched computations\n            to finish before starting the measurement window.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/metric/#zeus.metric.Metric.end_window","title":"end_window  <code>abstractmethod</code>","text":"<pre><code>end_window(name, sync_execution=True)\n</code></pre> <p>End the current measurement window and report metrics.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the measurement window.</p> required <code>sync_execution</code> <code>bool</code> <p>Whether to wait for asynchronously dispatched computations to finish before starting the measurement window.</p> <code>True</code> Source code in <code>zeus/metric.py</code> <pre><code>@abc.abstractmethod\ndef end_window(self, name: str, sync_execution: bool = True) -&gt; None:\n    \"\"\"End the current measurement window and report metrics.\n\n    Args:\n        name (str): Name of the measurement window.\n        sync_execution (bool): Whether to wait for asynchronously dispatched computations\n            to finish before starting the measurement window.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/metric/#zeus.metric.EnergyHistogram","title":"EnergyHistogram","text":"<p>               Bases: <code>Metric</code></p> <p>Measures the energy consumption a code range and exports a histogram metrics.</p> <p>Tracks energy consumption for GPUs, CPUs, and DRAM as Prometheus Histogram metrics.</p> Source code in <code>zeus/metric.py</code> <pre><code>class EnergyHistogram(Metric):\n    \"\"\"Measures the energy consumption a code range and exports a histogram metrics.\n\n    Tracks energy consumption for GPUs, CPUs, and DRAM as Prometheus Histogram metrics.\n    \"\"\"\n\n    def __init__(\n        self,\n        cpu_indices: list,\n        gpu_indices: list,\n        pushgateway_url: str,\n        job: str,\n        gpu_bucket_range: Sequence[float] = [50.0, 100.0, 200.0, 500.0, 1000.0],\n        cpu_bucket_range: Sequence[float] = [10.0, 50.0, 100.0, 500.0, 1000.0],\n        dram_bucket_range: Sequence[float] = [5.0, 10.0, 20.0, 50.0, 150.0],\n    ) -&gt; None:\n        \"\"\"Initialize the EnergyHistogram class.\n\n        Sets up the Prometheus Histogram metrics to track energy consumption for GPUs, CPUs, and DRAMs.\n        The data will be collected and pushed to the Prometheus Push Gateway at regular intervals.\n\n        Args:\n            cpu_indices (list): List of CPU indices to monitor.\n            gpu_indices (list): List of GPU indices to monitor.\n            pushgateway_url (str): URL of the Prometheus Push Gateway where metrics will be pushed.\n            job (str): Name of the Prometheus job to associate with the energy metrics.\n            gpu_bucket_range (list[float], optional): Bucket ranges for GPU energy histograms.\n                Defaults to [50.0, 100.0, 200.0, 500.0, 1000.0].\n            cpu_bucket_range (list[float], optional): Bucket ranges for CPU energy histograms.\n                Defaults to [10.0, 20.0, 50.0, 100.0, 200.0].\n            dram_bucket_range (list[float], optional): Bucket ranges for DRAM energy histograms.\n                Defaults to [5.0, 10.0, 20.0, 50.0, 150.0].\n\n        Raises:\n            ValueError: If any of the bucket ranges (GPU, CPU, DRAM) is an empty list.\n        \"\"\"\n        self.gpu_bucket_range = gpu_bucket_range\n        self.cpu_bucket_range = cpu_bucket_range\n        self.dram_bucket_range = dram_bucket_range\n        self.cpu_indices = cpu_indices\n        self.gpu_indices = gpu_indices\n        self.pushgateway_url = pushgateway_url\n        self.job = job\n        self.registry = CollectorRegistry()\n\n        if not gpu_bucket_range:\n            raise ValueError(\n                \"GPU bucket range cannot be empty. Please provide a valid range or omit the argument to use defaults.\"\n            )\n        if not cpu_bucket_range:\n            raise ValueError(\n                \"CPU bucket range cannot be empty. Please provide a valid range or omit the argument to use defaults.\"\n            )\n        if not dram_bucket_range:\n            raise ValueError(\n                \"DRAM bucket range cannot be empty. Please provide a valid range or omit the argument to use defaults.\"\n            )\n\n        # Initialize GPU histograms\n        if self.gpu_indices:\n            self.gpu_histograms = Histogram(\n                \"energy_monitor_gpu_energy_joules\",\n                \"GPU energy consumption\",\n                [\"window\", \"index\"],\n                buckets=self.gpu_bucket_range,\n                registry=self.registry,\n            )\n        # Initialize CPU histograms\n        if self.cpu_indices:\n            self.cpu_histograms = Histogram(\n                \"energy_monitor_cpu_energy_joules\",\n                \"CPU energy consumption\",\n                [\"window\", \"index\"],\n                buckets=self.cpu_bucket_range,\n                registry=self.registry,\n            )\n            # Initialize CPU and DRAM histograms\n            if any(cpu.supports_get_dram_energy_consumption() for cpu in get_cpus().cpus):\n                self.dram_histograms = Histogram(\n                    \"energy_monitor_dram_energy_joules\",\n                    \"DRAM energy consumption\",\n                    [\"window\", \"index\"],\n                    buckets=self.dram_bucket_range,\n                    registry=self.registry,\n                )\n\n        self.max_gpu_bucket = max(self.gpu_bucket_range)\n        self.max_cpu_bucket = max(self.cpu_bucket_range)\n        self.max_dram_bucket = max(self.dram_bucket_range)\n\n        self.min_gpu_bucket = min(self.gpu_bucket_range)\n        self.min_cpu_bucket = min(self.cpu_bucket_range)\n        self.min_dram_bucket = min(self.dram_bucket_range)\n\n        self.energy_monitor = ZeusMonitor(cpu_indices=cpu_indices, gpu_indices=gpu_indices)\n\n    def begin_window(self, name: str, sync_execution: bool = True) -&gt; None:\n        \"\"\"Begin the energy monitoring window.\n\n        Args:\n            name (str): The unique name of the measurement window. Must match between calls to 'begin_window' and 'end_window'.\n            sync_execution (bool): Whether to execute synchronously. Defaults to True. If assigned True, calls sync_execution_fn with the defined gpu\n        \"\"\"\n        if sync_execution:\n            sync_execution_fn(self.gpu_indices)\n\n        self.energy_monitor.begin_window(f\"__EnergyHistogram_{name}\", sync_execution=sync_execution)\n\n    def end_window(self, name: str, sync_execution: bool = True) -&gt; None:\n        \"\"\"End the current energy monitoring window and record the energy data.\n\n        Retrieves the energy consumption data (for GPUs, CPUs, and DRAMs) for the monitoring window\n        and updates the corresponding Histogram metrics. The data is then pushed to the Prometheus Push Gateway.\n\n        Args:\n            name (str): The unique name of the measurement window. Must match between calls to 'begin_window' and 'end_window'.\n            sync_execution (bool): Whether to execute synchronously. Defaults to True.\n        \"\"\"\n        if sync_execution:\n            sync_execution_fn(self.gpu_indices)\n\n        measurement = self.energy_monitor.end_window(f\"__EnergyHistogram_{name}\", sync_execution=sync_execution)\n\n        if measurement.gpu_energy:\n            for gpu_index, gpu_energy in measurement.gpu_energy.items():\n                self.gpu_histograms.labels(window=name, index=gpu_index).observe(gpu_energy)\n                if gpu_energy &gt; self.max_gpu_bucket:\n                    warnings.warn(\n                        f\"GPU {gpu_index} energy {gpu_energy} exceeds the maximum bucket value of {self.max_gpu_bucket}\",\n                        stacklevel=1,\n                    )\n                if gpu_energy &lt; self.min_gpu_bucket:\n                    warnings.warn(\n                        f\"GPU {gpu_index} energy {gpu_energy} exceeds the minimum bucket value of {self.min_gpu_bucket}\",\n                        stacklevel=1,\n                    )\n\n        if measurement.cpu_energy:\n            for cpu_index, cpu_energy in measurement.cpu_energy.items():\n                self.cpu_histograms.labels(window=name, index=cpu_index).observe(cpu_energy)\n                if cpu_energy &gt; self.max_cpu_bucket:\n                    warnings.warn(\n                        f\"CPU {cpu_index} energy {cpu_energy} exceeds the maximum bucket value of {self.max_cpu_bucket}\",\n                        stacklevel=1,\n                    )\n                if cpu_energy &lt; self.min_cpu_bucket:\n                    warnings.warn(\n                        f\"CPU {cpu_index} energy {cpu_energy} exceeds the minimum bucket value of {self.min_cpu_bucket}\",\n                        stacklevel=1,\n                    )\n\n        if measurement.dram_energy:\n            for dram_index, dram_energy in measurement.dram_energy.items():\n                self.dram_histograms.labels(window=name, index=dram_index).observe(dram_energy)\n                if dram_energy &gt; self.max_dram_bucket:\n                    warnings.warn(\n                        f\"DRAM {dram_index} energy {dram_energy} exceeds the maximum bucket value of {self.max_dram_bucket}\",\n                        stacklevel=1,\n                    )\n                if dram_energy &lt; self.min_dram_bucket:\n                    warnings.warn(\n                        f\"DRAM {dram_index} energy {dram_energy} exceeds the minimum bucket value of {self.min_dram_bucket}\",\n                        stacklevel=1,\n                    )\n\n        push_to_gateway(self.pushgateway_url, job=self.job, registry=self.registry)\n</code></pre>"},{"location":"reference/metric/#zeus.metric.EnergyHistogram.__init__","title":"__init__","text":"<pre><code>__init__(cpu_indices, gpu_indices, pushgateway_url, job, gpu_bucket_range=[50.0, 100.0, 200.0, 500.0, 1000.0], cpu_bucket_range=[10.0, 50.0, 100.0, 500.0, 1000.0], dram_bucket_range=[5.0, 10.0, 20.0, 50.0, 150.0])\n</code></pre> <p>Sets up the Prometheus Histogram metrics to track energy consumption for GPUs, CPUs, and DRAMs. The data will be collected and pushed to the Prometheus Push Gateway at regular intervals.</p> <p>Parameters:</p> Name Type Description Default <code>cpu_indices</code> <code>list</code> <p>List of CPU indices to monitor.</p> required <code>gpu_indices</code> <code>list</code> <p>List of GPU indices to monitor.</p> required <code>pushgateway_url</code> <code>str</code> <p>URL of the Prometheus Push Gateway where metrics will be pushed.</p> required <code>job</code> <code>str</code> <p>Name of the Prometheus job to associate with the energy metrics.</p> required <code>gpu_bucket_range</code> <code>list[float]</code> <p>Bucket ranges for GPU energy histograms. Defaults to [50.0, 100.0, 200.0, 500.0, 1000.0].</p> <code>[50.0, 100.0, 200.0, 500.0, 1000.0]</code> <code>cpu_bucket_range</code> <code>list[float]</code> <p>Bucket ranges for CPU energy histograms. Defaults to [10.0, 20.0, 50.0, 100.0, 200.0].</p> <code>[10.0, 50.0, 100.0, 500.0, 1000.0]</code> <code>dram_bucket_range</code> <code>list[float]</code> <p>Bucket ranges for DRAM energy histograms. Defaults to [5.0, 10.0, 20.0, 50.0, 150.0].</p> <code>[5.0, 10.0, 20.0, 50.0, 150.0]</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any of the bucket ranges (GPU, CPU, DRAM) is an empty list.</p> Source code in <code>zeus/metric.py</code> <pre><code>def __init__(\n    self,\n    cpu_indices: list,\n    gpu_indices: list,\n    pushgateway_url: str,\n    job: str,\n    gpu_bucket_range: Sequence[float] = [50.0, 100.0, 200.0, 500.0, 1000.0],\n    cpu_bucket_range: Sequence[float] = [10.0, 50.0, 100.0, 500.0, 1000.0],\n    dram_bucket_range: Sequence[float] = [5.0, 10.0, 20.0, 50.0, 150.0],\n) -&gt; None:\n    \"\"\"Initialize the EnergyHistogram class.\n\n    Sets up the Prometheus Histogram metrics to track energy consumption for GPUs, CPUs, and DRAMs.\n    The data will be collected and pushed to the Prometheus Push Gateway at regular intervals.\n\n    Args:\n        cpu_indices (list): List of CPU indices to monitor.\n        gpu_indices (list): List of GPU indices to monitor.\n        pushgateway_url (str): URL of the Prometheus Push Gateway where metrics will be pushed.\n        job (str): Name of the Prometheus job to associate with the energy metrics.\n        gpu_bucket_range (list[float], optional): Bucket ranges for GPU energy histograms.\n            Defaults to [50.0, 100.0, 200.0, 500.0, 1000.0].\n        cpu_bucket_range (list[float], optional): Bucket ranges for CPU energy histograms.\n            Defaults to [10.0, 20.0, 50.0, 100.0, 200.0].\n        dram_bucket_range (list[float], optional): Bucket ranges for DRAM energy histograms.\n            Defaults to [5.0, 10.0, 20.0, 50.0, 150.0].\n\n    Raises:\n        ValueError: If any of the bucket ranges (GPU, CPU, DRAM) is an empty list.\n    \"\"\"\n    self.gpu_bucket_range = gpu_bucket_range\n    self.cpu_bucket_range = cpu_bucket_range\n    self.dram_bucket_range = dram_bucket_range\n    self.cpu_indices = cpu_indices\n    self.gpu_indices = gpu_indices\n    self.pushgateway_url = pushgateway_url\n    self.job = job\n    self.registry = CollectorRegistry()\n\n    if not gpu_bucket_range:\n        raise ValueError(\n            \"GPU bucket range cannot be empty. Please provide a valid range or omit the argument to use defaults.\"\n        )\n    if not cpu_bucket_range:\n        raise ValueError(\n            \"CPU bucket range cannot be empty. Please provide a valid range or omit the argument to use defaults.\"\n        )\n    if not dram_bucket_range:\n        raise ValueError(\n            \"DRAM bucket range cannot be empty. Please provide a valid range or omit the argument to use defaults.\"\n        )\n\n    # Initialize GPU histograms\n    if self.gpu_indices:\n        self.gpu_histograms = Histogram(\n            \"energy_monitor_gpu_energy_joules\",\n            \"GPU energy consumption\",\n            [\"window\", \"index\"],\n            buckets=self.gpu_bucket_range,\n            registry=self.registry,\n        )\n    # Initialize CPU histograms\n    if self.cpu_indices:\n        self.cpu_histograms = Histogram(\n            \"energy_monitor_cpu_energy_joules\",\n            \"CPU energy consumption\",\n            [\"window\", \"index\"],\n            buckets=self.cpu_bucket_range,\n            registry=self.registry,\n        )\n        # Initialize CPU and DRAM histograms\n        if any(cpu.supports_get_dram_energy_consumption() for cpu in get_cpus().cpus):\n            self.dram_histograms = Histogram(\n                \"energy_monitor_dram_energy_joules\",\n                \"DRAM energy consumption\",\n                [\"window\", \"index\"],\n                buckets=self.dram_bucket_range,\n                registry=self.registry,\n            )\n\n    self.max_gpu_bucket = max(self.gpu_bucket_range)\n    self.max_cpu_bucket = max(self.cpu_bucket_range)\n    self.max_dram_bucket = max(self.dram_bucket_range)\n\n    self.min_gpu_bucket = min(self.gpu_bucket_range)\n    self.min_cpu_bucket = min(self.cpu_bucket_range)\n    self.min_dram_bucket = min(self.dram_bucket_range)\n\n    self.energy_monitor = ZeusMonitor(cpu_indices=cpu_indices, gpu_indices=gpu_indices)\n</code></pre>"},{"location":"reference/metric/#zeus.metric.EnergyHistogram.begin_window","title":"begin_window","text":"<pre><code>begin_window(name, sync_execution=True)\n</code></pre> <p>Begin the energy monitoring window.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The unique name of the measurement window. Must match between calls to 'begin_window' and 'end_window'.</p> required <code>sync_execution</code> <code>bool</code> <p>Whether to execute synchronously. Defaults to True. If assigned True, calls sync_execution_fn with the defined gpu</p> <code>True</code> Source code in <code>zeus/metric.py</code> <pre><code>def begin_window(self, name: str, sync_execution: bool = True) -&gt; None:\n    \"\"\"Begin the energy monitoring window.\n\n    Args:\n        name (str): The unique name of the measurement window. Must match between calls to 'begin_window' and 'end_window'.\n        sync_execution (bool): Whether to execute synchronously. Defaults to True. If assigned True, calls sync_execution_fn with the defined gpu\n    \"\"\"\n    if sync_execution:\n        sync_execution_fn(self.gpu_indices)\n\n    self.energy_monitor.begin_window(f\"__EnergyHistogram_{name}\", sync_execution=sync_execution)\n</code></pre>"},{"location":"reference/metric/#zeus.metric.EnergyHistogram.end_window","title":"end_window","text":"<pre><code>end_window(name, sync_execution=True)\n</code></pre> <p>End the current energy monitoring window and record the energy data.</p> <p>Retrieves the energy consumption data (for GPUs, CPUs, and DRAMs) for the monitoring window and updates the corresponding Histogram metrics. The data is then pushed to the Prometheus Push Gateway.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The unique name of the measurement window. Must match between calls to 'begin_window' and 'end_window'.</p> required <code>sync_execution</code> <code>bool</code> <p>Whether to execute synchronously. Defaults to True.</p> <code>True</code> Source code in <code>zeus/metric.py</code> <pre><code>def end_window(self, name: str, sync_execution: bool = True) -&gt; None:\n    \"\"\"End the current energy monitoring window and record the energy data.\n\n    Retrieves the energy consumption data (for GPUs, CPUs, and DRAMs) for the monitoring window\n    and updates the corresponding Histogram metrics. The data is then pushed to the Prometheus Push Gateway.\n\n    Args:\n        name (str): The unique name of the measurement window. Must match between calls to 'begin_window' and 'end_window'.\n        sync_execution (bool): Whether to execute synchronously. Defaults to True.\n    \"\"\"\n    if sync_execution:\n        sync_execution_fn(self.gpu_indices)\n\n    measurement = self.energy_monitor.end_window(f\"__EnergyHistogram_{name}\", sync_execution=sync_execution)\n\n    if measurement.gpu_energy:\n        for gpu_index, gpu_energy in measurement.gpu_energy.items():\n            self.gpu_histograms.labels(window=name, index=gpu_index).observe(gpu_energy)\n            if gpu_energy &gt; self.max_gpu_bucket:\n                warnings.warn(\n                    f\"GPU {gpu_index} energy {gpu_energy} exceeds the maximum bucket value of {self.max_gpu_bucket}\",\n                    stacklevel=1,\n                )\n            if gpu_energy &lt; self.min_gpu_bucket:\n                warnings.warn(\n                    f\"GPU {gpu_index} energy {gpu_energy} exceeds the minimum bucket value of {self.min_gpu_bucket}\",\n                    stacklevel=1,\n                )\n\n    if measurement.cpu_energy:\n        for cpu_index, cpu_energy in measurement.cpu_energy.items():\n            self.cpu_histograms.labels(window=name, index=cpu_index).observe(cpu_energy)\n            if cpu_energy &gt; self.max_cpu_bucket:\n                warnings.warn(\n                    f\"CPU {cpu_index} energy {cpu_energy} exceeds the maximum bucket value of {self.max_cpu_bucket}\",\n                    stacklevel=1,\n                )\n            if cpu_energy &lt; self.min_cpu_bucket:\n                warnings.warn(\n                    f\"CPU {cpu_index} energy {cpu_energy} exceeds the minimum bucket value of {self.min_cpu_bucket}\",\n                    stacklevel=1,\n                )\n\n    if measurement.dram_energy:\n        for dram_index, dram_energy in measurement.dram_energy.items():\n            self.dram_histograms.labels(window=name, index=dram_index).observe(dram_energy)\n            if dram_energy &gt; self.max_dram_bucket:\n                warnings.warn(\n                    f\"DRAM {dram_index} energy {dram_energy} exceeds the maximum bucket value of {self.max_dram_bucket}\",\n                    stacklevel=1,\n                )\n            if dram_energy &lt; self.min_dram_bucket:\n                warnings.warn(\n                    f\"DRAM {dram_index} energy {dram_energy} exceeds the minimum bucket value of {self.min_dram_bucket}\",\n                    stacklevel=1,\n                )\n\n    push_to_gateway(self.pushgateway_url, job=self.job, registry=self.registry)\n</code></pre>"},{"location":"reference/metric/#zeus.metric.EnergyCumulativeCounter","title":"EnergyCumulativeCounter","text":"<p>               Bases: <code>Metric</code></p> <p>EnergyCumulativeCounter class to monitor and record cumulative energy consumption.</p> <p>This class tracks GPU, CPU, and DRAM energy usage over time, and records the data as Prometheus Counter metrics. The energy consumption metrics are periodically updated and pushed to a Prometheus Push Gateway for monitoring and analysis.</p> <p>The cumulative nature of the Counter ensures that energy values are always incremented over time, never reset, which is ideal for tracking continuously increasing values like energy usage.</p> Source code in <code>zeus/metric.py</code> <pre><code>class EnergyCumulativeCounter(Metric):\n    \"\"\"EnergyCumulativeCounter class to monitor and record cumulative energy consumption.\n\n    This class tracks GPU, CPU, and DRAM energy usage over time, and records the data as Prometheus Counter metrics.\n    The energy consumption metrics are periodically updated and pushed to a Prometheus Push Gateway for monitoring and analysis.\n\n    The cumulative nature of the Counter ensures that energy values are always incremented over time, never reset,\n    which is ideal for tracking continuously increasing values like energy usage.\n    \"\"\"\n\n    def __init__(\n        self,\n        cpu_indices: list,\n        gpu_indices: list,\n        update_period: int,\n        pushgateway_url: str,\n        job: str,\n    ) -&gt; None:\n        \"\"\"Initialize the EnergyCumulativeCounter.\n\n        Args:\n            cpu_indices (list): List of CPU indices to monitor.\n            gpu_indices (list): List of GPU indices to monitor.\n            update_period: The time interval (in seconds) at which energy measurements are updated.\n            pushgateway_url: The URL for the Prometheus Push Gateway where the metrics will be pushed.\n            job: The name of the job to be associated with the Prometheus metrics.\n        \"\"\"\n        self.cpu_indices = cpu_indices\n        self.gpu_indices = gpu_indices\n        self.update_period = update_period\n        self.pushgateway_url = pushgateway_url\n        self.job = job\n        self.window_state: dict[str, MonitoringProcessState] = {}\n\n    def begin_window(self, name: str, sync_execution: bool = False) -&gt; None:\n        \"\"\"Begin the energy monitoring window.\n\n        Starts a new multiprocessing process that monitors energy usage periodically\n        and pushes the results to the Prometheus Push Gateway.\n\n        Args:\n            name (str): The unique name of the measurement window. Must match between calls to 'begin_window' and 'end_window'.\n            sync_execution (bool, optional): Whether to execute monitoring synchronously. Defaults to False.\n        \"\"\"\n        if sync_execution:\n            sync_execution_fn(self.gpu_indices)\n\n        context = mp.get_context(\"spawn\")\n        queue = context.Queue()\n        proc = context.Process(\n            target=energy_monitoring_loop,\n            args=(\n                name,\n                queue,\n                self.cpu_indices,\n                self.gpu_indices,\n                self.update_period,\n                self.pushgateway_url,\n                self.job,\n            ),\n        )\n        proc.start()\n        if not proc.is_alive():\n            raise RuntimeError(f\"Failed to start monitoring process for {name}.\")\n\n        self.window_state[name] = MonitoringProcessState(queue=queue, proc=proc)\n\n    def end_window(self, name: str, sync_execution: bool = False) -&gt; None:\n        \"\"\"End the energy monitoring window.\n\n        Args:\n            name (str): The unique name of the measurement window. Must match between calls to 'begin_window' and 'end_window'.\n            sync_execution (bool, optional): Whether to execute monitoring synchronously. Defaults to False.\n        \"\"\"\n        if name not in self.window_state:\n            raise ValueError(f\"No active monitoring process found for '{name}'.\")\n\n        if sync_execution:\n            sync_execution_fn(self.gpu_indices)\n\n        state = self.window_state.pop(name)\n        state.queue.put(\"stop\")\n        state.proc.join(timeout=20)\n\n        if state.proc.is_alive():\n            state.proc.terminate()\n</code></pre>"},{"location":"reference/metric/#zeus.metric.EnergyCumulativeCounter.__init__","title":"__init__","text":"<pre><code>__init__(cpu_indices, gpu_indices, update_period, pushgateway_url, job)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>cpu_indices</code> <code>list</code> <p>List of CPU indices to monitor.</p> required <code>gpu_indices</code> <code>list</code> <p>List of GPU indices to monitor.</p> required <code>update_period</code> <code>int</code> <p>The time interval (in seconds) at which energy measurements are updated.</p> required <code>pushgateway_url</code> <code>str</code> <p>The URL for the Prometheus Push Gateway where the metrics will be pushed.</p> required <code>job</code> <code>str</code> <p>The name of the job to be associated with the Prometheus metrics.</p> required Source code in <code>zeus/metric.py</code> <pre><code>def __init__(\n    self,\n    cpu_indices: list,\n    gpu_indices: list,\n    update_period: int,\n    pushgateway_url: str,\n    job: str,\n) -&gt; None:\n    \"\"\"Initialize the EnergyCumulativeCounter.\n\n    Args:\n        cpu_indices (list): List of CPU indices to monitor.\n        gpu_indices (list): List of GPU indices to monitor.\n        update_period: The time interval (in seconds) at which energy measurements are updated.\n        pushgateway_url: The URL for the Prometheus Push Gateway where the metrics will be pushed.\n        job: The name of the job to be associated with the Prometheus metrics.\n    \"\"\"\n    self.cpu_indices = cpu_indices\n    self.gpu_indices = gpu_indices\n    self.update_period = update_period\n    self.pushgateway_url = pushgateway_url\n    self.job = job\n    self.window_state: dict[str, MonitoringProcessState] = {}\n</code></pre>"},{"location":"reference/metric/#zeus.metric.EnergyCumulativeCounter.begin_window","title":"begin_window","text":"<pre><code>begin_window(name, sync_execution=False)\n</code></pre> <p>Begin the energy monitoring window.</p> <p>Starts a new multiprocessing process that monitors energy usage periodically and pushes the results to the Prometheus Push Gateway.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The unique name of the measurement window. Must match between calls to 'begin_window' and 'end_window'.</p> required <code>sync_execution</code> <code>bool</code> <p>Whether to execute monitoring synchronously. Defaults to False.</p> <code>False</code> Source code in <code>zeus/metric.py</code> <pre><code>def begin_window(self, name: str, sync_execution: bool = False) -&gt; None:\n    \"\"\"Begin the energy monitoring window.\n\n    Starts a new multiprocessing process that monitors energy usage periodically\n    and pushes the results to the Prometheus Push Gateway.\n\n    Args:\n        name (str): The unique name of the measurement window. Must match between calls to 'begin_window' and 'end_window'.\n        sync_execution (bool, optional): Whether to execute monitoring synchronously. Defaults to False.\n    \"\"\"\n    if sync_execution:\n        sync_execution_fn(self.gpu_indices)\n\n    context = mp.get_context(\"spawn\")\n    queue = context.Queue()\n    proc = context.Process(\n        target=energy_monitoring_loop,\n        args=(\n            name,\n            queue,\n            self.cpu_indices,\n            self.gpu_indices,\n            self.update_period,\n            self.pushgateway_url,\n            self.job,\n        ),\n    )\n    proc.start()\n    if not proc.is_alive():\n        raise RuntimeError(f\"Failed to start monitoring process for {name}.\")\n\n    self.window_state[name] = MonitoringProcessState(queue=queue, proc=proc)\n</code></pre>"},{"location":"reference/metric/#zeus.metric.EnergyCumulativeCounter.end_window","title":"end_window","text":"<pre><code>end_window(name, sync_execution=False)\n</code></pre> <p>End the energy monitoring window.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The unique name of the measurement window. Must match between calls to 'begin_window' and 'end_window'.</p> required <code>sync_execution</code> <code>bool</code> <p>Whether to execute monitoring synchronously. Defaults to False.</p> <code>False</code> Source code in <code>zeus/metric.py</code> <pre><code>def end_window(self, name: str, sync_execution: bool = False) -&gt; None:\n    \"\"\"End the energy monitoring window.\n\n    Args:\n        name (str): The unique name of the measurement window. Must match between calls to 'begin_window' and 'end_window'.\n        sync_execution (bool, optional): Whether to execute monitoring synchronously. Defaults to False.\n    \"\"\"\n    if name not in self.window_state:\n        raise ValueError(f\"No active monitoring process found for '{name}'.\")\n\n    if sync_execution:\n        sync_execution_fn(self.gpu_indices)\n\n    state = self.window_state.pop(name)\n    state.queue.put(\"stop\")\n    state.proc.join(timeout=20)\n\n    if state.proc.is_alive():\n        state.proc.terminate()\n</code></pre>"},{"location":"reference/metric/#zeus.metric.PowerGauge","title":"PowerGauge","text":"<p>               Bases: <code>Metric</code></p> <p>PowerGauge class to monitor and record power consumption.</p> <p>This class tracks GPU power usage in real time and records it as Prometheus Gauge metrics. The Gauge metric type is suitable for tracking values that can go up and down over time, like power consumption.</p> <p>Power usage data is collected at regular intervals and pushed to a Prometheus Push Gateway for monitoring.</p> Source code in <code>zeus/metric.py</code> <pre><code>class PowerGauge(Metric):\n    \"\"\"PowerGauge class to monitor and record power consumption.\n\n    This class tracks GPU power usage in real time and records it as **Prometheus Gauge** metrics.\n    The Gauge metric type is suitable for tracking values that can go up and down over time, like power consumption.\n\n    Power usage data is collected at regular intervals and pushed to a Prometheus Push Gateway for monitoring.\n    \"\"\"\n\n    def __init__(\n        self,\n        gpu_indices: list,\n        update_period: int,\n        pushgateway_url: str,\n        job: str,\n    ) -&gt; None:\n        \"\"\"Initialize the PowerGauge metric.\n\n        Args:\n            gpu_indices (list[int]): List of GPU indices to monitor for power consumption.\n            update_period (int): Interval (in seconds) between consecutive power measurements.\n            pushgateway_url (str): URL of the Prometheus Push Gateway where Gauge metrics are pushed.\n            job (str): Name of the Prometheus job to associate with the power metrics.\n        \"\"\"\n        self.gpu_indices = gpu_indices\n        self.update_period = update_period\n        self.pushgateway_url = pushgateway_url\n        self.job = job\n        self.window_state: dict[str, MonitoringProcessState] = {}\n\n    def begin_window(self, name: str, sync_execution: bool = False) -&gt; None:\n        \"\"\"Begin the power monitoring window.\n\n        Starts a new multiprocessing process that runs the power monitoring loop.\n        The process collects real-time power consumption data and updates the corresponding\n        Gauge metrics in Prometheus.\n\n        Args:\n            name (str): The unique name of the measurement window. Must match between calls to 'begin_window' and 'end_window'.\n            sync_execution (bool, optional): Whether to execute monitoring synchronously. Defaults to False.\n        \"\"\"\n        if name in self.window_state:\n            raise ValueError(f\"PowerGauge metric '{name}' already exists.\")\n\n        if sync_execution:\n            sync_execution_fn(self.gpu_indices)\n\n        context = mp.get_context(\"spawn\")\n        queue = context.Queue()\n        proc = context.Process(\n            target=power_monitoring_loop,\n            args=(\n                name,\n                queue,\n                self.gpu_indices,\n                self.update_period,\n                self.pushgateway_url,\n                self.job,\n            ),\n        )\n        proc.start()\n        if not proc.is_alive():\n            raise RuntimeError(f\"Failed to start power monitoring process for '{name}'.\")\n\n        self.window_state[name] = MonitoringProcessState(queue=queue, proc=proc)\n\n    def end_window(self, name: str, sync_execution: bool = False) -&gt; None:\n        \"\"\"End the power monitoring window.\n\n        Args:\n            name (str): The unique name of the measurement window. Must match between calls to 'begin_window' and 'end_window'.\n            sync_execution (bool, optional): Whether to execute monitoring synchronously. Defaults to False.\n        \"\"\"\n        if sync_execution:\n            sync_execution_fn(self.gpu_indices)\n\n        state = self.window_state.pop(name)\n        state.queue.put(\"stop\")\n        state.proc.join(timeout=20)\n\n        if state.proc.is_alive():\n            state.proc.terminate()\n</code></pre>"},{"location":"reference/metric/#zeus.metric.PowerGauge.__init__","title":"__init__","text":"<pre><code>__init__(gpu_indices, update_period, pushgateway_url, job)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>gpu_indices</code> <code>list[int]</code> <p>List of GPU indices to monitor for power consumption.</p> required <code>update_period</code> <code>int</code> <p>Interval (in seconds) between consecutive power measurements.</p> required <code>pushgateway_url</code> <code>str</code> <p>URL of the Prometheus Push Gateway where Gauge metrics are pushed.</p> required <code>job</code> <code>str</code> <p>Name of the Prometheus job to associate with the power metrics.</p> required Source code in <code>zeus/metric.py</code> <pre><code>def __init__(\n    self,\n    gpu_indices: list,\n    update_period: int,\n    pushgateway_url: str,\n    job: str,\n) -&gt; None:\n    \"\"\"Initialize the PowerGauge metric.\n\n    Args:\n        gpu_indices (list[int]): List of GPU indices to monitor for power consumption.\n        update_period (int): Interval (in seconds) between consecutive power measurements.\n        pushgateway_url (str): URL of the Prometheus Push Gateway where Gauge metrics are pushed.\n        job (str): Name of the Prometheus job to associate with the power metrics.\n    \"\"\"\n    self.gpu_indices = gpu_indices\n    self.update_period = update_period\n    self.pushgateway_url = pushgateway_url\n    self.job = job\n    self.window_state: dict[str, MonitoringProcessState] = {}\n</code></pre>"},{"location":"reference/metric/#zeus.metric.PowerGauge.begin_window","title":"begin_window","text":"<pre><code>begin_window(name, sync_execution=False)\n</code></pre> <p>Begin the power monitoring window.</p> <p>Starts a new multiprocessing process that runs the power monitoring loop. The process collects real-time power consumption data and updates the corresponding Gauge metrics in Prometheus.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The unique name of the measurement window. Must match between calls to 'begin_window' and 'end_window'.</p> required <code>sync_execution</code> <code>bool</code> <p>Whether to execute monitoring synchronously. Defaults to False.</p> <code>False</code> Source code in <code>zeus/metric.py</code> <pre><code>def begin_window(self, name: str, sync_execution: bool = False) -&gt; None:\n    \"\"\"Begin the power monitoring window.\n\n    Starts a new multiprocessing process that runs the power monitoring loop.\n    The process collects real-time power consumption data and updates the corresponding\n    Gauge metrics in Prometheus.\n\n    Args:\n        name (str): The unique name of the measurement window. Must match between calls to 'begin_window' and 'end_window'.\n        sync_execution (bool, optional): Whether to execute monitoring synchronously. Defaults to False.\n    \"\"\"\n    if name in self.window_state:\n        raise ValueError(f\"PowerGauge metric '{name}' already exists.\")\n\n    if sync_execution:\n        sync_execution_fn(self.gpu_indices)\n\n    context = mp.get_context(\"spawn\")\n    queue = context.Queue()\n    proc = context.Process(\n        target=power_monitoring_loop,\n        args=(\n            name,\n            queue,\n            self.gpu_indices,\n            self.update_period,\n            self.pushgateway_url,\n            self.job,\n        ),\n    )\n    proc.start()\n    if not proc.is_alive():\n        raise RuntimeError(f\"Failed to start power monitoring process for '{name}'.\")\n\n    self.window_state[name] = MonitoringProcessState(queue=queue, proc=proc)\n</code></pre>"},{"location":"reference/metric/#zeus.metric.PowerGauge.end_window","title":"end_window","text":"<pre><code>end_window(name, sync_execution=False)\n</code></pre> <p>End the power monitoring window.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The unique name of the measurement window. Must match between calls to 'begin_window' and 'end_window'.</p> required <code>sync_execution</code> <code>bool</code> <p>Whether to execute monitoring synchronously. Defaults to False.</p> <code>False</code> Source code in <code>zeus/metric.py</code> <pre><code>def end_window(self, name: str, sync_execution: bool = False) -&gt; None:\n    \"\"\"End the power monitoring window.\n\n    Args:\n        name (str): The unique name of the measurement window. Must match between calls to 'begin_window' and 'end_window'.\n        sync_execution (bool, optional): Whether to execute monitoring synchronously. Defaults to False.\n    \"\"\"\n    if sync_execution:\n        sync_execution_fn(self.gpu_indices)\n\n    state = self.window_state.pop(name)\n    state.queue.put(\"stop\")\n    state.proc.join(timeout=20)\n\n    if state.proc.is_alive():\n        state.proc.terminate()\n</code></pre>"},{"location":"reference/metric/#zeus.metric.energy_monitoring_loop","title":"energy_monitoring_loop","text":"<pre><code>energy_monitoring_loop(name, pipe, cpu_indices, gpu_indices, update_period, pushgateway_url, job)\n</code></pre> <p>Runs in a separate process to collect and update energy consumption metrics (for GPUs, CPUs, and DRAM).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The user-defined name of the monitoring window (used as a label for Prometheus metrics).</p> required <code>pipe</code> <code>Queue</code> <p>A multiprocessing queue for inter-process communication, used to signal when to stop the process.</p> required <code>cpu_indices</code> <code>list</code> <p>List of CPU indices to monitor.</p> required <code>gpu_indices</code> <code>list</code> <p>List of GPU indices to monitor.</p> required <code>update_period</code> <code>int</code> <p>The interval (in seconds) between consecutive energy data updates.</p> required <code>pushgateway_url</code> <code>str</code> <p>The URL of the Prometheus Push Gateway where the metrics will be pushed.</p> required <code>job</code> <code>str</code> <p>The name of the Prometheus job associated with these metrics.</p> required Source code in <code>zeus/metric.py</code> <pre><code>def energy_monitoring_loop(\n    name: str,\n    pipe: mp.Queue,\n    cpu_indices: list,\n    gpu_indices: list,\n    update_period: int,\n    pushgateway_url: str,\n    job: str,\n) -&gt; None:\n    \"\"\"Runs in a separate process to collect and update energy consumption metrics (for GPUs, CPUs, and DRAM).\n\n    Args:\n        name (str): The user-defined name of the monitoring window (used as a label for Prometheus metrics).\n        pipe (mp.Queue): A multiprocessing queue for inter-process communication, used to signal when to stop the process.\n        cpu_indices (list): List of CPU indices to monitor.\n        gpu_indices (list): List of GPU indices to monitor.\n        update_period (int): The interval (in seconds) between consecutive energy data updates.\n        pushgateway_url (str): The URL of the Prometheus Push Gateway where the metrics will be pushed.\n        job (str): The name of the Prometheus job associated with these metrics.\n    \"\"\"\n    registry = CollectorRegistry()\n    energy_monitor = ZeusMonitor(cpu_indices=cpu_indices, gpu_indices=gpu_indices)\n    gpu_counters = None\n    cpu_counters = None\n    dram_counters = None\n\n    if energy_monitor.gpu_indices:\n        gpu_counters = Counter(\n            \"energy_monitor_gpu_energy_joules\",\n            \"GPU energy consumption\",\n            [\"window\", \"index\"],\n            registry=registry,\n        )\n\n    if energy_monitor.cpu_indices:\n        cpu_counters = Counter(\n            \"energy_monitor_cpu_energy_joules\",\n            \"CPU energy consumption\",\n            [\"window\", \"index\"],\n            registry=registry,\n        )\n        if any(cpu.supports_get_dram_energy_consumption() for cpu in get_cpus().cpus):\n            dram_counters = Counter(\n                \"energy_monitor_dram_energy_joules\",\n                \"DRAM energy consumption\",\n                [\"window\", \"index\"],\n                registry=registry,\n            )\n\n    while True:\n        if not pipe.empty():\n            break\n        # Begin and end monitoring window using sync_execution\n        energy_monitor.begin_window(f\"__EnergyCumulativeCounter_{name}\", sync_execution=False)\n        time.sleep(update_period)\n        measurement = energy_monitor.end_window(f\"__EnergyCumulativeCounter_{name}\", sync_execution=False)\n\n        if measurement.gpu_energy:\n            for gpu_index, energy in measurement.gpu_energy.items():\n                if gpu_counters:\n                    gpu_counters.labels(window=name, index=gpu_index).inc(energy)\n\n        if measurement.cpu_energy:\n            for cpu_index, energy in measurement.cpu_energy.items():\n                if cpu_counters:\n                    cpu_counters.labels(window=name, index=cpu_index).inc(energy)\n\n        if measurement.dram_energy:\n            for dram_index, energy in measurement.dram_energy.items():\n                if dram_counters:\n                    dram_counters.labels(window=name, index=dram_index).inc(energy)\n        # Push metrics to Prometheus\n        push_to_gateway(pushgateway_url, job=job, registry=registry)\n</code></pre>"},{"location":"reference/metric/#zeus.metric.power_monitoring_loop","title":"power_monitoring_loop","text":"<pre><code>power_monitoring_loop(name, pipe, gpu_indices, update_period, pushgateway_url, job)\n</code></pre> <p>Runs in a separate process and periodically collects power consumption data for each GPU and pushes the results to the Prometheus Push Gateway.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique name for the monitoring window (used as a label in Prometheus metrics).</p> required <code>pipe</code> <code>Queue</code> <p>Queue to receive control signals (e.g., \"stop\").</p> required <code>gpu_indices</code> <code>list[int]</code> <p>List of GPU indices to monitor for power consumption.</p> required <code>update_period</code> <code>int</code> <p>Interval (in seconds) between consecutive power data polls.</p> required <code>pushgateway_url</code> <code>str</code> <p>URL of the Prometheus Push Gateway where metrics are pushed.</p> required <code>job</code> <code>str</code> <p>Name of the Prometheus job to associate with the metrics.</p> required Source code in <code>zeus/metric.py</code> <pre><code>def power_monitoring_loop(\n    name: str,\n    pipe: mp.Queue,\n    gpu_indices: list[int],\n    update_period: int,\n    pushgateway_url: str,\n    job: str,\n) -&gt; None:\n    \"\"\"Runs in a separate process and periodically collects power consumption data for each GPU and pushes the results to the Prometheus Push Gateway.\n\n    Args:\n        name (str): Unique name for the monitoring window (used as a label in Prometheus metrics).\n        pipe (multiprocessing.Queue): Queue to receive control signals (e.g., \"stop\").\n        gpu_indices (list[int]): List of GPU indices to monitor for power consumption.\n        update_period (int): Interval (in seconds) between consecutive power data polls.\n        pushgateway_url (str): URL of the Prometheus Push Gateway where metrics are pushed.\n        job (str): Name of the Prometheus job to associate with the metrics.\n    \"\"\"\n    power_monitor = PowerMonitor(gpu_indices=gpu_indices)\n    registry = CollectorRegistry()\n\n    gpu_gauges = Gauge(\n        \"power_monitor_gpu_power_watts\",\n        \"Records power consumption for GPU over time\",\n        [\"window\", \"index\"],\n        registry=registry,\n    )\n\n    while True:\n        if not pipe.empty():\n            break\n\n        power_measurement = power_monitor.get_power()\n\n        try:\n            if power_measurement:\n                for gpu_index, power_value in power_measurement.items():\n                    gpu_gauges.labels(window=name, index=gpu_index).set(power_value)\n        except Exception as e:\n            print(f\"Error during processing power measurement: {e}\")\n\n        try:\n            push_to_gateway(pushgateway_url, job=job, registry=registry)\n        except Exception as e:\n            print(f\"Error pushing metrics: {e}\")\n\n        time.sleep(update_period)\n</code></pre>"},{"location":"reference/show_env/","title":"show_env","text":""},{"location":"reference/show_env/#zeus.show_env","title":"zeus.show_env","text":"<p>Collect information about the environment and display it.</p> <ul> <li>Python version</li> <li>Package availability and versions: Zeus, PyTorch, JAX, CuPy.</li> <li>NVIDIA GPU availability: Number of GPUs and models.</li> <li>AMD GPU availability: Number of GPUs and models.</li> <li>Intel RAPL availability: Number of CPUs and whether DRAM measurements are available.</li> <li>Zeusd daemon connectivity and capabilities (when configured).</li> </ul>"},{"location":"reference/show_env/#zeus.show_env.show_env","title":"show_env","text":"<pre><code>show_env()\n</code></pre> <p>Collect information about the environment and display it.</p> Source code in <code>zeus/show_env.py</code> <pre><code>def show_env():\n    \"\"\"Collect information about the environment and display it.\"\"\"\n    print(SECTION_SEPARATOR)\n    print(\"## Package availability and versions\\n\")\n    print(\"Logging output:\")\n\n    package_availability = \"  Python version: \" + platform.python_version() + \"\\n\"\n    package_availability += f\"  Zeus: {zeus.__version__}\\n\"\n\n    try:\n        torch_available = framework.torch_is_available()\n        torch_cuda_available = True\n    except RuntimeError:\n        torch_available = framework.torch_is_available(ensure_cuda=False)\n        torch_cuda_available = False\n\n    if torch_available and torch_cuda_available:\n        torch = framework.MODULE_CACHE[\"torch\"]\n        package_availability += f\"  PyTorch: {torch.__version__} (with CUDA support)\\n\"\n    elif torch_available and not torch_cuda_available:\n        torch = framework.MODULE_CACHE[\"torch\"]\n        package_availability += f\"  PyTorch: {torch.__version__} (without CUDA support)\\n\"\n    else:\n        package_availability += \"  PyTorch: not available\\n\"\n\n    try:\n        jax_available = framework.jax_is_available()\n        jax_cuda_available = True\n    except RuntimeError:\n        jax_available = framework.jax_is_available(ensure_cuda=False)\n        jax_cuda_available = False\n\n    if jax_available and jax_cuda_available:\n        jax = framework.MODULE_CACHE[\"jax\"]\n        package_availability += f\"  JAX: {jax.__version__} (with CUDA support)\\n\"\n    elif jax_available and not jax_cuda_available:\n        jax = framework.MODULE_CACHE[\"jax\"]\n        package_availability += f\"  JAX: {jax.__version__} (without CUDA support)\\n\"\n    else:\n        package_availability += \"  JAX: not available\\n\"\n\n    cupy_available = framework.cupy_is_available()\n    if cupy_available:\n        cupy = framework.MODULE_CACHE[\"cupy\"]\n        package_availability += f\"  CuPy: {cupy.__version__}\\n\"\n    else:\n        package_availability += \"  CuPy: not available\\n\"\n\n    print(\"\\nDetected:\\n\" + package_availability)\n\n    print(SECTION_SEPARATOR)\n    print(\"## Device availability\\n\")\n    print(\"Logging output:\")\n\n    gpu_availability = \"\"\n    try:\n        gpus = get_gpus()\n    except ZeusGPUInitError:\n        gpus = EmptyGPUs()\n    except ZeusBaseGPUError as e:\n        gpu_availability += f\"  Error initializing GPUs: {e}\\n\"\n        gpus = EmptyGPUs()\n\n    if len(gpus) &gt; 0:\n        # Check for visibility environment variables\n        cuda_visible_devices = os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"[NOT SET]\")\n        hip_visible_devices = os.environ.get(\"HIP_VISIBLE_DEVICES\", \"[NOT SET]\")\n        gpu_availability += f\"  CUDA_VISIBLE_DEVICES={cuda_visible_devices}\\n\"\n        gpu_availability += f\"  HIP_VISIBLE_DEVICES={hip_visible_devices}\\n\"\n\n        # Use Zeus's internal state to get the physical GPU indices\n        physical_indices = [gpu.gpu_index for gpu in gpus.gpus]\n\n        # Show device index mapping if env var restricts visibility\n        gpu_availability += \"  Device index mapping: Physical (e.g., nvidia-smi) -&gt; Zeus (or any CUDA application)\\n\"\n        for zeus_idx, physical_idx in enumerate(physical_indices):\n            gpu_availability += f\"    GPU {physical_idx} -&gt; GPU {zeus_idx} ({gpus.get_name(zeus_idx)})\\n\"\n    else:\n        gpu_availability += \"  No GPUs available.\\n\"\n    print(\"\\nDetected:\\n\" + gpu_availability)\n\n    print(SECTION_SEPARATOR)\n    print(\"## CPU availability\\n\")\n    print(\"Logging output:\")\n\n    cpu_availability = \"\"\n    try:\n        cpus = get_cpus()\n    except ZeusCPUInitError:\n        cpus = EmptyCPUs()\n    except ZeusBaseCPUError as e:\n        cpu_availability += f\"  Error initializing CPUs: {e}\\n\"\n        cpus = EmptyCPUs()\n\n    if len(cpus) &gt; 0:\n        assert isinstance(cpus, RAPLCPUs)\n        for i, cpu in enumerate(cpus.cpus):\n            if isinstance(cpu, ZeusdRAPLCPU):\n                cpu_availability += f\"  CPU {i}:\\n    CPU measurements available (Zeusd at {cpu._client.endpoint})\\n\"\n                if cpu.supports_get_dram_energy_consumption():\n                    cpu_availability += f\"    DRAM measurements available (Zeusd at {cpu._client.endpoint})\\n\"\n                else:\n                    cpu_availability += \"    DRAM measurements unavailable\\n\"\n            elif isinstance(cpu, RAPLCPU):\n                cpu_availability += f\"  CPU {i}:\\n    CPU measurements available ({cpu.rapl_file.path})\\n\"\n                if cpu.supports_get_dram_energy_consumption():\n                    dram = cpu.dram\n                    assert dram is not None\n                    cpu_availability += f\"    DRAM measurements available ({dram.path})\\n\"\n                else:\n                    cpu_availability += \"    DRAM measurements unavailable\\n\"\n            else:\n                raise TypeError(\"Unexpected CPU type: \" + str(type(cpu)))\n    else:\n        cpu_availability += \"  No CPUs available.\\n\"\n    print(\"\\nDetected:\\n\" + cpu_availability)\n\n    print(SECTION_SEPARATOR)\n    print(\"## SoC availability\\n\")\n    print(\"Logging output:\")\n\n    soc_availability = \"\"\n    try:\n        soc = get_soc()\n    except ZeusSoCInitError:\n        soc = EmptySoC()\n    except ZeusBaseSoCError as e:\n        soc_availability += f\"  Error initializing SoC: {e}\\n\"\n        soc = EmptySoC()\n\n    if isinstance(soc, AppleSilicon):\n        metrics = soc.get_available_metrics()\n        soc_availability += \"  Apple Silicon SoC available.\\n\"\n        soc_availability += f\"  Available metrics: {', '.join(metrics)}\\n\"\n    elif isinstance(soc, Jetson):\n        metrics = soc.get_available_metrics()\n        soc_availability += \"  NVIDIA Jetson SoC available.\\n\"\n        soc_availability += f\"  Available metrics: {', '.join(metrics)}\\n\"\n    else:\n        soc_availability += \"  No SoC available.\\n\"\n\n        # If this is an Apple Silicon device but AppleSilicon was not detected,\n        # print out a warning.\n        if platform.system() == \"Darwin\" and platform.machine() in [\"arm64\", \"aarch64\"]:\n            soc_availability += (\n                \"\\nThis appears to be an Apple Silicon device, but it wasn't picked up by Zeus.\\n\"\n                \"Have you installed Zeus with the `apple` extra dependencies?\\n\"\n                \"    pip install 'zeus[apple]'\\n\"\n            )\n    print(\"\\nDetected:\\n\" + soc_availability)\n\n    print(SECTION_SEPARATOR)\n    print(\"## Zeusd daemon\\n\")\n\n    config = ZeusdConfig.from_env()\n    if config is None:\n        print(\"  Not configured (set ZEUSD_SOCK_PATH or ZEUSD_HOST_PORT to connect).\\n\")\n    else:\n        zeusd_info = f\"  Endpoint: {config.endpoint}\\n\"\n        try:\n            client = ZeusdClient(config)\n            zeusd_info += f\"  GPU IDs: {client.gpu_ids}\\n\"\n            zeusd_info += f\"  CPU IDs: {client.cpu_ids}\\n\"\n            zeusd_info += f\"  DRAM available: {client.dram_available}\\n\"\n            zeusd_info += f\"  Auth required: {client.auth_required}\\n\"\n\n            # Daemon clock info.\n            t1 = time.time()\n            daemon_time = client.get_time()\n            t2 = time.time()\n            client_midpoint = (t1 + t2) / 2.0\n            clock_offset = client_midpoint - daemon_time\n            zeusd_info += f\"  Clock offset: {clock_offset:+.4f} s (client - daemon)\\n\"\n\n            # Capabilities.\n            zeusd_info += f\"  can_read_gpu: {client.can_read_gpu}\\n\"\n            zeusd_info += f\"  can_control_gpu: {client.can_control_gpu}\\n\"\n            zeusd_info += f\"  can_read_cpu: {client.can_read_cpu}\\n\"\n\n            # Auth details.\n            if client.auth_required:\n                if client.auth_error:\n                    zeusd_info += f\"  Auth error: {client.auth_error}\\n\"\n                else:\n                    zeusd_info += f\"  Authenticated as: {client._whoami_sub}\\n\"\n                    zeusd_info += f\"  Granted scopes: {sorted(client.granted_scopes)}\\n\"\n                    if client._whoami_exp is not None:\n                        exp_dt = datetime.datetime.fromtimestamp(client._whoami_exp).astimezone()\n                        status = \"expired\" if client._whoami_exp &lt; time.time() else \"valid\"\n                        zeusd_info += f\"  Token expires: {exp_dt.strftime('%Y-%m-%d %H:%M:%S %Z')} ({status})\\n\"\n                    else:\n                        zeusd_info += \"  Token expires: never\\n\"\n        except Exception as e:\n            zeusd_info += f\"  Error: {e}\\n\"\n        print(zeusd_info)\n\n    print(SECTION_SEPARATOR)\n</code></pre>"},{"location":"reference/_legacy/","title":"_legacy","text":""},{"location":"reference/_legacy/#zeus._legacy","title":"zeus._legacy","text":"<p>Zeus legacy batch size optimizer.</p> <p>In order to reproduce the paper's result, use this legacy code. To actually use the batch size optimizer, use the <code>zeus.optimizer.batch_size</code></p>"},{"location":"reference/_legacy/job/","title":"job","text":""},{"location":"reference/_legacy/job/#zeus._legacy.job","title":"zeus._legacy.job","text":"<p>Defines the Job specification dataclass.</p>"},{"location":"reference/_legacy/job/#zeus._legacy.job.Job","title":"Job  <code>dataclass</code>","text":"<p>Job specification tuple.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>str</code> <p>Name of the dataset.</p> <code>network</code> <code>str</code> <p>Name of the DNN model.</p> <code>optimizer</code> <code>str</code> <p>Name of the optimizer, e.g. Adam.</p> <code>target_metric</code> <code>float</code> <p>Target validation metric.</p> <code>max_epochs</code> <code>int</code> <p>Maximum number of epochs to train before terminating.</p> <code>default_bs</code> <code>int | None</code> <p>Initial batch size (b0) provided by the user.</p> <code>default_lr</code> <code>float | None</code> <p>Learning rate corresponding to the default batch size.</p> <code>workdir</code> <code>str | None</code> <p>Working directory in which to launch the job command.</p> <code>command</code> <code>list[str] | None</code> <p>Job command template. See <code>gen_command</code>.</p> Source code in <code>zeus/_legacy/job.py</code> <pre><code>@dataclass(frozen=True, unsafe_hash=True)\nclass Job:\n    \"\"\"Job specification tuple.\n\n    Attributes:\n        dataset: Name of the dataset.\n        network: Name of the DNN model.\n        optimizer: Name of the optimizer, e.g. Adam.\n        target_metric: Target validation metric.\n        max_epochs: Maximum number of epochs to train before terminating.\n        default_bs: Initial batch size (b0) provided by the user.\n        default_lr: Learning rate corresponding to the default batch size.\n        workdir: Working directory in which to launch the job command.\n        command: Job command template. See [`gen_command`][zeus._legacy.job.Job.gen_command].\n    \"\"\"\n\n    dataset: str\n    network: str\n    optimizer: str\n    target_metric: float\n    max_epochs: int\n    default_bs: int | None = None\n    default_lr: float | None = None\n    workdir: str | None = None\n    command: list[str] | None = field(default=None, hash=False, compare=False)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Generate a more conside representation of the object.\"\"\"\n        return (\n            f\"Job({self.dataset},{self.network},{self.optimizer},{self.target_metric}\"\n            f\"{f',bs{self.default_bs}' if self.default_bs is not None else ''}~{self.max_epochs})\"\n        )\n\n    def to_logdir(self) -&gt; str:\n        \"\"\"Generate a logdir name that explains this job.\"\"\"\n        return (\n            f\"{self.dataset}+{self.network}+bs{self.default_bs}\"\n            f\"+{self.optimizer}+lr{self.default_lr}\"\n            f\"+tm{self.target_metric}+me{self.max_epochs}\"\n        )\n\n    def filter_df(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Pick out the rows corresponding to this job from the DataFrame.\"\"\"\n        return df.loc[\n            (df.dataset == self.dataset)\n            &amp; (df.network == self.network)\n            &amp; (df.optimizer == self.optimizer)\n            &amp; (df.target_metric == self.target_metric)\n        ]\n\n    def gen_command(\n        self,\n        batch_size: int,\n        learning_rate: float,\n        seed: int,\n        rec_i: int,\n    ) -&gt; list[str]:\n        \"\"\"Format the job command with given arguments.\n\n        Args:\n            batch_size: Batch size to use for this job launch.\n            learning_rate: Learning rate to use for this job launch.\n            seed: Random seed to use for this job launch.\n            rec_i: Recurrence number of this job launch.\n        \"\"\"\n        assert self.command, \"You must provide a command format string for this job.\"\n        command = []\n        for piece in self.command:\n            if piece in [\"{bs}\", \"{batch_size}\"]:\n                command.append(str(batch_size))\n            elif piece in [\"{lr}\", \"{learning_rate}\"]:\n                command.append(str(learning_rate))\n            elif piece == \"{seed}\":\n                command.append(str(seed))\n            elif piece in [\"{epoch}\", \"{epochs}\"]:\n                command.append(str(self.max_epochs))\n            elif piece == \"{slice_number}\":\n                command.append(str(rec_i))\n            elif piece == \"{target_metric}\":\n                command.append(str(self.target_metric))\n            else:\n                command.append(piece)\n        return command\n\n    def scale_lr(self, batch_size: int) -&gt; float:\n        \"\"\"Scale the learning rate for the given batch size.\n\n        Assumes that `self.default_bs` and `self.default_lr` were given.\n        Then, `self.default_lr` is scaled for the given `batch_size` using\n        square root scaling for adaptive optimizers (e.g. Adam, Adadelta,\n        AdamW) and linear scaling for others (e.g. SGD).\n        \"\"\"\n        assert self.default_bs, \"You must provide default_bs to scale LR.\"\n        assert self.default_lr, \"You must provide default_lr to scale LR.\"\n\n        optimizer = self.optimizer.lower()\n        if optimizer in [\"adam\", \"adadelta\", \"adamw\"]:\n            scaler = SquareRootScaler(bs=self.default_bs, lr=self.default_lr)\n            return scaler.compute_lr(batch_size)\n        if optimizer in [\"sgd\"]:\n            scaler = LinearScaler(bs=self.default_bs, lr=self.default_lr)\n            return scaler.compute_lr(batch_size)\n        raise NotImplementedError(f\"LR scaling for {self.optimizer} is not supported.\")\n</code></pre>"},{"location":"reference/_legacy/job/#zeus._legacy.job.Job.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>Generate a more conside representation of the object.</p> Source code in <code>zeus/_legacy/job.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Generate a more conside representation of the object.\"\"\"\n    return (\n        f\"Job({self.dataset},{self.network},{self.optimizer},{self.target_metric}\"\n        f\"{f',bs{self.default_bs}' if self.default_bs is not None else ''}~{self.max_epochs})\"\n    )\n</code></pre>"},{"location":"reference/_legacy/job/#zeus._legacy.job.Job.to_logdir","title":"to_logdir","text":"<pre><code>to_logdir()\n</code></pre> <p>Generate a logdir name that explains this job.</p> Source code in <code>zeus/_legacy/job.py</code> <pre><code>def to_logdir(self) -&gt; str:\n    \"\"\"Generate a logdir name that explains this job.\"\"\"\n    return (\n        f\"{self.dataset}+{self.network}+bs{self.default_bs}\"\n        f\"+{self.optimizer}+lr{self.default_lr}\"\n        f\"+tm{self.target_metric}+me{self.max_epochs}\"\n    )\n</code></pre>"},{"location":"reference/_legacy/job/#zeus._legacy.job.Job.filter_df","title":"filter_df","text":"<pre><code>filter_df(df)\n</code></pre> <p>Pick out the rows corresponding to this job from the DataFrame.</p> Source code in <code>zeus/_legacy/job.py</code> <pre><code>def filter_df(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Pick out the rows corresponding to this job from the DataFrame.\"\"\"\n    return df.loc[\n        (df.dataset == self.dataset)\n        &amp; (df.network == self.network)\n        &amp; (df.optimizer == self.optimizer)\n        &amp; (df.target_metric == self.target_metric)\n    ]\n</code></pre>"},{"location":"reference/_legacy/job/#zeus._legacy.job.Job.gen_command","title":"gen_command","text":"<pre><code>gen_command(batch_size, learning_rate, seed, rec_i)\n</code></pre> <p>Format the job command with given arguments.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size to use for this job launch.</p> required <code>learning_rate</code> <code>float</code> <p>Learning rate to use for this job launch.</p> required <code>seed</code> <code>int</code> <p>Random seed to use for this job launch.</p> required <code>rec_i</code> <code>int</code> <p>Recurrence number of this job launch.</p> required Source code in <code>zeus/_legacy/job.py</code> <pre><code>def gen_command(\n    self,\n    batch_size: int,\n    learning_rate: float,\n    seed: int,\n    rec_i: int,\n) -&gt; list[str]:\n    \"\"\"Format the job command with given arguments.\n\n    Args:\n        batch_size: Batch size to use for this job launch.\n        learning_rate: Learning rate to use for this job launch.\n        seed: Random seed to use for this job launch.\n        rec_i: Recurrence number of this job launch.\n    \"\"\"\n    assert self.command, \"You must provide a command format string for this job.\"\n    command = []\n    for piece in self.command:\n        if piece in [\"{bs}\", \"{batch_size}\"]:\n            command.append(str(batch_size))\n        elif piece in [\"{lr}\", \"{learning_rate}\"]:\n            command.append(str(learning_rate))\n        elif piece == \"{seed}\":\n            command.append(str(seed))\n        elif piece in [\"{epoch}\", \"{epochs}\"]:\n            command.append(str(self.max_epochs))\n        elif piece == \"{slice_number}\":\n            command.append(str(rec_i))\n        elif piece == \"{target_metric}\":\n            command.append(str(self.target_metric))\n        else:\n            command.append(piece)\n    return command\n</code></pre>"},{"location":"reference/_legacy/job/#zeus._legacy.job.Job.scale_lr","title":"scale_lr","text":"<pre><code>scale_lr(batch_size)\n</code></pre> <p>Scale the learning rate for the given batch size.</p> <p>Assumes that <code>self.default_bs</code> and <code>self.default_lr</code> were given. Then, <code>self.default_lr</code> is scaled for the given <code>batch_size</code> using square root scaling for adaptive optimizers (e.g. Adam, Adadelta, AdamW) and linear scaling for others (e.g. SGD).</p> Source code in <code>zeus/_legacy/job.py</code> <pre><code>def scale_lr(self, batch_size: int) -&gt; float:\n    \"\"\"Scale the learning rate for the given batch size.\n\n    Assumes that `self.default_bs` and `self.default_lr` were given.\n    Then, `self.default_lr` is scaled for the given `batch_size` using\n    square root scaling for adaptive optimizers (e.g. Adam, Adadelta,\n    AdamW) and linear scaling for others (e.g. SGD).\n    \"\"\"\n    assert self.default_bs, \"You must provide default_bs to scale LR.\"\n    assert self.default_lr, \"You must provide default_lr to scale LR.\"\n\n    optimizer = self.optimizer.lower()\n    if optimizer in [\"adam\", \"adadelta\", \"adamw\"]:\n        scaler = SquareRootScaler(bs=self.default_bs, lr=self.default_lr)\n        return scaler.compute_lr(batch_size)\n    if optimizer in [\"sgd\"]:\n        scaler = LinearScaler(bs=self.default_bs, lr=self.default_lr)\n        return scaler.compute_lr(batch_size)\n    raise NotImplementedError(f\"LR scaling for {self.optimizer} is not supported.\")\n</code></pre>"},{"location":"reference/_legacy/simulate/","title":"simulate","text":""},{"location":"reference/_legacy/simulate/#zeus._legacy.simulate","title":"zeus._legacy.simulate","text":"<p>A simulator for running trace-driven Zeus experiments.</p>"},{"location":"reference/_legacy/simulate/#zeus._legacy.simulate.HistoryEntry","title":"HistoryEntry  <code>dataclass</code>","text":"<p>Represents the config and result of a job run that may have failed.</p> <p>Attributes:</p> Name Type Description <code>bs</code> <code>int</code> <p>Batch size</p> <code>pl</code> <code>int</code> <p>Power limit</p> <code>energy</code> <code>float</code> <p>Energy consumption in Joules</p> <code>reached</code> <code>bool</code> <p>Whether the target metric was reached at the end</p> <code>time</code> <code>float</code> <p>Time consumption in seconds</p> Source code in <code>zeus/_legacy/simulate.py</code> <pre><code>@dataclass\nclass HistoryEntry:\n    \"\"\"Represents the config and result of a job run that may have failed.\n\n    Attributes:\n        bs: Batch size\n        pl: Power limit\n        energy: Energy consumption in Joules\n        reached: Whether the target metric was reached at the end\n        time: Time consumption in seconds\n    \"\"\"\n\n    bs: int\n    pl: int\n    energy: float\n    reached: bool\n    time: float\n</code></pre>"},{"location":"reference/_legacy/simulate/#zeus._legacy.simulate.Simulator","title":"Simulator","text":"<p>Simulates job execution optimized by Zeus.</p> Source code in <code>zeus/_legacy/simulate.py</code> <pre><code>class Simulator:\n    \"\"\"Simulates job execution optimized by Zeus.\"\"\"\n\n    def __init__(\n        self,\n        summary_train: str | pd.DataFrame,\n        summary_power: str | pd.DataFrame,\n        batch_size_optimizer: BatchSizeOptimizer,\n        power_limit_optimizer: PowerLimitOptimizer,\n        seed: int = 123456,\n        verbose: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize the simulator.\n\n        Args:\n            summary_train: Path to or `pd.DataFrame` of the train trace.\n            summary_power: Path to or `pd.DataFrame` of the power trace.\n            batch_size_optimizer: The user is expected to construct\n                the BSO with the desired policy and pass it into the simulator.\n            power_limit_optimizer: The user is expected to construct\n                the PLO with the desired policy and pass it into the simulator.\n            seed: The random seed. Every invocation of any simulation method in this\n                class is deterministic given the random seed, because the internal RNG is\n                deepcopied before running the simulation.\n            verbose: Whether to log out the internal states of the simulator.\n        \"\"\"\n        # Generate relevant data.\n        train_df = pd.read_csv(summary_train) if isinstance(summary_train, str) else summary_train\n        power_df = pd.read_csv(summary_power) if isinstance(summary_power, str) else summary_power\n        df = train_df.merge(power_df, how=\"inner\")\n        df[\"TTA\"] = df.target_epoch * df.time_per_epoch\n        df[\"ETA\"] = df.TTA * df.average_power\n        # 'energy_per_epoch' is used to compare different power limits with the same batch size\n        # when trying to figure out which power limit is the best one.\n        df[\"energy_per_epoch\"] = df.time_per_epoch * df.average_power\n        self.df = df\n\n        # Knob optimizers.\n        self.bso = batch_size_optimizer\n        self.plo = power_limit_optimizer\n\n        # Save arguments.\n        self.seed = seed\n        self.verbose = verbose\n\n    def simulate_one_job(\n        self,\n        job: Job,\n        num_recurrence: int,\n        beta_knob: float,\n        eta_knob: float,\n    ) -&gt; list[HistoryEntry]:\n        r\"\"\"Simulate a sequentially recurring job. Explore with early stopping.\n\n        Args:\n            job: Job spec to simulate.\n            num_recurrence: How many times the job recurs.\n            beta_knob: `beta_knob * min_eta` is the early stopping cost threshold.\n                Set to `np.inf` to disable early stopping.\n            eta_knob: $\\eta$ used in the hybrid cost metric.\n                $\\textrm{cost} = \\eta \\cdot \\textrm{ETA} + (1 - \\eta) \\cdot \\textrm{MaxPower} \\cdot \\textrm{TTA}$\n\n        Returns:\n            A list of [`HistoryEntry`][zeus._legacy.simulate.HistoryEntry] objects for each job run.\n        \"\"\"\n        # Copy all internal state so that simulation does not modify any\n        # internal state and is deterministic w.r.t. the random seed.\n        bso = deepcopy(self.bso)\n        plo = deepcopy(self.plo)\n        rng = np.random.default_rng(self.seed)\n\n        # Figure out MAXPOWER.\n        max_power = self.df.power_limit.max().item()\n        if self.verbose:\n            print(f\"[Simulator] Max power = {max_power}W\")\n\n        # Track the minimum cost observed for the early stopping energy threshold.\n        min_cost = np.inf\n\n        # Simulate each job one at a time.\n        history: list[HistoryEntry] = []\n\n        if self.verbose:\n            print(f\"[Simulator] {job} recurring {num_recurrence} times.\")\n\n        # A new job. Profile the feasible batch size range.\n        batch_sizes = self._profile_batch_size_range(job)\n\n        # Register the job in the BSO.\n        bso.register_job(job, batch_sizes)\n\n        # Job recurs.\n        for i in range(num_recurrence):\n            if self.verbose:\n                print(f\"\\nRecurrence: {i + 1}\")\n\n            # Run the job until convergence. Upper bound the number of retries to 20.\n            # Accumulate the cost of retries before convergence.\n            cost_acc = 0.0\n            for tries in range(1, 21):\n                # Whether this run of the job needed to profile power.\n                profiled_power = False\n\n                # Fetch knobs to use.\n                bs = bso.predict(job)\n                pl = plo.predict(job, bs)\n\n                # When the batch size is first explored, we need to profile power limit.\n                if pl is None:\n                    profiled_power = True\n                    result = self._profile_power_limit(job, bs, eta_knob)\n                    for pl, epe in result.items():\n                        plo.observe(job, bs, pl, epe)\n                    pl = plo.predict(job, bs)\n                    assert pl\n\n                # Run the job, potentially early stopping it.\n                eta, tta, reached = self._run_job(\n                    job=job,\n                    batch_size=bs,\n                    power_limit=pl,\n                    rng=rng,\n                    cost_ub=beta_knob * min_cost,\n                    eta_knob=eta_knob,\n                    profile_power=profiled_power,\n                )\n\n                # The job never ran because even one epoch exceeds the cost threshold.\n                # Let the BSO observe that this batch size is bad, but since the job\n                # did not run, do not add to the history and retry.\n                if eta == 0 and tta == 0 and not reached:\n                    bso.observe(job, bs, 100 * beta_knob * min_cost, False)\n                    continue\n\n                # Compute the hybrid cost metric.\n                cost = zeus_cost(eta, tta, eta_knob, max_power)\n                cost_acc += cost\n\n                # Provide feedback to the BSO.\n                bso.observe(job, bs, cost, reached)\n\n                # Record history for analysis.\n                history.append(HistoryEntry(bs, pl, eta, reached, tta))\n\n                # Reached the target metric. Update min_cost and go on to the next recurrence.\n                if reached:\n                    if self.verbose:\n                        print()\n                        print(f\"[Simulator] Reached target metric in {tries} {'try' if tries == 1 else 'tries'}.\")\n                    if min_cost &gt; cost_acc:\n                        if self.verbose:\n                            print(f\"[Simulator] Minimum cost updated from {min_cost:.2f} to {cost_acc:.2f}.\")\n                        min_cost = cost_acc\n                    break\n                # Didn't reach the target metric.\n                # We assume that the default BS (set by the user) will always converge.\n                # That is, reaching the target metric with the model should be a feasible task.\n                if i == 0:\n                    raise RuntimeError(f\"The default batch size {job.default_bs} did not converge.\")\n\n            # Target metric was not reached in 20 tries. We consider this target metric to be unreachable.\n            else:\n                raise RuntimeError(\"Job did not reach the target metric in 20 tries.\")\n\n        if self.verbose:\n            print()\n            print(f\"[Simulator] {job} (BS, PL, ETA, whether_reached, TTA) history: \\n{history}\")\n\n        return history\n\n    def simulate_one_alibaba_group(\n        self,\n        job: Job,\n        group_df: pd.DataFrame,\n        beta_knob: float,\n        eta_knob: float,\n    ) -&gt; list[HistoryEntry]:\n        r\"\"\"Run simulation on one group in the Alibaba trace.\n\n        Concurrent job submissions (jobs that start before the previous job\n        finishes) are launched with the batch size known to be of minimum\n        cost at that time. The BSO also observes the results of these jobs\n        when they are done, and these jobs may well finish before a job that\n        started before it. See `observe` in PruningGTSBatchSizeOptimizer for\n        an example of handing such a scenario.\n\n        Args:\n            job: Job spec of this group.\n            group_df: DataFrame of this group. Fields:\n                - group: Group ID in trace. Identical across all rows.\n                - dataset: Dataset name. Identical across all rows.\n                - start_time: Job start time in the trace.\n                - end_time: Job end time in the trace.\n                - runtime_ratio: runtime of this job over the mean runtime\n                    of all the jobs of this dataset. Captures intra-dataset\n                    job runtime differences.\n            beta_knob: `beta_knob * min_eta` is the early stopping cost threshold.\n                Set to `np.inf` to disable early stopping.\n            eta_knob: $\\eta$ used in the hybrid cost metric.\n                $\\textrm{cost} = \\eta \\cdot \\textrm{ETA} + (1 - \\eta) \\cdot \\textrm{MaxPower} \\cdot \\textrm{TTA}$\n\n        Returns:\n            A list of [`HistoryEntry`][zeus._legacy.simulate.HistoryEntry] objects for each job run.\n        \"\"\"\n        # Copy all internal state so that simulation does not modify any\n        # internal state and is deterministic w.r.t. the random seed.\n        bso = deepcopy(self.bso)\n        plo = deepcopy(self.plo)\n        rng = np.random.default_rng(self.seed)\n\n        # Sanity check\n        if job.default_bs is None:\n            raise ValueError(\"You must give the job a default batch size.\")\n\n        # Figure out MAXPOWER.\n        max_power = self.df.power_limit.max().item()\n        if self.verbose:\n            print(f\"[Simulator] Max power = {max_power}W\")\n\n        # Track the minimum cost observed for the early stopping energy threshold.\n        # Also track the corresponding minimum cost BS to handle concurrent jobs.\n        min_cost = np.inf\n        best_bs = job.default_bs\n\n        # Simulate each job one at a time.\n        history: list[HistoryEntry] = []\n\n        if self.verbose:\n            print(f\"[Simulator] {job} recurring {len(group_df)} times.\")\n\n        # A new job. Profile the feasible batch size range.\n        batch_sizes = self._profile_batch_size_range(job)\n\n        # Register the job in the BSO.\n        bso.register_job(job, batch_sizes)\n\n        # List of jobs in flight.\n        @dataclass\n        class RunningJob:\n            \"\"\"Represents a job that is running.\n\n            We know what's going to happen to this job when we launch it.\n            Thus, pre-compute all results (using self.run_job) and have this\n            instance hold the information. Then, jobs will be removed from the\n            list of running jobs when the current time passes self.end_time and\n            the result will be observed.\n            \"\"\"\n\n            start_time: float\n            end_time: float\n            runtime_ratio: float\n            batch_size: int\n            power_limit: int\n            reached: bool\n            time: float\n            energy: float\n            cost: float\n\n        running_jobs: list[RunningJob] = []\n\n        # Jobs in group_df are already sorted by start_time.\n        current_time = 0.0\n        for rec_i, (_, job_row) in enumerate(group_df.iterrows()):\n            if self.verbose:\n                print(f\"\\nRecurrence: {rec_i + 1}\")\n\n            # Update the current time.\n            current_time = job_row.start_time\n            if self.verbose:\n                print(f\"[Simulator] Current time is {current_time:.1f}\")\n\n            # Scan the in-flight job list to reap jobs that have finished.\n            # We need a while loop here because we might have submitted a retry job\n            # while reaping jobs that failed to reach the target metric, and that retry job\n            # may finish before the current job.\n            while any(map(lambda j: j.end_time &lt;= current_time, running_jobs)):\n                if self.verbose:\n                    print(f\"[Simulator] Running jobs: {running_jobs}\")\n\n                # We copy running_jobs because we want to mutate it as we traverse it.\n                running_jobs_copy = deepcopy(running_jobs)\n\n                # Sort the jobs in the order they end.\n                for rjob in sorted(running_jobs_copy, key=operator.attrgetter(\"end_time\")):\n                    # We're only interested in jobs that finished at this point in time.\n                    if rjob.end_time &gt; current_time:\n                        continue\n\n                    # Job is finished at this point in time.\n                    if self.verbose:\n                        print(\n                            f\"[Simulator] Job(BS={rjob.batch_size},time={rjob.time},\"\n                            f\"energy={rjob.energy},reached={rjob.reached}) finished\"\n                        )\n\n                    # Remove the job from the in-flight job list.\n                    running_jobs.remove(rjob)\n\n                    # Let the BSO observe the cost for this batch size.\n                    bso.observe(job, rjob.batch_size, rjob.cost, rjob.reached)\n\n                    # If the job never ran because even one epoch exceeds the cost threshold,\n                    # do not add to the history and retry.\n                    if rjob.energy != 0 or rjob.time != 0 or rjob.reached:\n                        # Record history for analysis.\n                        history.append(\n                            HistoryEntry(\n                                rjob.batch_size,\n                                rjob.power_limit,\n                                rjob.energy * rjob.runtime_ratio,  # Scale the energy of this job by the runtime ratio.\n                                rjob.reached,\n                                rjob.time * rjob.runtime_ratio,  # Scale the runtime of this job by the runtime ratio.\n                            )\n                        )\n\n                    # Reached target metric (no need to retry)\n                    if rjob.reached:\n                        if min_cost &gt; rjob.cost:\n                            if self.verbose:\n                                print(f\"[Simulator] Minimum cost updated from {min_cost:.2f} to {rjob.cost:.2f}\")\n                            min_cost = rjob.cost\n                            best_bs = rjob.batch_size\n\n                    # Didn't reach target metric. Need to run a retry job.\n                    else:\n                        profiled_power = False\n\n                        # If there are jobs in-flight, we just run additional concurrent\n                        # submissions with the best known knobs.\n                        if running_jobs:\n                            if self.verbose:\n                                print(f\"[Simulator] There are in-flight jobs. Use BS {best_bs}.\")\n                            bs = best_bs\n                            pl = plo.predict(job, bs)\n                            assert pl, f\"Power not profiled for best known BS {bs}\"\n\n                        # There are no jobs in-flight. Just submit the job normally.\n                        else:\n                            # Determine the knobs.\n                            bs = bso.predict(job)\n                            pl = plo.predict(job, bs)\n\n                            if self.verbose:\n                                print(f\"[Simulator] There are no in-flight jobs. Use BSO's prediction {bs}.\")\n\n                            # When the batch size is first explored, we need to profile power limit.\n                            if pl is None:\n                                profiled_power = True\n                                result = self._profile_power_limit(job, bs, eta_knob)\n                                for pl, epe in result.items():\n                                    plo.observe(job, bs, pl, epe)\n                                pl = plo.predict(job, bs)\n                                assert pl\n\n                        # Pre-compute the result of the job.\n                        eta, tta, reached = self._run_job(\n                            job=job,\n                            batch_size=bs,\n                            power_limit=pl,\n                            rng=rng,\n                            cost_ub=beta_knob * min_cost,\n                            eta_knob=eta_knob,\n                            profile_power=profiled_power,\n                        )\n\n                        # Compute the hybrid cost metric.\n                        cost = zeus_cost(eta, tta, eta_knob, max_power)\n\n                        # Create the RunningJob instance.\n                        running_job = RunningJob(\n                            start_time=rjob.end_time,\n                            end_time=rjob.end_time + (rjob.end_time - rjob.start_time),  # Assume same runtime.\n                            runtime_ratio=rjob.runtime_ratio,\n                            batch_size=bs,\n                            power_limit=pl,\n                            reached=reached,\n                            time=tta,\n                            energy=eta,\n                            cost=cost,\n                        )\n                        running_jobs.append(running_job)\n\n                        if self.verbose:\n                            print(f\"[Simulator] Started retry job {running_job}\")\n\n                        # We must break from the loop that scans the running_jobs list, because\n                        # the job we just submitted might actually be the next job that finishes.\n                        break\n\n            # We're (finally) done reaping all finished jobs. Run the current job.\n            profiled_power = False\n\n            # If there are jobs in-flight, we just run additional concurrent\n            # submissions with the best known knobs.\n            if running_jobs:\n                if self.verbose:\n                    print(f\"[Simulator] There are in-flight jobs. Use BS {best_bs}.\")\n                bs = best_bs\n                pl = plo.predict(job, bs)\n                assert pl is not None, f\"Power not profiled for best known BS {bs}\"\n\n            # There are no jobs in-flight. Just submit the job.\n            else:\n                # Determine the knobs.\n                bs = bso.predict(job)\n                pl = plo.predict(job, bs)\n\n                if self.verbose:\n                    print(f\"[Simulator] There are no in-flight jobs. Use BSO's prediction {bs}.\")\n\n                # When the batch size is first explored, we need to profile power limit.\n                if pl is None:\n                    profiled_power = True\n                    result = self._profile_power_limit(job, bs, eta_knob)\n                    for pl, epe in result.items():\n                        plo.observe(job, bs, pl, epe)\n                    pl = plo.predict(job, bs)\n                    assert pl\n\n            # Run the job, potentially early stopping it.\n            eta, tta, reached = self._run_job(\n                job=job,\n                batch_size=bs,\n                power_limit=pl,\n                rng=rng,\n                cost_ub=beta_knob * min_cost,\n                eta_knob=eta_knob,\n                profile_power=profiled_power,\n            )\n\n            # Compute the hybrid cost metric.\n            cost = zeus_cost(eta, tta, eta_knob, max_power)\n\n            # Create the RunningJob instance and enqueue.\n            running_job = RunningJob(\n                start_time=job_row.start_time,\n                end_time=job_row.end_time,\n                runtime_ratio=job_row.runtime_ratio,\n                batch_size=bs,\n                power_limit=pl,\n                reached=reached,\n                time=tta,\n                energy=eta,\n                cost=cost,\n            )\n            running_jobs.append(running_job)\n\n            if self.verbose:\n                print(f\"[Simulator] Started job {running_job}\")\n\n        # Now, we're done iterating the rows of group_df.\n        # Set the current time to infinity so that all running jobs finish.\n        current_time = np.inf\n        if self.verbose:\n            print(\"\\n[Simulator] Reap all jobs\")\n\n        # Scan the remaining in-flight job list to reap jobs that have finished.\n        # Since current_time is infinity, this while loop will reap all the jobs ever launched.\n        while any(map(lambda j: j.end_time &lt;= current_time, running_jobs)):\n            if self.verbose:\n                print(f\"[Simulator] Running jobs: {running_jobs}\")\n\n            # We copy running_jobs because we want to mutate it as we traverse it.\n            running_jobs_copy = deepcopy(running_jobs)\n\n            # Sort the jobs in the order they end.\n            for rjob in sorted(running_jobs_copy, key=operator.attrgetter(\"end_time\")):\n                # We're only interested in jobs that finished at this point in time.\n                if rjob.end_time &gt; current_time:\n                    continue\n\n                # Job is finished at this point in time.\n                if self.verbose:\n                    print(\n                        f\"[Simulator] Job(BS={rjob.batch_size},time={rjob.time},\"\n                        f\"energy={rjob.energy},reached={rjob.reached}) finished\"\n                    )\n\n                # Remove the job from the in-flight job list.\n                running_jobs.remove(rjob)\n\n                # Let the BSO observe the cost for this batch size.\n                bso.observe(job, rjob.batch_size, rjob.cost, rjob.reached)\n\n                # If the job never ran because even one epoch exceeds the cost threshold,\n                # do not add to the history and retry.\n                if rjob.energy != 0 or rjob.time != 0 or rjob.reached:\n                    # Record history for analysis.\n                    history.append(\n                        HistoryEntry(\n                            rjob.batch_size,\n                            rjob.power_limit,\n                            rjob.energy * rjob.runtime_ratio,  # Scale the energy of this job by the runtime ratio.\n                            rjob.reached,\n                            rjob.time * rjob.runtime_ratio,  # Scale the runtime of this job by the runtime ratio.\n                        )\n                    )\n\n                # Reached target metric (no need to retry)\n                if rjob.reached:\n                    if min_cost &gt; rjob.cost:\n                        if self.verbose:\n                            print(f\"[Simulator] Minimum cost updated from {min_cost:.2f} to {rjob.cost:.2f}\")\n                        min_cost = rjob.cost\n                        best_bs = rjob.batch_size\n\n                # Didin't reach target metric. Need to run a retry job.\n                else:\n                    profiled_power = False\n\n                    # If there are jobs in-flight, we just run additional concurrent\n                    # submissions with the best known knobs.\n                    if running_jobs:\n                        if self.verbose:\n                            print(f\"[Simulator] There are in-flight jobs. Use BS {best_bs}.\")\n                        bs = best_bs\n                        pl = plo.predict(job, bs)\n                        assert pl, f\"Power not profiled for best known BS {bs}\"\n\n                    # There are no jobs in-flight. Just submit the job normally.\n                    else:\n                        # Determine the knobs.\n                        bs = bso.predict(job)\n                        pl = plo.predict(job, bs)\n\n                        if self.verbose:\n                            print(f\"[Simulator] There are no in-flight jobs. Use BSO's prediction {bs}.\")\n\n                        # When the batch size is first explored, we need to profile power limit.\n                        if pl is None:\n                            profiled_power = True\n                            result = self._profile_power_limit(job, bs, eta_knob)\n                            for pl, epe in result.items():\n                                plo.observe(job, bs, pl, epe)\n                            pl = plo.predict(job, bs)\n                            assert pl\n\n                    # Pre-compute the result of the job.\n                    eta, tta, reached = self._run_job(\n                        job=job,\n                        batch_size=bs,\n                        power_limit=pl,\n                        rng=rng,\n                        cost_ub=beta_knob * min_cost,\n                        eta_knob=eta_knob,\n                        profile_power=profiled_power,\n                    )\n\n                    # Compute the hybrid cost metric.\n                    cost = zeus_cost(eta, tta, eta_knob, max_power)\n\n                    # Create the RunningJob instance.\n                    running_job = RunningJob(\n                        start_time=rjob.end_time,\n                        end_time=rjob.end_time + (rjob.end_time - rjob.start_time),  # Assume same runtime.\n                        runtime_ratio=rjob.runtime_ratio,\n                        batch_size=bs,\n                        power_limit=pl,\n                        reached=reached,\n                        time=tta,\n                        energy=eta,\n                        cost=cost,\n                    )\n                    running_jobs.append(running_job)\n\n                    if self.verbose:\n                        print(f\"[Simulator] Started retry job {running_job}\")\n\n                    # We must break from the loop that scans the running_jobs list, because\n                    # the job we just submitted might actually be the next job that finishes.\n                    break\n\n        return history\n\n    def _run_job(\n        self,\n        job: Job,\n        batch_size: int,\n        power_limit: int,\n        rng: np.random.Generator,\n        cost_ub: float,\n        eta_knob: float,\n        profile_power: bool,\n    ) -&gt; tuple[float, float, bool]:\n        r\"\"\"Simulate running the job and return the energy consumed and whether it converged.\n\n        This method will randomly choose one of the possible training \"paths\". Then,\n        based on cost_ub, it will compute the maximum number of epochs the job can run.\n        If the path's target_epoch is smaller than or equal to the maximum number of\n        epochs, the cost incurred until target_epoch is returned. Otherwise, the cost\n        incurred until the maximum number of epochs is returned.\n\n        It is important to note that the job may never run when the first epoch's cost\n        is already expected to exceed the cost upper bound. In such a case, the returned\n        time and energy consumptions will be zero. This case must be treated separately\n        in the calling code.\n\n        If profile_power is True, the first epoch will JIT-profile power limits coarsely\n        by dividing the epoch evenly into len(available_power_limits) pieces. Thus the\n        the first epoch's energy and time consumption will be slightly adjusted.\n\n        Args:\n            job: Job spec to run.\n            batch_size: Batch size to use.\n            power_limit: Power limit to use. Regardless of whether this run of this\n                batch size requires power profiling, the simulator will input the optimal\n                power limit for the batch size. The first epoch energy consumption from\n                power profiling is adjusted in the latter half of this method based on the\n                profile_power flag.\n            rng: Random number generator used to sample one training path.\n            cost_ub: Cost upper bound. The job is terminated when the next epoch is going\n                to exceed the cost upper bound.\n            eta_knob: $\\eta$ used in the hybrid cost metric.\n                $\\textrm{cost} = \\eta \\cdot \\textrm{ETA} + (1 - \\eta) \\cdot \\textrm{MaxPower} \\cdot \\textrm{TTA}$\n            profile_power: Whether this run of the job should profile power during the\n                first epoch.\n\n        Returns:\n            Tuple of energy consumption, time consumption, and whether the job reached the target metric.\n        \"\"\"\n        # df is filtered with job spec, BS, and PL. We sample one possible training path.\n        # power_df is filtered with job spec and BS. We use this to compute the energy\n        # consumption of profiling power during the first epoch.\n        df = job.filter_df(self.df)\n        power_df = df.loc[df.batch_size == batch_size]\n        df = power_df.loc[df.power_limit == power_limit]\n        path = df.sample(n=1, random_state=rng)\n\n        # Max number of epochs is bound by either the cost upper bound or the user-specified\n        # max_epochs, whichever is smaller.\n        if cost_ub == np.inf:\n            # cost_ub is infinity in two cases:\n            # 1. The simulator has never observed any cost value in the early part of simulation.\n            # 2. We're simulating with no early stopping, i.e. beta_knob is infinity.\n            max_epochs = job.max_epochs\n            if self.verbose:\n                print(f\"[run job] Cost UB is inf. {max_epochs=}\")\n        else:\n            # Stop right before the first epoch when cost will cross the upper bound.\n            cost_per_epoch = (\n                eta_knob * path.energy_per_epoch.item()\n                + (1 - eta_knob) * power_df.power_limit.max().item() * path.time_per_epoch.item()\n            )\n            max_epochs = min(cost_ub // cost_per_epoch, job.max_epochs)\n            if self.verbose:\n                print(f\"[run job] {cost_ub=}\")\n                print(f\"[run job] {cost_per_epoch=}\")\n                print(f\"[run job] {max_epochs=}\")\n\n        # The job virtually never ran. Time and Energy being zero will be treated specially outside.\n        # If the min_cost is so low, this might even prevent this BS from running at all.\n        if max_epochs == 0:\n            print(\n                f\"[run job] {job} cannot run even one epoch without exceeding the cost UB.\"\n                f\" BS {batch_size}, PL {power_limit}, {eta_knob=}\"\n            )\n            return 0.0, 0.0, False\n\n        def compute_energy_and_time(num_epochs: int, profile_power: bool) -&gt; tuple[float, float]:\n            \"\"\"Compute the energy and time consumed for running the job for num_epochs.\"\"\"\n            # This is the first run of this batch size, and we need to profile power\n            # during the first epoch.\n            if profile_power:\n                # Note that power_df holds rows with all power limits. Evenly splitting the\n                # epochs with the number of samples and running each slice with each power\n                # limit consumes (1/N) * e_100 + (1/N) * e_125 + ... + (1/N) * e_250.\n                # Also there are all runs 1, 2, ... included, but power info is actually\n                # completely duplicated across different runs in the DataFrame.\n                # Thus, taking the mean across the entire power_df gets us what we want.\n                energy_first_epoch = power_df.energy_per_epoch.mean().item()\n                energy_from_second_epoch = path.energy_per_epoch.item() * (num_epochs - 1)\n                energy_consumption = energy_first_epoch + energy_from_second_epoch\n                time_first_epoch = power_df.time_per_epoch.mean().item()\n                time_from_second_epoch = path.time_per_epoch.item() * (num_epochs - 1)\n                time_consumption = time_first_epoch + time_from_second_epoch\n            # Just run num_epochs with the given power limit. Simple.\n            else:\n                energy_consumption = path.energy_per_epoch.item() * num_epochs\n                time_consumption = path.time_per_epoch.item() * num_epochs\n            return energy_consumption, time_consumption\n\n        # Job reached target metric.\n        target_epoch = path.target_epoch.item()\n        if path.target_epoch.notnull().item() and target_epoch &lt;= max_epochs:\n            eta, tta = compute_energy_and_time(target_epoch, profile_power)\n            if self.verbose:\n                print(\n                    f\"[run job] {job} @ {batch_size},{power_limit}W{' prof' if profile_power else ''} \"\n                    f\"=&gt; \\033[31mReached in {int(target_epoch)} epochs, \"\n                    f\"TTA {tta:.2f} seconds, ETA {eta:.2f}\\033[0m\"\n                )\n            return eta, tta, True\n\n        # Job failed to reach the target metric.\n        energy_consumption, time_consumption = compute_energy_and_time(max_epochs, profile_power)\n        if self.verbose:\n            print(\n                f\"[run job] {job} @ {batch_size},{power_limit}W{' prof' if profile_power else ''} \"\n                f\"=&gt; \\033[31mFailed (stopped after {int(max_epochs)} epochs), \"\n                f\"TTA {time_consumption:.2f} seconds, ETA {energy_consumption:.2f}\\033[0m\"\n            )\n        return energy_consumption, time_consumption, False\n\n    def _profile_power_limit(self, job: Job, batch_size: int, eta_knob: float) -&gt; dict[int, float]:\n        \"\"\"Simulate running the job and profiling the power limit.\n\n        Returns:\n            Dictionary mapping PL to `energy_per_epoch`. PL is inserted in high to low order.\n        \"\"\"\n        # Filter by job spec and BS.\n        df = job.filter_df(self.df)\n        df = df.loc[(df.batch_size == batch_size)]\n\n        # Compute the epoch cost of each power limit (Equation 7).\n        max_pl = df.power_limit.max().item()\n        df = df.groupby([\"power_limit\"], as_index=False).mean(numeric_only=True)\n        df[\"epoch_cost\"] = (eta_knob * df[\"average_power\"] + (1 - eta_knob) * max_pl) * df[\"time_per_epoch\"]\n\n        # We'll be profiling energy from larger to smaller power limits.\n        df = df.sort_values(by=\"power_limit\", ascending=False)\n        result = {rec.power_limit: rec.epoch_cost for rec in df.to_records(index=False)}\n        if self.verbose:\n            print(f\"[PL profile] {job} @ {batch_size} =&gt; PL = {min(result, key=result.get)}W\")  # type: ignore\n        return result\n\n    def _profile_batch_size_range(self, job: Job) -&gt; list[int]:\n        \"\"\"Simulate profiling the available batch size range of the job.\n\n        Returns:\n            A list of feasible batch sizes.\n        \"\"\"\n        df = self.df\n        # Do not filter by target_metric here since we do not want to constrain\n        # the feasible batch size range to only those that reached the target metric.\n        df = df.loc[(df.dataset == job.dataset) &amp; (df.network == job.network) &amp; (df.optimizer == job.optimizer)]\n        result = sorted(list(df.batch_size.unique()))\n        if self.verbose:\n            print(f\"[BS profile] {job} =&gt; BS = {result}\")\n        return result\n</code></pre>"},{"location":"reference/_legacy/simulate/#zeus._legacy.simulate.Simulator.__init__","title":"__init__","text":"<pre><code>__init__(summary_train, summary_power, batch_size_optimizer, power_limit_optimizer, seed=123456, verbose=True)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>summary_train</code> <code>str | DataFrame</code> <p>Path to or <code>pd.DataFrame</code> of the train trace.</p> required <code>summary_power</code> <code>str | DataFrame</code> <p>Path to or <code>pd.DataFrame</code> of the power trace.</p> required <code>batch_size_optimizer</code> <code>BatchSizeOptimizer</code> <p>The user is expected to construct the BSO with the desired policy and pass it into the simulator.</p> required <code>power_limit_optimizer</code> <code>PowerLimitOptimizer</code> <p>The user is expected to construct the PLO with the desired policy and pass it into the simulator.</p> required <code>seed</code> <code>int</code> <p>The random seed. Every invocation of any simulation method in this class is deterministic given the random seed, because the internal RNG is deepcopied before running the simulation.</p> <code>123456</code> <code>verbose</code> <code>bool</code> <p>Whether to log out the internal states of the simulator.</p> <code>True</code> Source code in <code>zeus/_legacy/simulate.py</code> <pre><code>def __init__(\n    self,\n    summary_train: str | pd.DataFrame,\n    summary_power: str | pd.DataFrame,\n    batch_size_optimizer: BatchSizeOptimizer,\n    power_limit_optimizer: PowerLimitOptimizer,\n    seed: int = 123456,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the simulator.\n\n    Args:\n        summary_train: Path to or `pd.DataFrame` of the train trace.\n        summary_power: Path to or `pd.DataFrame` of the power trace.\n        batch_size_optimizer: The user is expected to construct\n            the BSO with the desired policy and pass it into the simulator.\n        power_limit_optimizer: The user is expected to construct\n            the PLO with the desired policy and pass it into the simulator.\n        seed: The random seed. Every invocation of any simulation method in this\n            class is deterministic given the random seed, because the internal RNG is\n            deepcopied before running the simulation.\n        verbose: Whether to log out the internal states of the simulator.\n    \"\"\"\n    # Generate relevant data.\n    train_df = pd.read_csv(summary_train) if isinstance(summary_train, str) else summary_train\n    power_df = pd.read_csv(summary_power) if isinstance(summary_power, str) else summary_power\n    df = train_df.merge(power_df, how=\"inner\")\n    df[\"TTA\"] = df.target_epoch * df.time_per_epoch\n    df[\"ETA\"] = df.TTA * df.average_power\n    # 'energy_per_epoch' is used to compare different power limits with the same batch size\n    # when trying to figure out which power limit is the best one.\n    df[\"energy_per_epoch\"] = df.time_per_epoch * df.average_power\n    self.df = df\n\n    # Knob optimizers.\n    self.bso = batch_size_optimizer\n    self.plo = power_limit_optimizer\n\n    # Save arguments.\n    self.seed = seed\n    self.verbose = verbose\n</code></pre>"},{"location":"reference/_legacy/simulate/#zeus._legacy.simulate.Simulator.simulate_one_job","title":"simulate_one_job","text":"<pre><code>simulate_one_job(job, num_recurrence, beta_knob, eta_knob)\n</code></pre> <p>Simulate a sequentially recurring job. Explore with early stopping.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>Job spec to simulate.</p> required <code>num_recurrence</code> <code>int</code> <p>How many times the job recurs.</p> required <code>beta_knob</code> <code>float</code> <p><code>beta_knob * min_eta</code> is the early stopping cost threshold. Set to <code>np.inf</code> to disable early stopping.</p> required <code>eta_knob</code> <code>float</code> <p>\\(\\eta\\) used in the hybrid cost metric. \\(\\textrm{cost} = \\eta \\cdot \\textrm{ETA} + (1 - \\eta) \\cdot \\textrm{MaxPower} \\cdot \\textrm{TTA}\\)</p> required <p>Returns:</p> Type Description <code>list[HistoryEntry]</code> <p>A list of <code>HistoryEntry</code> objects for each job run.</p> Source code in <code>zeus/_legacy/simulate.py</code> <pre><code>def simulate_one_job(\n    self,\n    job: Job,\n    num_recurrence: int,\n    beta_knob: float,\n    eta_knob: float,\n) -&gt; list[HistoryEntry]:\n    r\"\"\"Simulate a sequentially recurring job. Explore with early stopping.\n\n    Args:\n        job: Job spec to simulate.\n        num_recurrence: How many times the job recurs.\n        beta_knob: `beta_knob * min_eta` is the early stopping cost threshold.\n            Set to `np.inf` to disable early stopping.\n        eta_knob: $\\eta$ used in the hybrid cost metric.\n            $\\textrm{cost} = \\eta \\cdot \\textrm{ETA} + (1 - \\eta) \\cdot \\textrm{MaxPower} \\cdot \\textrm{TTA}$\n\n    Returns:\n        A list of [`HistoryEntry`][zeus._legacy.simulate.HistoryEntry] objects for each job run.\n    \"\"\"\n    # Copy all internal state so that simulation does not modify any\n    # internal state and is deterministic w.r.t. the random seed.\n    bso = deepcopy(self.bso)\n    plo = deepcopy(self.plo)\n    rng = np.random.default_rng(self.seed)\n\n    # Figure out MAXPOWER.\n    max_power = self.df.power_limit.max().item()\n    if self.verbose:\n        print(f\"[Simulator] Max power = {max_power}W\")\n\n    # Track the minimum cost observed for the early stopping energy threshold.\n    min_cost = np.inf\n\n    # Simulate each job one at a time.\n    history: list[HistoryEntry] = []\n\n    if self.verbose:\n        print(f\"[Simulator] {job} recurring {num_recurrence} times.\")\n\n    # A new job. Profile the feasible batch size range.\n    batch_sizes = self._profile_batch_size_range(job)\n\n    # Register the job in the BSO.\n    bso.register_job(job, batch_sizes)\n\n    # Job recurs.\n    for i in range(num_recurrence):\n        if self.verbose:\n            print(f\"\\nRecurrence: {i + 1}\")\n\n        # Run the job until convergence. Upper bound the number of retries to 20.\n        # Accumulate the cost of retries before convergence.\n        cost_acc = 0.0\n        for tries in range(1, 21):\n            # Whether this run of the job needed to profile power.\n            profiled_power = False\n\n            # Fetch knobs to use.\n            bs = bso.predict(job)\n            pl = plo.predict(job, bs)\n\n            # When the batch size is first explored, we need to profile power limit.\n            if pl is None:\n                profiled_power = True\n                result = self._profile_power_limit(job, bs, eta_knob)\n                for pl, epe in result.items():\n                    plo.observe(job, bs, pl, epe)\n                pl = plo.predict(job, bs)\n                assert pl\n\n            # Run the job, potentially early stopping it.\n            eta, tta, reached = self._run_job(\n                job=job,\n                batch_size=bs,\n                power_limit=pl,\n                rng=rng,\n                cost_ub=beta_knob * min_cost,\n                eta_knob=eta_knob,\n                profile_power=profiled_power,\n            )\n\n            # The job never ran because even one epoch exceeds the cost threshold.\n            # Let the BSO observe that this batch size is bad, but since the job\n            # did not run, do not add to the history and retry.\n            if eta == 0 and tta == 0 and not reached:\n                bso.observe(job, bs, 100 * beta_knob * min_cost, False)\n                continue\n\n            # Compute the hybrid cost metric.\n            cost = zeus_cost(eta, tta, eta_knob, max_power)\n            cost_acc += cost\n\n            # Provide feedback to the BSO.\n            bso.observe(job, bs, cost, reached)\n\n            # Record history for analysis.\n            history.append(HistoryEntry(bs, pl, eta, reached, tta))\n\n            # Reached the target metric. Update min_cost and go on to the next recurrence.\n            if reached:\n                if self.verbose:\n                    print()\n                    print(f\"[Simulator] Reached target metric in {tries} {'try' if tries == 1 else 'tries'}.\")\n                if min_cost &gt; cost_acc:\n                    if self.verbose:\n                        print(f\"[Simulator] Minimum cost updated from {min_cost:.2f} to {cost_acc:.2f}.\")\n                    min_cost = cost_acc\n                break\n            # Didn't reach the target metric.\n            # We assume that the default BS (set by the user) will always converge.\n            # That is, reaching the target metric with the model should be a feasible task.\n            if i == 0:\n                raise RuntimeError(f\"The default batch size {job.default_bs} did not converge.\")\n\n        # Target metric was not reached in 20 tries. We consider this target metric to be unreachable.\n        else:\n            raise RuntimeError(\"Job did not reach the target metric in 20 tries.\")\n\n    if self.verbose:\n        print()\n        print(f\"[Simulator] {job} (BS, PL, ETA, whether_reached, TTA) history: \\n{history}\")\n\n    return history\n</code></pre>"},{"location":"reference/_legacy/simulate/#zeus._legacy.simulate.Simulator.simulate_one_alibaba_group","title":"simulate_one_alibaba_group","text":"<pre><code>simulate_one_alibaba_group(job, group_df, beta_knob, eta_knob)\n</code></pre> <p>Run simulation on one group in the Alibaba trace.</p> <p>Concurrent job submissions (jobs that start before the previous job finishes) are launched with the batch size known to be of minimum cost at that time. The BSO also observes the results of these jobs when they are done, and these jobs may well finish before a job that started before it. See <code>observe</code> in PruningGTSBatchSizeOptimizer for an example of handing such a scenario.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>Job spec of this group.</p> required <code>group_df</code> <code>DataFrame</code> <p>DataFrame of this group. Fields: - group: Group ID in trace. Identical across all rows. - dataset: Dataset name. Identical across all rows. - start_time: Job start time in the trace. - end_time: Job end time in the trace. - runtime_ratio: runtime of this job over the mean runtime     of all the jobs of this dataset. Captures intra-dataset     job runtime differences.</p> required <code>beta_knob</code> <code>float</code> <p><code>beta_knob * min_eta</code> is the early stopping cost threshold. Set to <code>np.inf</code> to disable early stopping.</p> required <code>eta_knob</code> <code>float</code> <p>\\(\\eta\\) used in the hybrid cost metric. \\(\\textrm{cost} = \\eta \\cdot \\textrm{ETA} + (1 - \\eta) \\cdot \\textrm{MaxPower} \\cdot \\textrm{TTA}\\)</p> required <p>Returns:</p> Type Description <code>list[HistoryEntry]</code> <p>A list of <code>HistoryEntry</code> objects for each job run.</p> Source code in <code>zeus/_legacy/simulate.py</code> <pre><code>def simulate_one_alibaba_group(\n    self,\n    job: Job,\n    group_df: pd.DataFrame,\n    beta_knob: float,\n    eta_knob: float,\n) -&gt; list[HistoryEntry]:\n    r\"\"\"Run simulation on one group in the Alibaba trace.\n\n    Concurrent job submissions (jobs that start before the previous job\n    finishes) are launched with the batch size known to be of minimum\n    cost at that time. The BSO also observes the results of these jobs\n    when they are done, and these jobs may well finish before a job that\n    started before it. See `observe` in PruningGTSBatchSizeOptimizer for\n    an example of handing such a scenario.\n\n    Args:\n        job: Job spec of this group.\n        group_df: DataFrame of this group. Fields:\n            - group: Group ID in trace. Identical across all rows.\n            - dataset: Dataset name. Identical across all rows.\n            - start_time: Job start time in the trace.\n            - end_time: Job end time in the trace.\n            - runtime_ratio: runtime of this job over the mean runtime\n                of all the jobs of this dataset. Captures intra-dataset\n                job runtime differences.\n        beta_knob: `beta_knob * min_eta` is the early stopping cost threshold.\n            Set to `np.inf` to disable early stopping.\n        eta_knob: $\\eta$ used in the hybrid cost metric.\n            $\\textrm{cost} = \\eta \\cdot \\textrm{ETA} + (1 - \\eta) \\cdot \\textrm{MaxPower} \\cdot \\textrm{TTA}$\n\n    Returns:\n        A list of [`HistoryEntry`][zeus._legacy.simulate.HistoryEntry] objects for each job run.\n    \"\"\"\n    # Copy all internal state so that simulation does not modify any\n    # internal state and is deterministic w.r.t. the random seed.\n    bso = deepcopy(self.bso)\n    plo = deepcopy(self.plo)\n    rng = np.random.default_rng(self.seed)\n\n    # Sanity check\n    if job.default_bs is None:\n        raise ValueError(\"You must give the job a default batch size.\")\n\n    # Figure out MAXPOWER.\n    max_power = self.df.power_limit.max().item()\n    if self.verbose:\n        print(f\"[Simulator] Max power = {max_power}W\")\n\n    # Track the minimum cost observed for the early stopping energy threshold.\n    # Also track the corresponding minimum cost BS to handle concurrent jobs.\n    min_cost = np.inf\n    best_bs = job.default_bs\n\n    # Simulate each job one at a time.\n    history: list[HistoryEntry] = []\n\n    if self.verbose:\n        print(f\"[Simulator] {job} recurring {len(group_df)} times.\")\n\n    # A new job. Profile the feasible batch size range.\n    batch_sizes = self._profile_batch_size_range(job)\n\n    # Register the job in the BSO.\n    bso.register_job(job, batch_sizes)\n\n    # List of jobs in flight.\n    @dataclass\n    class RunningJob:\n        \"\"\"Represents a job that is running.\n\n        We know what's going to happen to this job when we launch it.\n        Thus, pre-compute all results (using self.run_job) and have this\n        instance hold the information. Then, jobs will be removed from the\n        list of running jobs when the current time passes self.end_time and\n        the result will be observed.\n        \"\"\"\n\n        start_time: float\n        end_time: float\n        runtime_ratio: float\n        batch_size: int\n        power_limit: int\n        reached: bool\n        time: float\n        energy: float\n        cost: float\n\n    running_jobs: list[RunningJob] = []\n\n    # Jobs in group_df are already sorted by start_time.\n    current_time = 0.0\n    for rec_i, (_, job_row) in enumerate(group_df.iterrows()):\n        if self.verbose:\n            print(f\"\\nRecurrence: {rec_i + 1}\")\n\n        # Update the current time.\n        current_time = job_row.start_time\n        if self.verbose:\n            print(f\"[Simulator] Current time is {current_time:.1f}\")\n\n        # Scan the in-flight job list to reap jobs that have finished.\n        # We need a while loop here because we might have submitted a retry job\n        # while reaping jobs that failed to reach the target metric, and that retry job\n        # may finish before the current job.\n        while any(map(lambda j: j.end_time &lt;= current_time, running_jobs)):\n            if self.verbose:\n                print(f\"[Simulator] Running jobs: {running_jobs}\")\n\n            # We copy running_jobs because we want to mutate it as we traverse it.\n            running_jobs_copy = deepcopy(running_jobs)\n\n            # Sort the jobs in the order they end.\n            for rjob in sorted(running_jobs_copy, key=operator.attrgetter(\"end_time\")):\n                # We're only interested in jobs that finished at this point in time.\n                if rjob.end_time &gt; current_time:\n                    continue\n\n                # Job is finished at this point in time.\n                if self.verbose:\n                    print(\n                        f\"[Simulator] Job(BS={rjob.batch_size},time={rjob.time},\"\n                        f\"energy={rjob.energy},reached={rjob.reached}) finished\"\n                    )\n\n                # Remove the job from the in-flight job list.\n                running_jobs.remove(rjob)\n\n                # Let the BSO observe the cost for this batch size.\n                bso.observe(job, rjob.batch_size, rjob.cost, rjob.reached)\n\n                # If the job never ran because even one epoch exceeds the cost threshold,\n                # do not add to the history and retry.\n                if rjob.energy != 0 or rjob.time != 0 or rjob.reached:\n                    # Record history for analysis.\n                    history.append(\n                        HistoryEntry(\n                            rjob.batch_size,\n                            rjob.power_limit,\n                            rjob.energy * rjob.runtime_ratio,  # Scale the energy of this job by the runtime ratio.\n                            rjob.reached,\n                            rjob.time * rjob.runtime_ratio,  # Scale the runtime of this job by the runtime ratio.\n                        )\n                    )\n\n                # Reached target metric (no need to retry)\n                if rjob.reached:\n                    if min_cost &gt; rjob.cost:\n                        if self.verbose:\n                            print(f\"[Simulator] Minimum cost updated from {min_cost:.2f} to {rjob.cost:.2f}\")\n                        min_cost = rjob.cost\n                        best_bs = rjob.batch_size\n\n                # Didn't reach target metric. Need to run a retry job.\n                else:\n                    profiled_power = False\n\n                    # If there are jobs in-flight, we just run additional concurrent\n                    # submissions with the best known knobs.\n                    if running_jobs:\n                        if self.verbose:\n                            print(f\"[Simulator] There are in-flight jobs. Use BS {best_bs}.\")\n                        bs = best_bs\n                        pl = plo.predict(job, bs)\n                        assert pl, f\"Power not profiled for best known BS {bs}\"\n\n                    # There are no jobs in-flight. Just submit the job normally.\n                    else:\n                        # Determine the knobs.\n                        bs = bso.predict(job)\n                        pl = plo.predict(job, bs)\n\n                        if self.verbose:\n                            print(f\"[Simulator] There are no in-flight jobs. Use BSO's prediction {bs}.\")\n\n                        # When the batch size is first explored, we need to profile power limit.\n                        if pl is None:\n                            profiled_power = True\n                            result = self._profile_power_limit(job, bs, eta_knob)\n                            for pl, epe in result.items():\n                                plo.observe(job, bs, pl, epe)\n                            pl = plo.predict(job, bs)\n                            assert pl\n\n                    # Pre-compute the result of the job.\n                    eta, tta, reached = self._run_job(\n                        job=job,\n                        batch_size=bs,\n                        power_limit=pl,\n                        rng=rng,\n                        cost_ub=beta_knob * min_cost,\n                        eta_knob=eta_knob,\n                        profile_power=profiled_power,\n                    )\n\n                    # Compute the hybrid cost metric.\n                    cost = zeus_cost(eta, tta, eta_knob, max_power)\n\n                    # Create the RunningJob instance.\n                    running_job = RunningJob(\n                        start_time=rjob.end_time,\n                        end_time=rjob.end_time + (rjob.end_time - rjob.start_time),  # Assume same runtime.\n                        runtime_ratio=rjob.runtime_ratio,\n                        batch_size=bs,\n                        power_limit=pl,\n                        reached=reached,\n                        time=tta,\n                        energy=eta,\n                        cost=cost,\n                    )\n                    running_jobs.append(running_job)\n\n                    if self.verbose:\n                        print(f\"[Simulator] Started retry job {running_job}\")\n\n                    # We must break from the loop that scans the running_jobs list, because\n                    # the job we just submitted might actually be the next job that finishes.\n                    break\n\n        # We're (finally) done reaping all finished jobs. Run the current job.\n        profiled_power = False\n\n        # If there are jobs in-flight, we just run additional concurrent\n        # submissions with the best known knobs.\n        if running_jobs:\n            if self.verbose:\n                print(f\"[Simulator] There are in-flight jobs. Use BS {best_bs}.\")\n            bs = best_bs\n            pl = plo.predict(job, bs)\n            assert pl is not None, f\"Power not profiled for best known BS {bs}\"\n\n        # There are no jobs in-flight. Just submit the job.\n        else:\n            # Determine the knobs.\n            bs = bso.predict(job)\n            pl = plo.predict(job, bs)\n\n            if self.verbose:\n                print(f\"[Simulator] There are no in-flight jobs. Use BSO's prediction {bs}.\")\n\n            # When the batch size is first explored, we need to profile power limit.\n            if pl is None:\n                profiled_power = True\n                result = self._profile_power_limit(job, bs, eta_knob)\n                for pl, epe in result.items():\n                    plo.observe(job, bs, pl, epe)\n                pl = plo.predict(job, bs)\n                assert pl\n\n        # Run the job, potentially early stopping it.\n        eta, tta, reached = self._run_job(\n            job=job,\n            batch_size=bs,\n            power_limit=pl,\n            rng=rng,\n            cost_ub=beta_knob * min_cost,\n            eta_knob=eta_knob,\n            profile_power=profiled_power,\n        )\n\n        # Compute the hybrid cost metric.\n        cost = zeus_cost(eta, tta, eta_knob, max_power)\n\n        # Create the RunningJob instance and enqueue.\n        running_job = RunningJob(\n            start_time=job_row.start_time,\n            end_time=job_row.end_time,\n            runtime_ratio=job_row.runtime_ratio,\n            batch_size=bs,\n            power_limit=pl,\n            reached=reached,\n            time=tta,\n            energy=eta,\n            cost=cost,\n        )\n        running_jobs.append(running_job)\n\n        if self.verbose:\n            print(f\"[Simulator] Started job {running_job}\")\n\n    # Now, we're done iterating the rows of group_df.\n    # Set the current time to infinity so that all running jobs finish.\n    current_time = np.inf\n    if self.verbose:\n        print(\"\\n[Simulator] Reap all jobs\")\n\n    # Scan the remaining in-flight job list to reap jobs that have finished.\n    # Since current_time is infinity, this while loop will reap all the jobs ever launched.\n    while any(map(lambda j: j.end_time &lt;= current_time, running_jobs)):\n        if self.verbose:\n            print(f\"[Simulator] Running jobs: {running_jobs}\")\n\n        # We copy running_jobs because we want to mutate it as we traverse it.\n        running_jobs_copy = deepcopy(running_jobs)\n\n        # Sort the jobs in the order they end.\n        for rjob in sorted(running_jobs_copy, key=operator.attrgetter(\"end_time\")):\n            # We're only interested in jobs that finished at this point in time.\n            if rjob.end_time &gt; current_time:\n                continue\n\n            # Job is finished at this point in time.\n            if self.verbose:\n                print(\n                    f\"[Simulator] Job(BS={rjob.batch_size},time={rjob.time},\"\n                    f\"energy={rjob.energy},reached={rjob.reached}) finished\"\n                )\n\n            # Remove the job from the in-flight job list.\n            running_jobs.remove(rjob)\n\n            # Let the BSO observe the cost for this batch size.\n            bso.observe(job, rjob.batch_size, rjob.cost, rjob.reached)\n\n            # If the job never ran because even one epoch exceeds the cost threshold,\n            # do not add to the history and retry.\n            if rjob.energy != 0 or rjob.time != 0 or rjob.reached:\n                # Record history for analysis.\n                history.append(\n                    HistoryEntry(\n                        rjob.batch_size,\n                        rjob.power_limit,\n                        rjob.energy * rjob.runtime_ratio,  # Scale the energy of this job by the runtime ratio.\n                        rjob.reached,\n                        rjob.time * rjob.runtime_ratio,  # Scale the runtime of this job by the runtime ratio.\n                    )\n                )\n\n            # Reached target metric (no need to retry)\n            if rjob.reached:\n                if min_cost &gt; rjob.cost:\n                    if self.verbose:\n                        print(f\"[Simulator] Minimum cost updated from {min_cost:.2f} to {rjob.cost:.2f}\")\n                    min_cost = rjob.cost\n                    best_bs = rjob.batch_size\n\n            # Didin't reach target metric. Need to run a retry job.\n            else:\n                profiled_power = False\n\n                # If there are jobs in-flight, we just run additional concurrent\n                # submissions with the best known knobs.\n                if running_jobs:\n                    if self.verbose:\n                        print(f\"[Simulator] There are in-flight jobs. Use BS {best_bs}.\")\n                    bs = best_bs\n                    pl = plo.predict(job, bs)\n                    assert pl, f\"Power not profiled for best known BS {bs}\"\n\n                # There are no jobs in-flight. Just submit the job normally.\n                else:\n                    # Determine the knobs.\n                    bs = bso.predict(job)\n                    pl = plo.predict(job, bs)\n\n                    if self.verbose:\n                        print(f\"[Simulator] There are no in-flight jobs. Use BSO's prediction {bs}.\")\n\n                    # When the batch size is first explored, we need to profile power limit.\n                    if pl is None:\n                        profiled_power = True\n                        result = self._profile_power_limit(job, bs, eta_knob)\n                        for pl, epe in result.items():\n                            plo.observe(job, bs, pl, epe)\n                        pl = plo.predict(job, bs)\n                        assert pl\n\n                # Pre-compute the result of the job.\n                eta, tta, reached = self._run_job(\n                    job=job,\n                    batch_size=bs,\n                    power_limit=pl,\n                    rng=rng,\n                    cost_ub=beta_knob * min_cost,\n                    eta_knob=eta_knob,\n                    profile_power=profiled_power,\n                )\n\n                # Compute the hybrid cost metric.\n                cost = zeus_cost(eta, tta, eta_knob, max_power)\n\n                # Create the RunningJob instance.\n                running_job = RunningJob(\n                    start_time=rjob.end_time,\n                    end_time=rjob.end_time + (rjob.end_time - rjob.start_time),  # Assume same runtime.\n                    runtime_ratio=rjob.runtime_ratio,\n                    batch_size=bs,\n                    power_limit=pl,\n                    reached=reached,\n                    time=tta,\n                    energy=eta,\n                    cost=cost,\n                )\n                running_jobs.append(running_job)\n\n                if self.verbose:\n                    print(f\"[Simulator] Started retry job {running_job}\")\n\n                # We must break from the loop that scans the running_jobs list, because\n                # the job we just submitted might actually be the next job that finishes.\n                break\n\n    return history\n</code></pre>"},{"location":"reference/_legacy/simulate/#zeus._legacy.simulate.Simulator._run_job","title":"_run_job","text":"<pre><code>_run_job(job, batch_size, power_limit, rng, cost_ub, eta_knob, profile_power)\n</code></pre> <p>Simulate running the job and return the energy consumed and whether it converged.</p> <p>This method will randomly choose one of the possible training \"paths\". Then, based on cost_ub, it will compute the maximum number of epochs the job can run. If the path's target_epoch is smaller than or equal to the maximum number of epochs, the cost incurred until target_epoch is returned. Otherwise, the cost incurred until the maximum number of epochs is returned.</p> <p>It is important to note that the job may never run when the first epoch's cost is already expected to exceed the cost upper bound. In such a case, the returned time and energy consumptions will be zero. This case must be treated separately in the calling code.</p> <p>If profile_power is True, the first epoch will JIT-profile power limits coarsely by dividing the epoch evenly into len(available_power_limits) pieces. Thus the the first epoch's energy and time consumption will be slightly adjusted.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>Job spec to run.</p> required <code>batch_size</code> <code>int</code> <p>Batch size to use.</p> required <code>power_limit</code> <code>int</code> <p>Power limit to use. Regardless of whether this run of this batch size requires power profiling, the simulator will input the optimal power limit for the batch size. The first epoch energy consumption from power profiling is adjusted in the latter half of this method based on the profile_power flag.</p> required <code>rng</code> <code>Generator</code> <p>Random number generator used to sample one training path.</p> required <code>cost_ub</code> <code>float</code> <p>Cost upper bound. The job is terminated when the next epoch is going to exceed the cost upper bound.</p> required <code>eta_knob</code> <code>float</code> <p>\\(\\eta\\) used in the hybrid cost metric. \\(\\textrm{cost} = \\eta \\cdot \\textrm{ETA} + (1 - \\eta) \\cdot \\textrm{MaxPower} \\cdot \\textrm{TTA}\\)</p> required <code>profile_power</code> <code>bool</code> <p>Whether this run of the job should profile power during the first epoch.</p> required <p>Returns:</p> Type Description <code>tuple[float, float, bool]</code> <p>Tuple of energy consumption, time consumption, and whether the job reached the target metric.</p> Source code in <code>zeus/_legacy/simulate.py</code> <pre><code>def _run_job(\n    self,\n    job: Job,\n    batch_size: int,\n    power_limit: int,\n    rng: np.random.Generator,\n    cost_ub: float,\n    eta_knob: float,\n    profile_power: bool,\n) -&gt; tuple[float, float, bool]:\n    r\"\"\"Simulate running the job and return the energy consumed and whether it converged.\n\n    This method will randomly choose one of the possible training \"paths\". Then,\n    based on cost_ub, it will compute the maximum number of epochs the job can run.\n    If the path's target_epoch is smaller than or equal to the maximum number of\n    epochs, the cost incurred until target_epoch is returned. Otherwise, the cost\n    incurred until the maximum number of epochs is returned.\n\n    It is important to note that the job may never run when the first epoch's cost\n    is already expected to exceed the cost upper bound. In such a case, the returned\n    time and energy consumptions will be zero. This case must be treated separately\n    in the calling code.\n\n    If profile_power is True, the first epoch will JIT-profile power limits coarsely\n    by dividing the epoch evenly into len(available_power_limits) pieces. Thus the\n    the first epoch's energy and time consumption will be slightly adjusted.\n\n    Args:\n        job: Job spec to run.\n        batch_size: Batch size to use.\n        power_limit: Power limit to use. Regardless of whether this run of this\n            batch size requires power profiling, the simulator will input the optimal\n            power limit for the batch size. The first epoch energy consumption from\n            power profiling is adjusted in the latter half of this method based on the\n            profile_power flag.\n        rng: Random number generator used to sample one training path.\n        cost_ub: Cost upper bound. The job is terminated when the next epoch is going\n            to exceed the cost upper bound.\n        eta_knob: $\\eta$ used in the hybrid cost metric.\n            $\\textrm{cost} = \\eta \\cdot \\textrm{ETA} + (1 - \\eta) \\cdot \\textrm{MaxPower} \\cdot \\textrm{TTA}$\n        profile_power: Whether this run of the job should profile power during the\n            first epoch.\n\n    Returns:\n        Tuple of energy consumption, time consumption, and whether the job reached the target metric.\n    \"\"\"\n    # df is filtered with job spec, BS, and PL. We sample one possible training path.\n    # power_df is filtered with job spec and BS. We use this to compute the energy\n    # consumption of profiling power during the first epoch.\n    df = job.filter_df(self.df)\n    power_df = df.loc[df.batch_size == batch_size]\n    df = power_df.loc[df.power_limit == power_limit]\n    path = df.sample(n=1, random_state=rng)\n\n    # Max number of epochs is bound by either the cost upper bound or the user-specified\n    # max_epochs, whichever is smaller.\n    if cost_ub == np.inf:\n        # cost_ub is infinity in two cases:\n        # 1. The simulator has never observed any cost value in the early part of simulation.\n        # 2. We're simulating with no early stopping, i.e. beta_knob is infinity.\n        max_epochs = job.max_epochs\n        if self.verbose:\n            print(f\"[run job] Cost UB is inf. {max_epochs=}\")\n    else:\n        # Stop right before the first epoch when cost will cross the upper bound.\n        cost_per_epoch = (\n            eta_knob * path.energy_per_epoch.item()\n            + (1 - eta_knob) * power_df.power_limit.max().item() * path.time_per_epoch.item()\n        )\n        max_epochs = min(cost_ub // cost_per_epoch, job.max_epochs)\n        if self.verbose:\n            print(f\"[run job] {cost_ub=}\")\n            print(f\"[run job] {cost_per_epoch=}\")\n            print(f\"[run job] {max_epochs=}\")\n\n    # The job virtually never ran. Time and Energy being zero will be treated specially outside.\n    # If the min_cost is so low, this might even prevent this BS from running at all.\n    if max_epochs == 0:\n        print(\n            f\"[run job] {job} cannot run even one epoch without exceeding the cost UB.\"\n            f\" BS {batch_size}, PL {power_limit}, {eta_knob=}\"\n        )\n        return 0.0, 0.0, False\n\n    def compute_energy_and_time(num_epochs: int, profile_power: bool) -&gt; tuple[float, float]:\n        \"\"\"Compute the energy and time consumed for running the job for num_epochs.\"\"\"\n        # This is the first run of this batch size, and we need to profile power\n        # during the first epoch.\n        if profile_power:\n            # Note that power_df holds rows with all power limits. Evenly splitting the\n            # epochs with the number of samples and running each slice with each power\n            # limit consumes (1/N) * e_100 + (1/N) * e_125 + ... + (1/N) * e_250.\n            # Also there are all runs 1, 2, ... included, but power info is actually\n            # completely duplicated across different runs in the DataFrame.\n            # Thus, taking the mean across the entire power_df gets us what we want.\n            energy_first_epoch = power_df.energy_per_epoch.mean().item()\n            energy_from_second_epoch = path.energy_per_epoch.item() * (num_epochs - 1)\n            energy_consumption = energy_first_epoch + energy_from_second_epoch\n            time_first_epoch = power_df.time_per_epoch.mean().item()\n            time_from_second_epoch = path.time_per_epoch.item() * (num_epochs - 1)\n            time_consumption = time_first_epoch + time_from_second_epoch\n        # Just run num_epochs with the given power limit. Simple.\n        else:\n            energy_consumption = path.energy_per_epoch.item() * num_epochs\n            time_consumption = path.time_per_epoch.item() * num_epochs\n        return energy_consumption, time_consumption\n\n    # Job reached target metric.\n    target_epoch = path.target_epoch.item()\n    if path.target_epoch.notnull().item() and target_epoch &lt;= max_epochs:\n        eta, tta = compute_energy_and_time(target_epoch, profile_power)\n        if self.verbose:\n            print(\n                f\"[run job] {job} @ {batch_size},{power_limit}W{' prof' if profile_power else ''} \"\n                f\"=&gt; \\033[31mReached in {int(target_epoch)} epochs, \"\n                f\"TTA {tta:.2f} seconds, ETA {eta:.2f}\\033[0m\"\n            )\n        return eta, tta, True\n\n    # Job failed to reach the target metric.\n    energy_consumption, time_consumption = compute_energy_and_time(max_epochs, profile_power)\n    if self.verbose:\n        print(\n            f\"[run job] {job} @ {batch_size},{power_limit}W{' prof' if profile_power else ''} \"\n            f\"=&gt; \\033[31mFailed (stopped after {int(max_epochs)} epochs), \"\n            f\"TTA {time_consumption:.2f} seconds, ETA {energy_consumption:.2f}\\033[0m\"\n        )\n    return energy_consumption, time_consumption, False\n</code></pre>"},{"location":"reference/_legacy/simulate/#zeus._legacy.simulate.Simulator._profile_power_limit","title":"_profile_power_limit","text":"<pre><code>_profile_power_limit(job, batch_size, eta_knob)\n</code></pre> <p>Simulate running the job and profiling the power limit.</p> <p>Returns:</p> Type Description <code>dict[int, float]</code> <p>Dictionary mapping PL to <code>energy_per_epoch</code>. PL is inserted in high to low order.</p> Source code in <code>zeus/_legacy/simulate.py</code> <pre><code>def _profile_power_limit(self, job: Job, batch_size: int, eta_knob: float) -&gt; dict[int, float]:\n    \"\"\"Simulate running the job and profiling the power limit.\n\n    Returns:\n        Dictionary mapping PL to `energy_per_epoch`. PL is inserted in high to low order.\n    \"\"\"\n    # Filter by job spec and BS.\n    df = job.filter_df(self.df)\n    df = df.loc[(df.batch_size == batch_size)]\n\n    # Compute the epoch cost of each power limit (Equation 7).\n    max_pl = df.power_limit.max().item()\n    df = df.groupby([\"power_limit\"], as_index=False).mean(numeric_only=True)\n    df[\"epoch_cost\"] = (eta_knob * df[\"average_power\"] + (1 - eta_knob) * max_pl) * df[\"time_per_epoch\"]\n\n    # We'll be profiling energy from larger to smaller power limits.\n    df = df.sort_values(by=\"power_limit\", ascending=False)\n    result = {rec.power_limit: rec.epoch_cost for rec in df.to_records(index=False)}\n    if self.verbose:\n        print(f\"[PL profile] {job} @ {batch_size} =&gt; PL = {min(result, key=result.get)}W\")  # type: ignore\n    return result\n</code></pre>"},{"location":"reference/_legacy/simulate/#zeus._legacy.simulate.Simulator._profile_batch_size_range","title":"_profile_batch_size_range","text":"<pre><code>_profile_batch_size_range(job)\n</code></pre> <p>Simulate profiling the available batch size range of the job.</p> <p>Returns:</p> Type Description <code>list[int]</code> <p>A list of feasible batch sizes.</p> Source code in <code>zeus/_legacy/simulate.py</code> <pre><code>def _profile_batch_size_range(self, job: Job) -&gt; list[int]:\n    \"\"\"Simulate profiling the available batch size range of the job.\n\n    Returns:\n        A list of feasible batch sizes.\n    \"\"\"\n    df = self.df\n    # Do not filter by target_metric here since we do not want to constrain\n    # the feasible batch size range to only those that reached the target metric.\n    df = df.loc[(df.dataset == job.dataset) &amp; (df.network == job.network) &amp; (df.optimizer == job.optimizer)]\n    result = sorted(list(df.batch_size.unique()))\n    if self.verbose:\n        print(f\"[BS profile] {job} =&gt; BS = {result}\")\n    return result\n</code></pre>"},{"location":"reference/_legacy/policy/","title":"policy","text":""},{"location":"reference/_legacy/policy/#zeus._legacy.policy","title":"zeus._legacy.policy","text":"<p>Optimization policies for Zeus.</p> <p><code>PowerLimitOptimizer</code> and <code>BatchSizeOptimizer</code> are abstract classes. Users can implement custom policies by extending the abstract classes, implementing required methods, and plugging them into the <code>Simulator</code>.</p>"},{"location":"reference/_legacy/policy/interface/","title":"interface","text":""},{"location":"reference/_legacy/policy/interface/#zeus._legacy.policy.interface","title":"zeus._legacy.policy.interface","text":"<p>Abstract classes for implementing custom optimization policies.</p>"},{"location":"reference/_legacy/policy/interface/#zeus._legacy.policy.interface.BatchSizeOptimizer","title":"BatchSizeOptimizer","text":"<p>               Bases: <code>ABC</code></p> <p>Finds out the best batch size to use for the job.</p> Source code in <code>zeus/_legacy/policy/interface.py</code> <pre><code>class BatchSizeOptimizer(ABC):\n    \"\"\"Finds out the best batch size to use for the job.\"\"\"\n\n    @property\n    @abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"Name of the batch size optimizer.\"\"\"\n\n    @abstractmethod\n    def register_job(self, job: Job, batch_sizes: list[int]) -&gt; None:\n        \"\"\"Prepare internal state so that it can handle the given job.\n\n        It is assumed that the state of each [`Job`][zeus._legacy.job.Job] will be\n        managed separately. Note that [`Job`][zeus._legacy.job.Job] is hashable,\n        and thus can be used as dictionary keys.\n\n        Args:\n            job: New jobs to register.\n            batch_sizes: Batch sizes to consider.\n        \"\"\"\n\n    @abstractmethod\n    def predict(self, job: Job) -&gt; int:\n        \"\"\"Return the best batch size to use for the job.\n\n        Args:\n            job: The job to pick the best batch size for.\n        \"\"\"\n\n    @abstractmethod\n    def observe(self, job: Job, batch_size: int, cost: float, converged: bool | None = None) -&gt; None:\n        \"\"\"Observe the cost of using the given batch size for the job.\n\n        Args:\n            job: The job from which this cost observation resulted.\n            batch_size: The batch size used for this run of the job.\n            cost: The energy-time cost of running the job.\n            converged: Whether the job reached its target metric. If may not have\n                reached its target if the job was early stopped based on cost or\n                the maximum epoch was reached. For BSO's that do not take this into\n                account, `None` can be passed.\n        \"\"\"\n\n    def _log(self, message: str) -&gt; None:\n        \"\"\"Log message with object name.\"\"\"\n        print(f\"[{self.name}] {message}\")\n</code></pre>"},{"location":"reference/_legacy/policy/interface/#zeus._legacy.policy.interface.BatchSizeOptimizer.name","title":"name  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>name\n</code></pre> <p>Name of the batch size optimizer.</p>"},{"location":"reference/_legacy/policy/interface/#zeus._legacy.policy.interface.BatchSizeOptimizer.register_job","title":"register_job  <code>abstractmethod</code>","text":"<pre><code>register_job(job, batch_sizes)\n</code></pre> <p>Prepare internal state so that it can handle the given job.</p> <p>It is assumed that the state of each <code>Job</code> will be managed separately. Note that <code>Job</code> is hashable, and thus can be used as dictionary keys.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>New jobs to register.</p> required <code>batch_sizes</code> <code>list[int]</code> <p>Batch sizes to consider.</p> required Source code in <code>zeus/_legacy/policy/interface.py</code> <pre><code>@abstractmethod\ndef register_job(self, job: Job, batch_sizes: list[int]) -&gt; None:\n    \"\"\"Prepare internal state so that it can handle the given job.\n\n    It is assumed that the state of each [`Job`][zeus._legacy.job.Job] will be\n    managed separately. Note that [`Job`][zeus._legacy.job.Job] is hashable,\n    and thus can be used as dictionary keys.\n\n    Args:\n        job: New jobs to register.\n        batch_sizes: Batch sizes to consider.\n    \"\"\"\n</code></pre>"},{"location":"reference/_legacy/policy/interface/#zeus._legacy.policy.interface.BatchSizeOptimizer.predict","title":"predict  <code>abstractmethod</code>","text":"<pre><code>predict(job)\n</code></pre> <p>Return the best batch size to use for the job.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>The job to pick the best batch size for.</p> required Source code in <code>zeus/_legacy/policy/interface.py</code> <pre><code>@abstractmethod\ndef predict(self, job: Job) -&gt; int:\n    \"\"\"Return the best batch size to use for the job.\n\n    Args:\n        job: The job to pick the best batch size for.\n    \"\"\"\n</code></pre>"},{"location":"reference/_legacy/policy/interface/#zeus._legacy.policy.interface.BatchSizeOptimizer.observe","title":"observe  <code>abstractmethod</code>","text":"<pre><code>observe(job, batch_size, cost, converged=None)\n</code></pre> <p>Observe the cost of using the given batch size for the job.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>The job from which this cost observation resulted.</p> required <code>batch_size</code> <code>int</code> <p>The batch size used for this run of the job.</p> required <code>cost</code> <code>float</code> <p>The energy-time cost of running the job.</p> required <code>converged</code> <code>bool | None</code> <p>Whether the job reached its target metric. If may not have reached its target if the job was early stopped based on cost or the maximum epoch was reached. For BSO's that do not take this into account, <code>None</code> can be passed.</p> <code>None</code> Source code in <code>zeus/_legacy/policy/interface.py</code> <pre><code>@abstractmethod\ndef observe(self, job: Job, batch_size: int, cost: float, converged: bool | None = None) -&gt; None:\n    \"\"\"Observe the cost of using the given batch size for the job.\n\n    Args:\n        job: The job from which this cost observation resulted.\n        batch_size: The batch size used for this run of the job.\n        cost: The energy-time cost of running the job.\n        converged: Whether the job reached its target metric. If may not have\n            reached its target if the job was early stopped based on cost or\n            the maximum epoch was reached. For BSO's that do not take this into\n            account, `None` can be passed.\n    \"\"\"\n</code></pre>"},{"location":"reference/_legacy/policy/interface/#zeus._legacy.policy.interface.BatchSizeOptimizer._log","title":"_log","text":"<pre><code>_log(message)\n</code></pre> <p>Log message with object name.</p> Source code in <code>zeus/_legacy/policy/interface.py</code> <pre><code>def _log(self, message: str) -&gt; None:\n    \"\"\"Log message with object name.\"\"\"\n    print(f\"[{self.name}] {message}\")\n</code></pre>"},{"location":"reference/_legacy/policy/interface/#zeus._legacy.policy.interface.PowerLimitOptimizer","title":"PowerLimitOptimizer","text":"<p>               Bases: <code>ABC</code></p> <p>Finds out the best power limit to use for the job and batch size.</p> Source code in <code>zeus/_legacy/policy/interface.py</code> <pre><code>class PowerLimitOptimizer(ABC):\n    \"\"\"Finds out the best power limit to use for the job and batch size.\"\"\"\n\n    @property\n    @abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"Name of the power limit optimizer.\"\"\"\n\n    @abstractmethod\n    def predict(self, job: Job, batch_size: int) -&gt; int | None:\n        \"\"\"Return the best power limit for the job and batch size.\n\n        Args:\n            job: The job to pick the best power limit for.\n            batch_size: The batch size chosen by the\n                [`BatchSizeOptimizer`][zeus._legacy.policy.BatchSizeOptimizer] for this job.\n\n        Returns:\n            The best power limit, or `None` if profiling results via\n            [`observe`][zeus._legacy.policy.interface.PowerLimitOptimizer.observe] are needed.\n        \"\"\"\n\n    @abstractmethod\n    def observe(self, job: Job, batch_size: int, power_limit: int, cost: float) -&gt; None:\n        \"\"\"Observe the cost of using the given batch size and power limit for the job.\n\n        Args:\n            job: The job from which this cost observation resulted.\n            batch_size: The batch size used for this run of the job.\n            power_limit: The power limit used for this run of the job.\n            cost: The cost of running the job.\n        \"\"\"\n\n    def _log(self, message: str) -&gt; None:\n        \"\"\"Log message with object name.\"\"\"\n        print(f\"[{self.name}] {message}\")\n</code></pre>"},{"location":"reference/_legacy/policy/interface/#zeus._legacy.policy.interface.PowerLimitOptimizer.name","title":"name  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>name\n</code></pre> <p>Name of the power limit optimizer.</p>"},{"location":"reference/_legacy/policy/interface/#zeus._legacy.policy.interface.PowerLimitOptimizer.predict","title":"predict  <code>abstractmethod</code>","text":"<pre><code>predict(job, batch_size)\n</code></pre> <p>Return the best power limit for the job and batch size.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>The job to pick the best power limit for.</p> required <code>batch_size</code> <code>int</code> <p>The batch size chosen by the <code>BatchSizeOptimizer</code> for this job.</p> required <p>Returns:</p> Type Description <code>int | None</code> <p>The best power limit, or <code>None</code> if profiling results via</p> <code>int | None</code> <p><code>observe</code> are needed.</p> Source code in <code>zeus/_legacy/policy/interface.py</code> <pre><code>@abstractmethod\ndef predict(self, job: Job, batch_size: int) -&gt; int | None:\n    \"\"\"Return the best power limit for the job and batch size.\n\n    Args:\n        job: The job to pick the best power limit for.\n        batch_size: The batch size chosen by the\n            [`BatchSizeOptimizer`][zeus._legacy.policy.BatchSizeOptimizer] for this job.\n\n    Returns:\n        The best power limit, or `None` if profiling results via\n        [`observe`][zeus._legacy.policy.interface.PowerLimitOptimizer.observe] are needed.\n    \"\"\"\n</code></pre>"},{"location":"reference/_legacy/policy/interface/#zeus._legacy.policy.interface.PowerLimitOptimizer.observe","title":"observe  <code>abstractmethod</code>","text":"<pre><code>observe(job, batch_size, power_limit, cost)\n</code></pre> <p>Observe the cost of using the given batch size and power limit for the job.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>Job</code> <p>The job from which this cost observation resulted.</p> required <code>batch_size</code> <code>int</code> <p>The batch size used for this run of the job.</p> required <code>power_limit</code> <code>int</code> <p>The power limit used for this run of the job.</p> required <code>cost</code> <code>float</code> <p>The cost of running the job.</p> required Source code in <code>zeus/_legacy/policy/interface.py</code> <pre><code>@abstractmethod\ndef observe(self, job: Job, batch_size: int, power_limit: int, cost: float) -&gt; None:\n    \"\"\"Observe the cost of using the given batch size and power limit for the job.\n\n    Args:\n        job: The job from which this cost observation resulted.\n        batch_size: The batch size used for this run of the job.\n        power_limit: The power limit used for this run of the job.\n        cost: The cost of running the job.\n    \"\"\"\n</code></pre>"},{"location":"reference/_legacy/policy/interface/#zeus._legacy.policy.interface.PowerLimitOptimizer._log","title":"_log","text":"<pre><code>_log(message)\n</code></pre> <p>Log message with object name.</p> Source code in <code>zeus/_legacy/policy/interface.py</code> <pre><code>def _log(self, message: str) -&gt; None:\n    \"\"\"Log message with object name.\"\"\"\n    print(f\"[{self.name}] {message}\")\n</code></pre>"},{"location":"reference/_legacy/policy/mab/","title":"mab","text":""},{"location":"reference/_legacy/policy/mab/#zeus._legacy.policy.mab","title":"zeus._legacy.policy.mab","text":"<p>Multi-Armed Bandit implementations.</p>"},{"location":"reference/_legacy/policy/mab/#zeus._legacy.policy.mab.GaussianTS","title":"GaussianTS","text":"<p>Thompson Sampling policy for Gaussian bandits.</p> <p>For each arm, the reward is modeled as a Gaussian distribution with known precision. The conjugate priors are also Gaussian distributions.</p> Source code in <code>zeus/_legacy/policy/mab.py</code> <pre><code>class GaussianTS:\n    \"\"\"Thompson Sampling policy for Gaussian bandits.\n\n    For each arm, the reward is modeled as a Gaussian distribution with\n    known precision. The conjugate priors are also Gaussian distributions.\n    \"\"\"\n\n    def __init__(\n        self,\n        arms: list[int],\n        reward_precision: list[float] | float,\n        prior_mean: float = 0.0,\n        prior_precision: float = 0.0,\n        num_exploration: int = 1,\n        seed: int = 123456,\n        verbose: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialze the object.\n\n        Args:\n            arms: Bandit arm values to use.\n            reward_precision: Precision (inverse variance) of the reward distribution.\n                Pass in a list of `float`s to set the reward precision differently for\n                each arm.\n            prior_mean: Mean of the belief prior distribution.\n            prior_precision: Precision of the belief prior distribution.\n            num_exploration: How many static explorations to run when no observations\n                are available.\n            seed: The random seed to use.\n            verbose: Whether to print out what's going on.\n        \"\"\"\n        self.name = \"GaussianTS\"\n\n        self.arms = arms\n        self.prior_mean = prior_mean\n        self.prior_prec = prior_precision\n        self.num_exploration = num_exploration\n        self.seed = seed\n        self.rng = np.random.default_rng(seed)\n        self.verbose = verbose\n\n        # Set the precision of the reward distribution of each arm.\n        if isinstance(reward_precision, list):\n            self.arm_reward_prec = dict(zip(arms, reward_precision))\n        else:\n            self.arm_reward_prec = {arm: reward_precision for arm in arms}\n\n        # Initialze the parameter distribution with the prior parameters.\n        self.arm_param_mean = dict.fromkeys(arms, prior_mean)\n        self.arm_param_prec = dict.fromkeys(arms, prior_precision)\n\n        # Track how many times an arm reward has been observed.\n        self.arm_num_observations = dict.fromkeys(arms, 0)\n\n    def fit(\n        self,\n        decisions: list[int] | np.ndarray,\n        rewards: list[float] | np.ndarray,\n        reset: bool,\n    ) -&gt; None:\n        \"\"\"Fit the bandit on the given list of observations.\n\n        Args:\n            decisions: A list of arms chosen.\n            rewards: A list of rewards that resulted from choosing the arms in `decisions`.\n            reset: Whether to reset all arms.\n        \"\"\"\n        decisions_arr = np.array(decisions)\n        rewards_arr = np.array(rewards)\n\n        # Fit all arms.\n        for arm in self.arms:\n            self.fit_arm(arm, rewards_arr[decisions_arr == arm], reset)\n\n    def fit_arm(self, arm: int, rewards: np.ndarray, reset: bool) -&gt; None:\n        \"\"\"Update the parameter distribution for one arm.\n\n        Reference: &lt;https://en.wikipedia.org/wiki/Conjugate_prior&gt;\n\n        Args:\n            arm: Arm to fit.\n            rewards: Array of rewards observed by pulling that arm.\n            reset: Whether to reset the parameters of the arm before fitting.\n        \"\"\"\n        if reset:\n            self.arm_param_mean[arm] = self.prior_mean\n            self.arm_param_prec[arm] = self.prior_prec\n            self.arm_num_observations[arm] = 0\n\n        if len(rewards) == 0:\n            return\n\n        # Read previous state.\n        reward_prec = self.arm_reward_prec[arm]\n        mean = self.arm_param_mean[arm]\n        prec = self.arm_param_prec[arm]\n\n        # Compute the parameters of the posterior distribution.\n        # The reward distribution's precision is given as infinite only when we\n        # have exactly one observation for the arm, s.t. sampling yields that\n        # exact observation.\n        if reward_prec == np.inf:\n            new_prec = np.inf\n            new_mean = rewards.mean()\n        else:\n            new_prec = prec + len(rewards) * reward_prec\n            new_mean = (prec * mean + reward_prec * rewards.sum()) / new_prec\n\n        # Update state.\n        self.arm_param_mean[arm] = new_mean\n        self.arm_param_prec[arm] = new_prec\n        self.arm_num_observations[arm] += len(rewards)\n\n    def predict(self) -&gt; int:\n        \"\"\"Return the arm with the largest sampled expected reward.\"\"\"\n        # Exploration-only phase.\n        # Order is random considering concurrent bandit scenarios.\n        arrms = np.array(self.arms)\n        for arm in self.rng.choice(arrms, len(arrms), replace=False):\n            if self.arm_num_observations[arm] &lt; self.num_exploration:\n                if self.verbose:\n                    print(f\"[{self.name}] Explore arm {arm}.\")\n                return arm\n\n        # Thomopson Sampling phase.\n        expectations = self.predict_expectations()\n        if self.verbose:\n            print(f\"[{self.name}] Sampled mean rewards:\")\n            for arm, sample in expectations.items():\n                print(\n                    f\"[{self.name}] Arm {arm:4d}: mu ~ N({self.arm_param_mean[arm]:.2f}, \"\n                    f\"{1 / self.arm_param_prec[arm]:.2f}) -&gt; {sample:.2f}\"\n                )\n        return max(expectations, key=expectations.get)  # type: ignore\n\n    def predict_expectations(self) -&gt; dict[int, float]:\n        \"\"\"Sample the expected reward for each arm.\n\n        Assumes that each arm has been explored at least once. Otherwise,\n        a value will be sampled from the prior.\n\n        Returns:\n            A mapping from every arm to their sampled expected reward.\n        \"\"\"\n        expectations = {}\n        for arm in self.arms:\n            mean = self.arm_param_mean[arm]\n            prec = self.arm_param_prec[arm]\n            if prec == self.prior_prec:\n                warnings.warn(\n                    f\"predict_expectations called when arm '{arm}' is cold.\",\n                    stacklevel=1,\n                )\n            expectations[arm] = self.rng.normal(mean, np.sqrt(np.reciprocal(prec)))\n        return expectations\n</code></pre>"},{"location":"reference/_legacy/policy/mab/#zeus._legacy.policy.mab.GaussianTS.__init__","title":"__init__","text":"<pre><code>__init__(arms, reward_precision, prior_mean=0.0, prior_precision=0.0, num_exploration=1, seed=123456, verbose=True)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>arms</code> <code>list[int]</code> <p>Bandit arm values to use.</p> required <code>reward_precision</code> <code>list[float] | float</code> <p>Precision (inverse variance) of the reward distribution. Pass in a list of <code>float</code>s to set the reward precision differently for each arm.</p> required <code>prior_mean</code> <code>float</code> <p>Mean of the belief prior distribution.</p> <code>0.0</code> <code>prior_precision</code> <code>float</code> <p>Precision of the belief prior distribution.</p> <code>0.0</code> <code>num_exploration</code> <code>int</code> <p>How many static explorations to run when no observations are available.</p> <code>1</code> <code>seed</code> <code>int</code> <p>The random seed to use.</p> <code>123456</code> <code>verbose</code> <code>bool</code> <p>Whether to print out what's going on.</p> <code>True</code> Source code in <code>zeus/_legacy/policy/mab.py</code> <pre><code>def __init__(\n    self,\n    arms: list[int],\n    reward_precision: list[float] | float,\n    prior_mean: float = 0.0,\n    prior_precision: float = 0.0,\n    num_exploration: int = 1,\n    seed: int = 123456,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"Initialze the object.\n\n    Args:\n        arms: Bandit arm values to use.\n        reward_precision: Precision (inverse variance) of the reward distribution.\n            Pass in a list of `float`s to set the reward precision differently for\n            each arm.\n        prior_mean: Mean of the belief prior distribution.\n        prior_precision: Precision of the belief prior distribution.\n        num_exploration: How many static explorations to run when no observations\n            are available.\n        seed: The random seed to use.\n        verbose: Whether to print out what's going on.\n    \"\"\"\n    self.name = \"GaussianTS\"\n\n    self.arms = arms\n    self.prior_mean = prior_mean\n    self.prior_prec = prior_precision\n    self.num_exploration = num_exploration\n    self.seed = seed\n    self.rng = np.random.default_rng(seed)\n    self.verbose = verbose\n\n    # Set the precision of the reward distribution of each arm.\n    if isinstance(reward_precision, list):\n        self.arm_reward_prec = dict(zip(arms, reward_precision))\n    else:\n        self.arm_reward_prec = {arm: reward_precision for arm in arms}\n\n    # Initialze the parameter distribution with the prior parameters.\n    self.arm_param_mean = dict.fromkeys(arms, prior_mean)\n    self.arm_param_prec = dict.fromkeys(arms, prior_precision)\n\n    # Track how many times an arm reward has been observed.\n    self.arm_num_observations = dict.fromkeys(arms, 0)\n</code></pre>"},{"location":"reference/_legacy/policy/mab/#zeus._legacy.policy.mab.GaussianTS.fit","title":"fit","text":"<pre><code>fit(decisions, rewards, reset)\n</code></pre> <p>Fit the bandit on the given list of observations.</p> <p>Parameters:</p> Name Type Description Default <code>decisions</code> <code>list[int] | ndarray</code> <p>A list of arms chosen.</p> required <code>rewards</code> <code>list[float] | ndarray</code> <p>A list of rewards that resulted from choosing the arms in <code>decisions</code>.</p> required <code>reset</code> <code>bool</code> <p>Whether to reset all arms.</p> required Source code in <code>zeus/_legacy/policy/mab.py</code> <pre><code>def fit(\n    self,\n    decisions: list[int] | np.ndarray,\n    rewards: list[float] | np.ndarray,\n    reset: bool,\n) -&gt; None:\n    \"\"\"Fit the bandit on the given list of observations.\n\n    Args:\n        decisions: A list of arms chosen.\n        rewards: A list of rewards that resulted from choosing the arms in `decisions`.\n        reset: Whether to reset all arms.\n    \"\"\"\n    decisions_arr = np.array(decisions)\n    rewards_arr = np.array(rewards)\n\n    # Fit all arms.\n    for arm in self.arms:\n        self.fit_arm(arm, rewards_arr[decisions_arr == arm], reset)\n</code></pre>"},{"location":"reference/_legacy/policy/mab/#zeus._legacy.policy.mab.GaussianTS.fit_arm","title":"fit_arm","text":"<pre><code>fit_arm(arm, rewards, reset)\n</code></pre> <p>Update the parameter distribution for one arm.</p> <p>Reference: https://en.wikipedia.org/wiki/Conjugate_prior</p> <p>Parameters:</p> Name Type Description Default <code>arm</code> <code>int</code> <p>Arm to fit.</p> required <code>rewards</code> <code>ndarray</code> <p>Array of rewards observed by pulling that arm.</p> required <code>reset</code> <code>bool</code> <p>Whether to reset the parameters of the arm before fitting.</p> required Source code in <code>zeus/_legacy/policy/mab.py</code> <pre><code>def fit_arm(self, arm: int, rewards: np.ndarray, reset: bool) -&gt; None:\n    \"\"\"Update the parameter distribution for one arm.\n\n    Reference: &lt;https://en.wikipedia.org/wiki/Conjugate_prior&gt;\n\n    Args:\n        arm: Arm to fit.\n        rewards: Array of rewards observed by pulling that arm.\n        reset: Whether to reset the parameters of the arm before fitting.\n    \"\"\"\n    if reset:\n        self.arm_param_mean[arm] = self.prior_mean\n        self.arm_param_prec[arm] = self.prior_prec\n        self.arm_num_observations[arm] = 0\n\n    if len(rewards) == 0:\n        return\n\n    # Read previous state.\n    reward_prec = self.arm_reward_prec[arm]\n    mean = self.arm_param_mean[arm]\n    prec = self.arm_param_prec[arm]\n\n    # Compute the parameters of the posterior distribution.\n    # The reward distribution's precision is given as infinite only when we\n    # have exactly one observation for the arm, s.t. sampling yields that\n    # exact observation.\n    if reward_prec == np.inf:\n        new_prec = np.inf\n        new_mean = rewards.mean()\n    else:\n        new_prec = prec + len(rewards) * reward_prec\n        new_mean = (prec * mean + reward_prec * rewards.sum()) / new_prec\n\n    # Update state.\n    self.arm_param_mean[arm] = new_mean\n    self.arm_param_prec[arm] = new_prec\n    self.arm_num_observations[arm] += len(rewards)\n</code></pre>"},{"location":"reference/_legacy/policy/mab/#zeus._legacy.policy.mab.GaussianTS.predict","title":"predict","text":"<pre><code>predict()\n</code></pre> <p>Return the arm with the largest sampled expected reward.</p> Source code in <code>zeus/_legacy/policy/mab.py</code> <pre><code>def predict(self) -&gt; int:\n    \"\"\"Return the arm with the largest sampled expected reward.\"\"\"\n    # Exploration-only phase.\n    # Order is random considering concurrent bandit scenarios.\n    arrms = np.array(self.arms)\n    for arm in self.rng.choice(arrms, len(arrms), replace=False):\n        if self.arm_num_observations[arm] &lt; self.num_exploration:\n            if self.verbose:\n                print(f\"[{self.name}] Explore arm {arm}.\")\n            return arm\n\n    # Thomopson Sampling phase.\n    expectations = self.predict_expectations()\n    if self.verbose:\n        print(f\"[{self.name}] Sampled mean rewards:\")\n        for arm, sample in expectations.items():\n            print(\n                f\"[{self.name}] Arm {arm:4d}: mu ~ N({self.arm_param_mean[arm]:.2f}, \"\n                f\"{1 / self.arm_param_prec[arm]:.2f}) -&gt; {sample:.2f}\"\n            )\n    return max(expectations, key=expectations.get)  # type: ignore\n</code></pre>"},{"location":"reference/_legacy/policy/mab/#zeus._legacy.policy.mab.GaussianTS.predict_expectations","title":"predict_expectations","text":"<pre><code>predict_expectations()\n</code></pre> <p>Sample the expected reward for each arm.</p> <p>Assumes that each arm has been explored at least once. Otherwise, a value will be sampled from the prior.</p> <p>Returns:</p> Type Description <code>dict[int, float]</code> <p>A mapping from every arm to their sampled expected reward.</p> Source code in <code>zeus/_legacy/policy/mab.py</code> <pre><code>def predict_expectations(self) -&gt; dict[int, float]:\n    \"\"\"Sample the expected reward for each arm.\n\n    Assumes that each arm has been explored at least once. Otherwise,\n    a value will be sampled from the prior.\n\n    Returns:\n        A mapping from every arm to their sampled expected reward.\n    \"\"\"\n    expectations = {}\n    for arm in self.arms:\n        mean = self.arm_param_mean[arm]\n        prec = self.arm_param_prec[arm]\n        if prec == self.prior_prec:\n            warnings.warn(\n                f\"predict_expectations called when arm '{arm}' is cold.\",\n                stacklevel=1,\n            )\n        expectations[arm] = self.rng.normal(mean, np.sqrt(np.reciprocal(prec)))\n    return expectations\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/","title":"optimizer","text":""},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer","title":"zeus._legacy.policy.optimizer","text":"<p>Implementations of various optimization policies.</p> <p><code>JITPowerLimitOptimizer</code> and <code>PruningGTSBatchSizeOptimizer</code> are the implementations used in Zeus's publication.</p>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.GTSBatchSizeOptimizer","title":"GTSBatchSizeOptimizer","text":"<p>               Bases: <code>BatchSizeOptimizer</code></p> <p>One Gaussian Thompson Sampling MAB for each job.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>class GTSBatchSizeOptimizer(BatchSizeOptimizer):\n    \"\"\"One Gaussian Thompson Sampling MAB for each job.\"\"\"\n\n    # ruff: noqa: D417\n    def __init__(\n        self,\n        learn_reward_precision: bool,\n        reward_precision: float = 0.0,\n        prior_mean: float = 0.0,\n        prior_precision: float = 0.0,\n        num_exploration: int = 1,\n        seed: int = 123456,\n        verbose: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialze the optimizer.\n\n        Refer to the constructor of [`GaussianTS`][zeus._legacy.policy.mab.GaussianTS]\n        for descriptions of other arguments.\n\n        Args:\n            learn_reward_precision: Whether to learn the reward precision of\n                each arm as we accumulate observations.\n        \"\"\"\n        self.learn_reward_precision = learn_reward_precision\n        self.reward_precision = reward_precision\n        self.prior_mean = prior_mean\n        self.prior_precision = prior_precision\n        self.num_exploration = num_exploration\n        self.seed = seed\n        self.verbose = verbose\n\n        # One MAB for each job.\n        self.mabs: dict[Job, GaussianTS] = {}\n\n        # Track the batch size range for each job.\n        self.batch_sizes: dict[Job, list[int]] = {}\n\n        # Observation history (batch size, reward) for each job.\n        self.history: dict[Job, defaultdict[int, list[float]]] = {}\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Name of the batch size optimizer.\"\"\"\n        return \"GaussianTS BSO\"\n\n    def register_job(self, job: Job, batch_sizes: list[int]) -&gt; None:\n        \"\"\"Instantiate a new GaussianTS MAB for the new job.\"\"\"\n        # We do not want to reset the state related to this job if\n        # anything already exists.\n        if job in self.mabs:\n            return\n        self.mabs[job] = GaussianTS(\n            arms=batch_sizes,\n            reward_precision=self.reward_precision,\n            prior_mean=self.prior_mean,\n            prior_precision=self.prior_precision,\n            num_exploration=self.num_exploration,\n            seed=self.seed,\n            verbose=self.verbose,\n        )\n        self.batch_sizes[job] = batch_sizes\n        self.history[job] = defaultdict(list)\n        if self.verbose:\n            self._log(f\"Registered {job}\")\n\n    def predict(self, job: Job) -&gt; int:\n        \"\"\"Return the batch size to use for the job.\"\"\"\n        if self.verbose:\n            self._log(f\"Prediction for {job}\")\n        pred = self.mabs[job].predict()\n        if self.verbose:\n            self._log(f\"{job} -&gt; \\033[31mBS = {pred}\\033[0m\")\n        return pred\n\n    def observe(self, job: Job, batch_size: int, cost: float, converged: bool | None = None) -&gt; None:\n        \"\"\"Learn from the cost of using the given batch size for the job.\"\"\"\n        if batch_size not in self.batch_sizes[job]:\n            raise ValueError(f\"Unknown batch size '{batch_size}' for {job}.\")\n\n        # No normalization needed since we learn a separate bandit for each job.\n        reward = -cost\n\n        # Add observation to history.\n        self.history[job][batch_size].append(reward)\n\n        # When we're not learning the reward precision, everyting is\n        # simple. We can just call `partial_fit` on the job's MAB instance.\n        if not self.learn_reward_precision:\n            self.mabs[job].fit([batch_size], [reward], reset=False)\n            if self.verbose:\n                self._log(f\"{job} @ {batch_size}: reward = {reward:.2f}\")\n\n        # When we're learning the reward precision, we need to\n        # 1. re-compute the precision this arm based on the history,\n        # 2. update the arm's reward precision\n        # 3. and `fit` the new MAB instance on all past data.\n        else:\n            arm_rewards = np.array(self.history[job][batch_size])\n            variance = np.var(arm_rewards)\n            # When there is only one observation for the arm, the variance is zero.\n            # NOTE: We might still want to have a pre-determined reward precision here\n            #       because sampling from an infinite precision Gaussian distribution\n            #       always returns the mean (the observation), and it will hamper\n            #       exploration in the early stage.\n            precision = np.inf if variance == 0.0 else np.reciprocal(variance)\n            mab = self.mabs[job]\n            mab.arm_reward_prec[batch_size] = precision\n            mab.fit_arm(batch_size, arm_rewards, reset=True)\n            self.mabs[job] = mab\n            if self.verbose:\n                arm_rewards_repr = \", \".join([f\"{r:.2f}\" for r in arm_rewards])\n                self._log(f\"{job} @ {batch_size}: arm_rewards = [{arm_rewards_repr}], reward_prec = {precision}\")\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.GTSBatchSizeOptimizer.name","title":"name  <code>property</code>","text":"<pre><code>name\n</code></pre> <p>Name of the batch size optimizer.</p>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.GTSBatchSizeOptimizer.__init__","title":"__init__","text":"<pre><code>__init__(learn_reward_precision, reward_precision=0.0, prior_mean=0.0, prior_precision=0.0, num_exploration=1, seed=123456, verbose=True)\n</code></pre> <p>Refer to the constructor of <code>GaussianTS</code> for descriptions of other arguments.</p> <p>Parameters:</p> Name Type Description Default <code>learn_reward_precision</code> <code>bool</code> <p>Whether to learn the reward precision of each arm as we accumulate observations.</p> required Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def __init__(\n    self,\n    learn_reward_precision: bool,\n    reward_precision: float = 0.0,\n    prior_mean: float = 0.0,\n    prior_precision: float = 0.0,\n    num_exploration: int = 1,\n    seed: int = 123456,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"Initialze the optimizer.\n\n    Refer to the constructor of [`GaussianTS`][zeus._legacy.policy.mab.GaussianTS]\n    for descriptions of other arguments.\n\n    Args:\n        learn_reward_precision: Whether to learn the reward precision of\n            each arm as we accumulate observations.\n    \"\"\"\n    self.learn_reward_precision = learn_reward_precision\n    self.reward_precision = reward_precision\n    self.prior_mean = prior_mean\n    self.prior_precision = prior_precision\n    self.num_exploration = num_exploration\n    self.seed = seed\n    self.verbose = verbose\n\n    # One MAB for each job.\n    self.mabs: dict[Job, GaussianTS] = {}\n\n    # Track the batch size range for each job.\n    self.batch_sizes: dict[Job, list[int]] = {}\n\n    # Observation history (batch size, reward) for each job.\n    self.history: dict[Job, defaultdict[int, list[float]]] = {}\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.GTSBatchSizeOptimizer.register_job","title":"register_job","text":"<pre><code>register_job(job, batch_sizes)\n</code></pre> <p>Instantiate a new GaussianTS MAB for the new job.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def register_job(self, job: Job, batch_sizes: list[int]) -&gt; None:\n    \"\"\"Instantiate a new GaussianTS MAB for the new job.\"\"\"\n    # We do not want to reset the state related to this job if\n    # anything already exists.\n    if job in self.mabs:\n        return\n    self.mabs[job] = GaussianTS(\n        arms=batch_sizes,\n        reward_precision=self.reward_precision,\n        prior_mean=self.prior_mean,\n        prior_precision=self.prior_precision,\n        num_exploration=self.num_exploration,\n        seed=self.seed,\n        verbose=self.verbose,\n    )\n    self.batch_sizes[job] = batch_sizes\n    self.history[job] = defaultdict(list)\n    if self.verbose:\n        self._log(f\"Registered {job}\")\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.GTSBatchSizeOptimizer.predict","title":"predict","text":"<pre><code>predict(job)\n</code></pre> <p>Return the batch size to use for the job.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def predict(self, job: Job) -&gt; int:\n    \"\"\"Return the batch size to use for the job.\"\"\"\n    if self.verbose:\n        self._log(f\"Prediction for {job}\")\n    pred = self.mabs[job].predict()\n    if self.verbose:\n        self._log(f\"{job} -&gt; \\033[31mBS = {pred}\\033[0m\")\n    return pred\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.GTSBatchSizeOptimizer.observe","title":"observe","text":"<pre><code>observe(job, batch_size, cost, converged=None)\n</code></pre> <p>Learn from the cost of using the given batch size for the job.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def observe(self, job: Job, batch_size: int, cost: float, converged: bool | None = None) -&gt; None:\n    \"\"\"Learn from the cost of using the given batch size for the job.\"\"\"\n    if batch_size not in self.batch_sizes[job]:\n        raise ValueError(f\"Unknown batch size '{batch_size}' for {job}.\")\n\n    # No normalization needed since we learn a separate bandit for each job.\n    reward = -cost\n\n    # Add observation to history.\n    self.history[job][batch_size].append(reward)\n\n    # When we're not learning the reward precision, everyting is\n    # simple. We can just call `partial_fit` on the job's MAB instance.\n    if not self.learn_reward_precision:\n        self.mabs[job].fit([batch_size], [reward], reset=False)\n        if self.verbose:\n            self._log(f\"{job} @ {batch_size}: reward = {reward:.2f}\")\n\n    # When we're learning the reward precision, we need to\n    # 1. re-compute the precision this arm based on the history,\n    # 2. update the arm's reward precision\n    # 3. and `fit` the new MAB instance on all past data.\n    else:\n        arm_rewards = np.array(self.history[job][batch_size])\n        variance = np.var(arm_rewards)\n        # When there is only one observation for the arm, the variance is zero.\n        # NOTE: We might still want to have a pre-determined reward precision here\n        #       because sampling from an infinite precision Gaussian distribution\n        #       always returns the mean (the observation), and it will hamper\n        #       exploration in the early stage.\n        precision = np.inf if variance == 0.0 else np.reciprocal(variance)\n        mab = self.mabs[job]\n        mab.arm_reward_prec[batch_size] = precision\n        mab.fit_arm(batch_size, arm_rewards, reset=True)\n        self.mabs[job] = mab\n        if self.verbose:\n            arm_rewards_repr = \", \".join([f\"{r:.2f}\" for r in arm_rewards])\n            self._log(f\"{job} @ {batch_size}: arm_rewards = [{arm_rewards_repr}], reward_prec = {precision}\")\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.PruningExploreManager","title":"PruningExploreManager","text":"<p>Helper class that generates batch sizes to explore and prune.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>class PruningExploreManager:\n    \"\"\"Helper class that generates batch sizes to explore and prune.\"\"\"\n\n    def __init__(\n        self,\n        batch_sizes: list[int],\n        default: int,\n        num_pruning_rounds: int = 2,\n    ) -&gt; None:\n        \"\"\"Initialze the object.\n\n        Args:\n            batch_sizes: The initial set of batch sizes to prune from.\n            default: The default batch size (b0) to begin exploration from.\n            num_pruning_rounds: How many rounds to run pruning.\n        \"\"\"\n        # Sanity checks.\n        if default not in batch_sizes:\n            raise ValueError(f\"Default batch size {default} not in {batch_sizes}.\")\n\n        # Save arguments.\n        self.batch_sizes = batch_sizes\n        self.default = default\n        self.num_pruning_rounds = num_pruning_rounds\n\n        # State\n        self.expecting = default\n\n        # Generator that returns batch sizes.\n        self.gen = self._exploration_engine()\n\n    def _exploration_engine(\n        self,\n    ) -&gt; Generator[int | None, tuple[int, float, bool], list[int]]:\n        \"\"\"Drive pruning exploration.\n\n        Yields the batch size to be explored.\n        The caller should `send` a tuple of (explored batch size, cost, whether reached).\n        As a safety measure, the explored batch size must match the most recently yielded\n        batch size, and otherwise a `RuntimeError` is raised.\n        Finally, when exploration is over, returns a sorted list of batch sizes that\n        survived pruning.\n        \"\"\"\n        for _ in range(self.num_pruning_rounds):\n            # A list of batch sizes that reached the target metric.\n            good: list[int] = []\n\n            # We first explore downwards form the default batch size, and then go upwards.\n            idx = self.batch_sizes.index(self.default)\n            down = sorted(self.batch_sizes[: idx + 1], reverse=True)\n            up = sorted(self.batch_sizes[idx + 1 :])\n\n            # We track the best cost because the default batch size is updated to the batch\n            # size that performed the best.\n            best_cost = np.inf\n\n            for bs_list in [down, up]:\n                for bs in bs_list:\n                    # We tell the outside world to explore `bs`, and we expect the outside\n                    # world to give us back the cost of that `bs`.\n                    self.expecting = bs\n                    batch_size, cost, reached = yield bs\n                    if self.expecting != batch_size:\n                        raise RuntimeError(f\"PruningExplorationManager: {self.expecting=}, {batch_size=}\")\n                    self.expecting = 0\n\n                    # An empty `yield` to not proceed to the next batch size when the caller\n                    # `send`s in the results.\n                    yield\n\n                    # Only batch sizes that reached the target mteric are good.\n                    if reached:\n                        if best_cost &gt; cost:\n                            best_cost = cost\n                            self.default = bs\n                        good.append(bs)\n                    # If the batch size did not reach the target metric, `break`ing here will\n                    # allow us to move on to either the next direction of exploration (upwards)\n                    # or end this round of pruning exploration.\n                    else:\n                        break\n\n            self.expecting = 0\n            self.batch_sizes = sorted(good)\n\n        return sorted(self.batch_sizes)\n\n    def next_batch_size(self) -&gt; int:\n        \"\"\"Return the next batch size to explore.\n\n        Raises `StopIteration` when pruning exploration phase is over.\n        The exception instance contains the final set of batch sizes to consider.\n        Access it through `exception.value`.\n        \"\"\"\n        batch_size = next(self.gen)\n        assert batch_size is not None, \"Call order may have been wrong.\"\n        return batch_size\n\n    def report_batch_size_result(self, batch_size: int, cost: float, reached: bool) -&gt; None:\n        \"\"\"Report whether the previous batch size reached the target metric.\n\n        Args:\n            batch_size: The batch size which this cost observation is from.\n            cost: The energy-time cost of running the job with this batch size.\n            reached: Whether the job reached the target metric.\n        \"\"\"\n        none = self.gen.send((batch_size, cost, reached))\n        assert none is None, \"Call order may have been wrong.\"\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.PruningExploreManager.__init__","title":"__init__","text":"<pre><code>__init__(batch_sizes, default, num_pruning_rounds=2)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>batch_sizes</code> <code>list[int]</code> <p>The initial set of batch sizes to prune from.</p> required <code>default</code> <code>int</code> <p>The default batch size (b0) to begin exploration from.</p> required <code>num_pruning_rounds</code> <code>int</code> <p>How many rounds to run pruning.</p> <code>2</code> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def __init__(\n    self,\n    batch_sizes: list[int],\n    default: int,\n    num_pruning_rounds: int = 2,\n) -&gt; None:\n    \"\"\"Initialze the object.\n\n    Args:\n        batch_sizes: The initial set of batch sizes to prune from.\n        default: The default batch size (b0) to begin exploration from.\n        num_pruning_rounds: How many rounds to run pruning.\n    \"\"\"\n    # Sanity checks.\n    if default not in batch_sizes:\n        raise ValueError(f\"Default batch size {default} not in {batch_sizes}.\")\n\n    # Save arguments.\n    self.batch_sizes = batch_sizes\n    self.default = default\n    self.num_pruning_rounds = num_pruning_rounds\n\n    # State\n    self.expecting = default\n\n    # Generator that returns batch sizes.\n    self.gen = self._exploration_engine()\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.PruningExploreManager._exploration_engine","title":"_exploration_engine","text":"<pre><code>_exploration_engine()\n</code></pre> <p>Drive pruning exploration.</p> <p>Yields the batch size to be explored. The caller should <code>send</code> a tuple of (explored batch size, cost, whether reached). As a safety measure, the explored batch size must match the most recently yielded batch size, and otherwise a <code>RuntimeError</code> is raised. Finally, when exploration is over, returns a sorted list of batch sizes that survived pruning.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def _exploration_engine(\n    self,\n) -&gt; Generator[int | None, tuple[int, float, bool], list[int]]:\n    \"\"\"Drive pruning exploration.\n\n    Yields the batch size to be explored.\n    The caller should `send` a tuple of (explored batch size, cost, whether reached).\n    As a safety measure, the explored batch size must match the most recently yielded\n    batch size, and otherwise a `RuntimeError` is raised.\n    Finally, when exploration is over, returns a sorted list of batch sizes that\n    survived pruning.\n    \"\"\"\n    for _ in range(self.num_pruning_rounds):\n        # A list of batch sizes that reached the target metric.\n        good: list[int] = []\n\n        # We first explore downwards form the default batch size, and then go upwards.\n        idx = self.batch_sizes.index(self.default)\n        down = sorted(self.batch_sizes[: idx + 1], reverse=True)\n        up = sorted(self.batch_sizes[idx + 1 :])\n\n        # We track the best cost because the default batch size is updated to the batch\n        # size that performed the best.\n        best_cost = np.inf\n\n        for bs_list in [down, up]:\n            for bs in bs_list:\n                # We tell the outside world to explore `bs`, and we expect the outside\n                # world to give us back the cost of that `bs`.\n                self.expecting = bs\n                batch_size, cost, reached = yield bs\n                if self.expecting != batch_size:\n                    raise RuntimeError(f\"PruningExplorationManager: {self.expecting=}, {batch_size=}\")\n                self.expecting = 0\n\n                # An empty `yield` to not proceed to the next batch size when the caller\n                # `send`s in the results.\n                yield\n\n                # Only batch sizes that reached the target mteric are good.\n                if reached:\n                    if best_cost &gt; cost:\n                        best_cost = cost\n                        self.default = bs\n                    good.append(bs)\n                # If the batch size did not reach the target metric, `break`ing here will\n                # allow us to move on to either the next direction of exploration (upwards)\n                # or end this round of pruning exploration.\n                else:\n                    break\n\n        self.expecting = 0\n        self.batch_sizes = sorted(good)\n\n    return sorted(self.batch_sizes)\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.PruningExploreManager.next_batch_size","title":"next_batch_size","text":"<pre><code>next_batch_size()\n</code></pre> <p>Return the next batch size to explore.</p> <p>Raises <code>StopIteration</code> when pruning exploration phase is over. The exception instance contains the final set of batch sizes to consider. Access it through <code>exception.value</code>.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def next_batch_size(self) -&gt; int:\n    \"\"\"Return the next batch size to explore.\n\n    Raises `StopIteration` when pruning exploration phase is over.\n    The exception instance contains the final set of batch sizes to consider.\n    Access it through `exception.value`.\n    \"\"\"\n    batch_size = next(self.gen)\n    assert batch_size is not None, \"Call order may have been wrong.\"\n    return batch_size\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.PruningExploreManager.report_batch_size_result","title":"report_batch_size_result","text":"<pre><code>report_batch_size_result(batch_size, cost, reached)\n</code></pre> <p>Report whether the previous batch size reached the target metric.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The batch size which this cost observation is from.</p> required <code>cost</code> <code>float</code> <p>The energy-time cost of running the job with this batch size.</p> required <code>reached</code> <code>bool</code> <p>Whether the job reached the target metric.</p> required Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def report_batch_size_result(self, batch_size: int, cost: float, reached: bool) -&gt; None:\n    \"\"\"Report whether the previous batch size reached the target metric.\n\n    Args:\n        batch_size: The batch size which this cost observation is from.\n        cost: The energy-time cost of running the job with this batch size.\n        reached: Whether the job reached the target metric.\n    \"\"\"\n    none = self.gen.send((batch_size, cost, reached))\n    assert none is None, \"Call order may have been wrong.\"\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.PruningGTSBatchSizeOptimizer","title":"PruningGTSBatchSizeOptimizer","text":"<p>               Bases: <code>BatchSizeOptimizer</code></p> <p>One Gaussian Thompson Sampling MAB for each job with double pruning exploration.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>class PruningGTSBatchSizeOptimizer(BatchSizeOptimizer):\n    \"\"\"One Gaussian Thompson Sampling MAB for each job with double pruning exploration.\"\"\"\n\n    def __init__(\n        self,\n        prior_mean: float = 0.0,\n        prior_precision: float = 0.0,\n        window_size: int = 0,\n        concurrency: bool = False,\n        seed: int = 123456,\n        verbose: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialze the optimizer.\n\n        Refer to the constructor of [`GaussianTS`][zeus._legacy.policy.mab.GaussianTS]\n        for descriptions of other arguments.\n\n        Args:\n            window_size: Size of the window for the MAB (for drift handling).\n            concurrency: Whether to support concurrent job submissions.\n        \"\"\"\n        self.prior_mean = prior_mean\n        self.prior_precision = prior_precision\n        self.window_size = window_size\n        self.concurrency = concurrency\n        self.seed = seed\n        self.verbose = verbose\n\n        # One MAB for each job.\n        self.mabs: dict[Job, GaussianTS] = {}\n\n        # One PruningExplorationManager for each job.\n        self.exp_manager: dict[Job, PruningExploreManager] = {}\n\n        # Observation history (batch size, reward) for each job.\n        self.history: dict[Job, list[tuple[int, float]]] = {}\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Name of the batch size optimizer.\"\"\"\n        return \"Pruning GaussianTS BSO\"\n\n    def register_job(self, job: Job, batch_sizes: list[int]) -&gt; None:\n        \"\"\"Register the job.\"\"\"\n        # Sanity checks.\n        if job.default_bs is None:\n            raise ValueError(f\"Default BS not specified for {job}.\")\n        if not batch_sizes:\n            raise ValueError(f\"Batch size list for {job} is empty.\")\n\n        # Set internal states.\n        self.exp_manager[job] = PruningExploreManager(sorted(batch_sizes), job.default_bs)\n        self.history[job] = []\n        if self.verbose:\n            self._log(f\"Registered {job}\")\n\n    def predict(self, job: Job) -&gt; int:\n        \"\"\"Return the batch size to use for the job.\"\"\"\n        # Try to see if the exploration manager has something.\n        try:\n            batch_size = self.exp_manager[job].next_batch_size()\n            if self.verbose:\n                self._log(f\"{job} in pruning stage -&gt; \\033[31mBS = {batch_size}\\033[0m\")\n        except StopIteration as exp:\n            # Pruning stage is over.\n            if job not in self.mabs:\n                self._construct_mab(job, exp.value)\n            batch_size = self.mabs[job].predict()\n            if self.verbose:\n                self._log(f\"{job} in Thompson Sampling stage -&gt; \\033[31mBS = {batch_size}\\033[0m\")\n\n        return batch_size\n\n    def observe(self, job: Job, batch_size: int, cost: float, converged: bool | None = None) -&gt; None:\n        \"\"\"Learn from the cost of using the given batch size for the job.\"\"\"\n        # Add observation to history.\n        self.history[job].append((batch_size, -cost))\n\n        # We're in Thompson Sampling stage.\n        if job in self.mabs:\n            # Since we're learning the reward precision, we need to\n            # 1. re-compute the precision of this arm based on the reward history,\n            # 2. update the arm's reward precision\n            # 3. and `fit` the new MAB instance on all the reward history.\n            # Note that `arm_rewards` always has more than one entry (and hence a\n            # non-zero variance) because we've been through pruning exploration.\n            arm_rewards = np.array(self._get_history_for_bs(job, batch_size))\n            precision = np.reciprocal(np.var(arm_rewards))\n            mab = self.mabs[job]\n            mab.arm_reward_prec[batch_size] = precision\n            mab.fit_arm(batch_size, arm_rewards, reset=True)\n            if self.verbose:\n                arm_rewards_repr = \", \".join([f\"{r:.2f}\" for r in arm_rewards])\n                self._log(f\"{job} @ {batch_size}: arm_rewards = [{arm_rewards_repr}], reward_prec = {precision}\")\n\n        # We're in pruning stage.\n        else:\n            assert converged is not None\n            # Log before we potentially error out.\n            if self.verbose:\n                self._log(\n                    f\"{job} in pruning stage, expecting BS {self.exp_manager[job].expecting}.\"\n                    f\" Current BS {batch_size} that did {'not ' * converged}converge.\"\n                )\n\n            # If we don't support concurrency, we can just pass the results to the\n            # exploration manager, and the manager will err if the order of batch sizes\n            # is screwed up.\n            if not self.concurrency:\n                self.exp_manager[job].report_batch_size_result(batch_size, cost, converged)\n                return\n\n            # If we are supporting concurrency, there's a subtle issue.\n            # Pruning exploration demands a specific order of trying out a batch size\n            # and receiving the results (cost and whether reached). This breaks in the\n            # following situation, for example:\n            # 1. Job with BS 32 that is part of pruning exploration starts.\n            # 2. Concurrent job comes in, and we launch it with the best known BS 64.\n            # 3. Job with BS 64 finishes first, and calls bso.observe with BS 64.\n            # This breaks the observation order assumption of PruningExplorationManager.\n            # Thus we check whether the current batch size is the one expected by\n            # PruningExplorationManager, and then only if so, call bso.observe.\n            # Otherwise, we silently insert the cost observation into the bso's history\n            # (first line of this method) and don't touch the PruningExplorationManager.\n            if self.exp_manager[job].expecting == batch_size:\n                self.exp_manager[job].report_batch_size_result(batch_size, cost, converged)\n\n    def _get_history_for_bs(self, job: Job, batch_size: int) -&gt; list[float]:\n        \"\"\"Return the windowed history for the given job's batch size.\"\"\"\n        history = self.history[job]\n        rewards = []\n        # Collect rewards starting from the most recent ones and backwards.\n        for bs, reward in reversed(history):\n            if bs == batch_size:\n                rewards.append(reward)\n                if len(rewards) == self.window_size:\n                    break\n        # There's no need to return this in time order, but just in case.\n        return list(reversed(rewards))\n\n    def _construct_mab(self, job: Job, batch_sizes: list[int]) -&gt; None:\n        \"\"\"When exploration is over, this method is called to construct and learn GTS.\"\"\"\n        # Sanity check.\n        if not batch_sizes:\n            raise ValueError(\"Empty batch size set when constructing MAB. Probably all batch sizes have been pruned.\")\n\n        if self.verbose:\n            self._log(f\"Construct MAB for {job} with arms {batch_sizes}\")\n\n        mab = GaussianTS(\n            arms=batch_sizes,  # The MAB only has \"good\" arms.\n            reward_precision=0.0,\n            prior_mean=self.prior_mean,\n            prior_precision=self.prior_precision,\n            num_exploration=2,\n            seed=self.seed,\n            verbose=self.verbose,\n        )\n        # Fit the arm for each good batch size.\n        for batch_size in self.exp_manager[job].batch_sizes:\n            arm_rewards = np.array(self._get_history_for_bs(job, batch_size))\n            assert len(arm_rewards) &gt;= 2, f\"Number of observations for {batch_size} is {len(arm_rewards)}.\"\n            mab.arm_reward_prec[batch_size] = np.reciprocal(np.var(arm_rewards))\n            mab.fit_arm(batch_size, arm_rewards, reset=True)\n        # Save the MAB.\n        self.mabs[job] = mab\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.PruningGTSBatchSizeOptimizer.name","title":"name  <code>property</code>","text":"<pre><code>name\n</code></pre> <p>Name of the batch size optimizer.</p>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.PruningGTSBatchSizeOptimizer.__init__","title":"__init__","text":"<pre><code>__init__(prior_mean=0.0, prior_precision=0.0, window_size=0, concurrency=False, seed=123456, verbose=True)\n</code></pre> <p>Refer to the constructor of <code>GaussianTS</code> for descriptions of other arguments.</p> <p>Parameters:</p> Name Type Description Default <code>window_size</code> <code>int</code> <p>Size of the window for the MAB (for drift handling).</p> <code>0</code> <code>concurrency</code> <code>bool</code> <p>Whether to support concurrent job submissions.</p> <code>False</code> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def __init__(\n    self,\n    prior_mean: float = 0.0,\n    prior_precision: float = 0.0,\n    window_size: int = 0,\n    concurrency: bool = False,\n    seed: int = 123456,\n    verbose: bool = True,\n) -&gt; None:\n    \"\"\"Initialze the optimizer.\n\n    Refer to the constructor of [`GaussianTS`][zeus._legacy.policy.mab.GaussianTS]\n    for descriptions of other arguments.\n\n    Args:\n        window_size: Size of the window for the MAB (for drift handling).\n        concurrency: Whether to support concurrent job submissions.\n    \"\"\"\n    self.prior_mean = prior_mean\n    self.prior_precision = prior_precision\n    self.window_size = window_size\n    self.concurrency = concurrency\n    self.seed = seed\n    self.verbose = verbose\n\n    # One MAB for each job.\n    self.mabs: dict[Job, GaussianTS] = {}\n\n    # One PruningExplorationManager for each job.\n    self.exp_manager: dict[Job, PruningExploreManager] = {}\n\n    # Observation history (batch size, reward) for each job.\n    self.history: dict[Job, list[tuple[int, float]]] = {}\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.PruningGTSBatchSizeOptimizer.register_job","title":"register_job","text":"<pre><code>register_job(job, batch_sizes)\n</code></pre> <p>Register the job.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def register_job(self, job: Job, batch_sizes: list[int]) -&gt; None:\n    \"\"\"Register the job.\"\"\"\n    # Sanity checks.\n    if job.default_bs is None:\n        raise ValueError(f\"Default BS not specified for {job}.\")\n    if not batch_sizes:\n        raise ValueError(f\"Batch size list for {job} is empty.\")\n\n    # Set internal states.\n    self.exp_manager[job] = PruningExploreManager(sorted(batch_sizes), job.default_bs)\n    self.history[job] = []\n    if self.verbose:\n        self._log(f\"Registered {job}\")\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.PruningGTSBatchSizeOptimizer.predict","title":"predict","text":"<pre><code>predict(job)\n</code></pre> <p>Return the batch size to use for the job.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def predict(self, job: Job) -&gt; int:\n    \"\"\"Return the batch size to use for the job.\"\"\"\n    # Try to see if the exploration manager has something.\n    try:\n        batch_size = self.exp_manager[job].next_batch_size()\n        if self.verbose:\n            self._log(f\"{job} in pruning stage -&gt; \\033[31mBS = {batch_size}\\033[0m\")\n    except StopIteration as exp:\n        # Pruning stage is over.\n        if job not in self.mabs:\n            self._construct_mab(job, exp.value)\n        batch_size = self.mabs[job].predict()\n        if self.verbose:\n            self._log(f\"{job} in Thompson Sampling stage -&gt; \\033[31mBS = {batch_size}\\033[0m\")\n\n    return batch_size\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.PruningGTSBatchSizeOptimizer.observe","title":"observe","text":"<pre><code>observe(job, batch_size, cost, converged=None)\n</code></pre> <p>Learn from the cost of using the given batch size for the job.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def observe(self, job: Job, batch_size: int, cost: float, converged: bool | None = None) -&gt; None:\n    \"\"\"Learn from the cost of using the given batch size for the job.\"\"\"\n    # Add observation to history.\n    self.history[job].append((batch_size, -cost))\n\n    # We're in Thompson Sampling stage.\n    if job in self.mabs:\n        # Since we're learning the reward precision, we need to\n        # 1. re-compute the precision of this arm based on the reward history,\n        # 2. update the arm's reward precision\n        # 3. and `fit` the new MAB instance on all the reward history.\n        # Note that `arm_rewards` always has more than one entry (and hence a\n        # non-zero variance) because we've been through pruning exploration.\n        arm_rewards = np.array(self._get_history_for_bs(job, batch_size))\n        precision = np.reciprocal(np.var(arm_rewards))\n        mab = self.mabs[job]\n        mab.arm_reward_prec[batch_size] = precision\n        mab.fit_arm(batch_size, arm_rewards, reset=True)\n        if self.verbose:\n            arm_rewards_repr = \", \".join([f\"{r:.2f}\" for r in arm_rewards])\n            self._log(f\"{job} @ {batch_size}: arm_rewards = [{arm_rewards_repr}], reward_prec = {precision}\")\n\n    # We're in pruning stage.\n    else:\n        assert converged is not None\n        # Log before we potentially error out.\n        if self.verbose:\n            self._log(\n                f\"{job} in pruning stage, expecting BS {self.exp_manager[job].expecting}.\"\n                f\" Current BS {batch_size} that did {'not ' * converged}converge.\"\n            )\n\n        # If we don't support concurrency, we can just pass the results to the\n        # exploration manager, and the manager will err if the order of batch sizes\n        # is screwed up.\n        if not self.concurrency:\n            self.exp_manager[job].report_batch_size_result(batch_size, cost, converged)\n            return\n\n        # If we are supporting concurrency, there's a subtle issue.\n        # Pruning exploration demands a specific order of trying out a batch size\n        # and receiving the results (cost and whether reached). This breaks in the\n        # following situation, for example:\n        # 1. Job with BS 32 that is part of pruning exploration starts.\n        # 2. Concurrent job comes in, and we launch it with the best known BS 64.\n        # 3. Job with BS 64 finishes first, and calls bso.observe with BS 64.\n        # This breaks the observation order assumption of PruningExplorationManager.\n        # Thus we check whether the current batch size is the one expected by\n        # PruningExplorationManager, and then only if so, call bso.observe.\n        # Otherwise, we silently insert the cost observation into the bso's history\n        # (first line of this method) and don't touch the PruningExplorationManager.\n        if self.exp_manager[job].expecting == batch_size:\n            self.exp_manager[job].report_batch_size_result(batch_size, cost, converged)\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.PruningGTSBatchSizeOptimizer._get_history_for_bs","title":"_get_history_for_bs","text":"<pre><code>_get_history_for_bs(job, batch_size)\n</code></pre> <p>Return the windowed history for the given job's batch size.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def _get_history_for_bs(self, job: Job, batch_size: int) -&gt; list[float]:\n    \"\"\"Return the windowed history for the given job's batch size.\"\"\"\n    history = self.history[job]\n    rewards = []\n    # Collect rewards starting from the most recent ones and backwards.\n    for bs, reward in reversed(history):\n        if bs == batch_size:\n            rewards.append(reward)\n            if len(rewards) == self.window_size:\n                break\n    # There's no need to return this in time order, but just in case.\n    return list(reversed(rewards))\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.PruningGTSBatchSizeOptimizer._construct_mab","title":"_construct_mab","text":"<pre><code>_construct_mab(job, batch_sizes)\n</code></pre> <p>When exploration is over, this method is called to construct and learn GTS.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def _construct_mab(self, job: Job, batch_sizes: list[int]) -&gt; None:\n    \"\"\"When exploration is over, this method is called to construct and learn GTS.\"\"\"\n    # Sanity check.\n    if not batch_sizes:\n        raise ValueError(\"Empty batch size set when constructing MAB. Probably all batch sizes have been pruned.\")\n\n    if self.verbose:\n        self._log(f\"Construct MAB for {job} with arms {batch_sizes}\")\n\n    mab = GaussianTS(\n        arms=batch_sizes,  # The MAB only has \"good\" arms.\n        reward_precision=0.0,\n        prior_mean=self.prior_mean,\n        prior_precision=self.prior_precision,\n        num_exploration=2,\n        seed=self.seed,\n        verbose=self.verbose,\n    )\n    # Fit the arm for each good batch size.\n    for batch_size in self.exp_manager[job].batch_sizes:\n        arm_rewards = np.array(self._get_history_for_bs(job, batch_size))\n        assert len(arm_rewards) &gt;= 2, f\"Number of observations for {batch_size} is {len(arm_rewards)}.\"\n        mab.arm_reward_prec[batch_size] = np.reciprocal(np.var(arm_rewards))\n        mab.fit_arm(batch_size, arm_rewards, reset=True)\n    # Save the MAB.\n    self.mabs[job] = mab\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.JITPowerLimitOptimizer","title":"JITPowerLimitOptimizer","text":"<p>               Bases: <code>PowerLimitOptimizer</code></p> <p>Returns the best power limit to use for the job &amp; batch size.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>class JITPowerLimitOptimizer(PowerLimitOptimizer):\n    \"\"\"Returns the best power limit to use for the job &amp; batch size.\"\"\"\n\n    def __init__(self, verbose: bool = True) -&gt; None:\n        \"\"\"Initialize the object.\"\"\"\n        self.verbose = verbose\n\n        self.best_pl: defaultdict[Job, dict[int, int]] = defaultdict(dict)\n        self.best_cost: defaultdict[Job, dict[int, float]] = defaultdict(dict)\n        self.observe_count: defaultdict[Job, defaultdict[int, int]] = defaultdict(lambda: defaultdict(int))\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Name of the power limit optimizer.\"\"\"\n        return \"JITPSO\"\n\n    def predict(self, job: Job, batch_size: int) -&gt; int | None:\n        \"\"\"Return the best power limit for the job, or None if unknown.\"\"\"\n        pred = self.best_pl[job].get(batch_size)\n        if self.verbose:\n            self._log(\n                f\"{job} @ {batch_size} -&gt; \\033[31mPL = {'needs profiling' if pred is None else str(pred) + 'W'}\\033[0m\"\n            )\n        return pred\n\n    def observe(self, job: Job, batch_size: int, power_limit: int, cost: float) -&gt; None:\n        \"\"\"Learn from the cost of using the given knobs for the job.\"\"\"\n        self.observe_count[job][batch_size] += 1\n        prev_best_cost = self.best_cost[job].get(batch_size)\n        if prev_best_cost is None or prev_best_cost &gt; cost:\n            self.best_pl[job][batch_size] = power_limit\n            self.best_cost[job][batch_size] = cost\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.JITPowerLimitOptimizer.name","title":"name  <code>property</code>","text":"<pre><code>name\n</code></pre> <p>Name of the power limit optimizer.</p>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.JITPowerLimitOptimizer.__init__","title":"__init__","text":"<pre><code>__init__(verbose=True)\n</code></pre> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def __init__(self, verbose: bool = True) -&gt; None:\n    \"\"\"Initialize the object.\"\"\"\n    self.verbose = verbose\n\n    self.best_pl: defaultdict[Job, dict[int, int]] = defaultdict(dict)\n    self.best_cost: defaultdict[Job, dict[int, float]] = defaultdict(dict)\n    self.observe_count: defaultdict[Job, defaultdict[int, int]] = defaultdict(lambda: defaultdict(int))\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.JITPowerLimitOptimizer.predict","title":"predict","text":"<pre><code>predict(job, batch_size)\n</code></pre> <p>Return the best power limit for the job, or None if unknown.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def predict(self, job: Job, batch_size: int) -&gt; int | None:\n    \"\"\"Return the best power limit for the job, or None if unknown.\"\"\"\n    pred = self.best_pl[job].get(batch_size)\n    if self.verbose:\n        self._log(\n            f\"{job} @ {batch_size} -&gt; \\033[31mPL = {'needs profiling' if pred is None else str(pred) + 'W'}\\033[0m\"\n        )\n    return pred\n</code></pre>"},{"location":"reference/_legacy/policy/optimizer/#zeus._legacy.policy.optimizer.JITPowerLimitOptimizer.observe","title":"observe","text":"<pre><code>observe(job, batch_size, power_limit, cost)\n</code></pre> <p>Learn from the cost of using the given knobs for the job.</p> Source code in <code>zeus/_legacy/policy/optimizer.py</code> <pre><code>def observe(self, job: Job, batch_size: int, power_limit: int, cost: float) -&gt; None:\n    \"\"\"Learn from the cost of using the given knobs for the job.\"\"\"\n    self.observe_count[job][batch_size] += 1\n    prev_best_cost = self.best_cost[job].get(batch_size)\n    if prev_best_cost is None or prev_best_cost &gt; cost:\n        self.best_pl[job][batch_size] = power_limit\n        self.best_cost[job][batch_size] = cost\n</code></pre>"},{"location":"reference/device/","title":"device","text":""},{"location":"reference/device/#zeus.device","title":"zeus.device","text":"<p>Abstraction layer over devices like GPUs.</p>"},{"location":"reference/device/common/","title":"common","text":""},{"location":"reference/device/common/#zeus.device.common","title":"zeus.device.common","text":"<p>Common utilities for device management.</p>"},{"location":"reference/device/common/#zeus.device.common.DeprecatedAliasABCMeta","title":"DeprecatedAliasABCMeta","text":"<p>               Bases: <code>ABCMeta</code></p> <p>Metaclass that combines ABC functionality with automatic deprecated alias creation.</p> <p>This metaclass looks for methods decorated with <code>@deprecated_alias</code> and automatically creates the old camelCase method names that emit deprecation warnings once and then call the new snake_case methods.</p> <p>Since this is frequently composed with <code>abc.ABCMeta</code>, this metaclass inherits from it to avoid metaclass conflicts.</p> <p>Example</p> <pre><code>class MyClass(abc.ABC, metaclass=DeprecatedAliasABCMeta):\n    @deprecated_alias(\"oldMethod\")\n    @abc.abstractmethod\n    def new_method(self):\n        pass\n\nclass MyImplementation(MyClass):\n    def new_method(self):\n        return \"implementation\"\n\nobj = MyImplementation()\nobj.new_method()  # No warning\nobj.oldMethod()   # Emits deprecation warning, calls new_method\n</code></pre> Source code in <code>zeus/device/common.py</code> <pre><code>class DeprecatedAliasABCMeta(abc.ABCMeta):\n    \"\"\"Metaclass that combines ABC functionality with automatic deprecated alias creation.\n\n    This metaclass looks for methods decorated with `@deprecated_alias` and automatically\n    creates the old camelCase method names that emit deprecation warnings once and then\n    call the new snake_case methods.\n\n    Since this is frequently composed with `abc.ABCMeta`, this metaclass inherits from it\n    to avoid metaclass conflicts.\n\n    !!! Example\n        ```python\n        class MyClass(abc.ABC, metaclass=DeprecatedAliasABCMeta):\n            @deprecated_alias(\"oldMethod\")\n            @abc.abstractmethod\n            def new_method(self):\n                pass\n\n        class MyImplementation(MyClass):\n            def new_method(self):\n                return \"implementation\"\n\n        obj = MyImplementation()\n        obj.new_method()  # No warning\n        obj.oldMethod()   # Emits deprecation warning, calls new_method\n        ```\n    \"\"\"\n\n    def __new__(mcs, name, bases, namespace):\n        \"\"\"Create the class and add deprecated alias methods.\"\"\"\n        cls = super().__new__(mcs, name, bases, namespace)\n\n        # Create deprecated aliases for methods marked with @deprecated_alias\n        for attr_name in dir(cls):\n            if attr_name.startswith(\"_\"):\n                continue\n\n            try:\n                attr = getattr(cls, attr_name)\n            except AttributeError:\n                continue\n\n            # Check if this method has the deprecated alias marker\n            if hasattr(attr, \"_deprecated_alias\"):\n                old_name = attr._deprecated_alias\n                # Create and attach the deprecated wrapper method\n                deprecated_method = _make_deprecated_method(attr, old_name, attr_name)\n                setattr(cls, old_name, deprecated_method)\n\n        return cls\n</code></pre>"},{"location":"reference/device/common/#zeus.device.common.DeprecatedAliasABCMeta.__new__","title":"__new__","text":"<pre><code>__new__(mcs, name, bases, namespace)\n</code></pre> <p>Create the class and add deprecated alias methods.</p> Source code in <code>zeus/device/common.py</code> <pre><code>def __new__(mcs, name, bases, namespace):\n    \"\"\"Create the class and add deprecated alias methods.\"\"\"\n    cls = super().__new__(mcs, name, bases, namespace)\n\n    # Create deprecated aliases for methods marked with @deprecated_alias\n    for attr_name in dir(cls):\n        if attr_name.startswith(\"_\"):\n            continue\n\n        try:\n            attr = getattr(cls, attr_name)\n        except AttributeError:\n            continue\n\n        # Check if this method has the deprecated alias marker\n        if hasattr(attr, \"_deprecated_alias\"):\n            old_name = attr._deprecated_alias\n            # Create and attach the deprecated wrapper method\n            deprecated_method = _make_deprecated_method(attr, old_name, attr_name)\n            setattr(cls, old_name, deprecated_method)\n\n    return cls\n</code></pre>"},{"location":"reference/device/common/#zeus.device.common.has_sys_admin","title":"has_sys_admin  <code>cached</code>","text":"<pre><code>has_sys_admin()\n</code></pre> <p>Check if the current process has <code>SYS_ADMIN</code> capabilities.</p> Source code in <code>zeus/device/common.py</code> <pre><code>@lru_cache(maxsize=1)\ndef has_sys_admin() -&gt; bool:\n    \"\"\"Check if the current process has `SYS_ADMIN` capabilities.\"\"\"\n    # First try to read procfs.\n    try:\n        with open(\"/proc/self/status\") as f:\n            for line in f:\n                if line.startswith(\"CapEff\"):\n                    bitmask = int(line.strip().split()[1], 16)\n                    has = bool(bitmask &amp; (1 &lt;&lt; 21))\n                    logger.info(\n                        \"Read security capabilities from /proc/self/status -- SYS_ADMIN: %s\",\n                        has,\n                    )\n                    return has\n    except Exception:\n        logger.info(\"Failed to read capabilities from /proc/self/status\", exc_info=True)\n\n    # If that fails, try to use the capget syscall.\n    class CapHeader(ctypes.Structure):\n        _fields_ = [(\"version\", ctypes.c_uint32), (\"pid\", ctypes.c_int)]\n\n    class CapData(ctypes.Structure):\n        _fields_ = [\n            (\"effective\", ctypes.c_uint32),\n            (\"permitted\", ctypes.c_uint32),\n            (\"inheritable\", ctypes.c_uint32),\n        ]\n\n    # Attempt to load libc and set up capget\n    try:\n        libc = ctypes.CDLL(\"libc.so.6\")\n        capget = libc.capget\n        capget.argtypes = [ctypes.POINTER(CapHeader), ctypes.POINTER(CapData)]\n        capget.restype = ctypes.c_int\n    except Exception:\n        logger.info(\"Failed to load libc.so.6\", exc_info=True)\n        return False\n\n    # Initialize the header and data structures\n    header = CapHeader(version=0x20080522, pid=0)  # Use the current process\n    data = CapData()\n\n    # Call capget and check for errors\n    if capget(ctypes.byref(header), ctypes.byref(data)) != 0:\n        errno = ctypes.get_errno()\n        logger.info(\"capget failed with error: %s (errno %s)\", os.strerror(errno), errno)\n        return False\n\n    bitmask = data.effective\n    has = bool(bitmask &amp; (1 &lt;&lt; 21))\n    logger.info(\"Read security capabilities from capget -- SYS_ADMIN: %s\", has)\n    return has\n</code></pre>"},{"location":"reference/device/common/#zeus.device.common.deprecated_alias","title":"deprecated_alias","text":"<pre><code>deprecated_alias(old_name)\n</code></pre> <p>Decorator that marks a method to have a deprecated camelCase alias.</p> <p>Apply this decorator to the new snake_case method. When the old camelCase name is called, it will emit a deprecation warning once and then call the new snake_case method.</p> Example <pre><code>@deprecated_alias(\"getName\")\ndef get_name(self):\n    return \"GPU Name\"\n</code></pre> <p>The class using this decorator should use <code>DeprecatedAliasABCMeta</code> as its metaclass.</p> <p>Parameters:</p> Name Type Description Default <code>old_name</code> <code>str</code> <p>The old camelCase method name to create as a deprecated alias.</p> required <p>Returns:</p> Type Description <code>Callable[[Callable], Callable]</code> <p>The decorated function with the <code>_deprecated_alias</code> attribute set.</p> Source code in <code>zeus/device/common.py</code> <pre><code>def deprecated_alias(old_name: str) -&gt; Callable[[Callable], Callable]:\n    \"\"\"Decorator that marks a method to have a deprecated camelCase alias.\n\n    Apply this decorator to the new snake_case method. When the old camelCase\n    name is called, it will emit a deprecation warning once and then call the\n    new snake_case method.\n\n    Example:\n        ```python\n        @deprecated_alias(\"getName\")\n        def get_name(self):\n            return \"GPU Name\"\n        ```\n\n    The class using this decorator should use `DeprecatedAliasABCMeta` as its metaclass.\n\n    Args:\n        old_name: The old camelCase method name to create as a deprecated alias.\n\n    Returns:\n        The decorated function with the `_deprecated_alias` attribute set.\n    \"\"\"\n\n    def decorator(func):\n        func._deprecated_alias = old_name\n        return func\n\n    return decorator\n</code></pre>"},{"location":"reference/device/common/#zeus.device.common._make_deprecated_method","title":"_make_deprecated_method","text":"<pre><code>_make_deprecated_method(new_method, old_name, new_name)\n</code></pre> <p>Create a deprecated method wrapper that warns once globally per method.</p> <p>Parameters:</p> Name Type Description Default <code>new_method</code> <code>Callable</code> <p>The new snake_case method to be called.</p> required <code>old_name</code> <code>str</code> <p>The old camelCase method name (for the warning message).</p> required <code>new_name</code> <code>str</code> <p>The new snake_case method name (for the warning message).</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>A wrapper function that emits a deprecation warning and calls the new method.</p> Source code in <code>zeus/device/common.py</code> <pre><code>def _make_deprecated_method(new_method: Callable, old_name: str, new_name: str) -&gt; Callable:\n    \"\"\"Create a deprecated method wrapper that warns once globally per method.\n\n    Args:\n        new_method: The new snake_case method to be called.\n        old_name: The old camelCase method name (for the warning message).\n        new_name: The new snake_case method name (for the warning message).\n\n    Returns:\n        A wrapper function that emits a deprecation warning and calls the new method.\n    \"\"\"\n    warned = [False]\n\n    @functools.wraps(new_method)\n    def deprecated_method(self, *args, **kwargs):\n        if not warned[0]:\n            warnings.warn(\n                f\"'{old_name}' is deprecated and will be removed in a future version. Use '{new_name}' instead.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            warned[0] = True\n        return getattr(self, new_name)(*args, **kwargs)\n\n    # Remove the _deprecated_alias attribute to prevent recursion when the metaclass\n    # processes this deprecated method during class creation\n    if hasattr(deprecated_method, \"_deprecated_alias\"):\n        delattr(deprecated_method, \"_deprecated_alias\")\n\n    return deprecated_method\n</code></pre>"},{"location":"reference/device/exception/","title":"exception","text":""},{"location":"reference/device/exception/#zeus.device.exception","title":"zeus.device.exception","text":"<p>Base device exception classes.</p>"},{"location":"reference/device/exception/#zeus.device.exception.ZeusBaseGPUError","title":"ZeusBaseGPUError","text":"<p>               Bases: <code>ZeusBaseError</code></p> <p>Zeus base GPU exception class.</p> Source code in <code>zeus/device/exception.py</code> <pre><code>class ZeusBaseGPUError(ZeusBaseError):\n    \"\"\"Zeus base GPU exception class.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Base Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/exception/#zeus.device.exception.ZeusBaseGPUError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/exception.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Base Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/exception/#zeus.device.exception.ZeusBaseCPUError","title":"ZeusBaseCPUError","text":"<p>               Bases: <code>ZeusBaseError</code></p> <p>Zeus base CPU exception class.</p> Source code in <code>zeus/device/exception.py</code> <pre><code>class ZeusBaseCPUError(ZeusBaseError):\n    \"\"\"Zeus base CPU exception class.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Base Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/exception/#zeus.device.exception.ZeusBaseCPUError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/exception.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Base Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/exception/#zeus.device.exception.ZeusBaseSoCError","title":"ZeusBaseSoCError","text":"<p>               Bases: <code>ZeusBaseError</code></p> <p>Zeus base SoC exception class.</p> Source code in <code>zeus/device/exception.py</code> <pre><code>class ZeusBaseSoCError(ZeusBaseError):\n    \"\"\"Zeus base SoC exception class.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Base Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/exception/#zeus.device.exception.ZeusBaseSoCError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/exception.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Base Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/exception/#zeus.device.exception.ZeusdError","title":"ZeusdError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Exception class for Zeus daemon-related errors.</p> Source code in <code>zeus/device/exception.py</code> <pre><code>class ZeusdError(ZeusBaseGPUError):\n    \"\"\"Exception class for Zeus daemon-related errors.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Zeusd error.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/exception/#zeus.device.exception.ZeusdError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/exception.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Zeusd error.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/cpu/","title":"cpu","text":""},{"location":"reference/device/cpu/#zeus.device.cpu","title":"zeus.device.cpu","text":"<p>Abstraction layer for CPU devices.</p> <p>The main function of this module is <code>get_cpus</code>, which returns a CPU Manager object specific to the platform.</p>"},{"location":"reference/device/cpu/#zeus.device.cpu.get_current_cpu_index","title":"get_current_cpu_index","text":"<pre><code>get_current_cpu_index(pid='current')\n</code></pre> <p>Retrieves the specific CPU index (socket) where the given PID is running.</p> <p>If no PID is given or pid is \"current\", the CPU index returned is of the CPU running the current process.</p> <p>Note</p> <p>Linux schedulers can preempt and reschedule processes to different CPUs. To prevent this from happening during monitoring, use <code>taskset</code> to pin processes to specific CPUs.</p> Source code in <code>zeus/device/cpu/__init__.py</code> <pre><code>def get_current_cpu_index(pid: int | Literal[\"current\"] = \"current\") -&gt; int:\n    \"\"\"Retrieves the specific CPU index (socket) where the given PID is running.\n\n    If no PID is given or pid is \"current\", the CPU index returned is of the CPU running the current process.\n\n    !!! Note\n        Linux schedulers can preempt and reschedule processes to different CPUs. To prevent this from happening\n        during monitoring, use `taskset` to pin processes to specific CPUs.\n    \"\"\"\n    if pid == \"current\":\n        pid = os.getpid()\n\n    with open(f\"/proc/{pid}/stat\") as stat_file:\n        cpu_core = int(stat_file.read().split()[38])\n\n    with open(f\"/sys/devices/system/cpu/cpu{cpu_core}/topology/physical_package_id\") as phys_package_file:\n        return int(phys_package_file.read().strip())\n</code></pre>"},{"location":"reference/device/cpu/#zeus.device.cpu.get_cpus","title":"get_cpus","text":"<pre><code>get_cpus()\n</code></pre> <p>Initialize and return a singleton CPU monitoring object for INTEL CPUs.</p> <p>The function returns a CPU management object that aims to abstract the underlying CPU monitoring libraries (RAPL for Intel CPUs).</p> <p>This function attempts to initialize CPU mointoring using RAPL. If this attempt fails, it raises a ZeusErrorInit exception.</p> Source code in <code>zeus/device/cpu/__init__.py</code> <pre><code>def get_cpus() -&gt; CPUs:\n    \"\"\"Initialize and return a singleton CPU monitoring object for INTEL CPUs.\n\n    The function returns a CPU management object that aims to abstract the underlying CPU monitoring libraries\n    (RAPL for Intel CPUs).\n\n    This function attempts to initialize CPU mointoring using RAPL. If this attempt fails, it raises\n    a ZeusErrorInit exception.\n    \"\"\"\n    global _cpus\n    if _cpus is not None:\n        return _cpus\n    if rapl_is_available():\n        _cpus = RAPLCPUs()\n        return _cpus\n    else:\n        raise ZeusCPUInitError(\"RAPL unvailable Failed to initialize CPU management library.\")\n</code></pre>"},{"location":"reference/device/cpu/common/","title":"common","text":""},{"location":"reference/device/cpu/common/#zeus.device.cpu.common","title":"zeus.device.cpu.common","text":"<p>Error wrappers and classes common to all CPU vendors.</p>"},{"location":"reference/device/cpu/common/#zeus.device.cpu.common.CpuDramMeasurement","title":"CpuDramMeasurement  <code>dataclass</code>","text":"<p>Represents a measurement of CPU and DRAM energy consumption.</p> <p>Attributes:</p> Name Type Description <code>cpu_mj</code> <code>int</code> <p>The CPU energy consumption in millijoules.</p> <code>dram_mj</code> <code>Optional[int]</code> <p>The DRAM energy consumption in millijoules. Defaults to None.</p> Source code in <code>zeus/device/cpu/common.py</code> <pre><code>@dataclass\nclass CpuDramMeasurement:\n    \"\"\"Represents a measurement of CPU and DRAM energy consumption.\n\n    Attributes:\n        cpu_mj (int): The CPU energy consumption in millijoules.\n        dram_mj (Optional[int]): The DRAM energy consumption in millijoules. Defaults to None.\n    \"\"\"\n\n    cpu_mj: float\n    dram_mj: float | None = None\n\n    def __sub__(self, other: CpuDramMeasurement) -&gt; CpuDramMeasurement:\n        \"\"\"Subtracts the values of another CpuDramMeasurement from this one.\n\n        Args:\n            other (CpuDramMeasurement): The other CpuDramMeasurement to subtract.\n\n        Returns:\n            CpuDramMeasurement: A new CpuDramMeasurement with the result of the subtraction.\n        \"\"\"\n        dram_mj = None\n        if self.dram_mj is not None and other.dram_mj is not None:\n            dram_mj = self.dram_mj - other.dram_mj\n        elif self.dram_mj is not None:\n            dram_mj = self.dram_mj\n        elif other.dram_mj is not None:\n            dram_mj = -other.dram_mj\n        return CpuDramMeasurement(self.cpu_mj - other.cpu_mj, dram_mj)\n\n    def __truediv__(self, other: int | float) -&gt; CpuDramMeasurement:\n        \"\"\"Divides the values of this CpuDramMeasurement by a float.\n\n        Args:\n            other: The float to divide by.\n\n        Returns:\n            CpuDramMeasurement: A new CpuDramMeasurement with the result of the division.\n\n        Raises:\n            ZeroDivisionError: If division by zero is attempted.\n        \"\"\"\n        if isinstance(other, (int, float)):\n            if other == 0:\n                raise ZeroDivisionError(\"Division by zero is not allowed\")\n            dram_mj = None\n            if self.dram_mj is not None:\n                dram_mj = self.dram_mj / other\n            return CpuDramMeasurement(self.cpu_mj / other, dram_mj)\n        else:\n            return NotImplemented\n</code></pre>"},{"location":"reference/device/cpu/common/#zeus.device.cpu.common.CpuDramMeasurement.__sub__","title":"__sub__","text":"<pre><code>__sub__(other)\n</code></pre> <p>Subtracts the values of another CpuDramMeasurement from this one.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>CpuDramMeasurement</code> <p>The other CpuDramMeasurement to subtract.</p> required <p>Returns:</p> Name Type Description <code>CpuDramMeasurement</code> <code>CpuDramMeasurement</code> <p>A new CpuDramMeasurement with the result of the subtraction.</p> Source code in <code>zeus/device/cpu/common.py</code> <pre><code>def __sub__(self, other: CpuDramMeasurement) -&gt; CpuDramMeasurement:\n    \"\"\"Subtracts the values of another CpuDramMeasurement from this one.\n\n    Args:\n        other (CpuDramMeasurement): The other CpuDramMeasurement to subtract.\n\n    Returns:\n        CpuDramMeasurement: A new CpuDramMeasurement with the result of the subtraction.\n    \"\"\"\n    dram_mj = None\n    if self.dram_mj is not None and other.dram_mj is not None:\n        dram_mj = self.dram_mj - other.dram_mj\n    elif self.dram_mj is not None:\n        dram_mj = self.dram_mj\n    elif other.dram_mj is not None:\n        dram_mj = -other.dram_mj\n    return CpuDramMeasurement(self.cpu_mj - other.cpu_mj, dram_mj)\n</code></pre>"},{"location":"reference/device/cpu/common/#zeus.device.cpu.common.CpuDramMeasurement.__truediv__","title":"__truediv__","text":"<pre><code>__truediv__(other)\n</code></pre> <p>Divides the values of this CpuDramMeasurement by a float.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>int | float</code> <p>The float to divide by.</p> required <p>Returns:</p> Name Type Description <code>CpuDramMeasurement</code> <code>CpuDramMeasurement</code> <p>A new CpuDramMeasurement with the result of the division.</p> <p>Raises:</p> Type Description <code>ZeroDivisionError</code> <p>If division by zero is attempted.</p> Source code in <code>zeus/device/cpu/common.py</code> <pre><code>def __truediv__(self, other: int | float) -&gt; CpuDramMeasurement:\n    \"\"\"Divides the values of this CpuDramMeasurement by a float.\n\n    Args:\n        other: The float to divide by.\n\n    Returns:\n        CpuDramMeasurement: A new CpuDramMeasurement with the result of the division.\n\n    Raises:\n        ZeroDivisionError: If division by zero is attempted.\n    \"\"\"\n    if isinstance(other, (int, float)):\n        if other == 0:\n            raise ZeroDivisionError(\"Division by zero is not allowed\")\n        dram_mj = None\n        if self.dram_mj is not None:\n            dram_mj = self.dram_mj / other\n        return CpuDramMeasurement(self.cpu_mj / other, dram_mj)\n    else:\n        return NotImplemented\n</code></pre>"},{"location":"reference/device/cpu/common/#zeus.device.cpu.common.ZeusCPUInitError","title":"ZeusCPUInitError","text":"<p>               Bases: <code>ZeusBaseCPUError</code></p> <p>Import error or CPU library initialization failures.</p> Source code in <code>zeus/device/cpu/common.py</code> <pre><code>class ZeusCPUInitError(ZeusBaseCPUError):\n    \"\"\"Import error or CPU library initialization failures.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/cpu/common/#zeus.device.cpu.common.ZeusCPUInitError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/cpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/cpu/common/#zeus.device.cpu.common.ZeusCPUNoPermissionError","title":"ZeusCPUNoPermissionError","text":"<p>               Bases: <code>ZeusBaseCPUError</code></p> <p>Zeus CPU exception class wrapper for No Permission to perform CPU operation.</p> Source code in <code>zeus/device/cpu/common.py</code> <pre><code>class ZeusCPUNoPermissionError(ZeusBaseCPUError):\n    \"\"\"Zeus CPU exception class wrapper for No Permission to perform CPU operation.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/cpu/common/#zeus.device.cpu.common.ZeusCPUNoPermissionError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/cpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/cpu/common/#zeus.device.cpu.common.ZeusCPUNotFoundError","title":"ZeusCPUNotFoundError","text":"<p>               Bases: <code>ZeusBaseCPUError</code></p> <p>Zeus CPU exception class wrapper for Not Found CPU.</p> Source code in <code>zeus/device/cpu/common.py</code> <pre><code>class ZeusCPUNotFoundError(ZeusBaseCPUError):\n    \"\"\"Zeus CPU exception class wrapper for Not Found CPU.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/cpu/common/#zeus.device.cpu.common.ZeusCPUNotFoundError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/cpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/cpu/common/#zeus.device.cpu.common.CPU","title":"CPU","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for CPU management.</p> <p>This class defines the interface for interacting with CPUs, subclasses should implement the methods to interact with specific CPU libraries.</p> Source code in <code>zeus/device/cpu/common.py</code> <pre><code>class CPU(abc.ABC, metaclass=DeprecatedAliasABCMeta):\n    \"\"\"Abstract base class for CPU management.\n\n    This class defines the interface for interacting with CPUs, subclasses should implement the methods to interact with specific CPU libraries.\n    \"\"\"\n\n    def __init__(self, cpu_index: int) -&gt; None:\n        \"\"\"Initialize the CPU with a specified index.\"\"\"\n        self.cpu_index = cpu_index\n\n    @deprecated_alias(\"getTotalEnergyConsumption\")\n    @abc.abstractmethod\n    def get_total_energy_consumption(self) -&gt; CpuDramMeasurement:\n        \"\"\"Returns the total energy consumption of the specified powerzone. Units: mJ.\"\"\"\n        pass\n\n    @deprecated_alias(\"supportsGetDramEnergyConsumption\")\n    @abc.abstractmethod\n    def supports_get_dram_energy_consumption(self) -&gt; bool:\n        \"\"\"Returns True if the specified CPU powerzone supports retrieving the subpackage energy consumption.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/device/cpu/common/#zeus.device.cpu.common.CPU.__init__","title":"__init__","text":"<pre><code>__init__(cpu_index)\n</code></pre> Source code in <code>zeus/device/cpu/common.py</code> <pre><code>def __init__(self, cpu_index: int) -&gt; None:\n    \"\"\"Initialize the CPU with a specified index.\"\"\"\n    self.cpu_index = cpu_index\n</code></pre>"},{"location":"reference/device/cpu/common/#zeus.device.cpu.common.CPU.get_total_energy_consumption","title":"get_total_energy_consumption  <code>abstractmethod</code>","text":"<pre><code>get_total_energy_consumption()\n</code></pre> <p>Returns the total energy consumption of the specified powerzone. Units: mJ.</p> Source code in <code>zeus/device/cpu/common.py</code> <pre><code>@deprecated_alias(\"getTotalEnergyConsumption\")\n@abc.abstractmethod\ndef get_total_energy_consumption(self) -&gt; CpuDramMeasurement:\n    \"\"\"Returns the total energy consumption of the specified powerzone. Units: mJ.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/cpu/common/#zeus.device.cpu.common.CPU.supports_get_dram_energy_consumption","title":"supports_get_dram_energy_consumption  <code>abstractmethod</code>","text":"<pre><code>supports_get_dram_energy_consumption()\n</code></pre> <p>Returns True if the specified CPU powerzone supports retrieving the subpackage energy consumption.</p> Source code in <code>zeus/device/cpu/common.py</code> <pre><code>@deprecated_alias(\"supportsGetDramEnergyConsumption\")\n@abc.abstractmethod\ndef supports_get_dram_energy_consumption(self) -&gt; bool:\n    \"\"\"Returns True if the specified CPU powerzone supports retrieving the subpackage energy consumption.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/cpu/common/#zeus.device.cpu.common.CPUs","title":"CPUs","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract base class for CPU manager object.</p> <p>This class defines the essential interface and common functionality for CPU management, instantiating multiple <code>CPU</code> objects for each CPU being tracked. Forwards the call for a specific method to the corresponding CPU object.</p> Source code in <code>zeus/device/cpu/common.py</code> <pre><code>class CPUs(abc.ABC, metaclass=DeprecatedAliasABCMeta):\n    \"\"\"An abstract base class for CPU manager object.\n\n    This class defines the essential interface and common functionality for CPU management, instantiating multiple `CPU` objects for each CPU being tracked.\n    Forwards the call for a specific method to the corresponding CPU object.\n    \"\"\"\n\n    @abc.abstractmethod\n    def __init__(self) -&gt; None:\n        \"\"\"Initializes the CPU management library to communicate with the CPU driver and sets up tracking for specified CPUs.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def __del__(self) -&gt; None:\n        \"\"\"Shuts down the CPU monitoring library to release resources and clean up.\"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def cpus(self) -&gt; Sequence[CPU]:\n        \"\"\"Returns a list of CPU objects being tracked.\"\"\"\n        pass\n\n    @deprecated_alias(\"getTotalEnergyConsumption\")\n    def get_total_energy_consumption(self, index: int) -&gt; CpuDramMeasurement:\n        \"\"\"Returns the total energy consumption of the specified powerzone. Units: mJ.\"\"\"\n        return self.cpus[index].get_total_energy_consumption()\n\n    @deprecated_alias(\"supportsGetDramEnergyConsumption\")\n    def supports_get_dram_energy_consumption(self, index: int) -&gt; bool:\n        \"\"\"Returns True if the specified CPU powerzone supports retrieving the subpackage energy consumption.\"\"\"\n        return self.cpus[index].supports_get_dram_energy_consumption()\n\n    def __len__(self) -&gt; int:\n        \"\"\"Returns the number of CPUs being tracked.\"\"\"\n        return len(self.cpus)\n</code></pre>"},{"location":"reference/device/cpu/common/#zeus.device.cpu.common.CPUs.cpus","title":"cpus  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>cpus\n</code></pre> <p>Returns a list of CPU objects being tracked.</p>"},{"location":"reference/device/cpu/common/#zeus.device.cpu.common.CPUs.__init__","title":"__init__  <code>abstractmethod</code>","text":"<pre><code>__init__()\n</code></pre> Source code in <code>zeus/device/cpu/common.py</code> <pre><code>@abc.abstractmethod\ndef __init__(self) -&gt; None:\n    \"\"\"Initializes the CPU management library to communicate with the CPU driver and sets up tracking for specified CPUs.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/cpu/common/#zeus.device.cpu.common.CPUs.__del__","title":"__del__  <code>abstractmethod</code>","text":"<pre><code>__del__()\n</code></pre> <p>Shuts down the CPU monitoring library to release resources and clean up.</p> Source code in <code>zeus/device/cpu/common.py</code> <pre><code>@abc.abstractmethod\ndef __del__(self) -&gt; None:\n    \"\"\"Shuts down the CPU monitoring library to release resources and clean up.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/cpu/common/#zeus.device.cpu.common.CPUs.get_total_energy_consumption","title":"get_total_energy_consumption","text":"<pre><code>get_total_energy_consumption(index)\n</code></pre> <p>Returns the total energy consumption of the specified powerzone. Units: mJ.</p> Source code in <code>zeus/device/cpu/common.py</code> <pre><code>@deprecated_alias(\"getTotalEnergyConsumption\")\ndef get_total_energy_consumption(self, index: int) -&gt; CpuDramMeasurement:\n    \"\"\"Returns the total energy consumption of the specified powerzone. Units: mJ.\"\"\"\n    return self.cpus[index].get_total_energy_consumption()\n</code></pre>"},{"location":"reference/device/cpu/common/#zeus.device.cpu.common.CPUs.supports_get_dram_energy_consumption","title":"supports_get_dram_energy_consumption","text":"<pre><code>supports_get_dram_energy_consumption(index)\n</code></pre> <p>Returns True if the specified CPU powerzone supports retrieving the subpackage energy consumption.</p> Source code in <code>zeus/device/cpu/common.py</code> <pre><code>@deprecated_alias(\"supportsGetDramEnergyConsumption\")\ndef supports_get_dram_energy_consumption(self, index: int) -&gt; bool:\n    \"\"\"Returns True if the specified CPU powerzone supports retrieving the subpackage energy consumption.\"\"\"\n    return self.cpus[index].supports_get_dram_energy_consumption()\n</code></pre>"},{"location":"reference/device/cpu/common/#zeus.device.cpu.common.CPUs.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Returns the number of CPUs being tracked.</p> Source code in <code>zeus/device/cpu/common.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of CPUs being tracked.\"\"\"\n    return len(self.cpus)\n</code></pre>"},{"location":"reference/device/cpu/common/#zeus.device.cpu.common.EmptyCPUs","title":"EmptyCPUs","text":"<p>               Bases: <code>CPUs</code></p> <p>Empty CPUs management object to be used when CPUs management object is unavailable.</p> <p>Calls to any methods will return a value error and the length of this object will be 0</p> Source code in <code>zeus/device/cpu/common.py</code> <pre><code>class EmptyCPUs(CPUs):\n    \"\"\"Empty CPUs management object to be used when CPUs management object is unavailable.\n\n    Calls to any methods will return a value error and the length of this object will be 0\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Instantiates empty CPUs object.\"\"\"\n        pass\n\n    def __del__(self) -&gt; None:\n        \"\"\"Shuts down the Intel CPU monitoring.\"\"\"\n        pass\n\n    @property\n    def cpus(self) -&gt; Sequence[CPU]:\n        \"\"\"Returns a list of CPU objects being tracked.\"\"\"\n        return []\n\n    @deprecated_alias(\"getTotalEnergyConsumption\")\n    def get_total_energy_consumption(self, index: int) -&gt; CpuDramMeasurement:\n        \"\"\"Returns the total energy consumption of the specified powerzone. Units: mJ.\"\"\"\n        raise ValueError(\"No CPUs available.\")\n\n    @deprecated_alias(\"supportsGetDramEnergyConsumption\")\n    def supports_get_dram_energy_consumption(self, index: int) -&gt; bool:\n        \"\"\"Returns True if the specified CPU powerzone supports retrieving the subpackage energy consumption.\"\"\"\n        raise ValueError(\"No CPUs available.\")\n\n    def __len__(self) -&gt; int:\n        \"\"\"Returns 0 since the object is empty.\"\"\"\n        return 0\n</code></pre>"},{"location":"reference/device/cpu/common/#zeus.device.cpu.common.EmptyCPUs.cpus","title":"cpus  <code>property</code>","text":"<pre><code>cpus\n</code></pre> <p>Returns a list of CPU objects being tracked.</p>"},{"location":"reference/device/cpu/common/#zeus.device.cpu.common.EmptyCPUs.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> Source code in <code>zeus/device/cpu/common.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Instantiates empty CPUs object.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/cpu/common/#zeus.device.cpu.common.EmptyCPUs.__del__","title":"__del__","text":"<pre><code>__del__()\n</code></pre> <p>Shuts down the Intel CPU monitoring.</p> Source code in <code>zeus/device/cpu/common.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Shuts down the Intel CPU monitoring.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/cpu/common/#zeus.device.cpu.common.EmptyCPUs.get_total_energy_consumption","title":"get_total_energy_consumption","text":"<pre><code>get_total_energy_consumption(index)\n</code></pre> <p>Returns the total energy consumption of the specified powerzone. Units: mJ.</p> Source code in <code>zeus/device/cpu/common.py</code> <pre><code>@deprecated_alias(\"getTotalEnergyConsumption\")\ndef get_total_energy_consumption(self, index: int) -&gt; CpuDramMeasurement:\n    \"\"\"Returns the total energy consumption of the specified powerzone. Units: mJ.\"\"\"\n    raise ValueError(\"No CPUs available.\")\n</code></pre>"},{"location":"reference/device/cpu/common/#zeus.device.cpu.common.EmptyCPUs.supports_get_dram_energy_consumption","title":"supports_get_dram_energy_consumption","text":"<pre><code>supports_get_dram_energy_consumption(index)\n</code></pre> <p>Returns True if the specified CPU powerzone supports retrieving the subpackage energy consumption.</p> Source code in <code>zeus/device/cpu/common.py</code> <pre><code>@deprecated_alias(\"supportsGetDramEnergyConsumption\")\ndef supports_get_dram_energy_consumption(self, index: int) -&gt; bool:\n    \"\"\"Returns True if the specified CPU powerzone supports retrieving the subpackage energy consumption.\"\"\"\n    raise ValueError(\"No CPUs available.\")\n</code></pre>"},{"location":"reference/device/cpu/common/#zeus.device.cpu.common.EmptyCPUs.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Returns 0 since the object is empty.</p> Source code in <code>zeus/device/cpu/common.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns 0 since the object is empty.\"\"\"\n    return 0\n</code></pre>"},{"location":"reference/device/cpu/rapl/","title":"rapl","text":""},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl","title":"zeus.device.cpu.rapl","text":"<p>RAPL CPUs.</p> <ul> <li> <p>RAPL (Running Average Power Limit):    RAPL is a technology introduced by Intel that allows for power consumption monitoring and control at the processor and memory subsystem level. It provides mechanisms to enforce power limits and manage thermal conditions effectively.</p> </li> <li> <p>Power Zone:    A power zone in the context of RAPL refers to a logical grouping of components within the CPU or system that share a common power domain. Each power zone can be monitored and controlled independently. Typical power zones include the entire package, specific cores, and memory subsystems.</p> </li> <li> <p>Package:    The package refers to the physical CPU chip, which may contain multiple cores and integrated components. In RAPL, the package power domain encompasses the power consumption of all the cores and integrated units within the CPU package.</p> </li> </ul>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl.RaplWraparoundTracker","title":"RaplWraparoundTracker","text":"<p>Monitor the wrapping around of RAPL counters.</p> <p>This class acts as a lower level wrapper around a Python process that polls the wrapping of RAPL counters. This is primarily used by <code>RAPLCPUs</code>.</p> <p>Warning</p> <p>Since the monitor spawns a child process, it should not be instantiated as a global variable. Python puts a protection to prevent creating a process in global scope. Refer to the \"Safe importing of main module\" section in the Python documentation for more details.</p> <p>Attributes:</p> Name Type Description <code>rapl_file_path</code> <code>str</code> <p>File path of rapl file to track wraparounds for.</p> <code>max_energy_uj</code> <code>float</code> <p>Max value of rapl counter for <code>rapl_file_path</code> file. Used to determine the sleep period between polls</p> Source code in <code>zeus/device/cpu/rapl.py</code> <pre><code>class RaplWraparoundTracker:\n    \"\"\"Monitor the wrapping around of RAPL counters.\n\n    This class acts as a lower level wrapper around a Python process that polls\n    the wrapping of RAPL counters. This is primarily used by\n    [`RAPLCPUs`][zeus.device.cpu.rapl.RAPLCPUs].\n\n    !!! Warning\n        Since the monitor spawns a child process, **it should not be instantiated as a global variable**.\n        Python puts a protection to prevent creating a process in global scope.\n        Refer to the \"Safe importing of main module\" section in the\n        [Python documentation](https://docs.python.org/3/library/multiprocessing.html#the-spawn-and-forkserver-start-methods)\n        for more details.\n\n    Attributes:\n        rapl_file_path (str): File path of rapl file to track wraparounds for.\n        max_energy_uj (float): Max value of rapl counter for `rapl_file_path` file. Used to\n            determine the sleep period between polls\n    \"\"\"\n\n    def __init__(\n        self,\n        rapl_file_path: str,\n        max_energy_uj: float,\n    ) -&gt; None:\n        \"\"\"Initialize the rapl monitor.\n\n        Args:\n            rapl_file_path: File path where the RAPL file is located\n            max_energy_uj: Max energy range uj value\n        \"\"\"\n        if not os.path.exists(rapl_file_path):\n            raise ValueError(f\"{rapl_file_path} is not a valid file path\")\n\n        # Set up logging.\n        logger.info(\"RaplWraparoundTracker is monitoring wrap around of %s\", rapl_file_path)\n\n        context = mp.get_context(\"spawn\")\n        self.wraparound_counter = context.Value(\"i\", 0)\n        # Spawn the power polling process.\n        atexit.register(self._stop)\n        self.process = context.Process(\n            target=_polling_process,\n            args=(rapl_file_path, max_energy_uj, self.wraparound_counter),\n        )\n        self.process.start()\n\n    def _stop(self) -&gt; None:\n        \"\"\"Stop monitoring power usage.\"\"\"\n        if self.process is not None:\n            self.process.terminate()\n            self.process.join(timeout=1.0)\n            self.process.kill()\n            self.process = None\n\n    def get_num_wraparounds(self) -&gt; int:\n        \"\"\"Get the number of wraparounds detected by the polling process.\"\"\"\n        with self.wraparound_counter.get_lock():\n            return self.wraparound_counter.value\n</code></pre>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl.RaplWraparoundTracker.__init__","title":"__init__","text":"<pre><code>__init__(rapl_file_path, max_energy_uj)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>rapl_file_path</code> <code>str</code> <p>File path where the RAPL file is located</p> required <code>max_energy_uj</code> <code>float</code> <p>Max energy range uj value</p> required Source code in <code>zeus/device/cpu/rapl.py</code> <pre><code>def __init__(\n    self,\n    rapl_file_path: str,\n    max_energy_uj: float,\n) -&gt; None:\n    \"\"\"Initialize the rapl monitor.\n\n    Args:\n        rapl_file_path: File path where the RAPL file is located\n        max_energy_uj: Max energy range uj value\n    \"\"\"\n    if not os.path.exists(rapl_file_path):\n        raise ValueError(f\"{rapl_file_path} is not a valid file path\")\n\n    # Set up logging.\n    logger.info(\"RaplWraparoundTracker is monitoring wrap around of %s\", rapl_file_path)\n\n    context = mp.get_context(\"spawn\")\n    self.wraparound_counter = context.Value(\"i\", 0)\n    # Spawn the power polling process.\n    atexit.register(self._stop)\n    self.process = context.Process(\n        target=_polling_process,\n        args=(rapl_file_path, max_energy_uj, self.wraparound_counter),\n    )\n    self.process.start()\n</code></pre>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl.RaplWraparoundTracker._stop","title":"_stop","text":"<pre><code>_stop()\n</code></pre> <p>Stop monitoring power usage.</p> Source code in <code>zeus/device/cpu/rapl.py</code> <pre><code>def _stop(self) -&gt; None:\n    \"\"\"Stop monitoring power usage.\"\"\"\n    if self.process is not None:\n        self.process.terminate()\n        self.process.join(timeout=1.0)\n        self.process.kill()\n        self.process = None\n</code></pre>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl.RaplWraparoundTracker.get_num_wraparounds","title":"get_num_wraparounds","text":"<pre><code>get_num_wraparounds()\n</code></pre> <p>Get the number of wraparounds detected by the polling process.</p> Source code in <code>zeus/device/cpu/rapl.py</code> <pre><code>def get_num_wraparounds(self) -&gt; int:\n    \"\"\"Get the number of wraparounds detected by the polling process.\"\"\"\n    with self.wraparound_counter.get_lock():\n        return self.wraparound_counter.value\n</code></pre>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl.ZeusRAPLNotSupportedError","title":"ZeusRAPLNotSupportedError","text":"<p>               Bases: <code>ZeusBaseCPUError</code></p> <p>Zeus CPU exception class wrapper for RAPL not supported on CPU.</p> Source code in <code>zeus/device/cpu/rapl.py</code> <pre><code>class ZeusRAPLNotSupportedError(ZeusBaseCPUError):\n    \"\"\"Zeus CPU exception class wrapper for RAPL not supported on CPU.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl.ZeusRAPLNotSupportedError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/cpu/rapl.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl.ZeusRAPLFileInitError","title":"ZeusRAPLFileInitError","text":"<p>               Bases: <code>ZeusBaseCPUError</code></p> <p>Zeus CPU exception class wrapper for RAPL file initialization error on CPU.</p> Source code in <code>zeus/device/cpu/rapl.py</code> <pre><code>class ZeusRAPLFileInitError(ZeusBaseCPUError):\n    \"\"\"Zeus CPU exception class wrapper for RAPL file initialization error on CPU.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl.ZeusRAPLFileInitError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/cpu/rapl.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl.ZeusRAPLPermissionError","title":"ZeusRAPLPermissionError","text":"<p>               Bases: <code>ZeusBaseCPUError</code></p> <p>Zeus GPU exception that wraps No Permission to perform GPU operation.</p> Source code in <code>zeus/device/cpu/rapl.py</code> <pre><code>class ZeusRAPLPermissionError(ZeusBaseCPUError):\n    \"\"\"Zeus GPU exception that wraps No Permission to perform GPU operation.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl.ZeusRAPLPermissionError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/cpu/rapl.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl.RAPLFile","title":"RAPLFile","text":"<p>RAPL File class for each RAPL file.</p> <p>This class defines the interface for interacting with a RAPL file for a package. A package can be a CPU or DRAM</p> Source code in <code>zeus/device/cpu/rapl.py</code> <pre><code>class RAPLFile:\n    \"\"\"RAPL File class for each RAPL file.\n\n    This class defines the interface for interacting with a RAPL file for a package. A package can\n    be a CPU or DRAM\n    \"\"\"\n\n    def __init__(self, path: str) -&gt; None:\n        \"\"\"Initialize RAPL file object. Each RAPL file object manages one energy_uj file.\"\"\"\n        self.path = path\n        self.energy_uj_path = os.path.join(path, \"energy_uj\")\n        try:\n            with open(os.path.join(path, \"name\"), \"r\") as name_file:\n                self.name: str = name_file.read().strip()\n        except FileNotFoundError as err:\n            raise ZeusRAPLFileInitError(\"Error reading package name\") from err\n        try:\n            with open(self.energy_uj_path) as energy_file:\n                self.last_energy = float(energy_file.read().strip())\n        except FileNotFoundError as err:\n            raise ZeusRAPLFileInitError(\"Error reading package energy\") from err\n        except PermissionError as err:\n            raise cpu_common.ZeusCPUNoPermissionError(\n                \"Can't read file due to permission error; RAPL requires root privileges. \"\n                \"Refer to https://ml.energy/zeus/getting_started/#system-privileges for more details.\"\n            ) from err\n        try:\n            with open(os.path.join(path, \"max_energy_range_uj\"), \"r\") as max_energy_file:\n                self.max_energy_range_uj = float(max_energy_file.read().strip())\n        except FileNotFoundError as err:\n            raise ZeusRAPLFileInitError(\"Error reading package max energy range\") from err\n\n        self.wraparound_tracker = RaplWraparoundTracker(self.energy_uj_path, self.max_energy_range_uj)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return a string representation of the RAPL file object.\"\"\"\n        return f\"RAPLFile(Path: {self.path}\\nEnergy_uj_path: {self.energy_uj_path}\\nName: {self.name}\\\n        \\nLast_energy: {self.last_energy}\\nMax_energy: {self.max_energy_range_uj})\"\n\n    def read(self) -&gt; float:\n        \"\"\"Read the current energy value from the energy_uj file.\n\n        Returns:\n            The current energy value in millijoules.\n        \"\"\"\n        with open(self.energy_uj_path) as energy_file:\n            new_energy_uj = float(energy_file.read().strip())\n        num_wraparounds = self.wraparound_tracker.get_num_wraparounds()\n        return (new_energy_uj + num_wraparounds * self.max_energy_range_uj) / 1000.0\n</code></pre>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl.RAPLFile.__init__","title":"__init__","text":"<pre><code>__init__(path)\n</code></pre> Source code in <code>zeus/device/cpu/rapl.py</code> <pre><code>def __init__(self, path: str) -&gt; None:\n    \"\"\"Initialize RAPL file object. Each RAPL file object manages one energy_uj file.\"\"\"\n    self.path = path\n    self.energy_uj_path = os.path.join(path, \"energy_uj\")\n    try:\n        with open(os.path.join(path, \"name\"), \"r\") as name_file:\n            self.name: str = name_file.read().strip()\n    except FileNotFoundError as err:\n        raise ZeusRAPLFileInitError(\"Error reading package name\") from err\n    try:\n        with open(self.energy_uj_path) as energy_file:\n            self.last_energy = float(energy_file.read().strip())\n    except FileNotFoundError as err:\n        raise ZeusRAPLFileInitError(\"Error reading package energy\") from err\n    except PermissionError as err:\n        raise cpu_common.ZeusCPUNoPermissionError(\n            \"Can't read file due to permission error; RAPL requires root privileges. \"\n            \"Refer to https://ml.energy/zeus/getting_started/#system-privileges for more details.\"\n        ) from err\n    try:\n        with open(os.path.join(path, \"max_energy_range_uj\"), \"r\") as max_energy_file:\n            self.max_energy_range_uj = float(max_energy_file.read().strip())\n    except FileNotFoundError as err:\n        raise ZeusRAPLFileInitError(\"Error reading package max energy range\") from err\n\n    self.wraparound_tracker = RaplWraparoundTracker(self.energy_uj_path, self.max_energy_range_uj)\n</code></pre>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl.RAPLFile.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>Return a string representation of the RAPL file object.</p> Source code in <code>zeus/device/cpu/rapl.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return a string representation of the RAPL file object.\"\"\"\n    return f\"RAPLFile(Path: {self.path}\\nEnergy_uj_path: {self.energy_uj_path}\\nName: {self.name}\\\n    \\nLast_energy: {self.last_energy}\\nMax_energy: {self.max_energy_range_uj})\"\n</code></pre>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl.RAPLFile.read","title":"read","text":"<pre><code>read()\n</code></pre> <p>Read the current energy value from the energy_uj file.</p> <p>Returns:</p> Type Description <code>float</code> <p>The current energy value in millijoules.</p> Source code in <code>zeus/device/cpu/rapl.py</code> <pre><code>def read(self) -&gt; float:\n    \"\"\"Read the current energy value from the energy_uj file.\n\n    Returns:\n        The current energy value in millijoules.\n    \"\"\"\n    with open(self.energy_uj_path) as energy_file:\n        new_energy_uj = float(energy_file.read().strip())\n    num_wraparounds = self.wraparound_tracker.get_num_wraparounds()\n    return (new_energy_uj + num_wraparounds * self.max_energy_range_uj) / 1000.0\n</code></pre>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl.RAPLCPU","title":"RAPLCPU","text":"<p>               Bases: <code>CPU</code></p> <p>Control a single CPU that supports RAPL.</p> Source code in <code>zeus/device/cpu/rapl.py</code> <pre><code>class RAPLCPU(cpu_common.CPU):\n    \"\"\"Control a single CPU that supports RAPL.\"\"\"\n\n    def __init__(self, cpu_index: int, rapl_dir: str) -&gt; None:\n        \"\"\"Initialize the Intel CPU with a specified index.\"\"\"\n        super().__init__(cpu_index)\n        self.rapl_dir = rapl_dir\n        self._get_powerzone()\n\n    _exception_map = {\n        FileNotFoundError: cpu_common.ZeusCPUNotFoundError,\n        PermissionError: cpu_common.ZeusCPUNoPermissionError,\n        OSError: cpu_common.ZeusCPUInitError,\n    }\n\n    def _get_powerzone(self) -&gt; None:\n        self.path = os.path.join(self.rapl_dir, f\"intel-rapl:{self.cpu_index}\")\n        self.rapl_file: RAPLFile = RAPLFile(self.path)\n        self.dram: RAPLFile | None = None\n        for dir in os.listdir(self.path):\n            if \"intel-rapl\" in dir:\n                try:\n                    rapl_file = RAPLFile(os.path.join(self.path, dir))\n                except ZeusRAPLFileInitError as err:\n                    warnings.warn(f\"Failed to initialize subpackage {err}\", stacklevel=1)\n                    continue\n                if rapl_file.name == \"dram\":\n                    self.dram = rapl_file\n\n    def get_total_energy_consumption(self) -&gt; CpuDramMeasurement:\n        \"\"\"Returns the total energy consumption of the specified powerzone. Units: mJ.\"\"\"\n        cpu_mj = self.rapl_file.read()\n        dram_mj = None\n        if self.dram is not None:\n            dram_mj = self.dram.read()\n        return CpuDramMeasurement(cpu_mj=cpu_mj, dram_mj=dram_mj)\n\n    def supports_get_dram_energy_consumption(self) -&gt; bool:\n        \"\"\"Returns True if the specified CPU powerzone supports retrieving the subpackage energy consumption.\"\"\"\n        return self.dram is not None\n</code></pre>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl.RAPLCPU.__init__","title":"__init__","text":"<pre><code>__init__(cpu_index, rapl_dir)\n</code></pre> Source code in <code>zeus/device/cpu/rapl.py</code> <pre><code>def __init__(self, cpu_index: int, rapl_dir: str) -&gt; None:\n    \"\"\"Initialize the Intel CPU with a specified index.\"\"\"\n    super().__init__(cpu_index)\n    self.rapl_dir = rapl_dir\n    self._get_powerzone()\n</code></pre>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl.RAPLCPU.get_total_energy_consumption","title":"get_total_energy_consumption","text":"<pre><code>get_total_energy_consumption()\n</code></pre> <p>Returns the total energy consumption of the specified powerzone. Units: mJ.</p> Source code in <code>zeus/device/cpu/rapl.py</code> <pre><code>def get_total_energy_consumption(self) -&gt; CpuDramMeasurement:\n    \"\"\"Returns the total energy consumption of the specified powerzone. Units: mJ.\"\"\"\n    cpu_mj = self.rapl_file.read()\n    dram_mj = None\n    if self.dram is not None:\n        dram_mj = self.dram.read()\n    return CpuDramMeasurement(cpu_mj=cpu_mj, dram_mj=dram_mj)\n</code></pre>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl.RAPLCPU.supports_get_dram_energy_consumption","title":"supports_get_dram_energy_consumption","text":"<pre><code>supports_get_dram_energy_consumption()\n</code></pre> <p>Returns True if the specified CPU powerzone supports retrieving the subpackage energy consumption.</p> Source code in <code>zeus/device/cpu/rapl.py</code> <pre><code>def supports_get_dram_energy_consumption(self) -&gt; bool:\n    \"\"\"Returns True if the specified CPU powerzone supports retrieving the subpackage energy consumption.\"\"\"\n    return self.dram is not None\n</code></pre>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl.ZeusdRAPLCPU","title":"ZeusdRAPLCPU","text":"<p>               Bases: <code>RAPLCPU</code></p> <p>A RAPLCPU that interfaces with RAPL via zeusd.</p> <p>The parent RAPLCPU class requires root privileges to interface with RAPL. ZeusdRAPLCPU (this class) overrides RAPLCPU's methods so that they instead send requests to the Zeus daemon, which will interface with RAPL on behalf of ZeusdRAPLCPU. As a result, ZeusdRAPLCPU does not need root privileges to monitor CPU and DRAM energy consumption.</p> <p>See here for details on system privileges required.</p> Source code in <code>zeus/device/cpu/rapl.py</code> <pre><code>class ZeusdRAPLCPU(RAPLCPU):\n    \"\"\"A RAPLCPU that interfaces with RAPL via zeusd.\n\n    The parent RAPLCPU class requires root privileges to interface with RAPL.\n    ZeusdRAPLCPU (this class) overrides RAPLCPU's methods so that they instead send requests\n    to the Zeus daemon, which will interface with RAPL on behalf of ZeusdRAPLCPU. As a result,\n    ZeusdRAPLCPU does not need root privileges to monitor CPU and DRAM energy consumption.\n\n    See [here](https://ml.energy/zeus/getting_started/#system-privileges)\n    for details on system privileges required.\n    \"\"\"\n\n    def __init__(self, cpu_index: int, client: ZeusdClient) -&gt; None:\n        \"\"\"Initialize the Intel CPU with a specified index.\n\n        Args:\n            cpu_index: Index of the CPU.\n            client: ZeusdClient connected to the daemon.\n        \"\"\"\n        self.cpu_index = cpu_index\n        self._client = client\n        require_capabilities(client, read_cpu=True, cpu_ids=[cpu_index])\n        idx = client.cpu_ids.index(cpu_index)\n        self.dram_available = client.dram_available[idx]\n\n    def get_total_energy_consumption(self) -&gt; CpuDramMeasurement:\n        \"\"\"Returns the total energy consumption of the specified powerzone. Units: mJ.\"\"\"\n        result = self._client.get_cpu_energy([self.cpu_index], cpu=True, dram=True)\n        entry = result[self.cpu_index]\n        if entry.cpu_energy_uj is None:\n            raise ZeusdError(f\"CPU {self.cpu_index}: cpu_energy_uj is null in response\")\n        cpu_mj = entry.cpu_energy_uj / 1000\n\n        dram_mj = None\n        if entry.dram_energy_uj is None:\n            if self.dram_available:\n                raise ZeusdError(\"DRAM energy should be available but no measurement was found\")\n        else:\n            dram_mj = entry.dram_energy_uj / 1000\n\n        return CpuDramMeasurement(cpu_mj=cpu_mj, dram_mj=dram_mj)\n\n    def supports_get_dram_energy_consumption(self) -&gt; bool:\n        \"\"\"Returns True if the specified CPU powerzone supports retrieving the subpackage energy consumption.\"\"\"\n        return self.dram_available\n</code></pre>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl.ZeusdRAPLCPU.__init__","title":"__init__","text":"<pre><code>__init__(cpu_index, client)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>cpu_index</code> <code>int</code> <p>Index of the CPU.</p> required <code>client</code> <code>ZeusdClient</code> <p>ZeusdClient connected to the daemon.</p> required Source code in <code>zeus/device/cpu/rapl.py</code> <pre><code>def __init__(self, cpu_index: int, client: ZeusdClient) -&gt; None:\n    \"\"\"Initialize the Intel CPU with a specified index.\n\n    Args:\n        cpu_index: Index of the CPU.\n        client: ZeusdClient connected to the daemon.\n    \"\"\"\n    self.cpu_index = cpu_index\n    self._client = client\n    require_capabilities(client, read_cpu=True, cpu_ids=[cpu_index])\n    idx = client.cpu_ids.index(cpu_index)\n    self.dram_available = client.dram_available[idx]\n</code></pre>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl.ZeusdRAPLCPU.get_total_energy_consumption","title":"get_total_energy_consumption","text":"<pre><code>get_total_energy_consumption()\n</code></pre> <p>Returns the total energy consumption of the specified powerzone. Units: mJ.</p> Source code in <code>zeus/device/cpu/rapl.py</code> <pre><code>def get_total_energy_consumption(self) -&gt; CpuDramMeasurement:\n    \"\"\"Returns the total energy consumption of the specified powerzone. Units: mJ.\"\"\"\n    result = self._client.get_cpu_energy([self.cpu_index], cpu=True, dram=True)\n    entry = result[self.cpu_index]\n    if entry.cpu_energy_uj is None:\n        raise ZeusdError(f\"CPU {self.cpu_index}: cpu_energy_uj is null in response\")\n    cpu_mj = entry.cpu_energy_uj / 1000\n\n    dram_mj = None\n    if entry.dram_energy_uj is None:\n        if self.dram_available:\n            raise ZeusdError(\"DRAM energy should be available but no measurement was found\")\n    else:\n        dram_mj = entry.dram_energy_uj / 1000\n\n    return CpuDramMeasurement(cpu_mj=cpu_mj, dram_mj=dram_mj)\n</code></pre>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl.ZeusdRAPLCPU.supports_get_dram_energy_consumption","title":"supports_get_dram_energy_consumption","text":"<pre><code>supports_get_dram_energy_consumption()\n</code></pre> <p>Returns True if the specified CPU powerzone supports retrieving the subpackage energy consumption.</p> Source code in <code>zeus/device/cpu/rapl.py</code> <pre><code>def supports_get_dram_energy_consumption(self) -&gt; bool:\n    \"\"\"Returns True if the specified CPU powerzone supports retrieving the subpackage energy consumption.\"\"\"\n    return self.dram_available\n</code></pre>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl.RAPLCPUs","title":"RAPLCPUs","text":"<p>               Bases: <code>CPUs</code></p> <p>RAPL CPU Manager object, containing individual RAPLCPU objects, abstracting RAPL calls and handling related exceptions.</p> Source code in <code>zeus/device/cpu/rapl.py</code> <pre><code>class RAPLCPUs(cpu_common.CPUs):\n    \"\"\"RAPL CPU Manager object, containing individual RAPLCPU objects, abstracting RAPL calls and handling related exceptions.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Instantiates IntelCPUs object, setting up tracking for specified Intel CPUs.\"\"\"\n        if not rapl_is_available():\n            raise ZeusRAPLNotSupportedError(\"RAPL is not supported on this CPU.\")\n\n        self.rapl_dir = RAPL_DIR if os.path.exists(RAPL_DIR) else CONTAINER_RAPL_DIR\n        self._init_cpus()\n\n    @property\n    def cpus(self) -&gt; Sequence[RAPLCPU]:\n        \"\"\"Returns a list of CPU objects being tracked.\"\"\"\n        return self._cpus\n\n    def _init_cpus(self) -&gt; None:\n        \"\"\"Initialize all Intel CPUs.\"\"\"\n        self._cpus = []\n\n        cpu_indices = []\n\n        def sort_key(dir):\n            return int(dir.split(\":\")[1])\n\n        for dir in sorted(glob(f\"{self.rapl_dir}/intel-rapl:*\"), key=sort_key):\n            parts = dir.split(\":\")\n            if len(parts) &gt; 1 and parts[1].isdigit():\n                cpu_indices.append(int(parts[1]))\n\n        # If Zeusd env vars are set, use ZeusdRAPLCPU backed by a shared client.\n        config = ZeusdConfig.from_env()\n        if config is not None:\n            try:\n                client = ZeusdClient(config)\n                self._cpus = [ZeusdRAPLCPU(cpu_index, client) for cpu_index in cpu_indices]\n            except ZeusBaseError as e:\n                raise cpu_common.ZeusCPUInitError(str(e)) from e\n        else:\n            self._cpus = [RAPLCPU(cpu_index, self.rapl_dir) for cpu_index in cpu_indices]\n\n    def __del__(self) -&gt; None:\n        \"\"\"Shuts down the Intel CPU monitoring.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl.RAPLCPUs.cpus","title":"cpus  <code>property</code>","text":"<pre><code>cpus\n</code></pre> <p>Returns a list of CPU objects being tracked.</p>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl.RAPLCPUs.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> Source code in <code>zeus/device/cpu/rapl.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Instantiates IntelCPUs object, setting up tracking for specified Intel CPUs.\"\"\"\n    if not rapl_is_available():\n        raise ZeusRAPLNotSupportedError(\"RAPL is not supported on this CPU.\")\n\n    self.rapl_dir = RAPL_DIR if os.path.exists(RAPL_DIR) else CONTAINER_RAPL_DIR\n    self._init_cpus()\n</code></pre>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl.RAPLCPUs._init_cpus","title":"_init_cpus","text":"<pre><code>_init_cpus()\n</code></pre> <p>Initialize all Intel CPUs.</p> Source code in <code>zeus/device/cpu/rapl.py</code> <pre><code>def _init_cpus(self) -&gt; None:\n    \"\"\"Initialize all Intel CPUs.\"\"\"\n    self._cpus = []\n\n    cpu_indices = []\n\n    def sort_key(dir):\n        return int(dir.split(\":\")[1])\n\n    for dir in sorted(glob(f\"{self.rapl_dir}/intel-rapl:*\"), key=sort_key):\n        parts = dir.split(\":\")\n        if len(parts) &gt; 1 and parts[1].isdigit():\n            cpu_indices.append(int(parts[1]))\n\n    # If Zeusd env vars are set, use ZeusdRAPLCPU backed by a shared client.\n    config = ZeusdConfig.from_env()\n    if config is not None:\n        try:\n            client = ZeusdClient(config)\n            self._cpus = [ZeusdRAPLCPU(cpu_index, client) for cpu_index in cpu_indices]\n        except ZeusBaseError as e:\n            raise cpu_common.ZeusCPUInitError(str(e)) from e\n    else:\n        self._cpus = [RAPLCPU(cpu_index, self.rapl_dir) for cpu_index in cpu_indices]\n</code></pre>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl.RAPLCPUs.__del__","title":"__del__","text":"<pre><code>__del__()\n</code></pre> <p>Shuts down the Intel CPU monitoring.</p> Source code in <code>zeus/device/cpu/rapl.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Shuts down the Intel CPU monitoring.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl._polling_process","title":"_polling_process","text":"<pre><code>_polling_process(rapl_file_path, max_energy_uj, wraparound_counter)\n</code></pre> <p>Check for wraparounds in the specified rapl file.</p> Source code in <code>zeus/device/cpu/rapl.py</code> <pre><code>def _polling_process(rapl_file_path: str, max_energy_uj: float, wraparound_counter: Synchronized[int]) -&gt; None:\n    \"\"\"Check for wraparounds in the specified rapl file.\"\"\"\n    try:\n        with open(rapl_file_path) as rapl_file:\n            last_energy_uj = float(rapl_file.read().strip())\n        while True:\n            sleep_time = 1.0\n            with open(rapl_file_path, \"r\") as rapl_file:\n                energy_uj = float(rapl_file.read().strip())\n            if max_energy_uj - energy_uj &lt; RAPL_COUNTER_MAX_INCREASE:\n                sleep_time = 0.1\n            if energy_uj &lt; last_energy_uj:\n                with wraparound_counter.get_lock():\n                    wraparound_counter.value += 1\n                    logger.debug(\n                        \"RAPL wraparound detected for %s; counter now: %d\",\n                        rapl_file_path,\n                        wraparound_counter.value,\n                    )\n            last_energy_uj = energy_uj\n            time.sleep(sleep_time)\n    except KeyboardInterrupt:\n        return\n</code></pre>"},{"location":"reference/device/cpu/rapl/#zeus.device.cpu.rapl.rapl_is_available","title":"rapl_is_available  <code>cached</code>","text":"<pre><code>rapl_is_available()\n</code></pre> <p>Check if RAPL is available.</p> Source code in <code>zeus/device/cpu/rapl.py</code> <pre><code>@lru_cache(maxsize=1)\ndef rapl_is_available() -&gt; bool:\n    \"\"\"Check if RAPL is available.\"\"\"\n    if os.path.exists(RAPL_DIR):\n        logger.info(\"RAPL directory (%s) is available.\", RAPL_DIR)\n        return True\n    if os.path.exists(CONTAINER_RAPL_DIR):\n        logger.info(\n            \"You are likely in a container, and RAPL directory (%s) is available.\",\n            CONTAINER_RAPL_DIR,\n        )\n        return True\n    logger.info(\"RAPL is not supported on this CPU.\")\n    return False\n</code></pre>"},{"location":"reference/device/gpu/","title":"gpu","text":""},{"location":"reference/device/gpu/#zeus.device.gpu","title":"zeus.device.gpu","text":"<p>Abstraction layer for GPU devices.</p> <p>The main function of this module is <code>get_gpus</code>, which returns a GPU Manager object specific to the platform.</p> <p>Important</p> <p>In theory, any NVIDIA GPU would be supported. On the other hand, for AMD GPUs, we currently only support ROCm 6.1 and later.</p>"},{"location":"reference/device/gpu/#zeus.device.gpu--getting-handles-to-gpus","title":"Getting handles to GPUs","text":"<p>The main API exported from this module is the <code>get_gpus</code> function. It returns either <code>NVIDIAGPUs</code> or <code>AMDGPUs</code> depending on the platform.</p> <pre><code>from zeus.device import get_gpus\ngpus = get_gpus()\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu--calling-gpu-management-apis","title":"Calling GPU management APIs","text":"<p>GPU management library APIs are mapped to methods on <code>GPU</code>.</p> <p>For example, for NVIDIA GPUs (which uses <code>pynvml</code>), you would have called:</p> <pre><code>handle = pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\nconstraints = pynvml.nvmlDeviceGetPowerManagementLimitConstraints(handle)\n</code></pre> <p>With the Zeus GPU abstraction layer, you would now call:</p> <pre><code>gpus = get_gpus() # returns an NVIDIAGPUs object\nconstraints = gpus.getPowerManagementLimitConstraints(gpu_index)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu--non-blocking-calls","title":"Non-blocking calls","text":"<p>Some implementations of <code>GPU</code> support non-blocking calls to setters. If non-blocking calls are not supported, setting <code>block</code> will be ignored and the call will block. Check <code>GPU.supports_non_blocking</code> to see if non-blocking calls are supported. Note that non-blocking calls will not raise exceptions even if the call fails.</p> <p>Currently, only <code>ZeusdNVIDIAGPU</code> supports non-blocking calls to methods that set the GPU's power limit, GPU frequency, memory frequency, and persistence mode. This is possible because the Zeus daemon supports a <code>block: bool</code> parameter in HTTP requests, which can be set to <code>False</code> to make the call return immediately without checking the result.</p>"},{"location":"reference/device/gpu/#zeus.device.gpu--error-handling","title":"Error handling","text":"<p>The following exceptions are defined in this module:</p> <ul> <li><code>ZeusGPUInitError</code>: Base class for initialization errors.</li> <li><code>ZeusGPUInvalidArgError</code>: Error for invalid arguments.</li> <li><code>ZeusGPUNotSupportedError</code>: Error for unsupported GPUs.</li> <li><code>ZeusGPUNoPermissionError</code>: Error for permission issues.</li> <li><code>ZeusGPUAlreadyInitializedError</code>: Error for reinitialization.</li> <li><code>ZeusGPUNotFoundError</code>: Error for missing GPUs.</li> <li><code>ZeusGPUInsufficientSizeError</code>: Error for insufficient buffer size.</li> <li><code>ZeusGPUInsufficientPowerError</code>: Error for insufficient power.</li> <li><code>ZeusGPUDriverNotLoadedError</code>: Error for driver issues.</li> <li><code>ZeusGPUTimeoutError</code>: Error for timeout issues.</li> <li><code>ZeusGPUIRQError</code>: Error for IRQ issues.</li> <li><code>ZeusGPULibraryNotFoundError</code>: Error for missing libraries.</li> <li><code>ZeusGPUFunctionNotFoundError</code>: Error for missing functions.</li> <li><code>ZeusGPUCorruptedInfoROMError</code>: Error for corrupted info ROM.</li> <li><code>ZeusGPULostError</code>: Error for lost GPUs.</li> <li><code>ZeusGPUResetRequiredError</code>: Error for GPUs requiring reset.</li> <li><code>ZeusGPUOperatingSystemError</code>: Error for OS issues.</li> <li><code>ZeusGPULibRMVersionMismatchError</code>: Error for library version mismatch.</li> <li><code>ZeusGPUMemoryError</code>: Error for memory issues.</li> <li><code>ZeusGPUUnknownError</code>: Error for unknown issues.</li> </ul>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusBaseGPUError","title":"ZeusBaseGPUError","text":"<p>               Bases: <code>ZeusBaseError</code></p> <p>Zeus base GPU exception class.</p> Source code in <code>zeus/device/exception.py</code> <pre><code>class ZeusBaseGPUError(ZeusBaseError):\n    \"\"\"Zeus base GPU exception class.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Base Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusBaseGPUError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/exception.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Base Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.DeprecatedAliasABCMeta","title":"DeprecatedAliasABCMeta","text":"<p>               Bases: <code>ABCMeta</code></p> <p>Metaclass that combines ABC functionality with automatic deprecated alias creation.</p> <p>This metaclass looks for methods decorated with <code>@deprecated_alias</code> and automatically creates the old camelCase method names that emit deprecation warnings once and then call the new snake_case methods.</p> <p>Since this is frequently composed with <code>abc.ABCMeta</code>, this metaclass inherits from it to avoid metaclass conflicts.</p> <p>Example</p> <pre><code>class MyClass(abc.ABC, metaclass=DeprecatedAliasABCMeta):\n    @deprecated_alias(\"oldMethod\")\n    @abc.abstractmethod\n    def new_method(self):\n        pass\n\nclass MyImplementation(MyClass):\n    def new_method(self):\n        return \"implementation\"\n\nobj = MyImplementation()\nobj.new_method()  # No warning\nobj.oldMethod()   # Emits deprecation warning, calls new_method\n</code></pre> Source code in <code>zeus/device/common.py</code> <pre><code>class DeprecatedAliasABCMeta(abc.ABCMeta):\n    \"\"\"Metaclass that combines ABC functionality with automatic deprecated alias creation.\n\n    This metaclass looks for methods decorated with `@deprecated_alias` and automatically\n    creates the old camelCase method names that emit deprecation warnings once and then\n    call the new snake_case methods.\n\n    Since this is frequently composed with `abc.ABCMeta`, this metaclass inherits from it\n    to avoid metaclass conflicts.\n\n    !!! Example\n        ```python\n        class MyClass(abc.ABC, metaclass=DeprecatedAliasABCMeta):\n            @deprecated_alias(\"oldMethod\")\n            @abc.abstractmethod\n            def new_method(self):\n                pass\n\n        class MyImplementation(MyClass):\n            def new_method(self):\n                return \"implementation\"\n\n        obj = MyImplementation()\n        obj.new_method()  # No warning\n        obj.oldMethod()   # Emits deprecation warning, calls new_method\n        ```\n    \"\"\"\n\n    def __new__(mcs, name, bases, namespace):\n        \"\"\"Create the class and add deprecated alias methods.\"\"\"\n        cls = super().__new__(mcs, name, bases, namespace)\n\n        # Create deprecated aliases for methods marked with @deprecated_alias\n        for attr_name in dir(cls):\n            if attr_name.startswith(\"_\"):\n                continue\n\n            try:\n                attr = getattr(cls, attr_name)\n            except AttributeError:\n                continue\n\n            # Check if this method has the deprecated alias marker\n            if hasattr(attr, \"_deprecated_alias\"):\n                old_name = attr._deprecated_alias\n                # Create and attach the deprecated wrapper method\n                deprecated_method = _make_deprecated_method(attr, old_name, attr_name)\n                setattr(cls, old_name, deprecated_method)\n\n        return cls\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.DeprecatedAliasABCMeta.__new__","title":"__new__","text":"<pre><code>__new__(mcs, name, bases, namespace)\n</code></pre> <p>Create the class and add deprecated alias methods.</p> Source code in <code>zeus/device/common.py</code> <pre><code>def __new__(mcs, name, bases, namespace):\n    \"\"\"Create the class and add deprecated alias methods.\"\"\"\n    cls = super().__new__(mcs, name, bases, namespace)\n\n    # Create deprecated aliases for methods marked with @deprecated_alias\n    for attr_name in dir(cls):\n        if attr_name.startswith(\"_\"):\n            continue\n\n        try:\n            attr = getattr(cls, attr_name)\n        except AttributeError:\n            continue\n\n        # Check if this method has the deprecated alias marker\n        if hasattr(attr, \"_deprecated_alias\"):\n            old_name = attr._deprecated_alias\n            # Create and attach the deprecated wrapper method\n            deprecated_method = _make_deprecated_method(attr, old_name, attr_name)\n            setattr(cls, old_name, deprecated_method)\n\n    return cls\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.GPU","title":"GPU","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for managing one GPU.</p> <p>For each method, child classes should call into vendor-specific GPU management libraries (e.g., NVML for NVIDIA GPUs).</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class GPU(abc.ABC, metaclass=DeprecatedAliasABCMeta):\n    \"\"\"Abstract base class for managing one GPU.\n\n    For each method, child classes should call into vendor-specific\n    GPU management libraries (e.g., NVML for NVIDIA GPUs).\n    \"\"\"\n\n    def __init__(self, gpu_index: int) -&gt; None:\n        \"\"\"Initializ the GPU with a specified index.\"\"\"\n        self.gpu_index = gpu_index\n\n    def _warn_sys_admin(self) -&gt; None:\n        \"\"\"Warn the user if the current process doesn't have `SYS_ADMIN` privileges.\"\"\"\n        # Deriving classes can disable this warning by setting this attribute.\n        if not getattr(self, \"_disable_sys_admin_warning\", False) and not has_sys_admin():\n            warnings.warn(\n                \"You are about to call a GPU management API that requires \"\n                \"`SYS_ADMIN` privileges. Some energy optimizers that change the \"\n                \"GPU's power settings need this.\\nSee \"\n                \"https://ml.energy/zeus/getting_started/#system-privileges \"\n                \"for more information and how to obtain `SYS_ADMIN`.\",\n                stacklevel=2,\n            )\n            # Only warn once.\n            self._disable_sys_admin_warning = True\n\n    @property\n    @abc.abstractmethod\n    def supports_nonblocking_setters(self) -&gt; bool:\n        \"\"\"Return True if the GPU object supports non-blocking configuration setters.\"\"\"\n        return False\n\n    @deprecated_alias(\"getName\")\n    @abc.abstractmethod\n    def get_name(self) -&gt; str:\n        \"\"\"Return the name of the GPU model.\"\"\"\n        pass\n\n    @deprecated_alias(\"getPowerManagementLimitConstraints\")\n    @abc.abstractmethod\n    def get_power_management_limit_constraints(self) -&gt; tuple[int, int]:\n        \"\"\"Return the minimum and maximum power management limits. Units: mW.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def get_power_management_limit(self) -&gt; int:\n        \"\"\"Return the current power management limit. Units: mW.\"\"\"\n        pass\n\n    @deprecated_alias(\"setPowerManagementLimit\")\n    @abc.abstractmethod\n    def set_power_management_limit(self, power_limit_mw: int, block: bool = True) -&gt; None:\n        \"\"\"Set the GPU's power management limit. Unit: mW.\"\"\"\n        pass\n\n    @deprecated_alias(\"resetPowerManagementLimit\")\n    @abc.abstractmethod\n    def reset_power_management_limit(self, block: bool = True) -&gt; None:\n        \"\"\"Reset the GPU's power management limit to the default value.\"\"\"\n        pass\n\n    @deprecated_alias(\"setPersistenceMode\")\n    @abc.abstractmethod\n    def set_persistence_mode(self, enabled: bool, block: bool = True) -&gt; None:\n        \"\"\"Set persistence mode.\"\"\"\n        pass\n\n    @deprecated_alias(\"getSupportedMemoryClocks\")\n    @abc.abstractmethod\n    def get_supported_memory_clocks(self) -&gt; list[int]:\n        \"\"\"Return a list of supported memory clock frequencies. Units: MHz.\"\"\"\n        pass\n\n    @deprecated_alias(\"setMemoryLockedClocks\")\n    @abc.abstractmethod\n    def set_memory_locked_clocks(self, min_clock_mhz: int, max_clock_mhz: int, block: bool = True) -&gt; None:\n        \"\"\"Lock the memory clock to a specified range. Units: MHz.\"\"\"\n        pass\n\n    @deprecated_alias(\"resetMemoryLockedClocks\")\n    @abc.abstractmethod\n    def reset_memory_locked_clocks(self, block: bool = True) -&gt; None:\n        \"\"\"Reset the locked memory clocks to the default.\"\"\"\n        pass\n\n    @deprecated_alias(\"getSupportedGraphicsClocks\")\n    @abc.abstractmethod\n    def get_supported_graphics_clocks(self, memory_clock_mhz: int | None = None) -&gt; list[int]:\n        \"\"\"Return a list of supported graphics clock frequencies. Units: MHz.\n\n        Args:\n            memory_clock_mhz: Memory clock frequency to use. Some GPUs have\n                different supported graphics clocks depending on the memory clock.\n        \"\"\"\n        pass\n\n    @deprecated_alias(\"setGpuLockedClocks\")\n    @abc.abstractmethod\n    def set_gpu_locked_clocks(self, min_clock_mhz: int, max_clock_mhz: int, block: bool = True) -&gt; None:\n        \"\"\"Lock the GPU clock to a specified range. Units: MHz.\"\"\"\n        pass\n\n    @deprecated_alias(\"resetGpuLockedClocks\")\n    @abc.abstractmethod\n    def reset_gpu_locked_clocks(self, block: bool = True) -&gt; None:\n        \"\"\"Reset the locked GPU clocks to the default.\"\"\"\n        pass\n\n    @deprecated_alias(\"getAveragePowerUsage\")\n    @abc.abstractmethod\n    def get_average_power_usage(self) -&gt; int:\n        \"\"\"Return the average power usage of the GPU. Units: mW.\"\"\"\n        pass\n\n    @deprecated_alias(\"getInstantPowerUsage\")\n    @abc.abstractmethod\n    def get_instant_power_usage(self) -&gt; int:\n        \"\"\"Return the current power draw of the GPU. Units: mW.\"\"\"\n        pass\n\n    @deprecated_alias(\"getAverageMemoryPowerUsage\")\n    @abc.abstractmethod\n    def get_average_memory_power_usage(self) -&gt; int:\n        \"\"\"Return the average power usage of the GPU's memory. Units: mW.\"\"\"\n        pass\n\n    @deprecated_alias(\"supportsGetTotalEnergyConsumption\")\n    @abc.abstractmethod\n    def supports_get_total_energy_consumption(self) -&gt; bool:\n        \"\"\"Check if the GPU supports retrieving total energy consumption.\"\"\"\n        pass\n\n    @deprecated_alias(\"getTotalEnergyConsumption\")\n    @abc.abstractmethod\n    def get_total_energy_consumption(self) -&gt; int:\n        \"\"\"Return the total energy consumption of the GPU since driver load. Units: mJ.\"\"\"\n        pass\n\n    @deprecated_alias(\"getGpuTemperature\")\n    @abc.abstractmethod\n    def get_gpu_temperature(self) -&gt; int:\n        \"\"\"Return the current GPU temperature. Units: Celsius.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.GPU.supports_nonblocking_setters","title":"supports_nonblocking_setters  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>supports_nonblocking_setters\n</code></pre> <p>Return True if the GPU object supports non-blocking configuration setters.</p>"},{"location":"reference/device/gpu/#zeus.device.gpu.GPU.__init__","title":"__init__","text":"<pre><code>__init__(gpu_index)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, gpu_index: int) -&gt; None:\n    \"\"\"Initializ the GPU with a specified index.\"\"\"\n    self.gpu_index = gpu_index\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.GPU._warn_sys_admin","title":"_warn_sys_admin","text":"<pre><code>_warn_sys_admin()\n</code></pre> <p>Warn the user if the current process doesn't have <code>SYS_ADMIN</code> privileges.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def _warn_sys_admin(self) -&gt; None:\n    \"\"\"Warn the user if the current process doesn't have `SYS_ADMIN` privileges.\"\"\"\n    # Deriving classes can disable this warning by setting this attribute.\n    if not getattr(self, \"_disable_sys_admin_warning\", False) and not has_sys_admin():\n        warnings.warn(\n            \"You are about to call a GPU management API that requires \"\n            \"`SYS_ADMIN` privileges. Some energy optimizers that change the \"\n            \"GPU's power settings need this.\\nSee \"\n            \"https://ml.energy/zeus/getting_started/#system-privileges \"\n            \"for more information and how to obtain `SYS_ADMIN`.\",\n            stacklevel=2,\n        )\n        # Only warn once.\n        self._disable_sys_admin_warning = True\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.GPU.get_name","title":"get_name  <code>abstractmethod</code>","text":"<pre><code>get_name()\n</code></pre> <p>Return the name of the GPU model.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getName\")\n@abc.abstractmethod\ndef get_name(self) -&gt; str:\n    \"\"\"Return the name of the GPU model.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.GPU.get_power_management_limit_constraints","title":"get_power_management_limit_constraints  <code>abstractmethod</code>","text":"<pre><code>get_power_management_limit_constraints()\n</code></pre> <p>Return the minimum and maximum power management limits. Units: mW.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getPowerManagementLimitConstraints\")\n@abc.abstractmethod\ndef get_power_management_limit_constraints(self) -&gt; tuple[int, int]:\n    \"\"\"Return the minimum and maximum power management limits. Units: mW.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.GPU.get_power_management_limit","title":"get_power_management_limit  <code>abstractmethod</code>","text":"<pre><code>get_power_management_limit()\n</code></pre> <p>Return the current power management limit. Units: mW.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@abc.abstractmethod\ndef get_power_management_limit(self) -&gt; int:\n    \"\"\"Return the current power management limit. Units: mW.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.GPU.set_power_management_limit","title":"set_power_management_limit  <code>abstractmethod</code>","text":"<pre><code>set_power_management_limit(power_limit_mw, block=True)\n</code></pre> <p>Set the GPU's power management limit. Unit: mW.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"setPowerManagementLimit\")\n@abc.abstractmethod\ndef set_power_management_limit(self, power_limit_mw: int, block: bool = True) -&gt; None:\n    \"\"\"Set the GPU's power management limit. Unit: mW.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.GPU.reset_power_management_limit","title":"reset_power_management_limit  <code>abstractmethod</code>","text":"<pre><code>reset_power_management_limit(block=True)\n</code></pre> <p>Reset the GPU's power management limit to the default value.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"resetPowerManagementLimit\")\n@abc.abstractmethod\ndef reset_power_management_limit(self, block: bool = True) -&gt; None:\n    \"\"\"Reset the GPU's power management limit to the default value.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.GPU.set_persistence_mode","title":"set_persistence_mode  <code>abstractmethod</code>","text":"<pre><code>set_persistence_mode(enabled, block=True)\n</code></pre> <p>Set persistence mode.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"setPersistenceMode\")\n@abc.abstractmethod\ndef set_persistence_mode(self, enabled: bool, block: bool = True) -&gt; None:\n    \"\"\"Set persistence mode.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.GPU.get_supported_memory_clocks","title":"get_supported_memory_clocks  <code>abstractmethod</code>","text":"<pre><code>get_supported_memory_clocks()\n</code></pre> <p>Return a list of supported memory clock frequencies. Units: MHz.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getSupportedMemoryClocks\")\n@abc.abstractmethod\ndef get_supported_memory_clocks(self) -&gt; list[int]:\n    \"\"\"Return a list of supported memory clock frequencies. Units: MHz.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.GPU.set_memory_locked_clocks","title":"set_memory_locked_clocks  <code>abstractmethod</code>","text":"<pre><code>set_memory_locked_clocks(min_clock_mhz, max_clock_mhz, block=True)\n</code></pre> <p>Lock the memory clock to a specified range. Units: MHz.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"setMemoryLockedClocks\")\n@abc.abstractmethod\ndef set_memory_locked_clocks(self, min_clock_mhz: int, max_clock_mhz: int, block: bool = True) -&gt; None:\n    \"\"\"Lock the memory clock to a specified range. Units: MHz.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.GPU.reset_memory_locked_clocks","title":"reset_memory_locked_clocks  <code>abstractmethod</code>","text":"<pre><code>reset_memory_locked_clocks(block=True)\n</code></pre> <p>Reset the locked memory clocks to the default.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"resetMemoryLockedClocks\")\n@abc.abstractmethod\ndef reset_memory_locked_clocks(self, block: bool = True) -&gt; None:\n    \"\"\"Reset the locked memory clocks to the default.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.GPU.get_supported_graphics_clocks","title":"get_supported_graphics_clocks  <code>abstractmethod</code>","text":"<pre><code>get_supported_graphics_clocks(memory_clock_mhz=None)\n</code></pre> <p>Return a list of supported graphics clock frequencies. Units: MHz.</p> <p>Parameters:</p> Name Type Description Default <code>memory_clock_mhz</code> <code>int | None</code> <p>Memory clock frequency to use. Some GPUs have different supported graphics clocks depending on the memory clock.</p> <code>None</code> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getSupportedGraphicsClocks\")\n@abc.abstractmethod\ndef get_supported_graphics_clocks(self, memory_clock_mhz: int | None = None) -&gt; list[int]:\n    \"\"\"Return a list of supported graphics clock frequencies. Units: MHz.\n\n    Args:\n        memory_clock_mhz: Memory clock frequency to use. Some GPUs have\n            different supported graphics clocks depending on the memory clock.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.GPU.set_gpu_locked_clocks","title":"set_gpu_locked_clocks  <code>abstractmethod</code>","text":"<pre><code>set_gpu_locked_clocks(min_clock_mhz, max_clock_mhz, block=True)\n</code></pre> <p>Lock the GPU clock to a specified range. Units: MHz.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"setGpuLockedClocks\")\n@abc.abstractmethod\ndef set_gpu_locked_clocks(self, min_clock_mhz: int, max_clock_mhz: int, block: bool = True) -&gt; None:\n    \"\"\"Lock the GPU clock to a specified range. Units: MHz.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.GPU.reset_gpu_locked_clocks","title":"reset_gpu_locked_clocks  <code>abstractmethod</code>","text":"<pre><code>reset_gpu_locked_clocks(block=True)\n</code></pre> <p>Reset the locked GPU clocks to the default.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"resetGpuLockedClocks\")\n@abc.abstractmethod\ndef reset_gpu_locked_clocks(self, block: bool = True) -&gt; None:\n    \"\"\"Reset the locked GPU clocks to the default.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.GPU.get_average_power_usage","title":"get_average_power_usage  <code>abstractmethod</code>","text":"<pre><code>get_average_power_usage()\n</code></pre> <p>Return the average power usage of the GPU. Units: mW.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getAveragePowerUsage\")\n@abc.abstractmethod\ndef get_average_power_usage(self) -&gt; int:\n    \"\"\"Return the average power usage of the GPU. Units: mW.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.GPU.get_instant_power_usage","title":"get_instant_power_usage  <code>abstractmethod</code>","text":"<pre><code>get_instant_power_usage()\n</code></pre> <p>Return the current power draw of the GPU. Units: mW.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getInstantPowerUsage\")\n@abc.abstractmethod\ndef get_instant_power_usage(self) -&gt; int:\n    \"\"\"Return the current power draw of the GPU. Units: mW.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.GPU.get_average_memory_power_usage","title":"get_average_memory_power_usage  <code>abstractmethod</code>","text":"<pre><code>get_average_memory_power_usage()\n</code></pre> <p>Return the average power usage of the GPU's memory. Units: mW.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getAverageMemoryPowerUsage\")\n@abc.abstractmethod\ndef get_average_memory_power_usage(self) -&gt; int:\n    \"\"\"Return the average power usage of the GPU's memory. Units: mW.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.GPU.supports_get_total_energy_consumption","title":"supports_get_total_energy_consumption  <code>abstractmethod</code>","text":"<pre><code>supports_get_total_energy_consumption()\n</code></pre> <p>Check if the GPU supports retrieving total energy consumption.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"supportsGetTotalEnergyConsumption\")\n@abc.abstractmethod\ndef supports_get_total_energy_consumption(self) -&gt; bool:\n    \"\"\"Check if the GPU supports retrieving total energy consumption.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.GPU.get_total_energy_consumption","title":"get_total_energy_consumption  <code>abstractmethod</code>","text":"<pre><code>get_total_energy_consumption()\n</code></pre> <p>Return the total energy consumption of the GPU since driver load. Units: mJ.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getTotalEnergyConsumption\")\n@abc.abstractmethod\ndef get_total_energy_consumption(self) -&gt; int:\n    \"\"\"Return the total energy consumption of the GPU since driver load. Units: mJ.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.GPU.get_gpu_temperature","title":"get_gpu_temperature  <code>abstractmethod</code>","text":"<pre><code>get_gpu_temperature()\n</code></pre> <p>Return the current GPU temperature. Units: Celsius.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getGpuTemperature\")\n@abc.abstractmethod\ndef get_gpu_temperature(self) -&gt; int:\n    \"\"\"Return the current GPU temperature. Units: Celsius.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.EmptyGPUs","title":"EmptyGPUs","text":"<p>               Bases: <code>GPUs</code></p> <p>A concrete class implementing the GPUs abstract base class, but representing an empty collection of GPUs.</p> <p>This class is used to represent a scenario where no GPUs are available or detected. Any method call attempting to interact with a GPU will raise a ValueError.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class EmptyGPUs(GPUs):\n    \"\"\"A concrete class implementing the GPUs abstract base class, but representing an empty collection of GPUs.\n\n    This class is used to represent a scenario where no GPUs are available or detected.\n    Any method call attempting to interact with a GPU will raise a ValueError.\n    \"\"\"\n\n    def __init__(self, ensure_homogeneous: bool = False) -&gt; None:\n        \"\"\"Initialize the EMPTYGPUs class.\n\n        Since this class represents an empty collection of GPUs, no actual initialization of GPU objects is performed.\n        \"\"\"\n        pass\n\n    def __del__(self) -&gt; None:\n        \"\"\"Clean up any resources if necessary.\n\n        As this class represents an empty collection of GPUs, no specific cleanup is required.\n        \"\"\"\n        pass\n\n    @property\n    def gpus(self) -&gt; Sequence[\"GPU\"]:\n        \"\"\"Return an empty list as no GPUs are being tracked.\"\"\"\n        return []\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return 0, indicating no GPUs are being tracked.\"\"\"\n        return 0\n\n    def _ensure_homogeneous(self) -&gt; None:\n        \"\"\"Raise a ValueError as no GPUs are being tracked.\"\"\"\n        raise ValueError(\"No GPUs available to ensure homogeneity.\")\n\n    def _warn_sys_admin(self) -&gt; None:\n        \"\"\"Raise a ValueError as no GPUs are being tracked.\"\"\"\n        raise ValueError(\"No GPUs available to warn about SYS_ADMIN privileges.\")\n\n    @deprecated_alias(\"getName\")\n    def get_name(self, gpu_index: int) -&gt; str:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    @deprecated_alias(\"getPowerManagementLimitConstraints\")\n    def get_power_management_limit_constraints(self, gpu_index: int) -&gt; tuple[int, int]:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    def get_power_management_limit(self, gpu_index: int) -&gt; int:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    @deprecated_alias(\"setPowerManagementLimit\")\n    def set_power_management_limit(self, gpu_index: int, power_limit_mw: int, block: bool = True) -&gt; None:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    @deprecated_alias(\"resetPowerManagementLimit\")\n    def reset_power_management_limit(self, gpu_index: int, block: bool = True) -&gt; None:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    @deprecated_alias(\"setPersistenceMode\")\n    def set_persistence_mode(self, gpu_index: int, enabled: bool, block: bool = True) -&gt; None:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    @deprecated_alias(\"getSupportedMemoryClocks\")\n    def get_supported_memory_clocks(self, gpu_index: int) -&gt; list[int]:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    @deprecated_alias(\"setMemoryLockedClocks\")\n    def set_memory_locked_clocks(\n        self,\n        gpu_index: int,\n        min_clock_mhz: int,\n        max_clock_mhz: int,\n        block: bool = True,\n    ) -&gt; None:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    @deprecated_alias(\"resetMemoryLockedClocks\")\n    def reset_memory_locked_clocks(self, gpu_index: int, block: bool = True) -&gt; None:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    @deprecated_alias(\"getSupportedGraphicsClocks\")\n    def get_supported_graphics_clocks(self, gpu_index: int, memory_clock_mhz: int | None = None) -&gt; list[int]:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    @deprecated_alias(\"setGpuLockedClocks\")\n    def set_gpu_locked_clocks(\n        self,\n        gpu_index: int,\n        min_clock_mhz: int,\n        max_clock_mhz: int,\n        block: bool = True,\n    ) -&gt; None:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    @deprecated_alias(\"resetGpuLockedClocks\")\n    def reset_gpu_locked_clocks(self, gpu_index: int, block: bool = True) -&gt; None:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    @deprecated_alias(\"getInstantPowerUsage\")\n    def get_instant_power_usage(self, gpu_index: int) -&gt; int:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    @deprecated_alias(\"supportsGetTotalEnergyConsumption\")\n    def supports_get_total_energy_consumption(self, gpu_index: int) -&gt; bool:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    @deprecated_alias(\"getTotalEnergyConsumption\")\n    def get_total_energy_consumption(self, gpu_index: int) -&gt; int:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    @deprecated_alias(\"getGpuTemperature\")\n    def get_gpu_temperature(self, gpu_index: int) -&gt; int:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.EmptyGPUs.gpus","title":"gpus  <code>property</code>","text":"<pre><code>gpus\n</code></pre> <p>Return an empty list as no GPUs are being tracked.</p>"},{"location":"reference/device/gpu/#zeus.device.gpu.EmptyGPUs.__init__","title":"__init__","text":"<pre><code>__init__(ensure_homogeneous=False)\n</code></pre> <p>Since this class represents an empty collection of GPUs, no actual initialization of GPU objects is performed.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, ensure_homogeneous: bool = False) -&gt; None:\n    \"\"\"Initialize the EMPTYGPUs class.\n\n    Since this class represents an empty collection of GPUs, no actual initialization of GPU objects is performed.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.EmptyGPUs.__del__","title":"__del__","text":"<pre><code>__del__()\n</code></pre> <p>Clean up any resources if necessary.</p> <p>As this class represents an empty collection of GPUs, no specific cleanup is required.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Clean up any resources if necessary.\n\n    As this class represents an empty collection of GPUs, no specific cleanup is required.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.EmptyGPUs.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return 0, indicating no GPUs are being tracked.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return 0, indicating no GPUs are being tracked.\"\"\"\n    return 0\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.EmptyGPUs._ensure_homogeneous","title":"_ensure_homogeneous","text":"<pre><code>_ensure_homogeneous()\n</code></pre> <p>Raise a ValueError as no GPUs are being tracked.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def _ensure_homogeneous(self) -&gt; None:\n    \"\"\"Raise a ValueError as no GPUs are being tracked.\"\"\"\n    raise ValueError(\"No GPUs available to ensure homogeneity.\")\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.EmptyGPUs._warn_sys_admin","title":"_warn_sys_admin","text":"<pre><code>_warn_sys_admin()\n</code></pre> <p>Raise a ValueError as no GPUs are being tracked.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def _warn_sys_admin(self) -&gt; None:\n    \"\"\"Raise a ValueError as no GPUs are being tracked.\"\"\"\n    raise ValueError(\"No GPUs available to warn about SYS_ADMIN privileges.\")\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.EmptyGPUs.get_name","title":"get_name","text":"<pre><code>get_name(gpu_index)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getName\")\ndef get_name(self, gpu_index: int) -&gt; str:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.EmptyGPUs.get_power_management_limit_constraints","title":"get_power_management_limit_constraints","text":"<pre><code>get_power_management_limit_constraints(gpu_index)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getPowerManagementLimitConstraints\")\ndef get_power_management_limit_constraints(self, gpu_index: int) -&gt; tuple[int, int]:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.EmptyGPUs.get_power_management_limit","title":"get_power_management_limit","text":"<pre><code>get_power_management_limit(gpu_index)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def get_power_management_limit(self, gpu_index: int) -&gt; int:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.EmptyGPUs.set_power_management_limit","title":"set_power_management_limit","text":"<pre><code>set_power_management_limit(gpu_index, power_limit_mw, block=True)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"setPowerManagementLimit\")\ndef set_power_management_limit(self, gpu_index: int, power_limit_mw: int, block: bool = True) -&gt; None:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.EmptyGPUs.reset_power_management_limit","title":"reset_power_management_limit","text":"<pre><code>reset_power_management_limit(gpu_index, block=True)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"resetPowerManagementLimit\")\ndef reset_power_management_limit(self, gpu_index: int, block: bool = True) -&gt; None:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.EmptyGPUs.set_persistence_mode","title":"set_persistence_mode","text":"<pre><code>set_persistence_mode(gpu_index, enabled, block=True)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"setPersistenceMode\")\ndef set_persistence_mode(self, gpu_index: int, enabled: bool, block: bool = True) -&gt; None:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.EmptyGPUs.get_supported_memory_clocks","title":"get_supported_memory_clocks","text":"<pre><code>get_supported_memory_clocks(gpu_index)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getSupportedMemoryClocks\")\ndef get_supported_memory_clocks(self, gpu_index: int) -&gt; list[int]:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.EmptyGPUs.set_memory_locked_clocks","title":"set_memory_locked_clocks","text":"<pre><code>set_memory_locked_clocks(gpu_index, min_clock_mhz, max_clock_mhz, block=True)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"setMemoryLockedClocks\")\ndef set_memory_locked_clocks(\n    self,\n    gpu_index: int,\n    min_clock_mhz: int,\n    max_clock_mhz: int,\n    block: bool = True,\n) -&gt; None:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.EmptyGPUs.reset_memory_locked_clocks","title":"reset_memory_locked_clocks","text":"<pre><code>reset_memory_locked_clocks(gpu_index, block=True)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"resetMemoryLockedClocks\")\ndef reset_memory_locked_clocks(self, gpu_index: int, block: bool = True) -&gt; None:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.EmptyGPUs.get_supported_graphics_clocks","title":"get_supported_graphics_clocks","text":"<pre><code>get_supported_graphics_clocks(gpu_index, memory_clock_mhz=None)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getSupportedGraphicsClocks\")\ndef get_supported_graphics_clocks(self, gpu_index: int, memory_clock_mhz: int | None = None) -&gt; list[int]:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.EmptyGPUs.set_gpu_locked_clocks","title":"set_gpu_locked_clocks","text":"<pre><code>set_gpu_locked_clocks(gpu_index, min_clock_mhz, max_clock_mhz, block=True)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"setGpuLockedClocks\")\ndef set_gpu_locked_clocks(\n    self,\n    gpu_index: int,\n    min_clock_mhz: int,\n    max_clock_mhz: int,\n    block: bool = True,\n) -&gt; None:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.EmptyGPUs.reset_gpu_locked_clocks","title":"reset_gpu_locked_clocks","text":"<pre><code>reset_gpu_locked_clocks(gpu_index, block=True)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"resetGpuLockedClocks\")\ndef reset_gpu_locked_clocks(self, gpu_index: int, block: bool = True) -&gt; None:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.EmptyGPUs.get_instant_power_usage","title":"get_instant_power_usage","text":"<pre><code>get_instant_power_usage(gpu_index)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getInstantPowerUsage\")\ndef get_instant_power_usage(self, gpu_index: int) -&gt; int:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.EmptyGPUs.supports_get_total_energy_consumption","title":"supports_get_total_energy_consumption","text":"<pre><code>supports_get_total_energy_consumption(gpu_index)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"supportsGetTotalEnergyConsumption\")\ndef supports_get_total_energy_consumption(self, gpu_index: int) -&gt; bool:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.EmptyGPUs.get_total_energy_consumption","title":"get_total_energy_consumption","text":"<pre><code>get_total_energy_consumption(gpu_index)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getTotalEnergyConsumption\")\ndef get_total_energy_consumption(self, gpu_index: int) -&gt; int:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.EmptyGPUs.get_gpu_temperature","title":"get_gpu_temperature","text":"<pre><code>get_gpu_temperature(gpu_index)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getGpuTemperature\")\ndef get_gpu_temperature(self, gpu_index: int) -&gt; int:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUInvalidArgError","title":"ZeusGPUInvalidArgError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Invalid Argument.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUInvalidArgError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Invalid Argument.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUInvalidArgError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUNotSupportedError","title":"ZeusGPUNotSupportedError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Not Supported Operation on GPU.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUNotSupportedError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Not Supported Operation on GPU.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUNotSupportedError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUNoPermissionError","title":"ZeusGPUNoPermissionError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps No Permission to perform GPU operation.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUNoPermissionError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps No Permission to perform GPU operation.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUNoPermissionError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUAlreadyInitializedError","title":"ZeusGPUAlreadyInitializedError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Already Initialized GPU.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUAlreadyInitializedError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Already Initialized GPU.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUAlreadyInitializedError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUNotFoundError","title":"ZeusGPUNotFoundError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Not Found GPU.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUNotFoundError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Not Found GPU.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUNotFoundError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUInsufficientSizeError","title":"ZeusGPUInsufficientSizeError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Insufficient Size.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUInsufficientSizeError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Insufficient Size.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUInsufficientSizeError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUInsufficientPowerError","title":"ZeusGPUInsufficientPowerError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Insufficient Power.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUInsufficientPowerError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Insufficient Power.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUInsufficientPowerError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUDriverNotLoadedError","title":"ZeusGPUDriverNotLoadedError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Driver Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUDriverNotLoadedError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Driver Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUDriverNotLoadedError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUTimeoutError","title":"ZeusGPUTimeoutError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Timeout Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUTimeoutError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Timeout Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUTimeoutError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUIRQError","title":"ZeusGPUIRQError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps IRQ Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUIRQError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps IRQ Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUIRQError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPULibraryNotFoundError","title":"ZeusGPULibraryNotFoundError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Library Not Found Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPULibraryNotFoundError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Library Not Found Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPULibraryNotFoundError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUFunctionNotFoundError","title":"ZeusGPUFunctionNotFoundError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Function Not Found Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUFunctionNotFoundError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Function Not Found Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUFunctionNotFoundError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUCorruptedInfoROMError","title":"ZeusGPUCorruptedInfoROMError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Corrupted Info ROM Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUCorruptedInfoROMError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Corrupted Info ROM Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUCorruptedInfoROMError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPULostError","title":"ZeusGPULostError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Lost GPU Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPULostError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Lost GPU Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPULostError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUResetRequiredError","title":"ZeusGPUResetRequiredError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Reset Required Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUResetRequiredError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Reset Required Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUResetRequiredError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUOperatingSystemError","title":"ZeusGPUOperatingSystemError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Operating System Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUOperatingSystemError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Operating System Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUOperatingSystemError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPULibRMVersionMismatchError","title":"ZeusGPULibRMVersionMismatchError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps LibRM Version Mismatch Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPULibRMVersionMismatchError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps LibRM Version Mismatch Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPULibRMVersionMismatchError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUMemoryError","title":"ZeusGPUMemoryError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Insufficient Memory Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUMemoryError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Insufficient Memory Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUMemoryError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUUnknownError","title":"ZeusGPUUnknownError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Unknown Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUUnknownError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Unknown Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUUnknownError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUHeterogeneousError","title":"ZeusGPUHeterogeneousError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Exception for when GPUs are not homogeneous.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUHeterogeneousError(ZeusBaseGPUError):\n    \"\"\"Exception for when GPUs are not homogeneous.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.ZeusGPUHeterogeneousError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.has_sys_admin","title":"has_sys_admin  <code>cached</code>","text":"<pre><code>has_sys_admin()\n</code></pre> <p>Check if the current process has <code>SYS_ADMIN</code> capabilities.</p> Source code in <code>zeus/device/common.py</code> <pre><code>@lru_cache(maxsize=1)\ndef has_sys_admin() -&gt; bool:\n    \"\"\"Check if the current process has `SYS_ADMIN` capabilities.\"\"\"\n    # First try to read procfs.\n    try:\n        with open(\"/proc/self/status\") as f:\n            for line in f:\n                if line.startswith(\"CapEff\"):\n                    bitmask = int(line.strip().split()[1], 16)\n                    has = bool(bitmask &amp; (1 &lt;&lt; 21))\n                    logger.info(\n                        \"Read security capabilities from /proc/self/status -- SYS_ADMIN: %s\",\n                        has,\n                    )\n                    return has\n    except Exception:\n        logger.info(\"Failed to read capabilities from /proc/self/status\", exc_info=True)\n\n    # If that fails, try to use the capget syscall.\n    class CapHeader(ctypes.Structure):\n        _fields_ = [(\"version\", ctypes.c_uint32), (\"pid\", ctypes.c_int)]\n\n    class CapData(ctypes.Structure):\n        _fields_ = [\n            (\"effective\", ctypes.c_uint32),\n            (\"permitted\", ctypes.c_uint32),\n            (\"inheritable\", ctypes.c_uint32),\n        ]\n\n    # Attempt to load libc and set up capget\n    try:\n        libc = ctypes.CDLL(\"libc.so.6\")\n        capget = libc.capget\n        capget.argtypes = [ctypes.POINTER(CapHeader), ctypes.POINTER(CapData)]\n        capget.restype = ctypes.c_int\n    except Exception:\n        logger.info(\"Failed to load libc.so.6\", exc_info=True)\n        return False\n\n    # Initialize the header and data structures\n    header = CapHeader(version=0x20080522, pid=0)  # Use the current process\n    data = CapData()\n\n    # Call capget and check for errors\n    if capget(ctypes.byref(header), ctypes.byref(data)) != 0:\n        errno = ctypes.get_errno()\n        logger.info(\"capget failed with error: %s (errno %s)\", os.strerror(errno), errno)\n        return False\n\n    bitmask = data.effective\n    has = bool(bitmask &amp; (1 &lt;&lt; 21))\n    logger.info(\"Read security capabilities from capget -- SYS_ADMIN: %s\", has)\n    return has\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.deprecated_alias","title":"deprecated_alias","text":"<pre><code>deprecated_alias(old_name)\n</code></pre> <p>Decorator that marks a method to have a deprecated camelCase alias.</p> <p>Apply this decorator to the new snake_case method. When the old camelCase name is called, it will emit a deprecation warning once and then call the new snake_case method.</p> Example <pre><code>@deprecated_alias(\"getName\")\ndef get_name(self):\n    return \"GPU Name\"\n</code></pre> <p>The class using this decorator should use <code>DeprecatedAliasABCMeta</code> as its metaclass.</p> <p>Parameters:</p> Name Type Description Default <code>old_name</code> <code>str</code> <p>The old camelCase method name to create as a deprecated alias.</p> required <p>Returns:</p> Type Description <code>Callable[[Callable], Callable]</code> <p>The decorated function with the <code>_deprecated_alias</code> attribute set.</p> Source code in <code>zeus/device/common.py</code> <pre><code>def deprecated_alias(old_name: str) -&gt; Callable[[Callable], Callable]:\n    \"\"\"Decorator that marks a method to have a deprecated camelCase alias.\n\n    Apply this decorator to the new snake_case method. When the old camelCase\n    name is called, it will emit a deprecation warning once and then call the\n    new snake_case method.\n\n    Example:\n        ```python\n        @deprecated_alias(\"getName\")\n        def get_name(self):\n            return \"GPU Name\"\n        ```\n\n    The class using this decorator should use `DeprecatedAliasABCMeta` as its metaclass.\n\n    Args:\n        old_name: The old camelCase method name to create as a deprecated alias.\n\n    Returns:\n        The decorated function with the `_deprecated_alias` attribute set.\n    \"\"\"\n\n    def decorator(func):\n        func._deprecated_alias = old_name\n        return func\n\n    return decorator\n</code></pre>"},{"location":"reference/device/gpu/#zeus.device.gpu.get_gpus","title":"get_gpus","text":"<pre><code>get_gpus(ensure_homogeneous=False)\n</code></pre> <p>Initialize and return a singleton object for GPU management.</p> <p>This function returns a GPU management object that aims to abstract the underlying GPU vendor and their specific monitoring library (pynvml for NVIDIA GPUs and amdsmi for AMD GPUs). Management APIs are mapped to methods on the returned <code>GPUs</code> object.</p> <p>GPU availability is checked in the following order:</p> <ol> <li>NVIDIA GPUs using <code>pynvml</code></li> <li>AMD GPUs using <code>amdsmi</code></li> <li>If both are unavailable, a <code>ZeusGPUInitError</code> is raised.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>ensure_homogeneous</code> <code>bool</code> <p>If True, ensures that all tracked GPUs have the same name.</p> <code>False</code> Source code in <code>zeus/device/gpu/__init__.py</code> <pre><code>def get_gpus(ensure_homogeneous: bool = False) -&gt; GPUs:\n    \"\"\"Initialize and return a singleton object for GPU management.\n\n    This function returns a GPU management object that aims to abstract\n    the underlying GPU vendor and their specific monitoring library\n    (pynvml for NVIDIA GPUs and amdsmi for AMD GPUs). Management APIs\n    are mapped to methods on the returned [`GPUs`][zeus.device.gpu.GPUs] object.\n\n    GPU availability is checked in the following order:\n\n    1. NVIDIA GPUs using `pynvml`\n    1. AMD GPUs using `amdsmi`\n    1. If both are unavailable, a `ZeusGPUInitError` is raised.\n\n    Args:\n        ensure_homogeneous (bool): If True, ensures that all tracked GPUs have the same name.\n    \"\"\"\n    global _gpus\n    if _gpus is not None:\n        return _gpus\n\n    if nvml_is_available():\n        _gpus = NVIDIAGPUs(ensure_homogeneous)\n        return _gpus\n    elif amdsmi_is_available():\n        _gpus = AMDGPUs(ensure_homogeneous)\n        return _gpus\n    else:\n        raise ZeusGPUInitError(\"NVML and AMDSMI unavailable. Failed to initialize GPU management library.\")\n</code></pre>"},{"location":"reference/device/gpu/amd/","title":"amd","text":""},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd","title":"zeus.device.gpu.amd","text":"<p>AMD GPUs.</p>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.MockAMDSMI","title":"MockAMDSMI","text":"<p>Mock class for AMD SMI library.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>class MockAMDSMI:\n    \"\"\"Mock class for AMD SMI library.\"\"\"\n\n    def __getattr__(self, name):\n        \"\"\"Raise an error if any method is called.\n\n        Since this class is only used when `amdsmi` is not available,\n        something has gone wrong if any method is called.\n        \"\"\"\n        raise RuntimeError(f\"amdsmi is not available and amdsmi.{name} shouldn't have been called. This is a bug.\")\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.MockAMDSMI.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(name)\n</code></pre> <p>Raise an error if any method is called.</p> <p>Since this class is only used when <code>amdsmi</code> is not available, something has gone wrong if any method is called.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>def __getattr__(self, name):\n    \"\"\"Raise an error if any method is called.\n\n    Since this class is only used when `amdsmi` is not available,\n    something has gone wrong if any method is called.\n    \"\"\"\n    raise RuntimeError(f\"amdsmi is not available and amdsmi.{name} shouldn't have been called. This is a bug.\")\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU","title":"AMDGPU","text":"<p>               Bases: <code>GPU</code></p> <p>Implementation of <code>GPU</code> for AMD GPUs.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>class AMDGPU(gpu_common.GPU):\n    \"\"\"Implementation of `GPU` for AMD GPUs.\"\"\"\n\n    def __init__(self, gpu_index: int) -&gt; None:\n        \"\"\"Initialize the GPU object.\"\"\"\n        super().__init__(gpu_index)\n        self._get_handle()\n\n        # These values are updated in AMDGPUs constructor\n        self._supports_get_total_energy_consumption = True\n        self._supports_instant_power_usage = True\n        self._supports_average_power_usage = True\n        self._is_dual_die_odd_chiplet = False  # Set for MI250/MI250X odd-indexed GPUs\n\n    _exception_map = {\n        1: gpu_common.ZeusGPUInvalidArgError,  # amdsmi.amdsmi_wrapper.AMDSMI_STATUS_INVAL\n        2: gpu_common.ZeusGPUNotSupportedError,  # amdsmi.amdsmi_wrapper.AMDSMI_STATUS_NOT_SUPPORTED\n        8: gpu_common.ZeusGPUTimeoutError,  # amdsmi.amdsmi_wrapper.AMDSMI_STATUS_TIMEOUT\n        10: gpu_common.ZeusGPUNoPermissionError,  # amdsmi.amdsmi_wrapper.AMDSMI_STATUS_NO_PERM\n        15: gpu_common.ZeusGPUMemoryError,  # amdsmi.amdsmi_wrapper.AMDSMI_STATUS_OUT_OF_RESOURCES\n        18: gpu_common.ZeusGPUInitError,  # amdsmi.amdsmi_wrapper.AMDSMI_STATUS_INIT_ERROR\n        31: gpu_common.ZeusGPUNotFoundError,  # amdsmi.amdsmi_wrapper.AMDSMI_STATUS_NOT_FOUND\n        32: gpu_common.ZeusGPUInitError,  # amdsmi.amdsmi_wrapper.AMDSMI_STATUS_NOT_INIT\n        34: gpu_common.ZeusGPUDriverNotLoadedError,  # amdsmi.amdsmi_wrapper.AMDSMI_STATUS_DRIVER_NOT_LOADED\n        41: gpu_common.ZeusGPUInsufficientSizeError,  # amdsmi.amdsmi_wrapper.AMDSMI_STATUS_INSUFFICIENT_SIZE\n        45: gpu_common.ZeusGPUDriverNotLoadedError,  # amdsmi.amdsmi_wrapper.AMDSMI_NO_ENERGY_DRV\n        46: gpu_common.ZeusGPUDriverNotLoadedError,  # amdsmi.amdsmi_wrapper.AMDSMI_NO_MSR_DRV\n        47: gpu_common.ZeusGPUDriverNotLoadedError,  # amdsmi.amdsmi_wrapper.AMDSMI_NO_HSMP_DRV\n        48: gpu_common.ZeusGPUNotSupportedError,  # amdsmi.amdsmi_wrapper.AMDSMI_NO_HSMP_SUP\n        49: gpu_common.ZeusGPUNotSupportedError,  # amdsmi.amdsmi_wrapper.AMDSMI_NO_HSMP_MSG_SUP\n        50: gpu_common.ZeusGPUTimeoutError,  # amdsmi.amdsmi_wrapper.AMDSMI_HSMP_TIMEOUT\n        51: gpu_common.ZeusGPUDriverNotLoadedError,  # amdsmi.amdsmi_wrapper.AMDSMI_NO_DRV\n        52: gpu_common.ZeusGPULibraryNotFoundError,  # amdsmi.amdsmi_wrapper.AMDSMI_FILE_NOT_FOUND\n        53: gpu_common.ZeusGPUInvalidArgError,  # amdsmi.amdsmi_wrapper.AMDSMI_ARG_PTR_NULL\n        4294967295: gpu_common.ZeusGPUUnknownError,  # amdsmi.amdsmi_wrapper.AMDSMI_STATUS_UNKNOWN_ERROR\n    }\n\n    @_handle_amdsmi_errors\n    def _get_handle(self):\n        handles = amdsmi.amdsmi_get_processor_handles()\n        if len(handles) &lt;= self.gpu_index:\n            raise gpu_common.ZeusGPUNotFoundError(\n                f\"GPU with index {self.gpu_index} not found. Found {len(handles)} GPUs.\"\n            )\n        self.handle = amdsmi.amdsmi_get_processor_handles()[self.gpu_index]\n\n    @_handle_amdsmi_errors\n    def get_name(self) -&gt; str:\n        \"\"\"Return the name of the GPU model.\"\"\"\n        info = amdsmi.amdsmi_get_gpu_asic_info(self.handle)\n        return info[\"market_name\"]\n\n    @property\n    def supports_nonblocking_setters(self) -&gt; bool:\n        \"\"\"Return True if the GPU object supports non-blocking configuration setters.\"\"\"\n        return False\n\n    @_handle_amdsmi_errors\n    def get_power_management_limit_constraints(self) -&gt; tuple[int, int]:\n        \"\"\"Return the minimum and maximum power management limits. Units: mW.\"\"\"\n        info = amdsmi.amdsmi_get_power_cap_info(self.handle)  # Returns in uW\n        min_power_cap, res = divmod(info[\"min_power_cap\"], 1000)\n        if res != 0:\n            logger.warning(\n                \"Minimum power cap for GPU %d is not a multiple of 1000 uW: %d uW\",\n                self.gpu_index,\n                info[\"min_power_cap\"],\n            )\n        max_power_cap, res = divmod(info[\"max_power_cap\"], 1000)\n        if res != 0:\n            logger.warning(\n                \"Maximum power cap for GPU %d is not a multiple of 1000 uW: %d uW\",\n                self.gpu_index,\n                info[\"max_power_cap\"],\n            )\n        return (int(min_power_cap), int(max_power_cap))\n\n    @_handle_amdsmi_errors\n    def get_power_management_limit(self) -&gt; int:\n        \"\"\"Return the current power management limit. Units: mW.\"\"\"\n        info = amdsmi.amdsmi_get_power_cap_info(self.handle)  # Returns in uW\n        power_cap, res = divmod(info[\"power_cap\"], 1000)\n        if res != 0:\n            logger.warning(\n                \"Current power cap for GPU %d is not a multiple of 1000 uW: %d uW\",\n                self.gpu_index,\n                info[\"power_cap\"],\n            )\n        return int(power_cap)\n\n    @_handle_amdsmi_errors\n    def set_power_management_limit(self, power_limit_mw: int, block: bool = True) -&gt; None:\n        \"\"\"Set the GPU's power management limit. Unit: mW.\"\"\"\n        current_limit = self.get_power_management_limit()\n        if current_limit == power_limit_mw:\n            return\n\n        self._warn_sys_admin()\n        # Units for set_power_cap is uW\n        amdsmi.amdsmi_set_power_cap(self.handle, 0, int(power_limit_mw * 1000))\n\n    @_handle_amdsmi_errors\n    def reset_power_management_limit(self, block: bool = True) -&gt; None:\n        \"\"\"Reset the GPU's power management limit to the default value.\"\"\"\n        info = amdsmi.amdsmi_get_power_cap_info(self.handle)  # Returns in uW\n        default_power_cap_uw = int(info[\"default_power_cap\"])\n        current_limit_mw = self.get_power_management_limit()\n        if current_limit_mw * 1000 == default_power_cap_uw:\n            return\n\n        self._warn_sys_admin()\n        # Units for set_power_cap is uW\n        amdsmi.amdsmi_set_power_cap(self.handle, 0, cap=default_power_cap_uw)\n\n    @_handle_amdsmi_errors\n    def set_persistence_mode(self, enabled: bool, block: bool = True) -&gt; None:\n        \"\"\"Set persistence mode.\"\"\"\n        raise gpu_common.ZeusGPUNotSupportedError(\"Persistence mode is not supported on AMD GPUs.\")\n\n    @_handle_amdsmi_errors\n    def get_supported_memory_clocks(self) -&gt; list[int]:\n        \"\"\"Return a list of supported memory clock frequencies. Units: MHz.\"\"\"\n        info = amdsmi.amdsmi_get_clock_info(self.handle, amdsmi.AmdSmiClkType.MEM)  # returns MHz\n        return [info[\"max_clk\"], info[\"min_clk\"]]\n\n    @_handle_amdsmi_errors\n    def set_memory_locked_clocks(self, min_clock_mhz: int, max_clock_mhz: int, block: bool = True) -&gt; None:\n        \"\"\"Lock the memory clock to a specified range. Units: MHz.\"\"\"\n        self._warn_sys_admin()\n        amdsmi.amdsmi_set_gpu_clk_range(\n            self.handle,\n            min_clock_mhz,\n            max_clock_mhz,\n            clk_type=amdsmi.AmdSmiClkType.MEM,\n        )\n\n    @_handle_amdsmi_errors\n    def reset_memory_locked_clocks(self, block: bool = True) -&gt; None:\n        \"\"\"Reset the locked memory clocks to the default.\"\"\"\n        # Get default MEM clock values\n        info = amdsmi.amdsmi_get_clock_info(self.handle, amdsmi.AmdSmiClkType.MEM)  # returns MHz\n\n        self._warn_sys_admin()\n        amdsmi.amdsmi_set_gpu_clk_range(\n            self.handle,\n            info[\"min_clk\"],\n            info[\"max_clk\"],\n            clk_type=amdsmi.AmdSmiClkType.MEM,\n        )  # expects MHz\n\n    @_handle_amdsmi_errors\n    def get_supported_graphics_clocks(self, memory_clock_mhz: int | None = None) -&gt; list[int]:\n        \"\"\"Return a list of supported graphics clock frequencies. Units: MHz.\n\n        Args:\n            memory_clock_mhz: Memory clock frequency to use. Some GPUs have\n                different supported graphics clocks depending on the memory clock.\n        \"\"\"\n        pass\n        info = amdsmi.amdsmi_get_clock_info(self.handle, amdsmi.AmdSmiClkType.GFX)  # returns MHz\n        return [info[\"max_clk\"], info[\"min_clk\"]]\n\n    @_handle_amdsmi_errors\n    def set_gpu_locked_clocks(self, min_clock_mhz: int, max_clock_mhz: int, block: bool = True) -&gt; None:\n        \"\"\"Lock the GPU clock to a specified range. Units: MHz.\"\"\"\n        self._warn_sys_admin()\n        amdsmi.amdsmi_set_gpu_clk_range(\n            self.handle,\n            min_clock_mhz,\n            max_clock_mhz,\n            clk_type=amdsmi.AmdSmiClkType.GFX,\n        )\n\n    @_handle_amdsmi_errors\n    def reset_gpu_locked_clocks(self, block: bool = True) -&gt; None:\n        \"\"\"Reset the locked GPU clocks to the default.\"\"\"\n        # Get default GPU clock values\n        info = amdsmi.amdsmi_get_clock_info(self.handle, amdsmi.AmdSmiClkType.GFX)  # returns MHz\n\n        self._warn_sys_admin()\n        amdsmi.amdsmi_set_gpu_clk_range(\n            self.handle,\n            info[\"min_clk\"],\n            info[\"max_clk\"],\n            clk_type=amdsmi.AmdSmiClkType.GFX,\n        )  # expects MHz\n\n    def _ensure_not_dual_die_odd_chiplet(self) -&gt; None:\n        \"\"\"Raise an error if the GPU is a chiplet of a dual-die AMD Instinct MI250/MI250X GPU.\"\"\"\n        if self._is_dual_die_odd_chiplet:\n            raise gpu_common.ZeusGPUNotSupportedError(\n                f\"GPU {self.gpu_index} is a chiplet of a dual-die AMD Instinct MI250/MI250X GPU \"\n                f\"that does not support individual power monitoring. AMD's driver only reports power \"\n                f\"for GPU {self.gpu_index - 1}, which represents the COMBINED power draw of BOTH chiplets \"\n                f\"(GPU {self.gpu_index - 1} and GPU {self.gpu_index}).\\n\\n\"\n                f\"To measure power/energy for workloads on this GPU:\\n\"\n                f\"  1. Use GPU {self.gpu_index - 1} for measurements (e.g., PowerMonitor(gpu_indices=[{self.gpu_index - 1}]))\\n\"\n                f\"  2. Be aware that measurements include BOTH chiplets and cannot be separated\\n\"\n                f\"  3. If you run workloads on GPU {self.gpu_index}, its power consumption will be \"\n                f\"included in GPU {self.gpu_index - 1}'s readings\"\n            )\n\n    @_handle_amdsmi_errors\n    def get_average_power_usage(self) -&gt; int:\n        \"\"\"Return the average power draw of the GPU. Units: mW.\"\"\"\n        self._ensure_not_dual_die_odd_chiplet()\n\n        if not self._supports_average_power_usage:\n            raise gpu_common.ZeusGPUNotSupportedError(\n                \"Average power usage is not supported on this AMD GPU. \"\n                \"This is because amdsmi.amdsmi_get_power_info does not return a valid 'average_socket_power'. \"\n                \"Please use `get_instant_power_usage` instead.\"\n            )\n        # returns in W, convert to mW\n        power_info = amdsmi.amdsmi_get_power_info(self.handle)\n        avg_power = power_info[\"average_socket_power\"]\n        if not isinstance(avg_power, int):\n            raise gpu_common.ZeusGPUNotSupportedError(\n                f\"Average power usage is not supported on this AMD GPU. \"\n                f\"amdsmi.amdsmi_get_power_info returned '{avg_power}' for 'average_socket_power'. \"\n                f\"Please use `get_instant_power_usage` instead.\"\n            )\n        return avg_power * 1000\n\n    @_handle_amdsmi_errors\n    def get_instant_power_usage(self) -&gt; int:\n        \"\"\"Return the current power draw of the GPU. Units: mW.\"\"\"\n        self._ensure_not_dual_die_odd_chiplet()\n\n        if not self._supports_instant_power_usage:\n            raise gpu_common.ZeusGPUNotSupportedError(\n                \"Instant power usage is not supported on this AMD GPU. \"\n                \"This is because amdsmi.amdsmi_get_power_info does not return a valid 'current_socket_power'. \"\n                \"Please use `get_average_power_usage` instead.\"\n            )\n        # returns in W, convert to mW\n        return int(amdsmi.amdsmi_get_power_info(self.handle)[\"current_socket_power\"]) * 1000\n\n    @_handle_amdsmi_errors\n    def get_average_memory_power_usage(self) -&gt; int:\n        \"\"\"Return the average power usage of the GPU's memory. Units: mW.\"\"\"\n        raise gpu_common.ZeusGPUNotSupportedError(\"Average memory power usage is not supported on AMD GPUs.\")\n\n    @_handle_amdsmi_errors\n    def supports_get_total_energy_consumption(self) -&gt; bool:\n        \"\"\"Check if the GPU supports retrieving total energy consumption. Returns a future object of the result.\"\"\"\n        return self._supports_get_total_energy_consumption\n\n    @_handle_amdsmi_errors\n    def get_total_energy_consumption(self) -&gt; int:\n        \"\"\"Return the total energy consumption of the GPU since driver load. Units: mJ.\"\"\"\n        if not self._supports_get_total_energy_consumption:\n            raise gpu_common.ZeusGPUNotSupportedError(\n                \"Total energy consumption is not supported on this AMD GPU. \"\n                \"This is because the result of `amdsmi.amdsmi_get_energy_count` is not accurate. \"\n                \"Please use `get_average_power_usage` or `get_instant_power_usage` to calculate energy usage.\"\n            )\n        energy_dict = amdsmi.amdsmi_get_energy_count(self.handle)\n        if \"energy_accumulator\" in energy_dict:  # Changed since amdsmi 6.2.1\n            energy = energy_dict[\"energy_accumulator\"] * energy_dict[\"counter_resolution\"]\n        else:\n            # Old API: assume has key \"power\". If not, exception will be handled by _handle_amdsmi_errors.\n            energy = energy_dict[\"power\"] * energy_dict[\"counter_resolution\"]\n\n        return int(energy / 1e3)  # returns in micro Joules, convert to mili Joules\n\n    @_handle_amdsmi_errors\n    def get_gpu_temperature(self) -&gt; int:\n        \"\"\"Return the current GPU temperature. Units: Celsius.\n\n        We use the hotspot temperatue (as opposed to edge) as we believe it to be more representative\n        of the GPU core's temperature under load.\n        \"\"\"\n        # amdsmi_get_temp_metric returns millidegrees Celsius, convert to Celsius\n        temp_millidegrees = amdsmi.amdsmi_get_temp_metric(\n            self.handle,\n            amdsmi.AmdSmiTemperatureType.HOTSPOT,\n            amdsmi.AmdSmiTemperatureMetric.CURRENT,\n        )\n        return temp_millidegrees // 1000\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.supports_nonblocking_setters","title":"supports_nonblocking_setters  <code>property</code>","text":"<pre><code>supports_nonblocking_setters\n</code></pre> <p>Return True if the GPU object supports non-blocking configuration setters.</p>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.__init__","title":"__init__","text":"<pre><code>__init__(gpu_index)\n</code></pre> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>def __init__(self, gpu_index: int) -&gt; None:\n    \"\"\"Initialize the GPU object.\"\"\"\n    super().__init__(gpu_index)\n    self._get_handle()\n\n    # These values are updated in AMDGPUs constructor\n    self._supports_get_total_energy_consumption = True\n    self._supports_instant_power_usage = True\n    self._supports_average_power_usage = True\n    self._is_dual_die_odd_chiplet = False  # Set for MI250/MI250X odd-indexed GPUs\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.get_name","title":"get_name","text":"<pre><code>get_name()\n</code></pre> <p>Return the name of the GPU model.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef get_name(self) -&gt; str:\n    \"\"\"Return the name of the GPU model.\"\"\"\n    info = amdsmi.amdsmi_get_gpu_asic_info(self.handle)\n    return info[\"market_name\"]\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.get_power_management_limit_constraints","title":"get_power_management_limit_constraints","text":"<pre><code>get_power_management_limit_constraints()\n</code></pre> <p>Return the minimum and maximum power management limits. Units: mW.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef get_power_management_limit_constraints(self) -&gt; tuple[int, int]:\n    \"\"\"Return the minimum and maximum power management limits. Units: mW.\"\"\"\n    info = amdsmi.amdsmi_get_power_cap_info(self.handle)  # Returns in uW\n    min_power_cap, res = divmod(info[\"min_power_cap\"], 1000)\n    if res != 0:\n        logger.warning(\n            \"Minimum power cap for GPU %d is not a multiple of 1000 uW: %d uW\",\n            self.gpu_index,\n            info[\"min_power_cap\"],\n        )\n    max_power_cap, res = divmod(info[\"max_power_cap\"], 1000)\n    if res != 0:\n        logger.warning(\n            \"Maximum power cap for GPU %d is not a multiple of 1000 uW: %d uW\",\n            self.gpu_index,\n            info[\"max_power_cap\"],\n        )\n    return (int(min_power_cap), int(max_power_cap))\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.get_power_management_limit","title":"get_power_management_limit","text":"<pre><code>get_power_management_limit()\n</code></pre> <p>Return the current power management limit. Units: mW.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef get_power_management_limit(self) -&gt; int:\n    \"\"\"Return the current power management limit. Units: mW.\"\"\"\n    info = amdsmi.amdsmi_get_power_cap_info(self.handle)  # Returns in uW\n    power_cap, res = divmod(info[\"power_cap\"], 1000)\n    if res != 0:\n        logger.warning(\n            \"Current power cap for GPU %d is not a multiple of 1000 uW: %d uW\",\n            self.gpu_index,\n            info[\"power_cap\"],\n        )\n    return int(power_cap)\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.set_power_management_limit","title":"set_power_management_limit","text":"<pre><code>set_power_management_limit(power_limit_mw, block=True)\n</code></pre> <p>Set the GPU's power management limit. Unit: mW.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef set_power_management_limit(self, power_limit_mw: int, block: bool = True) -&gt; None:\n    \"\"\"Set the GPU's power management limit. Unit: mW.\"\"\"\n    current_limit = self.get_power_management_limit()\n    if current_limit == power_limit_mw:\n        return\n\n    self._warn_sys_admin()\n    # Units for set_power_cap is uW\n    amdsmi.amdsmi_set_power_cap(self.handle, 0, int(power_limit_mw * 1000))\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.reset_power_management_limit","title":"reset_power_management_limit","text":"<pre><code>reset_power_management_limit(block=True)\n</code></pre> <p>Reset the GPU's power management limit to the default value.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef reset_power_management_limit(self, block: bool = True) -&gt; None:\n    \"\"\"Reset the GPU's power management limit to the default value.\"\"\"\n    info = amdsmi.amdsmi_get_power_cap_info(self.handle)  # Returns in uW\n    default_power_cap_uw = int(info[\"default_power_cap\"])\n    current_limit_mw = self.get_power_management_limit()\n    if current_limit_mw * 1000 == default_power_cap_uw:\n        return\n\n    self._warn_sys_admin()\n    # Units for set_power_cap is uW\n    amdsmi.amdsmi_set_power_cap(self.handle, 0, cap=default_power_cap_uw)\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.set_persistence_mode","title":"set_persistence_mode","text":"<pre><code>set_persistence_mode(enabled, block=True)\n</code></pre> <p>Set persistence mode.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef set_persistence_mode(self, enabled: bool, block: bool = True) -&gt; None:\n    \"\"\"Set persistence mode.\"\"\"\n    raise gpu_common.ZeusGPUNotSupportedError(\"Persistence mode is not supported on AMD GPUs.\")\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.get_supported_memory_clocks","title":"get_supported_memory_clocks","text":"<pre><code>get_supported_memory_clocks()\n</code></pre> <p>Return a list of supported memory clock frequencies. Units: MHz.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef get_supported_memory_clocks(self) -&gt; list[int]:\n    \"\"\"Return a list of supported memory clock frequencies. Units: MHz.\"\"\"\n    info = amdsmi.amdsmi_get_clock_info(self.handle, amdsmi.AmdSmiClkType.MEM)  # returns MHz\n    return [info[\"max_clk\"], info[\"min_clk\"]]\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.set_memory_locked_clocks","title":"set_memory_locked_clocks","text":"<pre><code>set_memory_locked_clocks(min_clock_mhz, max_clock_mhz, block=True)\n</code></pre> <p>Lock the memory clock to a specified range. Units: MHz.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef set_memory_locked_clocks(self, min_clock_mhz: int, max_clock_mhz: int, block: bool = True) -&gt; None:\n    \"\"\"Lock the memory clock to a specified range. Units: MHz.\"\"\"\n    self._warn_sys_admin()\n    amdsmi.amdsmi_set_gpu_clk_range(\n        self.handle,\n        min_clock_mhz,\n        max_clock_mhz,\n        clk_type=amdsmi.AmdSmiClkType.MEM,\n    )\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.reset_memory_locked_clocks","title":"reset_memory_locked_clocks","text":"<pre><code>reset_memory_locked_clocks(block=True)\n</code></pre> <p>Reset the locked memory clocks to the default.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef reset_memory_locked_clocks(self, block: bool = True) -&gt; None:\n    \"\"\"Reset the locked memory clocks to the default.\"\"\"\n    # Get default MEM clock values\n    info = amdsmi.amdsmi_get_clock_info(self.handle, amdsmi.AmdSmiClkType.MEM)  # returns MHz\n\n    self._warn_sys_admin()\n    amdsmi.amdsmi_set_gpu_clk_range(\n        self.handle,\n        info[\"min_clk\"],\n        info[\"max_clk\"],\n        clk_type=amdsmi.AmdSmiClkType.MEM,\n    )  # expects MHz\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.get_supported_graphics_clocks","title":"get_supported_graphics_clocks","text":"<pre><code>get_supported_graphics_clocks(memory_clock_mhz=None)\n</code></pre> <p>Return a list of supported graphics clock frequencies. Units: MHz.</p> <p>Parameters:</p> Name Type Description Default <code>memory_clock_mhz</code> <code>int | None</code> <p>Memory clock frequency to use. Some GPUs have different supported graphics clocks depending on the memory clock.</p> <code>None</code> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef get_supported_graphics_clocks(self, memory_clock_mhz: int | None = None) -&gt; list[int]:\n    \"\"\"Return a list of supported graphics clock frequencies. Units: MHz.\n\n    Args:\n        memory_clock_mhz: Memory clock frequency to use. Some GPUs have\n            different supported graphics clocks depending on the memory clock.\n    \"\"\"\n    pass\n    info = amdsmi.amdsmi_get_clock_info(self.handle, amdsmi.AmdSmiClkType.GFX)  # returns MHz\n    return [info[\"max_clk\"], info[\"min_clk\"]]\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.set_gpu_locked_clocks","title":"set_gpu_locked_clocks","text":"<pre><code>set_gpu_locked_clocks(min_clock_mhz, max_clock_mhz, block=True)\n</code></pre> <p>Lock the GPU clock to a specified range. Units: MHz.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef set_gpu_locked_clocks(self, min_clock_mhz: int, max_clock_mhz: int, block: bool = True) -&gt; None:\n    \"\"\"Lock the GPU clock to a specified range. Units: MHz.\"\"\"\n    self._warn_sys_admin()\n    amdsmi.amdsmi_set_gpu_clk_range(\n        self.handle,\n        min_clock_mhz,\n        max_clock_mhz,\n        clk_type=amdsmi.AmdSmiClkType.GFX,\n    )\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.reset_gpu_locked_clocks","title":"reset_gpu_locked_clocks","text":"<pre><code>reset_gpu_locked_clocks(block=True)\n</code></pre> <p>Reset the locked GPU clocks to the default.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef reset_gpu_locked_clocks(self, block: bool = True) -&gt; None:\n    \"\"\"Reset the locked GPU clocks to the default.\"\"\"\n    # Get default GPU clock values\n    info = amdsmi.amdsmi_get_clock_info(self.handle, amdsmi.AmdSmiClkType.GFX)  # returns MHz\n\n    self._warn_sys_admin()\n    amdsmi.amdsmi_set_gpu_clk_range(\n        self.handle,\n        info[\"min_clk\"],\n        info[\"max_clk\"],\n        clk_type=amdsmi.AmdSmiClkType.GFX,\n    )  # expects MHz\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU._ensure_not_dual_die_odd_chiplet","title":"_ensure_not_dual_die_odd_chiplet","text":"<pre><code>_ensure_not_dual_die_odd_chiplet()\n</code></pre> <p>Raise an error if the GPU is a chiplet of a dual-die AMD Instinct MI250/MI250X GPU.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>def _ensure_not_dual_die_odd_chiplet(self) -&gt; None:\n    \"\"\"Raise an error if the GPU is a chiplet of a dual-die AMD Instinct MI250/MI250X GPU.\"\"\"\n    if self._is_dual_die_odd_chiplet:\n        raise gpu_common.ZeusGPUNotSupportedError(\n            f\"GPU {self.gpu_index} is a chiplet of a dual-die AMD Instinct MI250/MI250X GPU \"\n            f\"that does not support individual power monitoring. AMD's driver only reports power \"\n            f\"for GPU {self.gpu_index - 1}, which represents the COMBINED power draw of BOTH chiplets \"\n            f\"(GPU {self.gpu_index - 1} and GPU {self.gpu_index}).\\n\\n\"\n            f\"To measure power/energy for workloads on this GPU:\\n\"\n            f\"  1. Use GPU {self.gpu_index - 1} for measurements (e.g., PowerMonitor(gpu_indices=[{self.gpu_index - 1}]))\\n\"\n            f\"  2. Be aware that measurements include BOTH chiplets and cannot be separated\\n\"\n            f\"  3. If you run workloads on GPU {self.gpu_index}, its power consumption will be \"\n            f\"included in GPU {self.gpu_index - 1}'s readings\"\n        )\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.get_average_power_usage","title":"get_average_power_usage","text":"<pre><code>get_average_power_usage()\n</code></pre> <p>Return the average power draw of the GPU. Units: mW.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef get_average_power_usage(self) -&gt; int:\n    \"\"\"Return the average power draw of the GPU. Units: mW.\"\"\"\n    self._ensure_not_dual_die_odd_chiplet()\n\n    if not self._supports_average_power_usage:\n        raise gpu_common.ZeusGPUNotSupportedError(\n            \"Average power usage is not supported on this AMD GPU. \"\n            \"This is because amdsmi.amdsmi_get_power_info does not return a valid 'average_socket_power'. \"\n            \"Please use `get_instant_power_usage` instead.\"\n        )\n    # returns in W, convert to mW\n    power_info = amdsmi.amdsmi_get_power_info(self.handle)\n    avg_power = power_info[\"average_socket_power\"]\n    if not isinstance(avg_power, int):\n        raise gpu_common.ZeusGPUNotSupportedError(\n            f\"Average power usage is not supported on this AMD GPU. \"\n            f\"amdsmi.amdsmi_get_power_info returned '{avg_power}' for 'average_socket_power'. \"\n            f\"Please use `get_instant_power_usage` instead.\"\n        )\n    return avg_power * 1000\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.get_instant_power_usage","title":"get_instant_power_usage","text":"<pre><code>get_instant_power_usage()\n</code></pre> <p>Return the current power draw of the GPU. Units: mW.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef get_instant_power_usage(self) -&gt; int:\n    \"\"\"Return the current power draw of the GPU. Units: mW.\"\"\"\n    self._ensure_not_dual_die_odd_chiplet()\n\n    if not self._supports_instant_power_usage:\n        raise gpu_common.ZeusGPUNotSupportedError(\n            \"Instant power usage is not supported on this AMD GPU. \"\n            \"This is because amdsmi.amdsmi_get_power_info does not return a valid 'current_socket_power'. \"\n            \"Please use `get_average_power_usage` instead.\"\n        )\n    # returns in W, convert to mW\n    return int(amdsmi.amdsmi_get_power_info(self.handle)[\"current_socket_power\"]) * 1000\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.get_average_memory_power_usage","title":"get_average_memory_power_usage","text":"<pre><code>get_average_memory_power_usage()\n</code></pre> <p>Return the average power usage of the GPU's memory. Units: mW.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef get_average_memory_power_usage(self) -&gt; int:\n    \"\"\"Return the average power usage of the GPU's memory. Units: mW.\"\"\"\n    raise gpu_common.ZeusGPUNotSupportedError(\"Average memory power usage is not supported on AMD GPUs.\")\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.supports_get_total_energy_consumption","title":"supports_get_total_energy_consumption","text":"<pre><code>supports_get_total_energy_consumption()\n</code></pre> <p>Check if the GPU supports retrieving total energy consumption. Returns a future object of the result.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef supports_get_total_energy_consumption(self) -&gt; bool:\n    \"\"\"Check if the GPU supports retrieving total energy consumption. Returns a future object of the result.\"\"\"\n    return self._supports_get_total_energy_consumption\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.get_total_energy_consumption","title":"get_total_energy_consumption","text":"<pre><code>get_total_energy_consumption()\n</code></pre> <p>Return the total energy consumption of the GPU since driver load. Units: mJ.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef get_total_energy_consumption(self) -&gt; int:\n    \"\"\"Return the total energy consumption of the GPU since driver load. Units: mJ.\"\"\"\n    if not self._supports_get_total_energy_consumption:\n        raise gpu_common.ZeusGPUNotSupportedError(\n            \"Total energy consumption is not supported on this AMD GPU. \"\n            \"This is because the result of `amdsmi.amdsmi_get_energy_count` is not accurate. \"\n            \"Please use `get_average_power_usage` or `get_instant_power_usage` to calculate energy usage.\"\n        )\n    energy_dict = amdsmi.amdsmi_get_energy_count(self.handle)\n    if \"energy_accumulator\" in energy_dict:  # Changed since amdsmi 6.2.1\n        energy = energy_dict[\"energy_accumulator\"] * energy_dict[\"counter_resolution\"]\n    else:\n        # Old API: assume has key \"power\". If not, exception will be handled by _handle_amdsmi_errors.\n        energy = energy_dict[\"power\"] * energy_dict[\"counter_resolution\"]\n\n    return int(energy / 1e3)  # returns in micro Joules, convert to mili Joules\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPU.get_gpu_temperature","title":"get_gpu_temperature","text":"<pre><code>get_gpu_temperature()\n</code></pre> <p>Return the current GPU temperature. Units: Celsius.</p> <p>We use the hotspot temperatue (as opposed to edge) as we believe it to be more representative of the GPU core's temperature under load.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@_handle_amdsmi_errors\ndef get_gpu_temperature(self) -&gt; int:\n    \"\"\"Return the current GPU temperature. Units: Celsius.\n\n    We use the hotspot temperatue (as opposed to edge) as we believe it to be more representative\n    of the GPU core's temperature under load.\n    \"\"\"\n    # amdsmi_get_temp_metric returns millidegrees Celsius, convert to Celsius\n    temp_millidegrees = amdsmi.amdsmi_get_temp_metric(\n        self.handle,\n        amdsmi.AmdSmiTemperatureType.HOTSPOT,\n        amdsmi.AmdSmiTemperatureMetric.CURRENT,\n    )\n    return temp_millidegrees // 1000\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPUs","title":"AMDGPUs","text":"<p>               Bases: <code>GPUs</code></p> <p>AMD GPU Manager object, containing individual AMDGPU objects, abstracting amdsmi calls and handling related exceptions.</p> <p>Important</p> <p>Currently only ROCm &gt;= 6.1 is supported.</p> <p><code>HIP_VISIBLE_DEVICES</code> environment variable is respected if set. For example, if there are 4 GPUs on the node and <code>HIP_VISIBLE_DEVICES=0,2</code>, only GPUs 0 and 2 are instantiated. In this case, to access GPU of HIP index 0, use the index 0, and for HIP index 2, use the index 1.</p> <p>When <code>HIP_VISIBLE_DEVICES</code> is not set but <code>CUDA_VISIBLE_DEVICES</code> is set, <code>CUDA_VISIBLE_DEVICES</code> is honored as if it were <code>HIP_VISIBLE_DEVICES</code>.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>class AMDGPUs(gpu_common.GPUs):\n    \"\"\"AMD GPU Manager object, containing individual AMDGPU objects, abstracting amdsmi calls and handling related exceptions.\n\n    !!! Important\n        Currently only ROCm &gt;= 6.1 is supported.\n\n    `HIP_VISIBLE_DEVICES` environment variable is respected if set.\n    For example, if there are 4 GPUs on the node and `HIP_VISIBLE_DEVICES=0,2`,\n    only GPUs 0 and 2 are instantiated. In this case, to access\n    GPU of HIP index 0, use the index 0, and for HIP index 2, use the index 1.\n\n    When `HIP_VISIBLE_DEVICES` is not set but `CUDA_VISIBLE_DEVICES` is set,\n    `CUDA_VISIBLE_DEVICES` is honored as if it were `HIP_VISIBLE_DEVICES`.\n    \"\"\"\n\n    def __init__(self, ensure_homogeneous: bool = False) -&gt; None:\n        \"\"\"Initialize AMDSMI and sets up the GPUs.\n\n        Args:\n            ensure_homogeneous (bool): If True, ensures that all tracked GPUs have the same name.\n        \"\"\"\n        try:\n            amdsmi.amdsmi_init()\n            self._init_gpus()\n            if ensure_homogeneous:\n                self._ensure_homogeneous()\n        except amdsmi.AmdSmiLibraryException as e:\n            exception_class = AMDGPU._exception_map.get(e.get_error_code(), gpu_common.ZeusBaseGPUError)\n            raise exception_class(e.get_error_info()) from e\n\n    @property\n    def gpus(self) -&gt; Sequence[AMDGPU]:\n        \"\"\"Return a list of AMDGPU objects being tracked.\"\"\"\n        return self._gpus\n\n    def _init_gpus(self) -&gt; None:\n        # Must respect `HIP_VISIBLE_DEVICES` (or `CUDA_VISIBLE_DEVICES`) if set\n        if (visible_device := os.environ.get(\"HIP_VISIBLE_DEVICES\")) is not None or (\n            visible_device := os.environ.get(\"CUDA_VISIBLE_DEVICES\")\n        ) is not None:\n            if not visible_device:\n                raise gpu_common.ZeusGPUInitError(\n                    \"HIP_VISIBLE_DEVICES or CUDA_VISIBLE_DEVICES is set but empty. \"\n                    \"You can use either one for AMD GPUs, but it should either be unset \"\n                    \"or a comma-separated list of GPU indices.\"\n                )\n            visible_indices = [int(idx) for idx in visible_device.split(\",\")]\n        else:\n            visible_indices = list(range(len(amdsmi.amdsmi_get_processor_handles())))\n\n        # create the number of visible GPUs\n        self._gpus = [AMDGPU(gpu_num) for gpu_num in visible_indices]\n\n        # set _supports_instant_power_usage for all GPUs\n        # amdsmi.amdsmi_get_power_info[\"current_socket_power\"] returns \"N/A\" if not supported\n        for gpu in self._gpus:\n            gpu._supports_instant_power_usage = isinstance(\n                amdsmi.amdsmi_get_power_info(gpu.handle)[\"current_socket_power\"],\n                int,\n            )\n\n        # set _supports_average_power_usage for all GPUs\n        # amdsmi.amdsmi_get_power_info[\"average_socket_power\"] returns \"N/A\" if not supported\n        for gpu in self._gpus:\n            gpu._supports_average_power_usage = isinstance(\n                amdsmi.amdsmi_get_power_info(gpu.handle)[\"average_socket_power\"],\n                int,\n            )\n\n        # set _supports_get_total_energy_consumption for all GPUs\n        wait_time = 0.5  # seconds\n        # Try to get power for energy validation, fallback if needed\n        powers = []\n        for gpu in self._gpus:\n            try:\n                power = gpu.get_average_power_usage()\n            except gpu_common.ZeusGPUNotSupportedError:\n                try:\n                    power = gpu.get_instant_power_usage()\n                except gpu_common.ZeusGPUNotSupportedError:\n                    # Neither average nor instant power available, use 0\n                    power = 0\n            powers.append(power)\n        initial_energies = [gpu.get_total_energy_consumption() for gpu in self._gpus]\n        time.sleep(wait_time)\n        final_energies = [gpu.get_total_energy_consumption() for gpu in self._gpus]\n        measured_energies = [final - initial for final, initial in zip(final_energies, initial_energies)]\n        expected_energies = [power * wait_time for power in powers]  # energy = power * time\n\n        for gpu, measured_energy, expected_energy in zip(self._gpus, measured_energies, expected_energies):\n            # Check for MI250/MI250X dual-die GPUs and warn users about combined power reporting\n            gpu_name = gpu.get_name()\n            if \"MI250\" in gpu_name:\n                if gpu.gpu_index % 2 == 1 and expected_energy &lt; 0.001:\n                    # This is an odd-indexed MI250/MI250X GPU (chiplet without power reporting)\n                    gpu._is_dual_die_odd_chiplet = True\n                    gpu._supports_get_total_energy_consumption = False\n                    gpu._supports_instant_power_usage = False\n                    gpu._supports_average_power_usage = False\n                    logger.warning(\n                        \"GPU %d is a chiplet of a dual-die AMD Instinct MI250/MI250X GPU. \"\n                        \"AMD's driver only reports power for GPU %d, which represents the COMBINED \"\n                        \"power draw of BOTH chiplets (GPU %d and GPU %d). \"\n                        \"Power and energy measurements are not available for this GPU individually.\",\n                        gpu.gpu_index,\n                        gpu.gpu_index - 1,\n                        gpu.gpu_index - 1,\n                        gpu.gpu_index,\n                    )\n                elif gpu.gpu_index % 2 == 0 and expected_energy &gt;= 0.001:\n                    # This is an even-indexed MI250/MI250X GPU (reports combined power)\n                    logger.warning(\n                        \"GPU %d is a dual-die AMD Instinct MI250/MI250X GPU. \"\n                        \"Power and energy measurements for this GPU represent the COMBINED consumption \"\n                        \"of BOTH chiplets (GPU %d and GPU %d). Workloads running on either chiplet \"\n                        \"will be included in these measurements and cannot be separated.\",\n                        gpu.gpu_index,\n                        gpu.gpu_index,\n                        gpu.gpu_index + 1,\n                    )\n\n            # Check for zero or very small expected_energy to avoid division by zero\n            if expected_energy &lt; 0.001:\n                # Skip if already handled as MI250 odd chiplet above\n                if not gpu._is_dual_die_odd_chiplet:\n                    # Generic case: GPU reports zero power (idle or unsupported)\n                    gpu._supports_get_total_energy_consumption = False\n                    logger.info(\n                        \"Disabling `get_total_energy_consumption` for device %d. \"\n                        \"Power reading is zero or negligible (expected energy: %.3f mJ), \"\n                        \"so energy counter validation cannot be performed. \"\n                        \"You can still measure energy by polling either `get_instant_power_usage` or `get_average_power_usage` and integrating over time.\",\n                        gpu.gpu_index,\n                        expected_energy,\n                    )\n            # Loose bound to rule out very obvious counter problems\n            elif 0.1 &lt; measured_energy / expected_energy &lt; 10:\n                gpu._supports_get_total_energy_consumption = True\n            else:\n                gpu._supports_get_total_energy_consumption = False\n                logger.info(\n                    \"Disabling `get_total_energy_consumption` for device %d. The result of `amdsmi.amdsmi_get_energy_count` is not accurate. Expected energy: %d mJ, Measured energy: %d mJ. \"\n                    \"This is a known issue with some AMD GPUs, please see https://github.com/ROCm/amdsmi/issues/38 for more information. \"\n                    \"You can still measure energy by polling either `get_instant_power_usage` or `get_average_power_usage` and integrating over time.\",\n                    gpu.gpu_index,\n                    expected_energy,\n                    measured_energy,\n                )\n\n    def __del__(self) -&gt; None:\n        \"\"\"Shut down AMDSMI.\"\"\"\n        with contextlib.suppress(amdsmi.AmdSmiException):\n            amdsmi.amdsmi_shut_down()  # Ignore error on shutdown. Neccessary for proper cleanup and test functionality\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPUs.gpus","title":"gpus  <code>property</code>","text":"<pre><code>gpus\n</code></pre> <p>Return a list of AMDGPU objects being tracked.</p>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPUs.__init__","title":"__init__","text":"<pre><code>__init__(ensure_homogeneous=False)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>ensure_homogeneous</code> <code>bool</code> <p>If True, ensures that all tracked GPUs have the same name.</p> <code>False</code> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>def __init__(self, ensure_homogeneous: bool = False) -&gt; None:\n    \"\"\"Initialize AMDSMI and sets up the GPUs.\n\n    Args:\n        ensure_homogeneous (bool): If True, ensures that all tracked GPUs have the same name.\n    \"\"\"\n    try:\n        amdsmi.amdsmi_init()\n        self._init_gpus()\n        if ensure_homogeneous:\n            self._ensure_homogeneous()\n    except amdsmi.AmdSmiLibraryException as e:\n        exception_class = AMDGPU._exception_map.get(e.get_error_code(), gpu_common.ZeusBaseGPUError)\n        raise exception_class(e.get_error_info()) from e\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.AMDGPUs.__del__","title":"__del__","text":"<pre><code>__del__()\n</code></pre> <p>Shut down AMDSMI.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Shut down AMDSMI.\"\"\"\n    with contextlib.suppress(amdsmi.AmdSmiException):\n        amdsmi.amdsmi_shut_down()  # Ignore error on shutdown. Neccessary for proper cleanup and test functionality\n</code></pre>"},{"location":"reference/device/gpu/amd/#zeus.device.gpu.amd.amdsmi_is_available","title":"amdsmi_is_available  <code>cached</code>","text":"<pre><code>amdsmi_is_available()\n</code></pre> <p>Check if amdsmi is available.</p> Source code in <code>zeus/device/gpu/amd.py</code> <pre><code>@lru_cache(maxsize=1)\ndef amdsmi_is_available() -&gt; bool:\n    \"\"\"Check if amdsmi is available.\"\"\"\n    try:\n        # `amdsmi` prints to stdout on import when libamd_smi.so is not found.\n        with contextlib.redirect_stdout(None):\n            import amdsmi\n    except ImportError:\n        logger.info(\"amdsmi is not available.\")\n        return False\n    # usually thrown if amdsmi can't find libamd_smi.so\n    except OSError:\n        if os.getenv(\"ROCM_PATH\") is None:\n            logger.warning(\"`ROCM_PATH` is not set. Do you have ROCm installed?\")\n        return False\n    # usually thrown if versions of amdsmi and ROCm are incompatible.\n    except AttributeError:\n        logger.warning(\n            \"Failed to import amdsmi. Ensure amdsmi's version is at least as high as the current ROCm version.\"\n        )\n        return False\n    except KeyError as e:\n        logger.warning(\n            \"Failed to import amdsmi due to a key error on: [%s]. Ensure that amdsmi is installed on your system.\",\n            e,\n        )\n        return False\n    try:\n        amdsmi.amdsmi_init()\n        logger.info(\"amdsmi is available and initialized\")\n        return True\n    except amdsmi.AmdSmiLibraryException as e:\n        logger.info(\"amdsmi is available but could not initialize: %s\", e)\n        return False\n</code></pre>"},{"location":"reference/device/gpu/common/","title":"common","text":""},{"location":"reference/device/gpu/common/#zeus.device.gpu.common","title":"zeus.device.gpu.common","text":"<p>Error wrappers and classes common to all GPU vendors.</p>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU","title":"GPU","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for managing one GPU.</p> <p>For each method, child classes should call into vendor-specific GPU management libraries (e.g., NVML for NVIDIA GPUs).</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class GPU(abc.ABC, metaclass=DeprecatedAliasABCMeta):\n    \"\"\"Abstract base class for managing one GPU.\n\n    For each method, child classes should call into vendor-specific\n    GPU management libraries (e.g., NVML for NVIDIA GPUs).\n    \"\"\"\n\n    def __init__(self, gpu_index: int) -&gt; None:\n        \"\"\"Initializ the GPU with a specified index.\"\"\"\n        self.gpu_index = gpu_index\n\n    def _warn_sys_admin(self) -&gt; None:\n        \"\"\"Warn the user if the current process doesn't have `SYS_ADMIN` privileges.\"\"\"\n        # Deriving classes can disable this warning by setting this attribute.\n        if not getattr(self, \"_disable_sys_admin_warning\", False) and not has_sys_admin():\n            warnings.warn(\n                \"You are about to call a GPU management API that requires \"\n                \"`SYS_ADMIN` privileges. Some energy optimizers that change the \"\n                \"GPU's power settings need this.\\nSee \"\n                \"https://ml.energy/zeus/getting_started/#system-privileges \"\n                \"for more information and how to obtain `SYS_ADMIN`.\",\n                stacklevel=2,\n            )\n            # Only warn once.\n            self._disable_sys_admin_warning = True\n\n    @property\n    @abc.abstractmethod\n    def supports_nonblocking_setters(self) -&gt; bool:\n        \"\"\"Return True if the GPU object supports non-blocking configuration setters.\"\"\"\n        return False\n\n    @deprecated_alias(\"getName\")\n    @abc.abstractmethod\n    def get_name(self) -&gt; str:\n        \"\"\"Return the name of the GPU model.\"\"\"\n        pass\n\n    @deprecated_alias(\"getPowerManagementLimitConstraints\")\n    @abc.abstractmethod\n    def get_power_management_limit_constraints(self) -&gt; tuple[int, int]:\n        \"\"\"Return the minimum and maximum power management limits. Units: mW.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def get_power_management_limit(self) -&gt; int:\n        \"\"\"Return the current power management limit. Units: mW.\"\"\"\n        pass\n\n    @deprecated_alias(\"setPowerManagementLimit\")\n    @abc.abstractmethod\n    def set_power_management_limit(self, power_limit_mw: int, block: bool = True) -&gt; None:\n        \"\"\"Set the GPU's power management limit. Unit: mW.\"\"\"\n        pass\n\n    @deprecated_alias(\"resetPowerManagementLimit\")\n    @abc.abstractmethod\n    def reset_power_management_limit(self, block: bool = True) -&gt; None:\n        \"\"\"Reset the GPU's power management limit to the default value.\"\"\"\n        pass\n\n    @deprecated_alias(\"setPersistenceMode\")\n    @abc.abstractmethod\n    def set_persistence_mode(self, enabled: bool, block: bool = True) -&gt; None:\n        \"\"\"Set persistence mode.\"\"\"\n        pass\n\n    @deprecated_alias(\"getSupportedMemoryClocks\")\n    @abc.abstractmethod\n    def get_supported_memory_clocks(self) -&gt; list[int]:\n        \"\"\"Return a list of supported memory clock frequencies. Units: MHz.\"\"\"\n        pass\n\n    @deprecated_alias(\"setMemoryLockedClocks\")\n    @abc.abstractmethod\n    def set_memory_locked_clocks(self, min_clock_mhz: int, max_clock_mhz: int, block: bool = True) -&gt; None:\n        \"\"\"Lock the memory clock to a specified range. Units: MHz.\"\"\"\n        pass\n\n    @deprecated_alias(\"resetMemoryLockedClocks\")\n    @abc.abstractmethod\n    def reset_memory_locked_clocks(self, block: bool = True) -&gt; None:\n        \"\"\"Reset the locked memory clocks to the default.\"\"\"\n        pass\n\n    @deprecated_alias(\"getSupportedGraphicsClocks\")\n    @abc.abstractmethod\n    def get_supported_graphics_clocks(self, memory_clock_mhz: int | None = None) -&gt; list[int]:\n        \"\"\"Return a list of supported graphics clock frequencies. Units: MHz.\n\n        Args:\n            memory_clock_mhz: Memory clock frequency to use. Some GPUs have\n                different supported graphics clocks depending on the memory clock.\n        \"\"\"\n        pass\n\n    @deprecated_alias(\"setGpuLockedClocks\")\n    @abc.abstractmethod\n    def set_gpu_locked_clocks(self, min_clock_mhz: int, max_clock_mhz: int, block: bool = True) -&gt; None:\n        \"\"\"Lock the GPU clock to a specified range. Units: MHz.\"\"\"\n        pass\n\n    @deprecated_alias(\"resetGpuLockedClocks\")\n    @abc.abstractmethod\n    def reset_gpu_locked_clocks(self, block: bool = True) -&gt; None:\n        \"\"\"Reset the locked GPU clocks to the default.\"\"\"\n        pass\n\n    @deprecated_alias(\"getAveragePowerUsage\")\n    @abc.abstractmethod\n    def get_average_power_usage(self) -&gt; int:\n        \"\"\"Return the average power usage of the GPU. Units: mW.\"\"\"\n        pass\n\n    @deprecated_alias(\"getInstantPowerUsage\")\n    @abc.abstractmethod\n    def get_instant_power_usage(self) -&gt; int:\n        \"\"\"Return the current power draw of the GPU. Units: mW.\"\"\"\n        pass\n\n    @deprecated_alias(\"getAverageMemoryPowerUsage\")\n    @abc.abstractmethod\n    def get_average_memory_power_usage(self) -&gt; int:\n        \"\"\"Return the average power usage of the GPU's memory. Units: mW.\"\"\"\n        pass\n\n    @deprecated_alias(\"supportsGetTotalEnergyConsumption\")\n    @abc.abstractmethod\n    def supports_get_total_energy_consumption(self) -&gt; bool:\n        \"\"\"Check if the GPU supports retrieving total energy consumption.\"\"\"\n        pass\n\n    @deprecated_alias(\"getTotalEnergyConsumption\")\n    @abc.abstractmethod\n    def get_total_energy_consumption(self) -&gt; int:\n        \"\"\"Return the total energy consumption of the GPU since driver load. Units: mJ.\"\"\"\n        pass\n\n    @deprecated_alias(\"getGpuTemperature\")\n    @abc.abstractmethod\n    def get_gpu_temperature(self) -&gt; int:\n        \"\"\"Return the current GPU temperature. Units: Celsius.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.supports_nonblocking_setters","title":"supports_nonblocking_setters  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>supports_nonblocking_setters\n</code></pre> <p>Return True if the GPU object supports non-blocking configuration setters.</p>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.__init__","title":"__init__","text":"<pre><code>__init__(gpu_index)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, gpu_index: int) -&gt; None:\n    \"\"\"Initializ the GPU with a specified index.\"\"\"\n    self.gpu_index = gpu_index\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU._warn_sys_admin","title":"_warn_sys_admin","text":"<pre><code>_warn_sys_admin()\n</code></pre> <p>Warn the user if the current process doesn't have <code>SYS_ADMIN</code> privileges.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def _warn_sys_admin(self) -&gt; None:\n    \"\"\"Warn the user if the current process doesn't have `SYS_ADMIN` privileges.\"\"\"\n    # Deriving classes can disable this warning by setting this attribute.\n    if not getattr(self, \"_disable_sys_admin_warning\", False) and not has_sys_admin():\n        warnings.warn(\n            \"You are about to call a GPU management API that requires \"\n            \"`SYS_ADMIN` privileges. Some energy optimizers that change the \"\n            \"GPU's power settings need this.\\nSee \"\n            \"https://ml.energy/zeus/getting_started/#system-privileges \"\n            \"for more information and how to obtain `SYS_ADMIN`.\",\n            stacklevel=2,\n        )\n        # Only warn once.\n        self._disable_sys_admin_warning = True\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.get_name","title":"get_name  <code>abstractmethod</code>","text":"<pre><code>get_name()\n</code></pre> <p>Return the name of the GPU model.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getName\")\n@abc.abstractmethod\ndef get_name(self) -&gt; str:\n    \"\"\"Return the name of the GPU model.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.get_power_management_limit_constraints","title":"get_power_management_limit_constraints  <code>abstractmethod</code>","text":"<pre><code>get_power_management_limit_constraints()\n</code></pre> <p>Return the minimum and maximum power management limits. Units: mW.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getPowerManagementLimitConstraints\")\n@abc.abstractmethod\ndef get_power_management_limit_constraints(self) -&gt; tuple[int, int]:\n    \"\"\"Return the minimum and maximum power management limits. Units: mW.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.get_power_management_limit","title":"get_power_management_limit  <code>abstractmethod</code>","text":"<pre><code>get_power_management_limit()\n</code></pre> <p>Return the current power management limit. Units: mW.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@abc.abstractmethod\ndef get_power_management_limit(self) -&gt; int:\n    \"\"\"Return the current power management limit. Units: mW.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.set_power_management_limit","title":"set_power_management_limit  <code>abstractmethod</code>","text":"<pre><code>set_power_management_limit(power_limit_mw, block=True)\n</code></pre> <p>Set the GPU's power management limit. Unit: mW.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"setPowerManagementLimit\")\n@abc.abstractmethod\ndef set_power_management_limit(self, power_limit_mw: int, block: bool = True) -&gt; None:\n    \"\"\"Set the GPU's power management limit. Unit: mW.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.reset_power_management_limit","title":"reset_power_management_limit  <code>abstractmethod</code>","text":"<pre><code>reset_power_management_limit(block=True)\n</code></pre> <p>Reset the GPU's power management limit to the default value.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"resetPowerManagementLimit\")\n@abc.abstractmethod\ndef reset_power_management_limit(self, block: bool = True) -&gt; None:\n    \"\"\"Reset the GPU's power management limit to the default value.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.set_persistence_mode","title":"set_persistence_mode  <code>abstractmethod</code>","text":"<pre><code>set_persistence_mode(enabled, block=True)\n</code></pre> <p>Set persistence mode.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"setPersistenceMode\")\n@abc.abstractmethod\ndef set_persistence_mode(self, enabled: bool, block: bool = True) -&gt; None:\n    \"\"\"Set persistence mode.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.get_supported_memory_clocks","title":"get_supported_memory_clocks  <code>abstractmethod</code>","text":"<pre><code>get_supported_memory_clocks()\n</code></pre> <p>Return a list of supported memory clock frequencies. Units: MHz.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getSupportedMemoryClocks\")\n@abc.abstractmethod\ndef get_supported_memory_clocks(self) -&gt; list[int]:\n    \"\"\"Return a list of supported memory clock frequencies. Units: MHz.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.set_memory_locked_clocks","title":"set_memory_locked_clocks  <code>abstractmethod</code>","text":"<pre><code>set_memory_locked_clocks(min_clock_mhz, max_clock_mhz, block=True)\n</code></pre> <p>Lock the memory clock to a specified range. Units: MHz.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"setMemoryLockedClocks\")\n@abc.abstractmethod\ndef set_memory_locked_clocks(self, min_clock_mhz: int, max_clock_mhz: int, block: bool = True) -&gt; None:\n    \"\"\"Lock the memory clock to a specified range. Units: MHz.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.reset_memory_locked_clocks","title":"reset_memory_locked_clocks  <code>abstractmethod</code>","text":"<pre><code>reset_memory_locked_clocks(block=True)\n</code></pre> <p>Reset the locked memory clocks to the default.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"resetMemoryLockedClocks\")\n@abc.abstractmethod\ndef reset_memory_locked_clocks(self, block: bool = True) -&gt; None:\n    \"\"\"Reset the locked memory clocks to the default.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.get_supported_graphics_clocks","title":"get_supported_graphics_clocks  <code>abstractmethod</code>","text":"<pre><code>get_supported_graphics_clocks(memory_clock_mhz=None)\n</code></pre> <p>Return a list of supported graphics clock frequencies. Units: MHz.</p> <p>Parameters:</p> Name Type Description Default <code>memory_clock_mhz</code> <code>int | None</code> <p>Memory clock frequency to use. Some GPUs have different supported graphics clocks depending on the memory clock.</p> <code>None</code> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getSupportedGraphicsClocks\")\n@abc.abstractmethod\ndef get_supported_graphics_clocks(self, memory_clock_mhz: int | None = None) -&gt; list[int]:\n    \"\"\"Return a list of supported graphics clock frequencies. Units: MHz.\n\n    Args:\n        memory_clock_mhz: Memory clock frequency to use. Some GPUs have\n            different supported graphics clocks depending on the memory clock.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.set_gpu_locked_clocks","title":"set_gpu_locked_clocks  <code>abstractmethod</code>","text":"<pre><code>set_gpu_locked_clocks(min_clock_mhz, max_clock_mhz, block=True)\n</code></pre> <p>Lock the GPU clock to a specified range. Units: MHz.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"setGpuLockedClocks\")\n@abc.abstractmethod\ndef set_gpu_locked_clocks(self, min_clock_mhz: int, max_clock_mhz: int, block: bool = True) -&gt; None:\n    \"\"\"Lock the GPU clock to a specified range. Units: MHz.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.reset_gpu_locked_clocks","title":"reset_gpu_locked_clocks  <code>abstractmethod</code>","text":"<pre><code>reset_gpu_locked_clocks(block=True)\n</code></pre> <p>Reset the locked GPU clocks to the default.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"resetGpuLockedClocks\")\n@abc.abstractmethod\ndef reset_gpu_locked_clocks(self, block: bool = True) -&gt; None:\n    \"\"\"Reset the locked GPU clocks to the default.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.get_average_power_usage","title":"get_average_power_usage  <code>abstractmethod</code>","text":"<pre><code>get_average_power_usage()\n</code></pre> <p>Return the average power usage of the GPU. Units: mW.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getAveragePowerUsage\")\n@abc.abstractmethod\ndef get_average_power_usage(self) -&gt; int:\n    \"\"\"Return the average power usage of the GPU. Units: mW.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.get_instant_power_usage","title":"get_instant_power_usage  <code>abstractmethod</code>","text":"<pre><code>get_instant_power_usage()\n</code></pre> <p>Return the current power draw of the GPU. Units: mW.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getInstantPowerUsage\")\n@abc.abstractmethod\ndef get_instant_power_usage(self) -&gt; int:\n    \"\"\"Return the current power draw of the GPU. Units: mW.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.get_average_memory_power_usage","title":"get_average_memory_power_usage  <code>abstractmethod</code>","text":"<pre><code>get_average_memory_power_usage()\n</code></pre> <p>Return the average power usage of the GPU's memory. Units: mW.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getAverageMemoryPowerUsage\")\n@abc.abstractmethod\ndef get_average_memory_power_usage(self) -&gt; int:\n    \"\"\"Return the average power usage of the GPU's memory. Units: mW.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.supports_get_total_energy_consumption","title":"supports_get_total_energy_consumption  <code>abstractmethod</code>","text":"<pre><code>supports_get_total_energy_consumption()\n</code></pre> <p>Check if the GPU supports retrieving total energy consumption.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"supportsGetTotalEnergyConsumption\")\n@abc.abstractmethod\ndef supports_get_total_energy_consumption(self) -&gt; bool:\n    \"\"\"Check if the GPU supports retrieving total energy consumption.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.get_total_energy_consumption","title":"get_total_energy_consumption  <code>abstractmethod</code>","text":"<pre><code>get_total_energy_consumption()\n</code></pre> <p>Return the total energy consumption of the GPU since driver load. Units: mJ.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getTotalEnergyConsumption\")\n@abc.abstractmethod\ndef get_total_energy_consumption(self) -&gt; int:\n    \"\"\"Return the total energy consumption of the GPU since driver load. Units: mJ.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPU.get_gpu_temperature","title":"get_gpu_temperature  <code>abstractmethod</code>","text":"<pre><code>get_gpu_temperature()\n</code></pre> <p>Return the current GPU temperature. Units: Celsius.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getGpuTemperature\")\n@abc.abstractmethod\ndef get_gpu_temperature(self) -&gt; int:\n    \"\"\"Return the current GPU temperature. Units: Celsius.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs","title":"GPUs","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract base class for a collection of <code>GPU</code> objects.</p> <p>This is basically a list of <code>GPU</code> objects and forwards most API calls to the individual <code>GPU</code> objects. Still, a separate wrapper class is is needed to for group-level operations like:</p> <ul> <li><code>ensure_homogeneous</code> that ensures that all GPUs have the same name</li> <li>handling vendor-specific environment variables (e.g., <code>CUDA_VISIBLE_DEVICES</code> for NVIDIA)</li> </ul> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class GPUs(abc.ABC, metaclass=DeprecatedAliasABCMeta):\n    \"\"\"An abstract base class for a collection of `GPU` objects.\n\n    This is basically a list of [`GPU`][zeus.device.gpu.common.GPU] objects and forwards\n    most API calls to the individual `GPU` objects. Still, a separate wrapper class is\n    is needed to for group-level operations like:\n\n    - `ensure_homogeneous` that ensures that all GPUs have the same name\n    - handling vendor-specific environment variables (e.g., `CUDA_VISIBLE_DEVICES` for NVIDIA)\n    \"\"\"\n\n    @abc.abstractmethod\n    def __init__(self, ensure_homogeneous: bool = False) -&gt; None:\n        \"\"\"Initialize the GPU management library and initializes `GPU` objects.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def __del__(self) -&gt; None:\n        \"\"\"Shut down the GPU monitoring library to release resources and clean up.\"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def gpus(self) -&gt; Sequence[GPU]:\n        \"\"\"Return a list of GPU objects being tracked.\"\"\"\n        pass\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of GPUs being tracked.\"\"\"\n        return len(self.gpus)\n\n    def _ensure_homogeneous(self) -&gt; None:\n        \"\"\"Ensures that all tracked GPUs are homogeneous in terms of name.\"\"\"\n        gpu_names = [gpu.get_name() for gpu in self.gpus]\n        # Both zero (no GPUs found) and one are fine.\n        if len(set(gpu_names)) &gt; 1:\n            raise ZeusGPUHeterogeneousError(f\"Heterogeneous GPUs found: {gpu_names}\")\n\n    def _warn_sys_admin(self) -&gt; None:\n        \"\"\"Warn the user if the current process doesn't have `SYS_ADMIN` privileges.\"\"\"\n        # Deriving classes can disable this warning by setting this attribute.\n        if not getattr(self, \"_disable_sys_admin_warning\", False) and not has_sys_admin():\n            warnings.warn(\n                \"You are about to call a GPU management API that requires \"\n                \"`SYS_ADMIN` privileges. Some energy optimizers that change the \"\n                \"GPU's power settings need this.\\nSee \"\n                \"https://ml.energy/zeus/getting_started/#system-privileges \"\n                \"for more information and how to obtain `SYS_ADMIN`.\",\n                stacklevel=2,\n            )\n            # Only warn once.\n            self._disable_sys_admin_warning = True\n\n    @deprecated_alias(\"getName\")\n    def get_name(self, gpu_index: int) -&gt; str:\n        \"\"\"Return the name of the specified GPU.\"\"\"\n        return self.gpus[gpu_index].get_name()\n\n    @deprecated_alias(\"getPowerManagementLimitConstraints\")\n    def get_power_management_limit_constraints(self, gpu_index: int) -&gt; tuple[int, int]:\n        \"\"\"Return the minimum and maximum power management limits. Units: mW.\"\"\"\n        return self.gpus[gpu_index].get_power_management_limit_constraints()\n\n    def get_power_management_limit(self, gpu_index: int) -&gt; int:\n        \"\"\"Return the current power management limit. Units: mW.\"\"\"\n        return self.gpus[gpu_index].get_power_management_limit()\n\n    @deprecated_alias(\"setPowerManagementLimit\")\n    def set_power_management_limit(self, gpu_index: int, power_limit_mw: int, block: bool = True) -&gt; None:\n        \"\"\"Set the GPU's power management limit. Unit: mW.\"\"\"\n        self.gpus[gpu_index].set_power_management_limit(power_limit_mw, block)\n\n    @deprecated_alias(\"resetPowerManagementLimit\")\n    def reset_power_management_limit(self, gpu_index: int, block: bool = True) -&gt; None:\n        \"\"\"Reset the GPU's power management limit to the default value.\"\"\"\n        self.gpus[gpu_index].reset_power_management_limit(block)\n\n    @deprecated_alias(\"setPersistenceMode\")\n    def set_persistence_mode(self, gpu_index: int, enabled: bool, block: bool = True) -&gt; None:\n        \"\"\"Set persistence mode for the specified GPU.\"\"\"\n        self.gpus[gpu_index].set_persistence_mode(enabled, block)\n\n    @deprecated_alias(\"getSupportedMemoryClocks\")\n    def get_supported_memory_clocks(self, gpu_index: int) -&gt; list[int]:\n        \"\"\"Return a list of supported memory clock frequencies. Units: MHz.\"\"\"\n        return self.gpus[gpu_index].get_supported_memory_clocks()\n\n    @deprecated_alias(\"setMemoryLockedClocks\")\n    def set_memory_locked_clocks(\n        self,\n        gpu_index: int,\n        min_clock_mhz: int,\n        max_clock_mhz: int,\n        block: bool = True,\n    ) -&gt; None:\n        \"\"\"Lock the memory clock to a specified range. Units: MHz.\"\"\"\n        self.gpus[gpu_index].set_memory_locked_clocks(min_clock_mhz, max_clock_mhz, block)\n\n    @deprecated_alias(\"resetMemoryLockedClocks\")\n    def reset_memory_locked_clocks(self, gpu_index: int, block: bool = True) -&gt; None:\n        \"\"\"Reset the locked memory clocks to the default.\"\"\"\n        self.gpus[gpu_index].reset_memory_locked_clocks(block)\n\n    @deprecated_alias(\"getSupportedGraphicsClocks\")\n    def get_supported_graphics_clocks(self, gpu_index: int, memory_clock_mhz: int | None = None) -&gt; list[int]:\n        \"\"\"Return a list of supported graphics clock frequencies. Units: MHz.\n\n        Args:\n            gpu_index: Index of the GPU to query.\n            memory_clock_mhz: Memory clock frequency to use. Some GPUs have\n                different supported graphics clocks depending on the memory clock.\n        \"\"\"\n        return self.gpus[gpu_index].get_supported_graphics_clocks(memory_clock_mhz)\n\n    @deprecated_alias(\"setGpuLockedClocks\")\n    def set_gpu_locked_clocks(\n        self,\n        gpu_index: int,\n        min_clock_mhz: int,\n        max_clock_mhz: int,\n        block: bool = True,\n    ) -&gt; None:\n        \"\"\"Lock the GPU clock to a specified range. Units: MHz.\"\"\"\n        self.gpus[gpu_index].set_gpu_locked_clocks(min_clock_mhz, max_clock_mhz, block)\n\n    @deprecated_alias(\"resetGpuLockedClocks\")\n    def reset_gpu_locked_clocks(self, gpu_index: int, block: bool = True) -&gt; None:\n        \"\"\"Reset the locked GPU clocks to the default.\"\"\"\n        self.gpus[gpu_index].reset_gpu_locked_clocks(block)\n\n    @deprecated_alias(\"getAveragePowerUsage\")\n    def get_average_power_usage(self, gpu_index: int) -&gt; int:\n        \"\"\"Return the average power usage of the GPU. Units: mW.\"\"\"\n        return self.gpus[gpu_index].get_average_power_usage()\n\n    @deprecated_alias(\"getInstantPowerUsage\")\n    def get_instant_power_usage(self, gpu_index: int) -&gt; int:\n        \"\"\"Return the current power draw of the GPU. Units: mW.\"\"\"\n        return self.gpus[gpu_index].get_instant_power_usage()\n\n    @deprecated_alias(\"getAverageMemoryPowerUsage\")\n    def get_average_memory_power_usage(self, gpu_index: int) -&gt; int:\n        \"\"\"Return the average power usage of the GPU's memory. Units: mW.\"\"\"\n        return self.gpus[gpu_index].get_average_memory_power_usage()\n\n    @deprecated_alias(\"supportsGetTotalEnergyConsumption\")\n    def supports_get_total_energy_consumption(self, gpu_index: int) -&gt; bool:\n        \"\"\"Check if the GPU supports retrieving total energy consumption.\"\"\"\n        return self.gpus[gpu_index].supports_get_total_energy_consumption()\n\n    @deprecated_alias(\"getTotalEnergyConsumption\")\n    def get_total_energy_consumption(self, gpu_index: int) -&gt; int:\n        \"\"\"Return the total energy consumption of the GPU since driver load. Units: mJ.\"\"\"\n        return self.gpus[gpu_index].get_total_energy_consumption()\n\n    @deprecated_alias(\"getGpuTemperature\")\n    def get_gpu_temperature(self, gpu_index: int) -&gt; int:\n        \"\"\"Return the current GPU temperature. Units: Celsius.\"\"\"\n        return self.gpus[gpu_index].get_gpu_temperature()\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.gpus","title":"gpus  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>gpus\n</code></pre> <p>Return a list of GPU objects being tracked.</p>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.__init__","title":"__init__  <code>abstractmethod</code>","text":"<pre><code>__init__(ensure_homogeneous=False)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@abc.abstractmethod\ndef __init__(self, ensure_homogeneous: bool = False) -&gt; None:\n    \"\"\"Initialize the GPU management library and initializes `GPU` objects.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.__del__","title":"__del__  <code>abstractmethod</code>","text":"<pre><code>__del__()\n</code></pre> <p>Shut down the GPU monitoring library to release resources and clean up.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@abc.abstractmethod\ndef __del__(self) -&gt; None:\n    \"\"\"Shut down the GPU monitoring library to release resources and clean up.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the number of GPUs being tracked.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of GPUs being tracked.\"\"\"\n    return len(self.gpus)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs._ensure_homogeneous","title":"_ensure_homogeneous","text":"<pre><code>_ensure_homogeneous()\n</code></pre> <p>Ensures that all tracked GPUs are homogeneous in terms of name.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def _ensure_homogeneous(self) -&gt; None:\n    \"\"\"Ensures that all tracked GPUs are homogeneous in terms of name.\"\"\"\n    gpu_names = [gpu.get_name() for gpu in self.gpus]\n    # Both zero (no GPUs found) and one are fine.\n    if len(set(gpu_names)) &gt; 1:\n        raise ZeusGPUHeterogeneousError(f\"Heterogeneous GPUs found: {gpu_names}\")\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs._warn_sys_admin","title":"_warn_sys_admin","text":"<pre><code>_warn_sys_admin()\n</code></pre> <p>Warn the user if the current process doesn't have <code>SYS_ADMIN</code> privileges.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def _warn_sys_admin(self) -&gt; None:\n    \"\"\"Warn the user if the current process doesn't have `SYS_ADMIN` privileges.\"\"\"\n    # Deriving classes can disable this warning by setting this attribute.\n    if not getattr(self, \"_disable_sys_admin_warning\", False) and not has_sys_admin():\n        warnings.warn(\n            \"You are about to call a GPU management API that requires \"\n            \"`SYS_ADMIN` privileges. Some energy optimizers that change the \"\n            \"GPU's power settings need this.\\nSee \"\n            \"https://ml.energy/zeus/getting_started/#system-privileges \"\n            \"for more information and how to obtain `SYS_ADMIN`.\",\n            stacklevel=2,\n        )\n        # Only warn once.\n        self._disable_sys_admin_warning = True\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.get_name","title":"get_name","text":"<pre><code>get_name(gpu_index)\n</code></pre> <p>Return the name of the specified GPU.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getName\")\ndef get_name(self, gpu_index: int) -&gt; str:\n    \"\"\"Return the name of the specified GPU.\"\"\"\n    return self.gpus[gpu_index].get_name()\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.get_power_management_limit_constraints","title":"get_power_management_limit_constraints","text":"<pre><code>get_power_management_limit_constraints(gpu_index)\n</code></pre> <p>Return the minimum and maximum power management limits. Units: mW.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getPowerManagementLimitConstraints\")\ndef get_power_management_limit_constraints(self, gpu_index: int) -&gt; tuple[int, int]:\n    \"\"\"Return the minimum and maximum power management limits. Units: mW.\"\"\"\n    return self.gpus[gpu_index].get_power_management_limit_constraints()\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.get_power_management_limit","title":"get_power_management_limit","text":"<pre><code>get_power_management_limit(gpu_index)\n</code></pre> <p>Return the current power management limit. Units: mW.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def get_power_management_limit(self, gpu_index: int) -&gt; int:\n    \"\"\"Return the current power management limit. Units: mW.\"\"\"\n    return self.gpus[gpu_index].get_power_management_limit()\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.set_power_management_limit","title":"set_power_management_limit","text":"<pre><code>set_power_management_limit(gpu_index, power_limit_mw, block=True)\n</code></pre> <p>Set the GPU's power management limit. Unit: mW.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"setPowerManagementLimit\")\ndef set_power_management_limit(self, gpu_index: int, power_limit_mw: int, block: bool = True) -&gt; None:\n    \"\"\"Set the GPU's power management limit. Unit: mW.\"\"\"\n    self.gpus[gpu_index].set_power_management_limit(power_limit_mw, block)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.reset_power_management_limit","title":"reset_power_management_limit","text":"<pre><code>reset_power_management_limit(gpu_index, block=True)\n</code></pre> <p>Reset the GPU's power management limit to the default value.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"resetPowerManagementLimit\")\ndef reset_power_management_limit(self, gpu_index: int, block: bool = True) -&gt; None:\n    \"\"\"Reset the GPU's power management limit to the default value.\"\"\"\n    self.gpus[gpu_index].reset_power_management_limit(block)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.set_persistence_mode","title":"set_persistence_mode","text":"<pre><code>set_persistence_mode(gpu_index, enabled, block=True)\n</code></pre> <p>Set persistence mode for the specified GPU.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"setPersistenceMode\")\ndef set_persistence_mode(self, gpu_index: int, enabled: bool, block: bool = True) -&gt; None:\n    \"\"\"Set persistence mode for the specified GPU.\"\"\"\n    self.gpus[gpu_index].set_persistence_mode(enabled, block)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.get_supported_memory_clocks","title":"get_supported_memory_clocks","text":"<pre><code>get_supported_memory_clocks(gpu_index)\n</code></pre> <p>Return a list of supported memory clock frequencies. Units: MHz.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getSupportedMemoryClocks\")\ndef get_supported_memory_clocks(self, gpu_index: int) -&gt; list[int]:\n    \"\"\"Return a list of supported memory clock frequencies. Units: MHz.\"\"\"\n    return self.gpus[gpu_index].get_supported_memory_clocks()\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.set_memory_locked_clocks","title":"set_memory_locked_clocks","text":"<pre><code>set_memory_locked_clocks(gpu_index, min_clock_mhz, max_clock_mhz, block=True)\n</code></pre> <p>Lock the memory clock to a specified range. Units: MHz.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"setMemoryLockedClocks\")\ndef set_memory_locked_clocks(\n    self,\n    gpu_index: int,\n    min_clock_mhz: int,\n    max_clock_mhz: int,\n    block: bool = True,\n) -&gt; None:\n    \"\"\"Lock the memory clock to a specified range. Units: MHz.\"\"\"\n    self.gpus[gpu_index].set_memory_locked_clocks(min_clock_mhz, max_clock_mhz, block)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.reset_memory_locked_clocks","title":"reset_memory_locked_clocks","text":"<pre><code>reset_memory_locked_clocks(gpu_index, block=True)\n</code></pre> <p>Reset the locked memory clocks to the default.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"resetMemoryLockedClocks\")\ndef reset_memory_locked_clocks(self, gpu_index: int, block: bool = True) -&gt; None:\n    \"\"\"Reset the locked memory clocks to the default.\"\"\"\n    self.gpus[gpu_index].reset_memory_locked_clocks(block)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.get_supported_graphics_clocks","title":"get_supported_graphics_clocks","text":"<pre><code>get_supported_graphics_clocks(gpu_index, memory_clock_mhz=None)\n</code></pre> <p>Return a list of supported graphics clock frequencies. Units: MHz.</p> <p>Parameters:</p> Name Type Description Default <code>gpu_index</code> <code>int</code> <p>Index of the GPU to query.</p> required <code>memory_clock_mhz</code> <code>int | None</code> <p>Memory clock frequency to use. Some GPUs have different supported graphics clocks depending on the memory clock.</p> <code>None</code> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getSupportedGraphicsClocks\")\ndef get_supported_graphics_clocks(self, gpu_index: int, memory_clock_mhz: int | None = None) -&gt; list[int]:\n    \"\"\"Return a list of supported graphics clock frequencies. Units: MHz.\n\n    Args:\n        gpu_index: Index of the GPU to query.\n        memory_clock_mhz: Memory clock frequency to use. Some GPUs have\n            different supported graphics clocks depending on the memory clock.\n    \"\"\"\n    return self.gpus[gpu_index].get_supported_graphics_clocks(memory_clock_mhz)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.set_gpu_locked_clocks","title":"set_gpu_locked_clocks","text":"<pre><code>set_gpu_locked_clocks(gpu_index, min_clock_mhz, max_clock_mhz, block=True)\n</code></pre> <p>Lock the GPU clock to a specified range. Units: MHz.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"setGpuLockedClocks\")\ndef set_gpu_locked_clocks(\n    self,\n    gpu_index: int,\n    min_clock_mhz: int,\n    max_clock_mhz: int,\n    block: bool = True,\n) -&gt; None:\n    \"\"\"Lock the GPU clock to a specified range. Units: MHz.\"\"\"\n    self.gpus[gpu_index].set_gpu_locked_clocks(min_clock_mhz, max_clock_mhz, block)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.reset_gpu_locked_clocks","title":"reset_gpu_locked_clocks","text":"<pre><code>reset_gpu_locked_clocks(gpu_index, block=True)\n</code></pre> <p>Reset the locked GPU clocks to the default.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"resetGpuLockedClocks\")\ndef reset_gpu_locked_clocks(self, gpu_index: int, block: bool = True) -&gt; None:\n    \"\"\"Reset the locked GPU clocks to the default.\"\"\"\n    self.gpus[gpu_index].reset_gpu_locked_clocks(block)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.get_average_power_usage","title":"get_average_power_usage","text":"<pre><code>get_average_power_usage(gpu_index)\n</code></pre> <p>Return the average power usage of the GPU. Units: mW.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getAveragePowerUsage\")\ndef get_average_power_usage(self, gpu_index: int) -&gt; int:\n    \"\"\"Return the average power usage of the GPU. Units: mW.\"\"\"\n    return self.gpus[gpu_index].get_average_power_usage()\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.get_instant_power_usage","title":"get_instant_power_usage","text":"<pre><code>get_instant_power_usage(gpu_index)\n</code></pre> <p>Return the current power draw of the GPU. Units: mW.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getInstantPowerUsage\")\ndef get_instant_power_usage(self, gpu_index: int) -&gt; int:\n    \"\"\"Return the current power draw of the GPU. Units: mW.\"\"\"\n    return self.gpus[gpu_index].get_instant_power_usage()\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.get_average_memory_power_usage","title":"get_average_memory_power_usage","text":"<pre><code>get_average_memory_power_usage(gpu_index)\n</code></pre> <p>Return the average power usage of the GPU's memory. Units: mW.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getAverageMemoryPowerUsage\")\ndef get_average_memory_power_usage(self, gpu_index: int) -&gt; int:\n    \"\"\"Return the average power usage of the GPU's memory. Units: mW.\"\"\"\n    return self.gpus[gpu_index].get_average_memory_power_usage()\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.supports_get_total_energy_consumption","title":"supports_get_total_energy_consumption","text":"<pre><code>supports_get_total_energy_consumption(gpu_index)\n</code></pre> <p>Check if the GPU supports retrieving total energy consumption.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"supportsGetTotalEnergyConsumption\")\ndef supports_get_total_energy_consumption(self, gpu_index: int) -&gt; bool:\n    \"\"\"Check if the GPU supports retrieving total energy consumption.\"\"\"\n    return self.gpus[gpu_index].supports_get_total_energy_consumption()\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.get_total_energy_consumption","title":"get_total_energy_consumption","text":"<pre><code>get_total_energy_consumption(gpu_index)\n</code></pre> <p>Return the total energy consumption of the GPU since driver load. Units: mJ.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getTotalEnergyConsumption\")\ndef get_total_energy_consumption(self, gpu_index: int) -&gt; int:\n    \"\"\"Return the total energy consumption of the GPU since driver load. Units: mJ.\"\"\"\n    return self.gpus[gpu_index].get_total_energy_consumption()\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.GPUs.get_gpu_temperature","title":"get_gpu_temperature","text":"<pre><code>get_gpu_temperature(gpu_index)\n</code></pre> <p>Return the current GPU temperature. Units: Celsius.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getGpuTemperature\")\ndef get_gpu_temperature(self, gpu_index: int) -&gt; int:\n    \"\"\"Return the current GPU temperature. Units: Celsius.\"\"\"\n    return self.gpus[gpu_index].get_gpu_temperature()\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.EmptyGPUs","title":"EmptyGPUs","text":"<p>               Bases: <code>GPUs</code></p> <p>A concrete class implementing the GPUs abstract base class, but representing an empty collection of GPUs.</p> <p>This class is used to represent a scenario where no GPUs are available or detected. Any method call attempting to interact with a GPU will raise a ValueError.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class EmptyGPUs(GPUs):\n    \"\"\"A concrete class implementing the GPUs abstract base class, but representing an empty collection of GPUs.\n\n    This class is used to represent a scenario where no GPUs are available or detected.\n    Any method call attempting to interact with a GPU will raise a ValueError.\n    \"\"\"\n\n    def __init__(self, ensure_homogeneous: bool = False) -&gt; None:\n        \"\"\"Initialize the EMPTYGPUs class.\n\n        Since this class represents an empty collection of GPUs, no actual initialization of GPU objects is performed.\n        \"\"\"\n        pass\n\n    def __del__(self) -&gt; None:\n        \"\"\"Clean up any resources if necessary.\n\n        As this class represents an empty collection of GPUs, no specific cleanup is required.\n        \"\"\"\n        pass\n\n    @property\n    def gpus(self) -&gt; Sequence[\"GPU\"]:\n        \"\"\"Return an empty list as no GPUs are being tracked.\"\"\"\n        return []\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return 0, indicating no GPUs are being tracked.\"\"\"\n        return 0\n\n    def _ensure_homogeneous(self) -&gt; None:\n        \"\"\"Raise a ValueError as no GPUs are being tracked.\"\"\"\n        raise ValueError(\"No GPUs available to ensure homogeneity.\")\n\n    def _warn_sys_admin(self) -&gt; None:\n        \"\"\"Raise a ValueError as no GPUs are being tracked.\"\"\"\n        raise ValueError(\"No GPUs available to warn about SYS_ADMIN privileges.\")\n\n    @deprecated_alias(\"getName\")\n    def get_name(self, gpu_index: int) -&gt; str:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    @deprecated_alias(\"getPowerManagementLimitConstraints\")\n    def get_power_management_limit_constraints(self, gpu_index: int) -&gt; tuple[int, int]:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    def get_power_management_limit(self, gpu_index: int) -&gt; int:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    @deprecated_alias(\"setPowerManagementLimit\")\n    def set_power_management_limit(self, gpu_index: int, power_limit_mw: int, block: bool = True) -&gt; None:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    @deprecated_alias(\"resetPowerManagementLimit\")\n    def reset_power_management_limit(self, gpu_index: int, block: bool = True) -&gt; None:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    @deprecated_alias(\"setPersistenceMode\")\n    def set_persistence_mode(self, gpu_index: int, enabled: bool, block: bool = True) -&gt; None:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    @deprecated_alias(\"getSupportedMemoryClocks\")\n    def get_supported_memory_clocks(self, gpu_index: int) -&gt; list[int]:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    @deprecated_alias(\"setMemoryLockedClocks\")\n    def set_memory_locked_clocks(\n        self,\n        gpu_index: int,\n        min_clock_mhz: int,\n        max_clock_mhz: int,\n        block: bool = True,\n    ) -&gt; None:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    @deprecated_alias(\"resetMemoryLockedClocks\")\n    def reset_memory_locked_clocks(self, gpu_index: int, block: bool = True) -&gt; None:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    @deprecated_alias(\"getSupportedGraphicsClocks\")\n    def get_supported_graphics_clocks(self, gpu_index: int, memory_clock_mhz: int | None = None) -&gt; list[int]:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    @deprecated_alias(\"setGpuLockedClocks\")\n    def set_gpu_locked_clocks(\n        self,\n        gpu_index: int,\n        min_clock_mhz: int,\n        max_clock_mhz: int,\n        block: bool = True,\n    ) -&gt; None:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    @deprecated_alias(\"resetGpuLockedClocks\")\n    def reset_gpu_locked_clocks(self, gpu_index: int, block: bool = True) -&gt; None:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    @deprecated_alias(\"getInstantPowerUsage\")\n    def get_instant_power_usage(self, gpu_index: int) -&gt; int:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    @deprecated_alias(\"supportsGetTotalEnergyConsumption\")\n    def supports_get_total_energy_consumption(self, gpu_index: int) -&gt; bool:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    @deprecated_alias(\"getTotalEnergyConsumption\")\n    def get_total_energy_consumption(self, gpu_index: int) -&gt; int:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n\n    @deprecated_alias(\"getGpuTemperature\")\n    def get_gpu_temperature(self, gpu_index: int) -&gt; int:\n        \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n        raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.EmptyGPUs.gpus","title":"gpus  <code>property</code>","text":"<pre><code>gpus\n</code></pre> <p>Return an empty list as no GPUs are being tracked.</p>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.EmptyGPUs.__init__","title":"__init__","text":"<pre><code>__init__(ensure_homogeneous=False)\n</code></pre> <p>Since this class represents an empty collection of GPUs, no actual initialization of GPU objects is performed.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, ensure_homogeneous: bool = False) -&gt; None:\n    \"\"\"Initialize the EMPTYGPUs class.\n\n    Since this class represents an empty collection of GPUs, no actual initialization of GPU objects is performed.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.EmptyGPUs.__del__","title":"__del__","text":"<pre><code>__del__()\n</code></pre> <p>Clean up any resources if necessary.</p> <p>As this class represents an empty collection of GPUs, no specific cleanup is required.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Clean up any resources if necessary.\n\n    As this class represents an empty collection of GPUs, no specific cleanup is required.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.EmptyGPUs.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return 0, indicating no GPUs are being tracked.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return 0, indicating no GPUs are being tracked.\"\"\"\n    return 0\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.EmptyGPUs._ensure_homogeneous","title":"_ensure_homogeneous","text":"<pre><code>_ensure_homogeneous()\n</code></pre> <p>Raise a ValueError as no GPUs are being tracked.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def _ensure_homogeneous(self) -&gt; None:\n    \"\"\"Raise a ValueError as no GPUs are being tracked.\"\"\"\n    raise ValueError(\"No GPUs available to ensure homogeneity.\")\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.EmptyGPUs._warn_sys_admin","title":"_warn_sys_admin","text":"<pre><code>_warn_sys_admin()\n</code></pre> <p>Raise a ValueError as no GPUs are being tracked.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def _warn_sys_admin(self) -&gt; None:\n    \"\"\"Raise a ValueError as no GPUs are being tracked.\"\"\"\n    raise ValueError(\"No GPUs available to warn about SYS_ADMIN privileges.\")\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.EmptyGPUs.get_name","title":"get_name","text":"<pre><code>get_name(gpu_index)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getName\")\ndef get_name(self, gpu_index: int) -&gt; str:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.EmptyGPUs.get_power_management_limit_constraints","title":"get_power_management_limit_constraints","text":"<pre><code>get_power_management_limit_constraints(gpu_index)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getPowerManagementLimitConstraints\")\ndef get_power_management_limit_constraints(self, gpu_index: int) -&gt; tuple[int, int]:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.EmptyGPUs.get_power_management_limit","title":"get_power_management_limit","text":"<pre><code>get_power_management_limit(gpu_index)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def get_power_management_limit(self, gpu_index: int) -&gt; int:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.EmptyGPUs.set_power_management_limit","title":"set_power_management_limit","text":"<pre><code>set_power_management_limit(gpu_index, power_limit_mw, block=True)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"setPowerManagementLimit\")\ndef set_power_management_limit(self, gpu_index: int, power_limit_mw: int, block: bool = True) -&gt; None:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.EmptyGPUs.reset_power_management_limit","title":"reset_power_management_limit","text":"<pre><code>reset_power_management_limit(gpu_index, block=True)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"resetPowerManagementLimit\")\ndef reset_power_management_limit(self, gpu_index: int, block: bool = True) -&gt; None:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.EmptyGPUs.set_persistence_mode","title":"set_persistence_mode","text":"<pre><code>set_persistence_mode(gpu_index, enabled, block=True)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"setPersistenceMode\")\ndef set_persistence_mode(self, gpu_index: int, enabled: bool, block: bool = True) -&gt; None:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.EmptyGPUs.get_supported_memory_clocks","title":"get_supported_memory_clocks","text":"<pre><code>get_supported_memory_clocks(gpu_index)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getSupportedMemoryClocks\")\ndef get_supported_memory_clocks(self, gpu_index: int) -&gt; list[int]:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.EmptyGPUs.set_memory_locked_clocks","title":"set_memory_locked_clocks","text":"<pre><code>set_memory_locked_clocks(gpu_index, min_clock_mhz, max_clock_mhz, block=True)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"setMemoryLockedClocks\")\ndef set_memory_locked_clocks(\n    self,\n    gpu_index: int,\n    min_clock_mhz: int,\n    max_clock_mhz: int,\n    block: bool = True,\n) -&gt; None:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.EmptyGPUs.reset_memory_locked_clocks","title":"reset_memory_locked_clocks","text":"<pre><code>reset_memory_locked_clocks(gpu_index, block=True)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"resetMemoryLockedClocks\")\ndef reset_memory_locked_clocks(self, gpu_index: int, block: bool = True) -&gt; None:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.EmptyGPUs.get_supported_graphics_clocks","title":"get_supported_graphics_clocks","text":"<pre><code>get_supported_graphics_clocks(gpu_index, memory_clock_mhz=None)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getSupportedGraphicsClocks\")\ndef get_supported_graphics_clocks(self, gpu_index: int, memory_clock_mhz: int | None = None) -&gt; list[int]:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.EmptyGPUs.set_gpu_locked_clocks","title":"set_gpu_locked_clocks","text":"<pre><code>set_gpu_locked_clocks(gpu_index, min_clock_mhz, max_clock_mhz, block=True)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"setGpuLockedClocks\")\ndef set_gpu_locked_clocks(\n    self,\n    gpu_index: int,\n    min_clock_mhz: int,\n    max_clock_mhz: int,\n    block: bool = True,\n) -&gt; None:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.EmptyGPUs.reset_gpu_locked_clocks","title":"reset_gpu_locked_clocks","text":"<pre><code>reset_gpu_locked_clocks(gpu_index, block=True)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"resetGpuLockedClocks\")\ndef reset_gpu_locked_clocks(self, gpu_index: int, block: bool = True) -&gt; None:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.EmptyGPUs.get_instant_power_usage","title":"get_instant_power_usage","text":"<pre><code>get_instant_power_usage(gpu_index)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getInstantPowerUsage\")\ndef get_instant_power_usage(self, gpu_index: int) -&gt; int:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.EmptyGPUs.supports_get_total_energy_consumption","title":"supports_get_total_energy_consumption","text":"<pre><code>supports_get_total_energy_consumption(gpu_index)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"supportsGetTotalEnergyConsumption\")\ndef supports_get_total_energy_consumption(self, gpu_index: int) -&gt; bool:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.EmptyGPUs.get_total_energy_consumption","title":"get_total_energy_consumption","text":"<pre><code>get_total_energy_consumption(gpu_index)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getTotalEnergyConsumption\")\ndef get_total_energy_consumption(self, gpu_index: int) -&gt; int:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.EmptyGPUs.get_gpu_temperature","title":"get_gpu_temperature","text":"<pre><code>get_gpu_temperature(gpu_index)\n</code></pre> <p>Raise a ValueError as no GPUs are available.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>@deprecated_alias(\"getGpuTemperature\")\ndef get_gpu_temperature(self, gpu_index: int) -&gt; int:\n    \"\"\"Raise a ValueError as no GPUs are available.\"\"\"\n    raise ValueError(\"No GPUs available.\")\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUInitError","title":"ZeusGPUInitError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Import error or GPU library initialization failures.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUInitError(ZeusBaseGPUError):\n    \"\"\"Import error or GPU library initialization failures.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUInitError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUInvalidArgError","title":"ZeusGPUInvalidArgError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Invalid Argument.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUInvalidArgError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Invalid Argument.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUInvalidArgError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUNotSupportedError","title":"ZeusGPUNotSupportedError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Not Supported Operation on GPU.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUNotSupportedError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Not Supported Operation on GPU.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUNotSupportedError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUNoPermissionError","title":"ZeusGPUNoPermissionError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps No Permission to perform GPU operation.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUNoPermissionError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps No Permission to perform GPU operation.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUNoPermissionError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUAlreadyInitializedError","title":"ZeusGPUAlreadyInitializedError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Already Initialized GPU.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUAlreadyInitializedError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Already Initialized GPU.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUAlreadyInitializedError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUNotFoundError","title":"ZeusGPUNotFoundError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Not Found GPU.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUNotFoundError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Not Found GPU.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUNotFoundError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUInsufficientSizeError","title":"ZeusGPUInsufficientSizeError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Insufficient Size.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUInsufficientSizeError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Insufficient Size.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUInsufficientSizeError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUInsufficientPowerError","title":"ZeusGPUInsufficientPowerError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Insufficient Power.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUInsufficientPowerError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Insufficient Power.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUInsufficientPowerError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUDriverNotLoadedError","title":"ZeusGPUDriverNotLoadedError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Driver Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUDriverNotLoadedError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Driver Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUDriverNotLoadedError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUTimeoutError","title":"ZeusGPUTimeoutError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Timeout Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUTimeoutError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Timeout Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUTimeoutError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUIRQError","title":"ZeusGPUIRQError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps IRQ Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUIRQError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps IRQ Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUIRQError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPULibraryNotFoundError","title":"ZeusGPULibraryNotFoundError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Library Not Found Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPULibraryNotFoundError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Library Not Found Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPULibraryNotFoundError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUFunctionNotFoundError","title":"ZeusGPUFunctionNotFoundError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Function Not Found Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUFunctionNotFoundError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Function Not Found Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUFunctionNotFoundError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUCorruptedInfoROMError","title":"ZeusGPUCorruptedInfoROMError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Corrupted Info ROM Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUCorruptedInfoROMError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Corrupted Info ROM Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUCorruptedInfoROMError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPULostError","title":"ZeusGPULostError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Lost GPU Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPULostError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Lost GPU Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPULostError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUResetRequiredError","title":"ZeusGPUResetRequiredError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Reset Required Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUResetRequiredError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Reset Required Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUResetRequiredError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUOperatingSystemError","title":"ZeusGPUOperatingSystemError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Operating System Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUOperatingSystemError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Operating System Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUOperatingSystemError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPULibRMVersionMismatchError","title":"ZeusGPULibRMVersionMismatchError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps LibRM Version Mismatch Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPULibRMVersionMismatchError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps LibRM Version Mismatch Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPULibRMVersionMismatchError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUMemoryError","title":"ZeusGPUMemoryError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Insufficient Memory Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUMemoryError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Insufficient Memory Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUMemoryError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUUnknownError","title":"ZeusGPUUnknownError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Zeus GPU exception that wraps Unknown Error.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUUnknownError(ZeusBaseGPUError):\n    \"\"\"Zeus GPU exception that wraps Unknown Error.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUUnknownError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUHeterogeneousError","title":"ZeusGPUHeterogeneousError","text":"<p>               Bases: <code>ZeusBaseGPUError</code></p> <p>Exception for when GPUs are not homogeneous.</p> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>class ZeusGPUHeterogeneousError(ZeusBaseGPUError):\n    \"\"\"Exception for when GPUs are not homogeneous.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/common/#zeus.device.gpu.common.ZeusGPUHeterogeneousError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/gpu/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/","title":"nvidia","text":""},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia","title":"zeus.device.gpu.nvidia","text":"<p>NVIDIA GPUs.</p>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU","title":"NVIDIAGPU","text":"<p>               Bases: <code>GPU</code></p> <p>Implementation of <code>GPU</code> for NVIDIA GPUs.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>class NVIDIAGPU(gpu_common.GPU):\n    \"\"\"Implementation of `GPU` for NVIDIA GPUs.\"\"\"\n\n    def __init__(self, gpu_index: int) -&gt; None:\n        \"\"\"Initialize the GPU object.\"\"\"\n        super().__init__(gpu_index)\n        self._get_handle()\n        self._supportsGetTotalEnergyConsumption = None\n\n        # Check if it's a Grace Hopper chip\n        try:\n            c2c_mode_info = pynvml.nvmlDeviceGetC2cModeInfoV(self.handle)\n            self._is_grace_hopper = c2c_mode_info.isC2cEnabled\n        except pynvml.NVMLError as e:\n            e_value = e.value  # ty: ignore[unresolved-attribute]\n            if e_value != pynvml.NVML_ERROR_NOT_SUPPORTED:\n                logger.warning(\n                    \"Attempted to check whether the current chip is a Grace Hopper chip \"\n                    \"by calling `nvmlDeviceGetC2cModeInfoV`, which we expected to either \"\n                    \"return a valid response or raise `NVML_ERROR_NOT_SUPPORTED`. \"\n                    \"Instead, it raised an unexpected error: '%s'. Treating this as \"\n                    \"not a Grace Hopper chip.\",\n                    e,\n                )\n            self._is_grace_hopper = False\n\n    _exception_map = {\n        pynvml.NVML_ERROR_UNINITIALIZED: gpu_common.ZeusGPUInitError,\n        pynvml.NVML_ERROR_INVALID_ARGUMENT: gpu_common.ZeusGPUInvalidArgError,\n        pynvml.NVML_ERROR_NOT_SUPPORTED: gpu_common.ZeusGPUNotSupportedError,\n        pynvml.NVML_ERROR_NO_PERMISSION: gpu_common.ZeusGPUNoPermissionError,\n        pynvml.NVML_ERROR_ALREADY_INITIALIZED: gpu_common.ZeusGPUAlreadyInitializedError,\n        pynvml.NVML_ERROR_NOT_FOUND: gpu_common.ZeusGPUNotFoundError,\n        pynvml.NVML_ERROR_INSUFFICIENT_SIZE: gpu_common.ZeusGPUInsufficientSizeError,\n        pynvml.NVML_ERROR_INSUFFICIENT_POWER: gpu_common.ZeusGPUInsufficientPowerError,\n        pynvml.NVML_ERROR_DRIVER_NOT_LOADED: gpu_common.ZeusGPUDriverNotLoadedError,\n        pynvml.NVML_ERROR_TIMEOUT: gpu_common.ZeusGPUTimeoutError,\n        pynvml.NVML_ERROR_IRQ_ISSUE: gpu_common.ZeusGPUIRQError,\n        pynvml.NVML_ERROR_LIBRARY_NOT_FOUND: gpu_common.ZeusGPULibraryNotFoundError,\n        pynvml.NVML_ERROR_FUNCTION_NOT_FOUND: gpu_common.ZeusGPUFunctionNotFoundError,\n        pynvml.NVML_ERROR_CORRUPTED_INFOROM: gpu_common.ZeusGPUCorruptedInfoROMError,\n        pynvml.NVML_ERROR_GPU_IS_LOST: gpu_common.ZeusGPULostError,\n        pynvml.NVML_ERROR_RESET_REQUIRED: gpu_common.ZeusGPUResetRequiredError,\n        pynvml.NVML_ERROR_OPERATING_SYSTEM: gpu_common.ZeusGPUOperatingSystemError,\n        pynvml.NVML_ERROR_LIB_RM_VERSION_MISMATCH: gpu_common.ZeusGPULibRMVersionMismatchError,\n        pynvml.NVML_ERROR_MEMORY: gpu_common.ZeusGPUMemoryError,\n        pynvml.NVML_ERROR_UNKNOWN: gpu_common.ZeusGPUUnknownError,\n    }\n\n    @_handle_nvml_errors\n    def _get_handle(self):\n        self.handle = pynvml.nvmlDeviceGetHandleByIndex(self.gpu_index)\n\n    @_handle_nvml_errors\n    def get_name(self) -&gt; str:\n        \"\"\"Return the name of the GPU model.\"\"\"\n        return pynvml.nvmlDeviceGetName(self.handle)\n\n    @property\n    def supports_nonblocking_setters(self) -&gt; bool:\n        \"\"\"Return True if the GPU object supports non-blocking configuration setters.\"\"\"\n        return False\n\n    @_handle_nvml_errors\n    def get_power_management_limit_constraints(self) -&gt; tuple[int, int]:\n        \"\"\"Return the minimum and maximum power management limits. Units: mW.\"\"\"\n        min_, max_ = pynvml.nvmlDeviceGetPowerManagementLimitConstraints(self.handle)\n        return (min_, max_)\n\n    @_handle_nvml_errors\n    def get_power_management_limit(self) -&gt; int:\n        \"\"\"Return the current power management limit. Units: mW.\"\"\"\n        return pynvml.nvmlDeviceGetPowerManagementLimit(self.handle)\n\n    @_handle_nvml_errors\n    def set_power_management_limit(self, power_limit_mw: int, block: bool = True) -&gt; None:\n        \"\"\"Set the GPU's power management limit. Unit: mW.\"\"\"\n        current_limit = self.get_power_management_limit()\n        if current_limit != power_limit_mw:\n            self._warn_sys_admin()\n            pynvml.nvmlDeviceSetPowerManagementLimit(self.handle, power_limit_mw)\n\n    @_handle_nvml_errors\n    def reset_power_management_limit(self, block: bool = True) -&gt; None:\n        \"\"\"Reset the GPU's power management limit to the default value.\"\"\"\n        default_limit = pynvml.nvmlDeviceGetPowerManagementDefaultLimit(self.handle)\n        current_limit = self.get_power_management_limit()\n        if current_limit != default_limit:\n            self._warn_sys_admin()\n            pynvml.nvmlDeviceSetPowerManagementLimit(self.handle, default_limit)\n\n    @_handle_nvml_errors\n    def set_persistence_mode(self, enabled: bool, block: bool = True) -&gt; None:\n        \"\"\"Set persistence mode.\"\"\"\n        current_mode = pynvml.nvmlDeviceGetPersistenceMode(self.handle)\n        desired_mode = pynvml.NVML_FEATURE_ENABLED if enabled else pynvml.NVML_FEATURE_DISABLED\n        if current_mode != desired_mode:\n            self._warn_sys_admin()\n            pynvml.nvmlDeviceSetPersistenceMode(self.handle, desired_mode)\n\n    @_handle_nvml_errors\n    def get_supported_memory_clocks(self) -&gt; list[int]:\n        \"\"\"Return a list of supported memory clock frequencies. Units: MHz.\"\"\"\n        return pynvml.nvmlDeviceGetSupportedMemoryClocks(self.handle)\n\n    @_handle_nvml_errors\n    def set_memory_locked_clocks(self, min_clock_mhz: int, max_clock_mhz: int, block: bool = True) -&gt; None:\n        \"\"\"Lock the memory clock to a specified range. Units: MHz.\"\"\"\n        self._warn_sys_admin()\n        pynvml.nvmlDeviceSetMemoryLockedClocks(self.handle, min_clock_mhz, max_clock_mhz)\n\n    @_handle_nvml_errors\n    def reset_memory_locked_clocks(self, block: bool = True) -&gt; None:\n        \"\"\"Reset the locked memory clocks to the default.\"\"\"\n        self._warn_sys_admin()\n        pynvml.nvmlDeviceResetMemoryLockedClocks(self.handle)\n\n    @_handle_nvml_errors\n    def get_supported_graphics_clocks(self, memory_clock_mhz: int | None = None) -&gt; list[int]:\n        \"\"\"Return a list of supported graphics clock frequencies. Units: MHz.\n\n        Args:\n            memory_clock_mhz: Memory clock frequency to use. Some GPUs have\n                different supported graphics clocks depending on the memory clock.\n        \"\"\"\n        pass\n        return pynvml.nvmlDeviceGetSupportedGraphicsClocks(self.handle, memory_clock_mhz)\n\n    @_handle_nvml_errors\n    def set_gpu_locked_clocks(self, min_clock_mhz: int, max_clock_mhz: int, block: bool = True) -&gt; None:\n        \"\"\"Lock the GPU clock to a specified range. Units: MHz.\"\"\"\n        self._warn_sys_admin()\n        pynvml.nvmlDeviceSetGpuLockedClocks(self.handle, min_clock_mhz, max_clock_mhz)\n\n    @_handle_nvml_errors\n    def reset_gpu_locked_clocks(self, block: bool = True) -&gt; None:\n        \"\"\"Reset the locked GPU clocks to the default.\"\"\"\n        self._warn_sys_admin()\n        pynvml.nvmlDeviceResetGpuLockedClocks(self.handle)\n\n    @_handle_nvml_errors\n    def get_average_power_usage(self) -&gt; int:\n        \"\"\"Return the average power draw of the GPU. Units: mW.\"\"\"\n        if self._is_grace_hopper:\n            fields = [(pynvml.NVML_FI_DEV_POWER_AVERAGE, pynvml.NVML_POWER_SCOPE_MODULE)]\n        else:\n            fields = [(pynvml.NVML_FI_DEV_POWER_AVERAGE, pynvml.NVML_POWER_SCOPE_GPU)]\n\n        metric = pynvml.nvmlDeviceGetFieldValues(self.handle, fields)[0]\n        if (ret := metric.nvmlReturn) != pynvml.NVML_SUCCESS:\n            raise pynvml.NVMLError(ret)\n        return metric.value.uiVal\n\n    @_handle_nvml_errors\n    def get_instant_power_usage(self) -&gt; int:\n        \"\"\"Return the current power draw of the GPU. Units: mW.\"\"\"\n        if self._is_grace_hopper:\n            fields = [(pynvml.NVML_FI_DEV_POWER_INSTANT, pynvml.NVML_POWER_SCOPE_MODULE)]\n        else:\n            fields = [(pynvml.NVML_FI_DEV_POWER_INSTANT, pynvml.NVML_POWER_SCOPE_GPU)]\n\n        metric = pynvml.nvmlDeviceGetFieldValues(self.handle, fields)[0]\n        if (ret := metric.nvmlReturn) != pynvml.NVML_SUCCESS:\n            raise pynvml.NVMLError(ret)\n        return metric.value.uiVal\n\n    @_handle_nvml_errors\n    def get_average_memory_power_usage(self) -&gt; int:\n        \"\"\"Return the average power draw of the GPU's memory. Units: mW.\n\n        !!! Warning\n            This isn't exactly documented in NVML at the time of writing, but `nvidia-smi`\n            makes use of this API.\n\n            Confirmed working on H100 80GB HBM3. Confirmed not working on A40.\n        \"\"\"\n        metric = pynvml.nvmlDeviceGetFieldValues(\n            self.handle,\n            [(pynvml.NVML_FI_DEV_POWER_AVERAGE, pynvml.NVML_POWER_SCOPE_MEMORY)],\n        )[0]\n        if (ret := metric.nvmlReturn) != pynvml.NVML_SUCCESS:\n            raise pynvml.NVMLError(ret)\n        power = metric.value.uiVal\n        if power == 0:\n            warnings.warn(\n                \"Average memory power returned 0. The current GPU may not be supported.\",\n                stacklevel=1,\n            )\n        return power\n\n    @_handle_nvml_errors\n    def supports_get_total_energy_consumption(self) -&gt; bool:\n        \"\"\"Check if the GPU supports retrieving total energy consumption.\"\"\"\n        # Supported on Volta or newer microarchitectures\n        if self._supportsGetTotalEnergyConsumption is None:\n            self._supportsGetTotalEnergyConsumption = (\n                pynvml.nvmlDeviceGetArchitecture(self.handle) &gt;= pynvml.NVML_DEVICE_ARCH_VOLTA\n            )\n\n        return self._supportsGetTotalEnergyConsumption\n\n    @_handle_nvml_errors\n    def get_total_energy_consumption(self) -&gt; int:\n        \"\"\"Return the total energy consumption of the specified GPU. Units: mJ.\"\"\"\n        return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n\n    @_handle_nvml_errors\n    def get_gpu_temperature(self) -&gt; int:\n        \"\"\"Return the current GPU temperature. Units: Celsius.\"\"\"\n        temperature = pynvml.nvmlDeviceGetTemperatureV(self.handle, pynvml.NVML_TEMPERATURE_GPU)\n        return temperature\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.supports_nonblocking_setters","title":"supports_nonblocking_setters  <code>property</code>","text":"<pre><code>supports_nonblocking_setters\n</code></pre> <p>Return True if the GPU object supports non-blocking configuration setters.</p>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.__init__","title":"__init__","text":"<pre><code>__init__(gpu_index)\n</code></pre> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>def __init__(self, gpu_index: int) -&gt; None:\n    \"\"\"Initialize the GPU object.\"\"\"\n    super().__init__(gpu_index)\n    self._get_handle()\n    self._supportsGetTotalEnergyConsumption = None\n\n    # Check if it's a Grace Hopper chip\n    try:\n        c2c_mode_info = pynvml.nvmlDeviceGetC2cModeInfoV(self.handle)\n        self._is_grace_hopper = c2c_mode_info.isC2cEnabled\n    except pynvml.NVMLError as e:\n        e_value = e.value  # ty: ignore[unresolved-attribute]\n        if e_value != pynvml.NVML_ERROR_NOT_SUPPORTED:\n            logger.warning(\n                \"Attempted to check whether the current chip is a Grace Hopper chip \"\n                \"by calling `nvmlDeviceGetC2cModeInfoV`, which we expected to either \"\n                \"return a valid response or raise `NVML_ERROR_NOT_SUPPORTED`. \"\n                \"Instead, it raised an unexpected error: '%s'. Treating this as \"\n                \"not a Grace Hopper chip.\",\n                e,\n            )\n        self._is_grace_hopper = False\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.get_name","title":"get_name","text":"<pre><code>get_name()\n</code></pre> <p>Return the name of the GPU model.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef get_name(self) -&gt; str:\n    \"\"\"Return the name of the GPU model.\"\"\"\n    return pynvml.nvmlDeviceGetName(self.handle)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.get_power_management_limit_constraints","title":"get_power_management_limit_constraints","text":"<pre><code>get_power_management_limit_constraints()\n</code></pre> <p>Return the minimum and maximum power management limits. Units: mW.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef get_power_management_limit_constraints(self) -&gt; tuple[int, int]:\n    \"\"\"Return the minimum and maximum power management limits. Units: mW.\"\"\"\n    min_, max_ = pynvml.nvmlDeviceGetPowerManagementLimitConstraints(self.handle)\n    return (min_, max_)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.get_power_management_limit","title":"get_power_management_limit","text":"<pre><code>get_power_management_limit()\n</code></pre> <p>Return the current power management limit. Units: mW.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef get_power_management_limit(self) -&gt; int:\n    \"\"\"Return the current power management limit. Units: mW.\"\"\"\n    return pynvml.nvmlDeviceGetPowerManagementLimit(self.handle)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.set_power_management_limit","title":"set_power_management_limit","text":"<pre><code>set_power_management_limit(power_limit_mw, block=True)\n</code></pre> <p>Set the GPU's power management limit. Unit: mW.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef set_power_management_limit(self, power_limit_mw: int, block: bool = True) -&gt; None:\n    \"\"\"Set the GPU's power management limit. Unit: mW.\"\"\"\n    current_limit = self.get_power_management_limit()\n    if current_limit != power_limit_mw:\n        self._warn_sys_admin()\n        pynvml.nvmlDeviceSetPowerManagementLimit(self.handle, power_limit_mw)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.reset_power_management_limit","title":"reset_power_management_limit","text":"<pre><code>reset_power_management_limit(block=True)\n</code></pre> <p>Reset the GPU's power management limit to the default value.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef reset_power_management_limit(self, block: bool = True) -&gt; None:\n    \"\"\"Reset the GPU's power management limit to the default value.\"\"\"\n    default_limit = pynvml.nvmlDeviceGetPowerManagementDefaultLimit(self.handle)\n    current_limit = self.get_power_management_limit()\n    if current_limit != default_limit:\n        self._warn_sys_admin()\n        pynvml.nvmlDeviceSetPowerManagementLimit(self.handle, default_limit)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.set_persistence_mode","title":"set_persistence_mode","text":"<pre><code>set_persistence_mode(enabled, block=True)\n</code></pre> <p>Set persistence mode.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef set_persistence_mode(self, enabled: bool, block: bool = True) -&gt; None:\n    \"\"\"Set persistence mode.\"\"\"\n    current_mode = pynvml.nvmlDeviceGetPersistenceMode(self.handle)\n    desired_mode = pynvml.NVML_FEATURE_ENABLED if enabled else pynvml.NVML_FEATURE_DISABLED\n    if current_mode != desired_mode:\n        self._warn_sys_admin()\n        pynvml.nvmlDeviceSetPersistenceMode(self.handle, desired_mode)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.get_supported_memory_clocks","title":"get_supported_memory_clocks","text":"<pre><code>get_supported_memory_clocks()\n</code></pre> <p>Return a list of supported memory clock frequencies. Units: MHz.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef get_supported_memory_clocks(self) -&gt; list[int]:\n    \"\"\"Return a list of supported memory clock frequencies. Units: MHz.\"\"\"\n    return pynvml.nvmlDeviceGetSupportedMemoryClocks(self.handle)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.set_memory_locked_clocks","title":"set_memory_locked_clocks","text":"<pre><code>set_memory_locked_clocks(min_clock_mhz, max_clock_mhz, block=True)\n</code></pre> <p>Lock the memory clock to a specified range. Units: MHz.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef set_memory_locked_clocks(self, min_clock_mhz: int, max_clock_mhz: int, block: bool = True) -&gt; None:\n    \"\"\"Lock the memory clock to a specified range. Units: MHz.\"\"\"\n    self._warn_sys_admin()\n    pynvml.nvmlDeviceSetMemoryLockedClocks(self.handle, min_clock_mhz, max_clock_mhz)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.reset_memory_locked_clocks","title":"reset_memory_locked_clocks","text":"<pre><code>reset_memory_locked_clocks(block=True)\n</code></pre> <p>Reset the locked memory clocks to the default.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef reset_memory_locked_clocks(self, block: bool = True) -&gt; None:\n    \"\"\"Reset the locked memory clocks to the default.\"\"\"\n    self._warn_sys_admin()\n    pynvml.nvmlDeviceResetMemoryLockedClocks(self.handle)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.get_supported_graphics_clocks","title":"get_supported_graphics_clocks","text":"<pre><code>get_supported_graphics_clocks(memory_clock_mhz=None)\n</code></pre> <p>Return a list of supported graphics clock frequencies. Units: MHz.</p> <p>Parameters:</p> Name Type Description Default <code>memory_clock_mhz</code> <code>int | None</code> <p>Memory clock frequency to use. Some GPUs have different supported graphics clocks depending on the memory clock.</p> <code>None</code> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef get_supported_graphics_clocks(self, memory_clock_mhz: int | None = None) -&gt; list[int]:\n    \"\"\"Return a list of supported graphics clock frequencies. Units: MHz.\n\n    Args:\n        memory_clock_mhz: Memory clock frequency to use. Some GPUs have\n            different supported graphics clocks depending on the memory clock.\n    \"\"\"\n    pass\n    return pynvml.nvmlDeviceGetSupportedGraphicsClocks(self.handle, memory_clock_mhz)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.set_gpu_locked_clocks","title":"set_gpu_locked_clocks","text":"<pre><code>set_gpu_locked_clocks(min_clock_mhz, max_clock_mhz, block=True)\n</code></pre> <p>Lock the GPU clock to a specified range. Units: MHz.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef set_gpu_locked_clocks(self, min_clock_mhz: int, max_clock_mhz: int, block: bool = True) -&gt; None:\n    \"\"\"Lock the GPU clock to a specified range. Units: MHz.\"\"\"\n    self._warn_sys_admin()\n    pynvml.nvmlDeviceSetGpuLockedClocks(self.handle, min_clock_mhz, max_clock_mhz)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.reset_gpu_locked_clocks","title":"reset_gpu_locked_clocks","text":"<pre><code>reset_gpu_locked_clocks(block=True)\n</code></pre> <p>Reset the locked GPU clocks to the default.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef reset_gpu_locked_clocks(self, block: bool = True) -&gt; None:\n    \"\"\"Reset the locked GPU clocks to the default.\"\"\"\n    self._warn_sys_admin()\n    pynvml.nvmlDeviceResetGpuLockedClocks(self.handle)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.get_average_power_usage","title":"get_average_power_usage","text":"<pre><code>get_average_power_usage()\n</code></pre> <p>Return the average power draw of the GPU. Units: mW.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef get_average_power_usage(self) -&gt; int:\n    \"\"\"Return the average power draw of the GPU. Units: mW.\"\"\"\n    if self._is_grace_hopper:\n        fields = [(pynvml.NVML_FI_DEV_POWER_AVERAGE, pynvml.NVML_POWER_SCOPE_MODULE)]\n    else:\n        fields = [(pynvml.NVML_FI_DEV_POWER_AVERAGE, pynvml.NVML_POWER_SCOPE_GPU)]\n\n    metric = pynvml.nvmlDeviceGetFieldValues(self.handle, fields)[0]\n    if (ret := metric.nvmlReturn) != pynvml.NVML_SUCCESS:\n        raise pynvml.NVMLError(ret)\n    return metric.value.uiVal\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.get_instant_power_usage","title":"get_instant_power_usage","text":"<pre><code>get_instant_power_usage()\n</code></pre> <p>Return the current power draw of the GPU. Units: mW.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef get_instant_power_usage(self) -&gt; int:\n    \"\"\"Return the current power draw of the GPU. Units: mW.\"\"\"\n    if self._is_grace_hopper:\n        fields = [(pynvml.NVML_FI_DEV_POWER_INSTANT, pynvml.NVML_POWER_SCOPE_MODULE)]\n    else:\n        fields = [(pynvml.NVML_FI_DEV_POWER_INSTANT, pynvml.NVML_POWER_SCOPE_GPU)]\n\n    metric = pynvml.nvmlDeviceGetFieldValues(self.handle, fields)[0]\n    if (ret := metric.nvmlReturn) != pynvml.NVML_SUCCESS:\n        raise pynvml.NVMLError(ret)\n    return metric.value.uiVal\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.get_average_memory_power_usage","title":"get_average_memory_power_usage","text":"<pre><code>get_average_memory_power_usage()\n</code></pre> <p>Return the average power draw of the GPU's memory. Units: mW.</p> <p>Warning</p> <p>This isn't exactly documented in NVML at the time of writing, but <code>nvidia-smi</code> makes use of this API.</p> <p>Confirmed working on H100 80GB HBM3. Confirmed not working on A40.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef get_average_memory_power_usage(self) -&gt; int:\n    \"\"\"Return the average power draw of the GPU's memory. Units: mW.\n\n    !!! Warning\n        This isn't exactly documented in NVML at the time of writing, but `nvidia-smi`\n        makes use of this API.\n\n        Confirmed working on H100 80GB HBM3. Confirmed not working on A40.\n    \"\"\"\n    metric = pynvml.nvmlDeviceGetFieldValues(\n        self.handle,\n        [(pynvml.NVML_FI_DEV_POWER_AVERAGE, pynvml.NVML_POWER_SCOPE_MEMORY)],\n    )[0]\n    if (ret := metric.nvmlReturn) != pynvml.NVML_SUCCESS:\n        raise pynvml.NVMLError(ret)\n    power = metric.value.uiVal\n    if power == 0:\n        warnings.warn(\n            \"Average memory power returned 0. The current GPU may not be supported.\",\n            stacklevel=1,\n        )\n    return power\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.supports_get_total_energy_consumption","title":"supports_get_total_energy_consumption","text":"<pre><code>supports_get_total_energy_consumption()\n</code></pre> <p>Check if the GPU supports retrieving total energy consumption.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef supports_get_total_energy_consumption(self) -&gt; bool:\n    \"\"\"Check if the GPU supports retrieving total energy consumption.\"\"\"\n    # Supported on Volta or newer microarchitectures\n    if self._supportsGetTotalEnergyConsumption is None:\n        self._supportsGetTotalEnergyConsumption = (\n            pynvml.nvmlDeviceGetArchitecture(self.handle) &gt;= pynvml.NVML_DEVICE_ARCH_VOLTA\n        )\n\n    return self._supportsGetTotalEnergyConsumption\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.get_total_energy_consumption","title":"get_total_energy_consumption","text":"<pre><code>get_total_energy_consumption()\n</code></pre> <p>Return the total energy consumption of the specified GPU. Units: mJ.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef get_total_energy_consumption(self) -&gt; int:\n    \"\"\"Return the total energy consumption of the specified GPU. Units: mJ.\"\"\"\n    return pynvml.nvmlDeviceGetTotalEnergyConsumption(self.handle)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPU.get_gpu_temperature","title":"get_gpu_temperature","text":"<pre><code>get_gpu_temperature()\n</code></pre> <p>Return the current GPU temperature. Units: Celsius.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef get_gpu_temperature(self) -&gt; int:\n    \"\"\"Return the current GPU temperature. Units: Celsius.\"\"\"\n    temperature = pynvml.nvmlDeviceGetTemperatureV(self.handle, pynvml.NVML_TEMPERATURE_GPU)\n    return temperature\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.ZeusdNVIDIAGPU","title":"ZeusdNVIDIAGPU","text":"<p>               Bases: <code>NVIDIAGPU</code></p> <p>An NVIDIAGPU that sets GPU knobs that require <code>SYS_ADMIN</code> via zeusd.</p> <p>Some NVML APIs (e.g., setting persistence mode, power limit, frequency) requires the Linux security capability <code>SYS_ADMIN</code>, which is virtually <code>sudo</code>. This class overrides those methods so that they send a request to the Zeus daemon.</p> <p>See here for details on system privileges required.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>class ZeusdNVIDIAGPU(NVIDIAGPU):\n    \"\"\"An NVIDIAGPU that sets GPU knobs that require `SYS_ADMIN` via zeusd.\n\n    Some NVML APIs (e.g., setting persistence mode, power limit, frequency)\n    requires the Linux security capability `SYS_ADMIN`, which is virtually `sudo`.\n    This class overrides those methods so that they send a request to the\n    Zeus daemon.\n\n    See [here](https://ml.energy/zeus/getting_started/#system-privileges)\n    for details on system privileges required.\n    \"\"\"\n\n    def __init__(self, gpu_index: int, client: ZeusdClient) -&gt; None:\n        \"\"\"Initialize the GPU object backed by a Zeusd daemon.\n\n        Args:\n            gpu_index: Index of the GPU.\n            client: ZeusdClient connected to the daemon.\n        \"\"\"\n        super().__init__(gpu_index)\n        self._client = client\n        self._gpu_index = gpu_index\n        require_capabilities(client, read_gpu=True, control_gpu=True)\n\n    @property\n    def supports_nonblocking_setters(self) -&gt; bool:\n        \"\"\"Return True if the GPU object supports non-blocking configuration setters.\"\"\"\n        return True\n\n    def set_power_management_limit(self, power_limit_mw: int, block: bool = True) -&gt; None:\n        \"\"\"Set the GPU's power management limit. Unit: mW.\"\"\"\n        current_limit = self.get_power_management_limit()\n        if current_limit == power_limit_mw:\n            return\n        self._client.set_power_limit([self._gpu_index], power_limit_mw, block)\n\n    @_handle_nvml_errors\n    def reset_power_management_limit(self, block: bool = True) -&gt; None:\n        \"\"\"Reset the GPU's power management limit to the default value.\"\"\"\n        self.set_power_management_limit(\n            pynvml.nvmlDeviceGetPowerManagementDefaultLimit(self.handle),\n            block,\n        )\n\n    def set_persistence_mode(self, enabled: bool, block: bool = True) -&gt; None:\n        \"\"\"Set persistence mode.\"\"\"\n        self._client.set_persistence_mode([self._gpu_index], enabled, block)\n\n    def set_memory_locked_clocks(self, min_clock_mhz: int, max_clock_mhz: int, block: bool = True) -&gt; None:\n        \"\"\"Lock the memory clock to a specified range. Units: MHz.\"\"\"\n        self._client.set_mem_locked_clocks([self._gpu_index], min_clock_mhz, max_clock_mhz, block)\n\n    def reset_memory_locked_clocks(self, block: bool = True) -&gt; None:\n        \"\"\"Reset the locked memory clocks to the default.\"\"\"\n        self._client.reset_mem_locked_clocks([self._gpu_index], block)\n\n    def set_gpu_locked_clocks(self, min_clock_mhz: int, max_clock_mhz: int, block: bool = True) -&gt; None:\n        \"\"\"Lock the GPU clock to a specified range. Units: MHz.\"\"\"\n        self._client.set_gpu_locked_clocks([self._gpu_index], min_clock_mhz, max_clock_mhz, block)\n\n    def reset_gpu_locked_clocks(self, block: bool = True) -&gt; None:\n        \"\"\"Reset the locked GPU clocks to the default.\"\"\"\n        self._client.reset_gpu_locked_clocks([self._gpu_index], block)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.ZeusdNVIDIAGPU.supports_nonblocking_setters","title":"supports_nonblocking_setters  <code>property</code>","text":"<pre><code>supports_nonblocking_setters\n</code></pre> <p>Return True if the GPU object supports non-blocking configuration setters.</p>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.ZeusdNVIDIAGPU.__init__","title":"__init__","text":"<pre><code>__init__(gpu_index, client)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>gpu_index</code> <code>int</code> <p>Index of the GPU.</p> required <code>client</code> <code>ZeusdClient</code> <p>ZeusdClient connected to the daemon.</p> required Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>def __init__(self, gpu_index: int, client: ZeusdClient) -&gt; None:\n    \"\"\"Initialize the GPU object backed by a Zeusd daemon.\n\n    Args:\n        gpu_index: Index of the GPU.\n        client: ZeusdClient connected to the daemon.\n    \"\"\"\n    super().__init__(gpu_index)\n    self._client = client\n    self._gpu_index = gpu_index\n    require_capabilities(client, read_gpu=True, control_gpu=True)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.ZeusdNVIDIAGPU.set_power_management_limit","title":"set_power_management_limit","text":"<pre><code>set_power_management_limit(power_limit_mw, block=True)\n</code></pre> <p>Set the GPU's power management limit. Unit: mW.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>def set_power_management_limit(self, power_limit_mw: int, block: bool = True) -&gt; None:\n    \"\"\"Set the GPU's power management limit. Unit: mW.\"\"\"\n    current_limit = self.get_power_management_limit()\n    if current_limit == power_limit_mw:\n        return\n    self._client.set_power_limit([self._gpu_index], power_limit_mw, block)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.ZeusdNVIDIAGPU.reset_power_management_limit","title":"reset_power_management_limit","text":"<pre><code>reset_power_management_limit(block=True)\n</code></pre> <p>Reset the GPU's power management limit to the default value.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@_handle_nvml_errors\ndef reset_power_management_limit(self, block: bool = True) -&gt; None:\n    \"\"\"Reset the GPU's power management limit to the default value.\"\"\"\n    self.set_power_management_limit(\n        pynvml.nvmlDeviceGetPowerManagementDefaultLimit(self.handle),\n        block,\n    )\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.ZeusdNVIDIAGPU.set_persistence_mode","title":"set_persistence_mode","text":"<pre><code>set_persistence_mode(enabled, block=True)\n</code></pre> <p>Set persistence mode.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>def set_persistence_mode(self, enabled: bool, block: bool = True) -&gt; None:\n    \"\"\"Set persistence mode.\"\"\"\n    self._client.set_persistence_mode([self._gpu_index], enabled, block)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.ZeusdNVIDIAGPU.set_memory_locked_clocks","title":"set_memory_locked_clocks","text":"<pre><code>set_memory_locked_clocks(min_clock_mhz, max_clock_mhz, block=True)\n</code></pre> <p>Lock the memory clock to a specified range. Units: MHz.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>def set_memory_locked_clocks(self, min_clock_mhz: int, max_clock_mhz: int, block: bool = True) -&gt; None:\n    \"\"\"Lock the memory clock to a specified range. Units: MHz.\"\"\"\n    self._client.set_mem_locked_clocks([self._gpu_index], min_clock_mhz, max_clock_mhz, block)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.ZeusdNVIDIAGPU.reset_memory_locked_clocks","title":"reset_memory_locked_clocks","text":"<pre><code>reset_memory_locked_clocks(block=True)\n</code></pre> <p>Reset the locked memory clocks to the default.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>def reset_memory_locked_clocks(self, block: bool = True) -&gt; None:\n    \"\"\"Reset the locked memory clocks to the default.\"\"\"\n    self._client.reset_mem_locked_clocks([self._gpu_index], block)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.ZeusdNVIDIAGPU.set_gpu_locked_clocks","title":"set_gpu_locked_clocks","text":"<pre><code>set_gpu_locked_clocks(min_clock_mhz, max_clock_mhz, block=True)\n</code></pre> <p>Lock the GPU clock to a specified range. Units: MHz.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>def set_gpu_locked_clocks(self, min_clock_mhz: int, max_clock_mhz: int, block: bool = True) -&gt; None:\n    \"\"\"Lock the GPU clock to a specified range. Units: MHz.\"\"\"\n    self._client.set_gpu_locked_clocks([self._gpu_index], min_clock_mhz, max_clock_mhz, block)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.ZeusdNVIDIAGPU.reset_gpu_locked_clocks","title":"reset_gpu_locked_clocks","text":"<pre><code>reset_gpu_locked_clocks(block=True)\n</code></pre> <p>Reset the locked GPU clocks to the default.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>def reset_gpu_locked_clocks(self, block: bool = True) -&gt; None:\n    \"\"\"Reset the locked GPU clocks to the default.\"\"\"\n    self._client.reset_gpu_locked_clocks([self._gpu_index], block)\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPUs","title":"NVIDIAGPUs","text":"<p>               Bases: <code>GPUs</code></p> <p>Implementation of <code>GPUs</code> for NVIDIA GPUs.</p> <p><code>CUDA_VISIBLE_DEVICES</code> environment variable is respected if set. For example, if there are 4 GPUs on the node and <code>CUDA_VISIBLE_DEVICES=0,2</code>, only GPUs 0 and 2 are instantiated. In this case, to access GPU of CUDA index 0, use the index 0, and for CUDA index 2, use the index 1.</p> <p>If you have the Zeus daemon deployed, make sure you have set the <code>ZEUSD_SOCK_PATH</code> environment variable to the path of the Zeus daemon socket. This class will automatically use <code>ZeusdNVIDIAGPU</code> if <code>ZEUSD_SOCK_PATH</code> is set.</p> <p>Note</p> <p>For Grace Hopper, the power and energy values are for the entire superchip/module.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>class NVIDIAGPUs(gpu_common.GPUs):\n    \"\"\"Implementation of `GPUs` for NVIDIA GPUs.\n\n    `CUDA_VISIBLE_DEVICES` environment variable is respected if set.\n    For example, if there are 4 GPUs on the node and `CUDA_VISIBLE_DEVICES=0,2`,\n    only GPUs 0 and 2 are instantiated. In this case, to access\n    GPU of CUDA index 0, use the index 0, and for CUDA index 2, use the index 1.\n\n    If you have the Zeus daemon deployed, make sure you have set the `ZEUSD_SOCK_PATH`\n    environment variable to the path of the Zeus daemon socket. This class will\n    automatically use [`ZeusdNVIDIAGPU`][zeus.device.gpu.nvidia.ZeusdNVIDIAGPU]\n    if `ZEUSD_SOCK_PATH` is set.\n\n    !!! Note\n        For Grace Hopper, the power and energy values are for the entire superchip/module.\n    \"\"\"\n\n    def __init__(self, ensure_homogeneous: bool = False) -&gt; None:\n        \"\"\"Initialize NVML and sets up the GPUs.\n\n        Args:\n            ensure_homogeneous (bool): If True, ensures that all tracked GPUs have the same name.\n        \"\"\"\n        try:\n            pynvml.nvmlInit()\n            self._init_gpus()\n            if ensure_homogeneous:\n                self._ensure_homogeneous()\n        except pynvml.NVMLError as e:\n            exception_class = NVIDIAGPU._exception_map.get(\n                e.value,  # type: ignore\n                gpu_common.ZeusBaseGPUError,\n            )\n            raise exception_class(str(e)) from e\n\n    @property\n    def gpus(self) -&gt; Sequence[NVIDIAGPU]:\n        \"\"\"Return a list of NVIDIAGPU objects being tracked.\"\"\"\n        return self._gpus\n\n    def _init_gpus(self) -&gt; None:\n        # Must respect `CUDA_VISIBLE_DEVICES` if set\n        if (visible_device := os.environ.get(\"CUDA_VISIBLE_DEVICES\")) is not None:\n            if not visible_device:\n                raise gpu_common.ZeusGPUInitError(\n                    \"CUDA_VISIBLE_DEVICES is set to an empty string. \"\n                    \"It should either be unset or a comma-separated list of GPU indices.\"\n                )\n            if visible_device.startswith(\"MIG\"):\n                raise gpu_common.ZeusGPUInitError(\n                    \"CUDA_VISIBLE_DEVICES contains MIG devices. NVML (the library used by Zeus) \"\n                    \"currently does not support measuring the power or energy consumption of MIG \"\n                    \"slices. You can still measure the whole GPU by temporarily setting \"\n                    \"CUDA_VISIBLE_DEVICES to integer GPU indices and restoring it afterwards.\"\n                )\n            visible_indices = [int(idx) for idx in visible_device.split(\",\")]\n        else:\n            visible_indices = list(range(pynvml.nvmlDeviceGetCount()))\n\n        # If Zeusd env vars are set, use ZeusdNVIDIAGPU backed by a shared client.\n        config = ZeusdConfig.from_env()\n        if config is not None:\n            try:\n                client = ZeusdClient(config)\n                self._gpus = [ZeusdNVIDIAGPU(gpu_num, client) for gpu_num in visible_indices]\n            except ZeusBaseError as e:\n                raise gpu_common.ZeusGPUInitError(str(e)) from e\n            for gpu in self._gpus:\n                gpu._disable_sys_admin_warning = True\n        else:\n            self._gpus = [NVIDIAGPU(gpu_num) for gpu_num in visible_indices]\n\n    def __del__(self) -&gt; None:\n        \"\"\"Shut down NVML.\"\"\"\n        with contextlib.suppress(pynvml.NVMLError):\n            pynvml.nvmlShutdown()\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPUs.gpus","title":"gpus  <code>property</code>","text":"<pre><code>gpus\n</code></pre> <p>Return a list of NVIDIAGPU objects being tracked.</p>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPUs.__init__","title":"__init__","text":"<pre><code>__init__(ensure_homogeneous=False)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>ensure_homogeneous</code> <code>bool</code> <p>If True, ensures that all tracked GPUs have the same name.</p> <code>False</code> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>def __init__(self, ensure_homogeneous: bool = False) -&gt; None:\n    \"\"\"Initialize NVML and sets up the GPUs.\n\n    Args:\n        ensure_homogeneous (bool): If True, ensures that all tracked GPUs have the same name.\n    \"\"\"\n    try:\n        pynvml.nvmlInit()\n        self._init_gpus()\n        if ensure_homogeneous:\n            self._ensure_homogeneous()\n    except pynvml.NVMLError as e:\n        exception_class = NVIDIAGPU._exception_map.get(\n            e.value,  # type: ignore\n            gpu_common.ZeusBaseGPUError,\n        )\n        raise exception_class(str(e)) from e\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.NVIDIAGPUs.__del__","title":"__del__","text":"<pre><code>__del__()\n</code></pre> <p>Shut down NVML.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Shut down NVML.\"\"\"\n    with contextlib.suppress(pynvml.NVMLError):\n        pynvml.nvmlShutdown()\n</code></pre>"},{"location":"reference/device/gpu/nvidia/#zeus.device.gpu.nvidia.nvml_is_available","title":"nvml_is_available  <code>cached</code>","text":"<pre><code>nvml_is_available()\n</code></pre> <p>Check if NVML is available.</p> Source code in <code>zeus/device/gpu/nvidia.py</code> <pre><code>@lru_cache(maxsize=1)\ndef nvml_is_available() -&gt; bool:\n    \"\"\"Check if NVML is available.\"\"\"\n    try:\n        import pynvml\n    except ImportError:\n        logger.info(\"Failed to import `pynvml`. Make sure you have `nvidia-ml-py` installed.\")\n        return False\n\n    # Detect unofficial pynvml packages.\n    # If detected, this should be a critical error.\n    if not hasattr(pynvml, \"_nvmlGetFunctionPointer\"):\n        logger.error(\"Unoffical pynvml package detected!\")\n        raise ImportError(\n            \"Unofficial pynvml package detected! \"\n            \"This causes conflicts with the official NVIDIA bindings. \"\n            \"Please remove with `pip uninstall pynvml` and instead use the official \"\n            \"bindings from NVIDIA: `nvidia-ml-py`. \"\n        )\n\n    try:\n        pynvml.nvmlInit()\n        logger.info(\"pynvml is available and initialized.\")\n        return True\n    except pynvml.NVMLError as e:\n        logger.info(\"pynvml is available but could not initialize NVML: %s.\", e)\n        return False\n</code></pre>"},{"location":"reference/device/soc/","title":"soc","text":""},{"location":"reference/device/soc/#zeus.device.soc","title":"zeus.device.soc","text":"<p>Abstraction layer for SoC devices.</p> <p>The main function of this module is <code>get_soc</code>, which returns a SoC Manager object specific to the platform.</p>"},{"location":"reference/device/soc/#zeus.device.soc.get_soc","title":"get_soc","text":"<pre><code>get_soc()\n</code></pre> <p>Initialize and return a singleton monolithic SoC monitoring object.</p> <p>The function returns a SoC management object that aims to abstract underlying SoC monitoring functionalities.</p> Currently supported SoC devices <ul> <li>Apple Silicon</li> </ul> <p>If no SoC monitor object can be initialized, a <code>ZeusSoCInitError</code> exception will be raised.</p> Source code in <code>zeus/device/soc/__init__.py</code> <pre><code>def get_soc() -&gt; SoC:\n    \"\"\"Initialize and return a singleton monolithic SoC monitoring object.\n\n    The function returns a SoC management object that aims to abstract underlying SoC monitoring\n    functionalities.\n\n    Currently supported SoC devices:\n        - Apple Silicon\n\n    If no SoC monitor object can be initialized, a `ZeusSoCInitError` exception will be raised.\n    \"\"\"\n    global _soc\n    if _soc is not None:\n        return _soc\n\n    # --- Apple Silicon ---\n    if apple_silicon_is_available():\n        with suppress(ZeusAppleInitError):\n            _soc = AppleSilicon()\n\n    # --- Jetson Nano ---\n    elif jetson_is_available():\n        with suppress(ZeusJetsonInitError):\n            _soc = Jetson()\n\n    # For additional SoC's, add more initialization attempts.\n    if _soc is None:\n        raise ZeusSoCInitError(\"No observable SoC was found on the current machine.\")\n    return _soc\n</code></pre>"},{"location":"reference/device/soc/apple/","title":"apple","text":""},{"location":"reference/device/soc/apple/#zeus.device.soc.apple","title":"zeus.device.soc.apple","text":"<p>Apple Silicon SoC's.</p>"},{"location":"reference/device/soc/apple/#zeus.device.soc.apple.MockZeusAppleSilicon","title":"MockZeusAppleSilicon","text":"<p>Mock class for zeus-apple-silicon library.</p> Source code in <code>zeus/device/soc/apple.py</code> <pre><code>class MockZeusAppleSilicon:\n    \"\"\"Mock class for zeus-apple-silicon library.\"\"\"\n\n    def __getattr__(self, name):\n        \"\"\"Raise an error if any method is called.\n\n        Since this class is only used when `zeus-apple-silicon` is not\n        available, something has gone wrong if any method is called.\n        \"\"\"\n        raise RuntimeError(\n            f\"zeus-apple-silicon is not available and zeus-apple-silicon.{name} \"\n            \"shouldn't have been called. This is a bug.\"\n        )\n</code></pre>"},{"location":"reference/device/soc/apple/#zeus.device.soc.apple.MockZeusAppleSilicon.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(name)\n</code></pre> <p>Raise an error if any method is called.</p> <p>Since this class is only used when <code>zeus-apple-silicon</code> is not available, something has gone wrong if any method is called.</p> Source code in <code>zeus/device/soc/apple.py</code> <pre><code>def __getattr__(self, name):\n    \"\"\"Raise an error if any method is called.\n\n    Since this class is only used when `zeus-apple-silicon` is not\n    available, something has gone wrong if any method is called.\n    \"\"\"\n    raise RuntimeError(\n        f\"zeus-apple-silicon is not available and zeus-apple-silicon.{name} \"\n        \"shouldn't have been called. This is a bug.\"\n    )\n</code></pre>"},{"location":"reference/device/soc/apple/#zeus.device.soc.apple.ZeusAppleInitError","title":"ZeusAppleInitError","text":"<p>               Bases: <code>ZeusSoCInitError</code></p> <p>Import error for Apple SoC initialization failures.</p> Source code in <code>zeus/device/soc/apple.py</code> <pre><code>class ZeusAppleInitError(ZeusSoCInitError):\n    \"\"\"Import error for Apple SoC initialization failures.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/soc/apple/#zeus.device.soc.apple.ZeusAppleInitError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/soc/apple.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/soc/apple/#zeus.device.soc.apple.AppleSiliconMeasurement","title":"AppleSiliconMeasurement  <code>dataclass</code>","text":"<p>               Bases: <code>SoCMeasurement</code></p> <p>Represents energy consumption of various subsystems on an Apple processor.</p> <p>All measurements are in mJ.</p> Source code in <code>zeus/device/soc/apple.py</code> <pre><code>@dataclass\nclass AppleSiliconMeasurement(SoCMeasurement):\n    \"\"\"Represents energy consumption of various subsystems on an Apple processor.\n\n    All measurements are in mJ.\n    \"\"\"\n\n    # CPU related metrics\n    cpu_total_mj: int | None = None\n    efficiency_cores_mj: list[int] | None = None\n    performance_cores_mj: list[int] | None = None\n    efficiency_core_manager_mj: int | None = None\n    performance_core_manager_mj: int | None = None\n\n    # DRAM\n    dram_mj: int | None = None\n\n    # GPU related metrics\n    gpu_mj: int | None = None\n    gpu_sram_mj: int | None = None\n\n    # ANE (Apple Neural Engine)\n    ane_mj: int | None = None\n\n    def __sub__(self, other: AppleSiliconMeasurement) -&gt; AppleSiliconMeasurement:\n        \"\"\"Produce a single measurement object containing differences across all fields.\"\"\"\n        if not isinstance(other, type(self)):\n            raise TypeError(\"Subtraction is only supported between AppleSiliconMeasurement instances.\")\n\n        result = self.__class__()\n\n        for field in fields(self):\n            f_name = field.name\n            value1 = getattr(self, f_name)\n            value2 = getattr(other, f_name)\n            if value1 is None and value2 is None:\n                continue\n\n            if type(value1) is not type(value2):\n                raise ValueError(f\"Inconsistent field between two AppleSiliconMeasurement objects: {f_name}\")\n\n            if isinstance(value1, int):\n                setattr(result, f_name, value1 - value2)\n            elif isinstance(value1, list):\n                if len(value1) != len(value2):\n                    raise ValueError(f\"Inconsistent field between two AppleSiliconMeasurement objects: {f_name}\")\n                setattr(result, f_name, [x - y for x, y in zip(value1, value2)])\n\n        return result\n\n    def zero_all_fields(self) -&gt; None:\n        \"\"\"Set the value of all fields in the measurement object to zero.\"\"\"\n        for field in fields(self):\n            f_name = field.name\n            f_value = getattr(self, f_name)\n            if isinstance(f_value, int):\n                setattr(self, f_name, 0)\n            elif isinstance(f_value, list):\n                setattr(self, f_name, [])\n            else:\n                setattr(self, f_name, None)\n\n    @classmethod\n    def from_metrics(\n        cls,\n        metrics: zeus_apple_silicon.AppleEnergyMetrics,\n    ) -&gt; AppleSiliconMeasurement:\n        \"\"\"Return an AppleSiliconMeasurement object based on an AppleEnergyMetrics object.\"\"\"\n        return cls(\n            cpu_total_mj=metrics.cpu_total_mj,\n            efficiency_cores_mj=metrics.efficiency_cores_mj,\n            performance_cores_mj=metrics.performance_cores_mj,\n            efficiency_core_manager_mj=metrics.efficiency_core_manager_mj,\n            performance_core_manager_mj=metrics.performance_core_manager_mj,\n            dram_mj=metrics.dram_mj,\n            gpu_mj=metrics.gpu_mj,\n            gpu_sram_mj=metrics.gpu_sram_mj,\n            ane_mj=metrics.ane_mj,\n        )\n</code></pre>"},{"location":"reference/device/soc/apple/#zeus.device.soc.apple.AppleSiliconMeasurement.__sub__","title":"__sub__","text":"<pre><code>__sub__(other)\n</code></pre> <p>Produce a single measurement object containing differences across all fields.</p> Source code in <code>zeus/device/soc/apple.py</code> <pre><code>def __sub__(self, other: AppleSiliconMeasurement) -&gt; AppleSiliconMeasurement:\n    \"\"\"Produce a single measurement object containing differences across all fields.\"\"\"\n    if not isinstance(other, type(self)):\n        raise TypeError(\"Subtraction is only supported between AppleSiliconMeasurement instances.\")\n\n    result = self.__class__()\n\n    for field in fields(self):\n        f_name = field.name\n        value1 = getattr(self, f_name)\n        value2 = getattr(other, f_name)\n        if value1 is None and value2 is None:\n            continue\n\n        if type(value1) is not type(value2):\n            raise ValueError(f\"Inconsistent field between two AppleSiliconMeasurement objects: {f_name}\")\n\n        if isinstance(value1, int):\n            setattr(result, f_name, value1 - value2)\n        elif isinstance(value1, list):\n            if len(value1) != len(value2):\n                raise ValueError(f\"Inconsistent field between two AppleSiliconMeasurement objects: {f_name}\")\n            setattr(result, f_name, [x - y for x, y in zip(value1, value2)])\n\n    return result\n</code></pre>"},{"location":"reference/device/soc/apple/#zeus.device.soc.apple.AppleSiliconMeasurement.zero_all_fields","title":"zero_all_fields","text":"<pre><code>zero_all_fields()\n</code></pre> <p>Set the value of all fields in the measurement object to zero.</p> Source code in <code>zeus/device/soc/apple.py</code> <pre><code>def zero_all_fields(self) -&gt; None:\n    \"\"\"Set the value of all fields in the measurement object to zero.\"\"\"\n    for field in fields(self):\n        f_name = field.name\n        f_value = getattr(self, f_name)\n        if isinstance(f_value, int):\n            setattr(self, f_name, 0)\n        elif isinstance(f_value, list):\n            setattr(self, f_name, [])\n        else:\n            setattr(self, f_name, None)\n</code></pre>"},{"location":"reference/device/soc/apple/#zeus.device.soc.apple.AppleSiliconMeasurement.from_metrics","title":"from_metrics  <code>classmethod</code>","text":"<pre><code>from_metrics(metrics)\n</code></pre> <p>Return an AppleSiliconMeasurement object based on an AppleEnergyMetrics object.</p> Source code in <code>zeus/device/soc/apple.py</code> <pre><code>@classmethod\ndef from_metrics(\n    cls,\n    metrics: zeus_apple_silicon.AppleEnergyMetrics,\n) -&gt; AppleSiliconMeasurement:\n    \"\"\"Return an AppleSiliconMeasurement object based on an AppleEnergyMetrics object.\"\"\"\n    return cls(\n        cpu_total_mj=metrics.cpu_total_mj,\n        efficiency_cores_mj=metrics.efficiency_cores_mj,\n        performance_cores_mj=metrics.performance_cores_mj,\n        efficiency_core_manager_mj=metrics.efficiency_core_manager_mj,\n        performance_core_manager_mj=metrics.performance_core_manager_mj,\n        dram_mj=metrics.dram_mj,\n        gpu_mj=metrics.gpu_mj,\n        gpu_sram_mj=metrics.gpu_sram_mj,\n        ane_mj=metrics.ane_mj,\n    )\n</code></pre>"},{"location":"reference/device/soc/apple/#zeus.device.soc.apple.AppleSilicon","title":"AppleSilicon","text":"<p>               Bases: <code>SoC</code></p> <p>An interface for obtaining energy metrics of an Apple processor.</p> Source code in <code>zeus/device/soc/apple.py</code> <pre><code>class AppleSilicon(SoC):\n    \"\"\"An interface for obtaining energy metrics of an Apple processor.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize an instance of an Apple Silicon energy monitor.\"\"\"\n        self._monitor: zeus_apple_silicon.AppleEnergyMonitor\n        self.available_metrics: set[str] | None = None\n\n        try:\n            self._monitor = zeus_apple_silicon.AppleEnergyMonitor()\n\n        # This except block exists for failures the AppleEnergyMonitor\n        # object may encounter during its own construction.\n        except RuntimeError as e:\n            raise ZeusAppleInitError(f\"Failed to initialize `AppleEnergyMonitor`: {e}\") from None\n\n    def get_available_metrics(self) -&gt; set[str]:\n        \"\"\"Return a set of all observable metrics on the current processor.\"\"\"\n        if self.available_metrics is None:\n            result: SoCMeasurement = self.get_total_energy_consumption()\n            available_metrics = set()\n\n            metrics_dict = asdict(result)\n            for f_name, f_value in metrics_dict.items():\n                if f_value is not None:\n                    available_metrics.add(f_name)\n\n            self.available_metrics = available_metrics\n        return self.available_metrics\n\n    def get_total_energy_consumption(self) -&gt; AppleSiliconMeasurement:\n        \"\"\"Returns the total energy consumption of the SoC.\n\n        The measurement should be cumulative; different calls to this function throughout\n        the lifetime of a single `SoC` manager object should count from a fixed arbitrary\n        point in time.\n\n        Units: mJ.\n        \"\"\"\n        result = self._monitor.get_cumulative_energy()\n        return AppleSiliconMeasurement.from_metrics(result)\n\n    def begin_window(self, key) -&gt; None:\n        \"\"\"Begin a measurement interval labeled with `key`.\"\"\"\n        self._monitor.begin_window(key)\n\n    def end_window(self, key) -&gt; AppleSiliconMeasurement:\n        \"\"\"End a measurement window and return the energy consumption. Units: mJ.\"\"\"\n        result = self._monitor.end_window(key)\n        return AppleSiliconMeasurement.from_metrics(result)\n</code></pre>"},{"location":"reference/device/soc/apple/#zeus.device.soc.apple.AppleSilicon.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> Source code in <code>zeus/device/soc/apple.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize an instance of an Apple Silicon energy monitor.\"\"\"\n    self._monitor: zeus_apple_silicon.AppleEnergyMonitor\n    self.available_metrics: set[str] | None = None\n\n    try:\n        self._monitor = zeus_apple_silicon.AppleEnergyMonitor()\n\n    # This except block exists for failures the AppleEnergyMonitor\n    # object may encounter during its own construction.\n    except RuntimeError as e:\n        raise ZeusAppleInitError(f\"Failed to initialize `AppleEnergyMonitor`: {e}\") from None\n</code></pre>"},{"location":"reference/device/soc/apple/#zeus.device.soc.apple.AppleSilicon.get_available_metrics","title":"get_available_metrics","text":"<pre><code>get_available_metrics()\n</code></pre> <p>Return a set of all observable metrics on the current processor.</p> Source code in <code>zeus/device/soc/apple.py</code> <pre><code>def get_available_metrics(self) -&gt; set[str]:\n    \"\"\"Return a set of all observable metrics on the current processor.\"\"\"\n    if self.available_metrics is None:\n        result: SoCMeasurement = self.get_total_energy_consumption()\n        available_metrics = set()\n\n        metrics_dict = asdict(result)\n        for f_name, f_value in metrics_dict.items():\n            if f_value is not None:\n                available_metrics.add(f_name)\n\n        self.available_metrics = available_metrics\n    return self.available_metrics\n</code></pre>"},{"location":"reference/device/soc/apple/#zeus.device.soc.apple.AppleSilicon.get_total_energy_consumption","title":"get_total_energy_consumption","text":"<pre><code>get_total_energy_consumption()\n</code></pre> <p>Returns the total energy consumption of the SoC.</p> <p>The measurement should be cumulative; different calls to this function throughout the lifetime of a single <code>SoC</code> manager object should count from a fixed arbitrary point in time.</p> <p>Units: mJ.</p> Source code in <code>zeus/device/soc/apple.py</code> <pre><code>def get_total_energy_consumption(self) -&gt; AppleSiliconMeasurement:\n    \"\"\"Returns the total energy consumption of the SoC.\n\n    The measurement should be cumulative; different calls to this function throughout\n    the lifetime of a single `SoC` manager object should count from a fixed arbitrary\n    point in time.\n\n    Units: mJ.\n    \"\"\"\n    result = self._monitor.get_cumulative_energy()\n    return AppleSiliconMeasurement.from_metrics(result)\n</code></pre>"},{"location":"reference/device/soc/apple/#zeus.device.soc.apple.AppleSilicon.begin_window","title":"begin_window","text":"<pre><code>begin_window(key)\n</code></pre> <p>Begin a measurement interval labeled with <code>key</code>.</p> Source code in <code>zeus/device/soc/apple.py</code> <pre><code>def begin_window(self, key) -&gt; None:\n    \"\"\"Begin a measurement interval labeled with `key`.\"\"\"\n    self._monitor.begin_window(key)\n</code></pre>"},{"location":"reference/device/soc/apple/#zeus.device.soc.apple.AppleSilicon.end_window","title":"end_window","text":"<pre><code>end_window(key)\n</code></pre> <p>End a measurement window and return the energy consumption. Units: mJ.</p> Source code in <code>zeus/device/soc/apple.py</code> <pre><code>def end_window(self, key) -&gt; AppleSiliconMeasurement:\n    \"\"\"End a measurement window and return the energy consumption. Units: mJ.\"\"\"\n    result = self._monitor.end_window(key)\n    return AppleSiliconMeasurement.from_metrics(result)\n</code></pre>"},{"location":"reference/device/soc/apple/#zeus.device.soc.apple.apple_silicon_is_available","title":"apple_silicon_is_available  <code>cached</code>","text":"<pre><code>apple_silicon_is_available()\n</code></pre> <p>Check if Apple silicon is available.</p> Source code in <code>zeus/device/soc/apple.py</code> <pre><code>@lru_cache(maxsize=1)\ndef apple_silicon_is_available() -&gt; bool:\n    \"\"\"Check if Apple silicon is available.\"\"\"\n    if not zeus_apple_available:\n        return False\n    if sys.platform != \"darwin\" or platform.processor() != \"arm\":\n        return False\n    return True\n</code></pre>"},{"location":"reference/device/soc/common/","title":"common","text":""},{"location":"reference/device/soc/common/#zeus.device.soc.common","title":"zeus.device.soc.common","text":"<p>Error wrappers and classes common to all SoC devices.</p>"},{"location":"reference/device/soc/common/#zeus.device.soc.common.ZeusSoCInitError","title":"ZeusSoCInitError","text":"<p>               Bases: <code>ZeusBaseSoCError</code></p> <p>Import error for SoC initialization failures.</p> Source code in <code>zeus/device/soc/common.py</code> <pre><code>class ZeusSoCInitError(ZeusBaseSoCError):\n    \"\"\"Import error for SoC initialization failures.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Intialize the exception object.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/soc/common/#zeus.device.soc.common.ZeusSoCInitError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/soc/common.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Intialize the exception object.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/soc/common/#zeus.device.soc.common.SoCMeasurement","title":"SoCMeasurement  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Represents energy consumption metrics of various subsystems on a SoC processor.</p> <p>Since subsystems available on a SoC processor are highly variable, the fields of this dataclass are entirely up to each derived class.</p> <p>Fields available and implemented for a specific SoC processor architecture can be found by referring to the SoCMeasurement derived class corresponding to that particular architecture (e.g., <code>AppleSiliconMeasurement</code> for Apple silicon), or by simply printing an instance of that derived class.</p> <p>Units: mJ</p> Source code in <code>zeus/device/soc/common.py</code> <pre><code>@dataclass\nclass SoCMeasurement(abc.ABC, metaclass=DeprecatedAliasABCMeta):\n    \"\"\"Represents energy consumption metrics of various subsystems on a SoC processor.\n\n    Since subsystems available on a SoC processor are highly variable, the fields of\n    this dataclass are entirely up to each derived class.\n\n    Fields available and implemented for a specific SoC processor architecture can be\n    found by referring to the SoCMeasurement derived class corresponding to that\n    particular architecture (e.g., `AppleSiliconMeasurement` for Apple silicon),\n    or by simply printing an instance of that derived class.\n\n    Units: mJ\n    \"\"\"\n\n    @abc.abstractmethod\n    def __sub__(self, other) -&gt; SoCMeasurement:\n        \"\"\"Produce a single measurement object containing differences across all fields.\"\"\"\n        pass\n\n    @deprecated_alias(\"zeroAllFields\")\n    @abc.abstractmethod\n    def zero_all_fields(self) -&gt; None:\n        \"\"\"Set the value of all fields in the measurement object to zero.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/device/soc/common/#zeus.device.soc.common.SoCMeasurement.__sub__","title":"__sub__  <code>abstractmethod</code>","text":"<pre><code>__sub__(other)\n</code></pre> <p>Produce a single measurement object containing differences across all fields.</p> Source code in <code>zeus/device/soc/common.py</code> <pre><code>@abc.abstractmethod\ndef __sub__(self, other) -&gt; SoCMeasurement:\n    \"\"\"Produce a single measurement object containing differences across all fields.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/soc/common/#zeus.device.soc.common.SoCMeasurement.zero_all_fields","title":"zero_all_fields  <code>abstractmethod</code>","text":"<pre><code>zero_all_fields()\n</code></pre> <p>Set the value of all fields in the measurement object to zero.</p> Source code in <code>zeus/device/soc/common.py</code> <pre><code>@deprecated_alias(\"zeroAllFields\")\n@abc.abstractmethod\ndef zero_all_fields(self) -&gt; None:\n    \"\"\"Set the value of all fields in the measurement object to zero.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/soc/common/#zeus.device.soc.common.SoC","title":"SoC","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract base class for monitoring the energy consumption of a monolithic SoC processor.</p> <p>This class will be utilized by ZeusMonitor.</p> Source code in <code>zeus/device/soc/common.py</code> <pre><code>class SoC(abc.ABC, metaclass=DeprecatedAliasABCMeta):\n    \"\"\"An abstract base class for monitoring the energy consumption of a monolithic SoC processor.\n\n    This class will be utilized by ZeusMonitor.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the SoC class.\n\n        If a derived class implementation intends to rely on this base class's implementation of\n        `begin_window` and `end_window`, it must invoke this constructor in its own. Otherwise, if\n        it will override both of those methods, it can skip invoking this.\n        \"\"\"\n        self.measurement_states: dict[str, SoCMeasurement] = {}\n\n    @deprecated_alias(\"getAvailableMetrics\")\n    @abc.abstractmethod\n    def get_available_metrics(self) -&gt; set[str]:\n        \"\"\"Return a set of all observable metrics on the current processor.\"\"\"\n        pass\n\n    @deprecated_alias(\"getTotalEnergyConsumption\")\n    @abc.abstractmethod\n    def get_total_energy_consumption(self) -&gt; SoCMeasurement:\n        \"\"\"Returns the total energy consumption of the SoC.\n\n        The measurement should be cumulative; different calls to this function throughout\n        the lifetime of a single `SoC` manager object should count from a fixed arbitrary\n        point in time.\n\n        Units: mJ.\n        \"\"\"\n        pass\n\n    @deprecated_alias(\"beginWindow\")\n    def begin_window(self, key) -&gt; None:\n        \"\"\"Begin a measurement interval labeled with `key`.\"\"\"\n        if key in self.measurement_states:\n            raise KeyError(f\"Measurement window '{key}' already exists\")\n\n        self.measurement_states[key] = self.get_total_energy_consumption()\n\n    @deprecated_alias(\"endWindow\")\n    def end_window(self, key) -&gt; SoCMeasurement:\n        \"\"\"End a measurement window and return the energy consumption. Units: mJ.\"\"\"\n        # Retrieve the measurement taken at the start of the window.\n        try:\n            start_cumulative: SoCMeasurement = self.measurement_states.pop(key)\n        except KeyError:\n            raise KeyError(f\"Measurement window '{key}' does not exist\") from None\n\n        end_cumulative: SoCMeasurement = self.get_total_energy_consumption()\n        return end_cumulative - start_cumulative\n</code></pre>"},{"location":"reference/device/soc/common/#zeus.device.soc.common.SoC.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>If a derived class implementation intends to rely on this base class's implementation of <code>begin_window</code> and <code>end_window</code>, it must invoke this constructor in its own. Otherwise, if it will override both of those methods, it can skip invoking this.</p> Source code in <code>zeus/device/soc/common.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the SoC class.\n\n    If a derived class implementation intends to rely on this base class's implementation of\n    `begin_window` and `end_window`, it must invoke this constructor in its own. Otherwise, if\n    it will override both of those methods, it can skip invoking this.\n    \"\"\"\n    self.measurement_states: dict[str, SoCMeasurement] = {}\n</code></pre>"},{"location":"reference/device/soc/common/#zeus.device.soc.common.SoC.get_available_metrics","title":"get_available_metrics  <code>abstractmethod</code>","text":"<pre><code>get_available_metrics()\n</code></pre> <p>Return a set of all observable metrics on the current processor.</p> Source code in <code>zeus/device/soc/common.py</code> <pre><code>@deprecated_alias(\"getAvailableMetrics\")\n@abc.abstractmethod\ndef get_available_metrics(self) -&gt; set[str]:\n    \"\"\"Return a set of all observable metrics on the current processor.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/soc/common/#zeus.device.soc.common.SoC.get_total_energy_consumption","title":"get_total_energy_consumption  <code>abstractmethod</code>","text":"<pre><code>get_total_energy_consumption()\n</code></pre> <p>Returns the total energy consumption of the SoC.</p> <p>The measurement should be cumulative; different calls to this function throughout the lifetime of a single <code>SoC</code> manager object should count from a fixed arbitrary point in time.</p> <p>Units: mJ.</p> Source code in <code>zeus/device/soc/common.py</code> <pre><code>@deprecated_alias(\"getTotalEnergyConsumption\")\n@abc.abstractmethod\ndef get_total_energy_consumption(self) -&gt; SoCMeasurement:\n    \"\"\"Returns the total energy consumption of the SoC.\n\n    The measurement should be cumulative; different calls to this function throughout\n    the lifetime of a single `SoC` manager object should count from a fixed arbitrary\n    point in time.\n\n    Units: mJ.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/soc/common/#zeus.device.soc.common.SoC.begin_window","title":"begin_window","text":"<pre><code>begin_window(key)\n</code></pre> <p>Begin a measurement interval labeled with <code>key</code>.</p> Source code in <code>zeus/device/soc/common.py</code> <pre><code>@deprecated_alias(\"beginWindow\")\ndef begin_window(self, key) -&gt; None:\n    \"\"\"Begin a measurement interval labeled with `key`.\"\"\"\n    if key in self.measurement_states:\n        raise KeyError(f\"Measurement window '{key}' already exists\")\n\n    self.measurement_states[key] = self.get_total_energy_consumption()\n</code></pre>"},{"location":"reference/device/soc/common/#zeus.device.soc.common.SoC.end_window","title":"end_window","text":"<pre><code>end_window(key)\n</code></pre> <p>End a measurement window and return the energy consumption. Units: mJ.</p> Source code in <code>zeus/device/soc/common.py</code> <pre><code>@deprecated_alias(\"endWindow\")\ndef end_window(self, key) -&gt; SoCMeasurement:\n    \"\"\"End a measurement window and return the energy consumption. Units: mJ.\"\"\"\n    # Retrieve the measurement taken at the start of the window.\n    try:\n        start_cumulative: SoCMeasurement = self.measurement_states.pop(key)\n    except KeyError:\n        raise KeyError(f\"Measurement window '{key}' does not exist\") from None\n\n    end_cumulative: SoCMeasurement = self.get_total_energy_consumption()\n    return end_cumulative - start_cumulative\n</code></pre>"},{"location":"reference/device/soc/common/#zeus.device.soc.common.EmptySoC","title":"EmptySoC","text":"<p>               Bases: <code>SoC</code></p> <p>Empty SoC management object to be used when SoC management object is unavailable.</p> Source code in <code>zeus/device/soc/common.py</code> <pre><code>class EmptySoC(SoC):\n    \"\"\"Empty SoC management object to be used when SoC management object is unavailable.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize an empty SoC class.\"\"\"\n        pass\n\n    @deprecated_alias(\"getAvailableMetrics\")\n    def get_available_metrics(self) -&gt; set[str]:\n        \"\"\"Return a set of all observable metrics on the current processor.\"\"\"\n        return set()\n\n    @deprecated_alias(\"getTotalEnergyConsumption\")\n    def get_total_energy_consumption(self) -&gt; SoCMeasurement:\n        \"\"\"Returns the total energy consumption of the SoC.\n\n        The measurement should be cumulative, with different calls to this function all\n        counting from a fixed arbitrary point in time.\n\n        Units: mJ.\n        \"\"\"\n        raise ValueError(\"No SoC is available.\")\n\n    @deprecated_alias(\"beginWindow\")\n    def begin_window(self, key) -&gt; None:\n        \"\"\"Begin a measurement interval labeled with `key`.\"\"\"\n        raise ValueError(\"No SoC is available.\")\n\n    @deprecated_alias(\"endWindow\")\n    def end_window(self, key) -&gt; SoCMeasurement:\n        \"\"\"End a measurement window and return the energy consumption. Units: mJ.\"\"\"\n        raise ValueError(\"No SoC is available.\")\n</code></pre>"},{"location":"reference/device/soc/common/#zeus.device.soc.common.EmptySoC.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> Source code in <code>zeus/device/soc/common.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize an empty SoC class.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/soc/common/#zeus.device.soc.common.EmptySoC.get_available_metrics","title":"get_available_metrics","text":"<pre><code>get_available_metrics()\n</code></pre> <p>Return a set of all observable metrics on the current processor.</p> Source code in <code>zeus/device/soc/common.py</code> <pre><code>@deprecated_alias(\"getAvailableMetrics\")\ndef get_available_metrics(self) -&gt; set[str]:\n    \"\"\"Return a set of all observable metrics on the current processor.\"\"\"\n    return set()\n</code></pre>"},{"location":"reference/device/soc/common/#zeus.device.soc.common.EmptySoC.get_total_energy_consumption","title":"get_total_energy_consumption","text":"<pre><code>get_total_energy_consumption()\n</code></pre> <p>Returns the total energy consumption of the SoC.</p> <p>The measurement should be cumulative, with different calls to this function all counting from a fixed arbitrary point in time.</p> <p>Units: mJ.</p> Source code in <code>zeus/device/soc/common.py</code> <pre><code>@deprecated_alias(\"getTotalEnergyConsumption\")\ndef get_total_energy_consumption(self) -&gt; SoCMeasurement:\n    \"\"\"Returns the total energy consumption of the SoC.\n\n    The measurement should be cumulative, with different calls to this function all\n    counting from a fixed arbitrary point in time.\n\n    Units: mJ.\n    \"\"\"\n    raise ValueError(\"No SoC is available.\")\n</code></pre>"},{"location":"reference/device/soc/common/#zeus.device.soc.common.EmptySoC.begin_window","title":"begin_window","text":"<pre><code>begin_window(key)\n</code></pre> <p>Begin a measurement interval labeled with <code>key</code>.</p> Source code in <code>zeus/device/soc/common.py</code> <pre><code>@deprecated_alias(\"beginWindow\")\ndef begin_window(self, key) -&gt; None:\n    \"\"\"Begin a measurement interval labeled with `key`.\"\"\"\n    raise ValueError(\"No SoC is available.\")\n</code></pre>"},{"location":"reference/device/soc/common/#zeus.device.soc.common.EmptySoC.end_window","title":"end_window","text":"<pre><code>end_window(key)\n</code></pre> <p>End a measurement window and return the energy consumption. Units: mJ.</p> Source code in <code>zeus/device/soc/common.py</code> <pre><code>@deprecated_alias(\"endWindow\")\ndef end_window(self, key) -&gt; SoCMeasurement:\n    \"\"\"End a measurement window and return the energy consumption. Units: mJ.\"\"\"\n    raise ValueError(\"No SoC is available.\")\n</code></pre>"},{"location":"reference/device/soc/jetson/","title":"jetson","text":""},{"location":"reference/device/soc/jetson/#zeus.device.soc.jetson","title":"zeus.device.soc.jetson","text":"<p>NVIDIA Jetson platform support.</p>"},{"location":"reference/device/soc/jetson/#zeus.device.soc.jetson.ZeusJetsonInitError","title":"ZeusJetsonInitError","text":"<p>               Bases: <code>ZeusSoCInitError</code></p> <p>Jetson initialization failures.</p> Source code in <code>zeus/device/soc/jetson.py</code> <pre><code>class ZeusJetsonInitError(ZeusSoCInitError):\n    \"\"\"Jetson initialization failures.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize Zeus Exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/device/soc/jetson/#zeus.device.soc.jetson.ZeusJetsonInitError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/device/soc/jetson.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize Zeus Exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/device/soc/jetson/#zeus.device.soc.jetson.PowerMeasurementStrategy","title":"PowerMeasurementStrategy","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for two different power measurement strategies.</p> Source code in <code>zeus/device/soc/jetson.py</code> <pre><code>class PowerMeasurementStrategy(abc.ABC):\n    \"\"\"Abstract base class for two different power measurement strategies.\"\"\"\n\n    @abc.abstractmethod\n    def measure_power(self) -&gt; float:\n        \"\"\"Measure power in mW.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/device/soc/jetson/#zeus.device.soc.jetson.PowerMeasurementStrategy.measure_power","title":"measure_power  <code>abstractmethod</code>","text":"<pre><code>measure_power()\n</code></pre> <p>Measure power in mW.</p> Source code in <code>zeus/device/soc/jetson.py</code> <pre><code>@abc.abstractmethod\ndef measure_power(self) -&gt; float:\n    \"\"\"Measure power in mW.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/device/soc/jetson/#zeus.device.soc.jetson.DirectPower","title":"DirectPower","text":"<p>               Bases: <code>PowerMeasurementStrategy</code></p> <p>Reads power directly from a sysfs path.</p> Source code in <code>zeus/device/soc/jetson.py</code> <pre><code>class DirectPower(PowerMeasurementStrategy):\n    \"\"\"Reads power directly from a sysfs path.\"\"\"\n\n    def __init__(self, power_path: Path) -&gt; None:\n        \"\"\"Initialize DirectPower paths.\"\"\"\n        self.power_path: Path = power_path\n\n    def measure_power(self) -&gt; float:\n        \"\"\"Measure power by reading from sysfs paths.\n\n        Units: mW.\n        \"\"\"\n        power: float = float(self.power_path.read_text().strip())\n        return power\n</code></pre>"},{"location":"reference/device/soc/jetson/#zeus.device.soc.jetson.DirectPower.__init__","title":"__init__","text":"<pre><code>__init__(power_path)\n</code></pre> Source code in <code>zeus/device/soc/jetson.py</code> <pre><code>def __init__(self, power_path: Path) -&gt; None:\n    \"\"\"Initialize DirectPower paths.\"\"\"\n    self.power_path: Path = power_path\n</code></pre>"},{"location":"reference/device/soc/jetson/#zeus.device.soc.jetson.DirectPower.measure_power","title":"measure_power","text":"<pre><code>measure_power()\n</code></pre> <p>Measure power by reading from sysfs paths.</p> <p>Units: mW.</p> Source code in <code>zeus/device/soc/jetson.py</code> <pre><code>def measure_power(self) -&gt; float:\n    \"\"\"Measure power by reading from sysfs paths.\n\n    Units: mW.\n    \"\"\"\n    power: float = float(self.power_path.read_text().strip())\n    return power\n</code></pre>"},{"location":"reference/device/soc/jetson/#zeus.device.soc.jetson.VoltageCurrentProduct","title":"VoltageCurrentProduct","text":"<p>               Bases: <code>PowerMeasurementStrategy</code></p> <p>Computes power as product of voltage and current, read from two sysfs paths.</p> Source code in <code>zeus/device/soc/jetson.py</code> <pre><code>class VoltageCurrentProduct(PowerMeasurementStrategy):\n    \"\"\"Computes power as product of voltage and current, read from two sysfs paths.\"\"\"\n\n    def __init__(self, voltage_path: Path, current_path: Path) -&gt; None:\n        \"\"\"Initialize VoltageCurrentProduct paths.\"\"\"\n        self.voltage_path: Path = voltage_path\n        self.current_path: Path = current_path\n\n    def measure_power(self) -&gt; float:\n        \"\"\"Measure power by reading from sysfs paths.\n\n        Units: mW.\n        \"\"\"\n        voltage: float = float(self.voltage_path.read_text().strip())\n        current: float = float(self.current_path.read_text().strip())\n        return (voltage * current) / 1000\n</code></pre>"},{"location":"reference/device/soc/jetson/#zeus.device.soc.jetson.VoltageCurrentProduct.__init__","title":"__init__","text":"<pre><code>__init__(voltage_path, current_path)\n</code></pre> Source code in <code>zeus/device/soc/jetson.py</code> <pre><code>def __init__(self, voltage_path: Path, current_path: Path) -&gt; None:\n    \"\"\"Initialize VoltageCurrentProduct paths.\"\"\"\n    self.voltage_path: Path = voltage_path\n    self.current_path: Path = current_path\n</code></pre>"},{"location":"reference/device/soc/jetson/#zeus.device.soc.jetson.VoltageCurrentProduct.measure_power","title":"measure_power","text":"<pre><code>measure_power()\n</code></pre> <p>Measure power by reading from sysfs paths.</p> <p>Units: mW.</p> Source code in <code>zeus/device/soc/jetson.py</code> <pre><code>def measure_power(self) -&gt; float:\n    \"\"\"Measure power by reading from sysfs paths.\n\n    Units: mW.\n    \"\"\"\n    voltage: float = float(self.voltage_path.read_text().strip())\n    current: float = float(self.current_path.read_text().strip())\n    return (voltage * current) / 1000\n</code></pre>"},{"location":"reference/device/soc/jetson/#zeus.device.soc.jetson.JetsonMeasurement","title":"JetsonMeasurement  <code>dataclass</code>","text":"<p>               Bases: <code>SoCMeasurement</code></p> <p>Represents energy measurements for Jetson subsystems.</p> <p>All measurements are in mJ.</p> Source code in <code>zeus/device/soc/jetson.py</code> <pre><code>@dataclass\nclass JetsonMeasurement(SoCMeasurement):\n    \"\"\"Represents energy measurements for Jetson subsystems.\n\n    All measurements are in mJ.\n    \"\"\"\n\n    cpu_energy_mj: float | None = None\n    gpu_energy_mj: float | None = None\n    total_energy_mj: float | None = None\n\n    def __sub__(self, other: JetsonMeasurement) -&gt; JetsonMeasurement:\n        \"\"\"Produce a single measurement object containing differences across all fields.\"\"\"\n        if not isinstance(other, type(self)):\n            raise TypeError(\"Subtraction is only supported between Jetson instances.\")\n\n        result = self.__class__()\n\n        for field in fields(self):\n            f_name = field.name\n            value1 = getattr(self, f_name)\n            value2 = getattr(other, f_name)\n            if value1 is None and value2 is None:\n                continue\n            else:\n                setattr(result, f_name, value1 - value2)\n\n        return result\n\n    def zero_all_fields(self) -&gt; None:\n        \"\"\"Set all internal measurement values to zero.\"\"\"\n        for field in fields(self):\n            f_name = field.name\n            f_value = getattr(self, f_name)\n            if isinstance(f_value, float):\n                setattr(self, f_name, 0.0)\n            else:\n                setattr(self, f_name, None)\n</code></pre>"},{"location":"reference/device/soc/jetson/#zeus.device.soc.jetson.JetsonMeasurement.__sub__","title":"__sub__","text":"<pre><code>__sub__(other)\n</code></pre> <p>Produce a single measurement object containing differences across all fields.</p> Source code in <code>zeus/device/soc/jetson.py</code> <pre><code>def __sub__(self, other: JetsonMeasurement) -&gt; JetsonMeasurement:\n    \"\"\"Produce a single measurement object containing differences across all fields.\"\"\"\n    if not isinstance(other, type(self)):\n        raise TypeError(\"Subtraction is only supported between Jetson instances.\")\n\n    result = self.__class__()\n\n    for field in fields(self):\n        f_name = field.name\n        value1 = getattr(self, f_name)\n        value2 = getattr(other, f_name)\n        if value1 is None and value2 is None:\n            continue\n        else:\n            setattr(result, f_name, value1 - value2)\n\n    return result\n</code></pre>"},{"location":"reference/device/soc/jetson/#zeus.device.soc.jetson.JetsonMeasurement.zero_all_fields","title":"zero_all_fields","text":"<pre><code>zero_all_fields()\n</code></pre> <p>Set all internal measurement values to zero.</p> Source code in <code>zeus/device/soc/jetson.py</code> <pre><code>def zero_all_fields(self) -&gt; None:\n    \"\"\"Set all internal measurement values to zero.\"\"\"\n    for field in fields(self):\n        f_name = field.name\n        f_value = getattr(self, f_name)\n        if isinstance(f_value, float):\n            setattr(self, f_name, 0.0)\n        else:\n            setattr(self, f_name, None)\n</code></pre>"},{"location":"reference/device/soc/jetson/#zeus.device.soc.jetson.DeviceMap","title":"DeviceMap","text":"<p>               Bases: <code>TypedDict</code></p> <p>Map of device names to their corresponding power measurement strategies.</p> Source code in <code>zeus/device/soc/jetson.py</code> <pre><code>class DeviceMap(TypedDict, total=False):\n    \"\"\"Map of device names to their corresponding power measurement strategies.\"\"\"\n\n    cpu_power_mw: PowerMeasurementStrategy\n    gpu_power_mw: PowerMeasurementStrategy\n    total_power_mw: PowerMeasurementStrategy\n</code></pre>"},{"location":"reference/device/soc/jetson/#zeus.device.soc.jetson.Jetson","title":"Jetson","text":"<p>               Bases: <code>SoC</code></p> <p>An interface for obtaining the energy metrics of a Jetson processor.</p> Source code in <code>zeus/device/soc/jetson.py</code> <pre><code>class Jetson(SoC):\n    \"\"\"An interface for obtaining the energy metrics of a Jetson processor.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize an instance of a Jetson energy monitor.\"\"\"\n        if not jetson_is_available():\n            raise ZeusJetsonInitError(\"No Jetson processor was detected on the current device.\")\n\n        super().__init__()\n\n        # Maps each power rail (cpu, gpu, and total) to a power measurement strategy\n        self.power_measurement = self._discover_available_metrics()\n        self.available_metrics: set[str] | None = None\n\n        # Spawn polling process\n        context = mp.get_context(\"spawn\")\n        self.command_queue = context.Queue()\n        self.result_queue = context.Queue()\n        self.process = context.Process(\n            target=_polling_process_async_wrapper,\n            args=(self.command_queue, self.result_queue, self.power_measurement),\n        )\n        self.process.start()\n        atexit.register(self._stop_process)\n\n    def _discover_available_metrics(self) -&gt; DeviceMap:\n        \"\"\"Return available power measurement metrics per rail from the INA3221 sensor on Jetson devices.\n\n        All official NVIDIA Jetson devices have at least 1 INA3221 power monitor that measures per-rail power usage via 3 channels.\n\n          - https://docs.nvidia.com/jetson/archives/l4t-archived/l4t-3276/index.html#page/Tegra%20Linux%20Driver%20Package%20Development%20Guide/clock_power_setup.html#\n          - https://docs.nvidia.com/jetson/archives/r35.6.1/DeveloperGuide/SD/PlatformPowerAndPerformance/JetsonXavierNxSeriesAndJetsonAgxXavierSeries.html#software-based-power-consumption-modeling\n          - https://docs.nvidia.com/jetson/archives/r36.4.3/DeveloperGuide/SD/PlatformPowerAndPerformance/JetsonOrinNanoSeriesJetsonOrinNxSeriesAndJetsonAgxOrinSeries.html#\n        \"\"\"\n        path = Path(\"/sys/bus/i2c/drivers/ina3221x\")\n\n        metric_paths: dict[str, dict[str, Path]] = {}\n        power_measurement: DeviceMap = {}\n\n        def extract_directories(path: Path, rail_name: str, rail_index: str, type: str) -&gt; None:\n            \"\"\"Extract file paths for power, voltage, and current measurements based on the rail naming type.\"\"\"\n            rail_name_lower = rail_name.lower()\n\n            if \"cpu\" in rail_name_lower:\n                rail_name_simplified = \"cpu_power_mw\"\n            elif \"gpu\" in rail_name_lower:\n                rail_name_simplified = \"gpu_power_mw\"\n            elif \"system\" in rail_name_lower or \"_in\" in rail_name_lower or \"total\" in rail_name_lower:\n                rail_name_simplified = \"total_power_mw\"\n            else:\n                return  # Skip unsupported rail types\n\n            if type == \"label\":\n                power_path = path / f\"power{rail_index}_input\"\n                volt_path = path / f\"in{rail_index}_input\"\n                curr_path = path / f\"curr{rail_index}_input\"\n            else:\n                power_path = path / f\"in_power{rail_index}_input\"\n                volt_path = path / f\"in_voltage{rail_index}_input\"\n                curr_path = path / f\"in_current{rail_index}_input\"\n\n            if check_file(power_path):\n                metric_paths[rail_name_simplified] = {\"power\": Path(power_path)}\n            elif check_file(volt_path) and check_file(curr_path):\n                metric_paths[rail_name_simplified] = {\n                    \"volt\": Path(volt_path),\n                    \"curr\": Path(curr_path),\n                }\n            # Else, skip the rail due to insufficient metrics for power\n\n        for device in path.glob(\"*\"):\n            for subdevice in device.glob(\"*\"):\n                # Get the files containing rail names.\n                label_files = subdevice.glob(\"in*_label\")\n                rail_files = subdevice.glob(\"rail_name_*\")\n                # For each rail name, get its respective power, voltage, current paths.\n                for label_file in label_files:\n                    rail_name = label_file.read_text().strip()\n                    rail_index = label_file.name.split(\"_\")[0].lstrip(\"in\")\n                    extract_directories(subdevice, rail_name, rail_index, \"label\")\n                for rail_file in rail_files:\n                    rail_name = rail_file.read_text().strip()\n                    rail_index = rail_file.name.split(\"rail_name_\", 1)[-1]\n                    extract_directories(subdevice, rail_name, rail_index, \"rail_name\")\n\n        # Instantiate PowerMeasurementStrategy objects based on available metrics\n        for rail, metrics in metric_paths.items():\n            if \"power\" in metrics:\n                power_measurement[rail] = DirectPower(metrics[\"power\"])  # ty: ignore[invalid-key]\n            elif \"volt\" in metrics and \"curr\" in metrics:\n                power_measurement[rail] = VoltageCurrentProduct(metrics[\"volt\"], metrics[\"curr\"])  # ty: ignore[invalid-key]\n            # Else, skip the rail due to insufficient metrics for power\n        return power_measurement\n\n    def get_available_metrics(self) -&gt; set[str]:\n        \"\"\"Return a set of all observable metrics on the Jetson device.\"\"\"\n        if self.available_metrics is None:\n            result: JetsonMeasurement = self.get_total_energy_consumption()\n            available_metrics = set()\n\n            metrics_dict = asdict(result)\n            for f_name, f_value in metrics_dict.items():\n                if f_value is not None:\n                    available_metrics.add(f_name)\n\n            self.available_metrics = available_metrics\n        return self.available_metrics\n\n    def _stop_process(self) -&gt; None:\n        \"\"\"Kill the polling process.\"\"\"\n        self.command_queue.put_nowait(Command.STOP)\n        self.process.join(timeout=1.0)\n        self.process.kill()\n\n    def get_total_energy_consumption(self, timeout: float = 15.0) -&gt; JetsonMeasurement:\n        \"\"\"Returns the total energy consumption of the Jetson device. This measurement is cumulative.\n\n        Units: mJ.\n        \"\"\"\n        self.command_queue.put(Command.READ)\n        return self.result_queue.get(timeout=timeout)\n</code></pre>"},{"location":"reference/device/soc/jetson/#zeus.device.soc.jetson.Jetson.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> Source code in <code>zeus/device/soc/jetson.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize an instance of a Jetson energy monitor.\"\"\"\n    if not jetson_is_available():\n        raise ZeusJetsonInitError(\"No Jetson processor was detected on the current device.\")\n\n    super().__init__()\n\n    # Maps each power rail (cpu, gpu, and total) to a power measurement strategy\n    self.power_measurement = self._discover_available_metrics()\n    self.available_metrics: set[str] | None = None\n\n    # Spawn polling process\n    context = mp.get_context(\"spawn\")\n    self.command_queue = context.Queue()\n    self.result_queue = context.Queue()\n    self.process = context.Process(\n        target=_polling_process_async_wrapper,\n        args=(self.command_queue, self.result_queue, self.power_measurement),\n    )\n    self.process.start()\n    atexit.register(self._stop_process)\n</code></pre>"},{"location":"reference/device/soc/jetson/#zeus.device.soc.jetson.Jetson._discover_available_metrics","title":"_discover_available_metrics","text":"<pre><code>_discover_available_metrics()\n</code></pre> <p>Return available power measurement metrics per rail from the INA3221 sensor on Jetson devices.</p> <p>All official NVIDIA Jetson devices have at least 1 INA3221 power monitor that measures per-rail power usage via 3 channels.</p> <ul> <li>https://docs.nvidia.com/jetson/archives/l4t-archived/l4t-3276/index.html#page/Tegra%20Linux%20Driver%20Package%20Development%20Guide/clock_power_setup.html#</li> <li>https://docs.nvidia.com/jetson/archives/r35.6.1/DeveloperGuide/SD/PlatformPowerAndPerformance/JetsonXavierNxSeriesAndJetsonAgxXavierSeries.html#software-based-power-consumption-modeling</li> <li>https://docs.nvidia.com/jetson/archives/r36.4.3/DeveloperGuide/SD/PlatformPowerAndPerformance/JetsonOrinNanoSeriesJetsonOrinNxSeriesAndJetsonAgxOrinSeries.html#</li> </ul> Source code in <code>zeus/device/soc/jetson.py</code> <pre><code>def _discover_available_metrics(self) -&gt; DeviceMap:\n    \"\"\"Return available power measurement metrics per rail from the INA3221 sensor on Jetson devices.\n\n    All official NVIDIA Jetson devices have at least 1 INA3221 power monitor that measures per-rail power usage via 3 channels.\n\n      - https://docs.nvidia.com/jetson/archives/l4t-archived/l4t-3276/index.html#page/Tegra%20Linux%20Driver%20Package%20Development%20Guide/clock_power_setup.html#\n      - https://docs.nvidia.com/jetson/archives/r35.6.1/DeveloperGuide/SD/PlatformPowerAndPerformance/JetsonXavierNxSeriesAndJetsonAgxXavierSeries.html#software-based-power-consumption-modeling\n      - https://docs.nvidia.com/jetson/archives/r36.4.3/DeveloperGuide/SD/PlatformPowerAndPerformance/JetsonOrinNanoSeriesJetsonOrinNxSeriesAndJetsonAgxOrinSeries.html#\n    \"\"\"\n    path = Path(\"/sys/bus/i2c/drivers/ina3221x\")\n\n    metric_paths: dict[str, dict[str, Path]] = {}\n    power_measurement: DeviceMap = {}\n\n    def extract_directories(path: Path, rail_name: str, rail_index: str, type: str) -&gt; None:\n        \"\"\"Extract file paths for power, voltage, and current measurements based on the rail naming type.\"\"\"\n        rail_name_lower = rail_name.lower()\n\n        if \"cpu\" in rail_name_lower:\n            rail_name_simplified = \"cpu_power_mw\"\n        elif \"gpu\" in rail_name_lower:\n            rail_name_simplified = \"gpu_power_mw\"\n        elif \"system\" in rail_name_lower or \"_in\" in rail_name_lower or \"total\" in rail_name_lower:\n            rail_name_simplified = \"total_power_mw\"\n        else:\n            return  # Skip unsupported rail types\n\n        if type == \"label\":\n            power_path = path / f\"power{rail_index}_input\"\n            volt_path = path / f\"in{rail_index}_input\"\n            curr_path = path / f\"curr{rail_index}_input\"\n        else:\n            power_path = path / f\"in_power{rail_index}_input\"\n            volt_path = path / f\"in_voltage{rail_index}_input\"\n            curr_path = path / f\"in_current{rail_index}_input\"\n\n        if check_file(power_path):\n            metric_paths[rail_name_simplified] = {\"power\": Path(power_path)}\n        elif check_file(volt_path) and check_file(curr_path):\n            metric_paths[rail_name_simplified] = {\n                \"volt\": Path(volt_path),\n                \"curr\": Path(curr_path),\n            }\n        # Else, skip the rail due to insufficient metrics for power\n\n    for device in path.glob(\"*\"):\n        for subdevice in device.glob(\"*\"):\n            # Get the files containing rail names.\n            label_files = subdevice.glob(\"in*_label\")\n            rail_files = subdevice.glob(\"rail_name_*\")\n            # For each rail name, get its respective power, voltage, current paths.\n            for label_file in label_files:\n                rail_name = label_file.read_text().strip()\n                rail_index = label_file.name.split(\"_\")[0].lstrip(\"in\")\n                extract_directories(subdevice, rail_name, rail_index, \"label\")\n            for rail_file in rail_files:\n                rail_name = rail_file.read_text().strip()\n                rail_index = rail_file.name.split(\"rail_name_\", 1)[-1]\n                extract_directories(subdevice, rail_name, rail_index, \"rail_name\")\n\n    # Instantiate PowerMeasurementStrategy objects based on available metrics\n    for rail, metrics in metric_paths.items():\n        if \"power\" in metrics:\n            power_measurement[rail] = DirectPower(metrics[\"power\"])  # ty: ignore[invalid-key]\n        elif \"volt\" in metrics and \"curr\" in metrics:\n            power_measurement[rail] = VoltageCurrentProduct(metrics[\"volt\"], metrics[\"curr\"])  # ty: ignore[invalid-key]\n        # Else, skip the rail due to insufficient metrics for power\n    return power_measurement\n</code></pre>"},{"location":"reference/device/soc/jetson/#zeus.device.soc.jetson.Jetson.get_available_metrics","title":"get_available_metrics","text":"<pre><code>get_available_metrics()\n</code></pre> <p>Return a set of all observable metrics on the Jetson device.</p> Source code in <code>zeus/device/soc/jetson.py</code> <pre><code>def get_available_metrics(self) -&gt; set[str]:\n    \"\"\"Return a set of all observable metrics on the Jetson device.\"\"\"\n    if self.available_metrics is None:\n        result: JetsonMeasurement = self.get_total_energy_consumption()\n        available_metrics = set()\n\n        metrics_dict = asdict(result)\n        for f_name, f_value in metrics_dict.items():\n            if f_value is not None:\n                available_metrics.add(f_name)\n\n        self.available_metrics = available_metrics\n    return self.available_metrics\n</code></pre>"},{"location":"reference/device/soc/jetson/#zeus.device.soc.jetson.Jetson._stop_process","title":"_stop_process","text":"<pre><code>_stop_process()\n</code></pre> <p>Kill the polling process.</p> Source code in <code>zeus/device/soc/jetson.py</code> <pre><code>def _stop_process(self) -&gt; None:\n    \"\"\"Kill the polling process.\"\"\"\n    self.command_queue.put_nowait(Command.STOP)\n    self.process.join(timeout=1.0)\n    self.process.kill()\n</code></pre>"},{"location":"reference/device/soc/jetson/#zeus.device.soc.jetson.Jetson.get_total_energy_consumption","title":"get_total_energy_consumption","text":"<pre><code>get_total_energy_consumption(timeout=15.0)\n</code></pre> <p>Returns the total energy consumption of the Jetson device. This measurement is cumulative.</p> <p>Units: mJ.</p> Source code in <code>zeus/device/soc/jetson.py</code> <pre><code>def get_total_energy_consumption(self, timeout: float = 15.0) -&gt; JetsonMeasurement:\n    \"\"\"Returns the total energy consumption of the Jetson device. This measurement is cumulative.\n\n    Units: mJ.\n    \"\"\"\n    self.command_queue.put(Command.READ)\n    return self.result_queue.get(timeout=timeout)\n</code></pre>"},{"location":"reference/device/soc/jetson/#zeus.device.soc.jetson.Command","title":"Command","text":"<p>               Bases: <code>Enum</code></p> <p>Provide commands for the polling process.</p> Source code in <code>zeus/device/soc/jetson.py</code> <pre><code>class Command(enum.Enum):\n    \"\"\"Provide commands for the polling process.\"\"\"\n\n    READ = \"read\"\n    STOP = \"stop\"\n</code></pre>"},{"location":"reference/device/soc/jetson/#zeus.device.soc.jetson.check_file","title":"check_file","text":"<pre><code>check_file(path)\n</code></pre> <p>Check if the given path exists and is a file.</p> Source code in <code>zeus/device/soc/jetson.py</code> <pre><code>def check_file(path: Path) -&gt; bool:\n    \"\"\"Check if the given path exists and is a file.\"\"\"\n    return path.exists() and path.is_file()\n</code></pre>"},{"location":"reference/device/soc/jetson/#zeus.device.soc.jetson._polling_process_async_wrapper","title":"_polling_process_async_wrapper","text":"<pre><code>_polling_process_async_wrapper(command_queue, result_queue, power_measurement)\n</code></pre> <p>Function wrapper for the asynchronous energy polling process.</p> Source code in <code>zeus/device/soc/jetson.py</code> <pre><code>def _polling_process_async_wrapper(\n    command_queue: mp.Queue[Command],\n    result_queue: mp.Queue[JetsonMeasurement],\n    power_measurement: DeviceMap,\n) -&gt; None:\n    \"\"\"Function wrapper for the asynchronous energy polling process.\"\"\"\n    asyncio.run(\n        _polling_process_async(\n            command_queue,\n            result_queue,\n            power_measurement,\n        )\n    )\n</code></pre>"},{"location":"reference/device/soc/jetson/#zeus.device.soc.jetson._polling_process_async","title":"_polling_process_async  <code>async</code>","text":"<pre><code>_polling_process_async(command_queue, result_queue, power_measurement)\n</code></pre> <p>Continuously polls for accumulated energy measurements for CPU, GPU, and total power, listening for commands to stop or return the measurement.</p> Source code in <code>zeus/device/soc/jetson.py</code> <pre><code>async def _polling_process_async(\n    command_queue: mp.Queue[Command],\n    result_queue: mp.Queue[JetsonMeasurement],\n    power_measurement: DeviceMap,\n) -&gt; None:\n    \"\"\"Continuously polls for accumulated energy measurements for CPU, GPU, and total power, listening for commands to stop or return the measurement.\"\"\"\n    cumulative_measurement = JetsonMeasurement(\n        cpu_energy_mj=0.0 if \"cpu_power_mw\" in power_measurement else None,\n        gpu_energy_mj=0.0 if \"gpu_power_mw\" in power_measurement else None,\n        total_energy_mj=0.0 if \"total_power_mw\" in power_measurement else None,\n    )\n\n    prev_ts = time.monotonic()\n\n    while True:\n        current_ts: float = time.monotonic()\n        dt: float = current_ts - prev_ts\n\n        if \"cpu_power_mw\" in power_measurement:\n            cpu_power_mw = power_measurement[\"cpu_power_mw\"].measure_power()\n            cpu_energy_mj = cpu_power_mw * dt\n            cumulative_measurement.cpu_energy_mj = (cumulative_measurement.cpu_energy_mj or 0.0) + cpu_energy_mj\n        if \"gpu_power_mw\" in power_measurement:\n            gpu_power_mw = power_measurement[\"gpu_power_mw\"].measure_power()\n            gpu_energy_mj = gpu_power_mw * dt\n            cumulative_measurement.gpu_energy_mj = (cumulative_measurement.gpu_energy_mj or 0.0) + gpu_energy_mj\n        if \"total_power_mw\" in power_measurement:\n            total_power_mw = power_measurement[\"total_power_mw\"].measure_power()\n            total_energy_mj = total_power_mw * dt\n            cumulative_measurement.total_energy_mj = (cumulative_measurement.total_energy_mj or 0.0) + total_energy_mj\n\n        prev_ts = current_ts\n\n        try:\n            command = await asyncio.to_thread(\n                command_queue.get,\n                timeout=0.1,\n            )\n        except Empty:\n            # Update energy and do nothing\n            continue\n\n        if command == Command.STOP:\n            break\n        if command == Command.READ:\n            # Update and return energy measurement\n            result_queue.put(cumulative_measurement)\n</code></pre>"},{"location":"reference/device/soc/jetson/#zeus.device.soc.jetson.jetson_is_available","title":"jetson_is_available","text":"<pre><code>jetson_is_available()\n</code></pre> <p>Return if the current processor is a Jetson device.</p> Source code in <code>zeus/device/soc/jetson.py</code> <pre><code>def jetson_is_available() -&gt; bool:\n    \"\"\"Return if the current processor is a Jetson device.\"\"\"\n    if sys.platform != \"linux\" or platform.processor() != \"aarch64\":\n        return False\n\n    return os.path.exists(\"/usr/lib/aarch64-linux-gnu/tegra\") or os.path.exists(\"/etc/nv_tegra_release\")\n</code></pre>"},{"location":"reference/monitor/","title":"monitor","text":""},{"location":"reference/monitor/#zeus.monitor","title":"zeus.monitor","text":"<p>Time, energy, and power monitors for Zeus.</p> <p>The main class of this module is <code>ZeusMonitor</code>.</p> <p>If users wish to monitor power consumption over time, the <code>power</code> module can come in handy.</p> <p>If users wish to monitor GPU temperature over time, the <code>temperature</code> module can come in handy.</p>"},{"location":"reference/monitor/carbon/","title":"carbon","text":""},{"location":"reference/monitor/carbon/#zeus.monitor.carbon","title":"zeus.monitor.carbon","text":"<p>Carbon intensity providers used for carbon-aware optimizers.</p>"},{"location":"reference/monitor/carbon/#zeus.monitor.carbon.ZeusCarbonIntensityHTTPError","title":"ZeusCarbonIntensityHTTPError","text":"<p>               Bases: <code>ZeusBaseError</code></p> <p>Exception when HTTP request to carbon intensity provider fails.</p> Source code in <code>zeus/monitor/carbon.py</code> <pre><code>class ZeusCarbonIntensityHTTPError(ZeusBaseError):\n    \"\"\"Exception when HTTP request to carbon intensity provider fails.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize HTTP request exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/monitor/carbon/#zeus.monitor.carbon.ZeusCarbonIntensityHTTPError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/monitor/carbon.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize HTTP request exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/monitor/carbon/#zeus.monitor.carbon.ZeusCarbonIntensityNotFoundError","title":"ZeusCarbonIntensityNotFoundError","text":"<p>               Bases: <code>ZeusBaseError</code></p> <p>Exception when carbon intensity measurement could not be retrieved.</p> Source code in <code>zeus/monitor/carbon.py</code> <pre><code>class ZeusCarbonIntensityNotFoundError(ZeusBaseError):\n    \"\"\"Exception when carbon intensity measurement could not be retrieved.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize carbon not found exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/monitor/carbon/#zeus.monitor.carbon.ZeusCarbonIntensityNotFoundError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/monitor/carbon.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize carbon not found exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/monitor/carbon/#zeus.monitor.carbon.CarbonIntensityProvider","title":"CarbonIntensityProvider","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class for implementing ways to fetch carbon intensity.</p> Source code in <code>zeus/monitor/carbon.py</code> <pre><code>class CarbonIntensityProvider(abc.ABC):\n    \"\"\"Abstract class for implementing ways to fetch carbon intensity.\"\"\"\n\n    @abc.abstractmethod\n    def get_current_carbon_intensity(self) -&gt; float:\n        \"\"\"Abstract method for fetching the current carbon intensity of the set location of the class.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def get_recent_carbon_intensity(self) -&gt; dict[datetime, float]:\n        \"\"\"Abstract method for fetching the current carbon intensity of the set location of the class.\"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def update_period(self) -&gt; timedelta:\n        \"\"\"Abstract method for how long each carbon intensity value in the history dict remains current.\"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def history_length(self) -&gt; int:\n        \"\"\"Abstract method for how many carbon intensity values are in the history dict.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/monitor/carbon/#zeus.monitor.carbon.CarbonIntensityProvider.update_period","title":"update_period  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>update_period\n</code></pre> <p>Abstract method for how long each carbon intensity value in the history dict remains current.</p>"},{"location":"reference/monitor/carbon/#zeus.monitor.carbon.CarbonIntensityProvider.history_length","title":"history_length  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>history_length\n</code></pre> <p>Abstract method for how many carbon intensity values are in the history dict.</p>"},{"location":"reference/monitor/carbon/#zeus.monitor.carbon.CarbonIntensityProvider.get_current_carbon_intensity","title":"get_current_carbon_intensity  <code>abstractmethod</code>","text":"<pre><code>get_current_carbon_intensity()\n</code></pre> <p>Abstract method for fetching the current carbon intensity of the set location of the class.</p> Source code in <code>zeus/monitor/carbon.py</code> <pre><code>@abc.abstractmethod\ndef get_current_carbon_intensity(self) -&gt; float:\n    \"\"\"Abstract method for fetching the current carbon intensity of the set location of the class.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/monitor/carbon/#zeus.monitor.carbon.CarbonIntensityProvider.get_recent_carbon_intensity","title":"get_recent_carbon_intensity  <code>abstractmethod</code>","text":"<pre><code>get_recent_carbon_intensity()\n</code></pre> <p>Abstract method for fetching the current carbon intensity of the set location of the class.</p> Source code in <code>zeus/monitor/carbon.py</code> <pre><code>@abc.abstractmethod\ndef get_recent_carbon_intensity(self) -&gt; dict[datetime, float]:\n    \"\"\"Abstract method for fetching the current carbon intensity of the set location of the class.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/monitor/carbon/#zeus.monitor.carbon.ElectrictyMapsClient","title":"ElectrictyMapsClient","text":"<p>               Bases: <code>CarbonIntensityProvider</code></p> <p>Carbon Intensity Provider with ElectricityMaps API.</p> <p>Reference:</p> <ol> <li>ElectricityMaps</li> <li>ElectricityMaps API</li> <li>ElectricityMaps GitHub</li> </ol> Source code in <code>zeus/monitor/carbon.py</code> <pre><code>class ElectrictyMapsClient(CarbonIntensityProvider):\n    \"\"\"Carbon Intensity Provider with ElectricityMaps API.\n\n    Reference:\n\n    1. [ElectricityMaps](https://www.electricitymaps.com/)\n    2. [ElectricityMaps API](https://static.electricitymaps.com/api/docs/index.html)\n    3. [ElectricityMaps GitHub](https://github.com/electricitymaps/electricitymaps-contrib)\n    \"\"\"\n\n    def __init__(\n        self,\n        location: tuple[float, float],\n        estimate: bool = False,\n        emission_factor_type: Literal[\"direct\", \"lifecycle\"] = \"direct\",\n    ) -&gt; None:\n        \"\"\"Iniitializes ElectricityMaps Carbon Provider.\n\n        Args:\n            location: tuple of latitude and longitude (latitude, longitude)\n            estimate: bool to toggle whether carbon intensity is estimated or not\n            emission_factor_type: emission factor to be measured (`direct` or `lifestyle`)\n        \"\"\"\n        self.lat, self.long = location\n        self.estimate = estimate\n        self.emission_factor_type = emission_factor_type\n\n    def get_current_carbon_intensity(self) -&gt; float:\n        \"\"\"Fetches current carbon intensity of the location of the class.\n\n        In some locations, there is no recent carbon intensity data. `self.estimate` can be used to approximate the carbon intensity in such cases.\n        \"\"\"\n        try:\n            url = (\n                f\"https://api.electricitymap.org/v3/carbon-intensity/latest?lat={self.lat}&amp;lon={self.long}\"\n                + f\"&amp;disableEstimations={not self.estimate}&amp;emissionFactorType={self.emission_factor_type}\"\n            )\n            resp = requests.get(url)\n        except requests.exceptions.RequestException as e:\n            raise ZeusCarbonIntensityHTTPError(f\"Failed to retrieve current carbon intensity measurement: {e}\") from e\n\n        try:\n            return resp.json()[\"carbonIntensity\"]\n        except KeyError as e:\n            # Raise exception when carbonIntensity does not exist in response\n            raise ZeusCarbonIntensityNotFoundError(\n                f\"Current carbon intensity measurement not found at `({self.lat}, {self.long})` \"\n                f\"with estimate set to `{self.estimate}` and emission_factor_type set to `{self.emission_factor_type}`\\n\"\n                f\"JSON Response: {resp.text}\"\n            ) from e\n\n    def get_recent_carbon_intensity(self) -&gt; dict[datetime, float]:\n        \"\"\"Fetches recent (within last 24 hours) carbon intensity of the location of the class.\n\n        In some locations, there is no recent carbon intensity data. `self.estimate` can be used to approximate the carbon intensity in such cases.\n        \"\"\"\n        try:\n            url = (\n                f\"https://api.electricitymap.org/v3/carbon-intensity/history?lat={self.lat}&amp;lon={self.long}\"\n                + f\"&amp;disableEstimations={not self.estimate}&amp;emissionFactorType={self.emission_factor_type}\"\n            )\n            resp = requests.get(url)\n        except requests.exceptions.RequestException as e:\n            raise ZeusCarbonIntensityHTTPError(f\"Failed to retrieve recent carbon intensity measurement: {e}\") from e\n\n        try:\n            recent_carbon_intensities: dict[datetime, float] = {\n                parser.parse(measurement[\"datetime\"]): measurement[\"carbonIntensity\"]\n                for measurement in resp.json()[\"history\"]\n            }\n            return recent_carbon_intensities\n        except KeyError as e:\n            # Raise exception when carbonIntensity does not exist in response\n            raise ZeusCarbonIntensityNotFoundError(\n                f\"Recent carbon intensity measurement not found at `({self.lat}, {self.long})` \"\n                f\"with estimate set to `{self.estimate}` and emission_factor_type set to `{self.emission_factor_type}`\\n\"\n                f\"JSON Response: {resp.text}\"\n            ) from e\n\n    @property\n    def update_period(self) -&gt; timedelta:\n        \"\"\"Returns timedelta for how long each carbon intensity value in the history dict remains current.\"\"\"\n        return timedelta(hours=1)\n\n    @property\n    def history_length(self) -&gt; int:\n        \"\"\"Returns number of carbon intensity values in history dict.\"\"\"\n        return 24\n</code></pre>"},{"location":"reference/monitor/carbon/#zeus.monitor.carbon.ElectrictyMapsClient.update_period","title":"update_period  <code>property</code>","text":"<pre><code>update_period\n</code></pre> <p>Returns timedelta for how long each carbon intensity value in the history dict remains current.</p>"},{"location":"reference/monitor/carbon/#zeus.monitor.carbon.ElectrictyMapsClient.history_length","title":"history_length  <code>property</code>","text":"<pre><code>history_length\n</code></pre> <p>Returns number of carbon intensity values in history dict.</p>"},{"location":"reference/monitor/carbon/#zeus.monitor.carbon.ElectrictyMapsClient.__init__","title":"__init__","text":"<pre><code>__init__(location, estimate=False, emission_factor_type='direct')\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>tuple[float, float]</code> <p>tuple of latitude and longitude (latitude, longitude)</p> required <code>estimate</code> <code>bool</code> <p>bool to toggle whether carbon intensity is estimated or not</p> <code>False</code> <code>emission_factor_type</code> <code>Literal['direct', 'lifecycle']</code> <p>emission factor to be measured (<code>direct</code> or <code>lifestyle</code>)</p> <code>'direct'</code> Source code in <code>zeus/monitor/carbon.py</code> <pre><code>def __init__(\n    self,\n    location: tuple[float, float],\n    estimate: bool = False,\n    emission_factor_type: Literal[\"direct\", \"lifecycle\"] = \"direct\",\n) -&gt; None:\n    \"\"\"Iniitializes ElectricityMaps Carbon Provider.\n\n    Args:\n        location: tuple of latitude and longitude (latitude, longitude)\n        estimate: bool to toggle whether carbon intensity is estimated or not\n        emission_factor_type: emission factor to be measured (`direct` or `lifestyle`)\n    \"\"\"\n    self.lat, self.long = location\n    self.estimate = estimate\n    self.emission_factor_type = emission_factor_type\n</code></pre>"},{"location":"reference/monitor/carbon/#zeus.monitor.carbon.ElectrictyMapsClient.get_current_carbon_intensity","title":"get_current_carbon_intensity","text":"<pre><code>get_current_carbon_intensity()\n</code></pre> <p>Fetches current carbon intensity of the location of the class.</p> <p>In some locations, there is no recent carbon intensity data. <code>self.estimate</code> can be used to approximate the carbon intensity in such cases.</p> Source code in <code>zeus/monitor/carbon.py</code> <pre><code>def get_current_carbon_intensity(self) -&gt; float:\n    \"\"\"Fetches current carbon intensity of the location of the class.\n\n    In some locations, there is no recent carbon intensity data. `self.estimate` can be used to approximate the carbon intensity in such cases.\n    \"\"\"\n    try:\n        url = (\n            f\"https://api.electricitymap.org/v3/carbon-intensity/latest?lat={self.lat}&amp;lon={self.long}\"\n            + f\"&amp;disableEstimations={not self.estimate}&amp;emissionFactorType={self.emission_factor_type}\"\n        )\n        resp = requests.get(url)\n    except requests.exceptions.RequestException as e:\n        raise ZeusCarbonIntensityHTTPError(f\"Failed to retrieve current carbon intensity measurement: {e}\") from e\n\n    try:\n        return resp.json()[\"carbonIntensity\"]\n    except KeyError as e:\n        # Raise exception when carbonIntensity does not exist in response\n        raise ZeusCarbonIntensityNotFoundError(\n            f\"Current carbon intensity measurement not found at `({self.lat}, {self.long})` \"\n            f\"with estimate set to `{self.estimate}` and emission_factor_type set to `{self.emission_factor_type}`\\n\"\n            f\"JSON Response: {resp.text}\"\n        ) from e\n</code></pre>"},{"location":"reference/monitor/carbon/#zeus.monitor.carbon.ElectrictyMapsClient.get_recent_carbon_intensity","title":"get_recent_carbon_intensity","text":"<pre><code>get_recent_carbon_intensity()\n</code></pre> <p>Fetches recent (within last 24 hours) carbon intensity of the location of the class.</p> <p>In some locations, there is no recent carbon intensity data. <code>self.estimate</code> can be used to approximate the carbon intensity in such cases.</p> Source code in <code>zeus/monitor/carbon.py</code> <pre><code>def get_recent_carbon_intensity(self) -&gt; dict[datetime, float]:\n    \"\"\"Fetches recent (within last 24 hours) carbon intensity of the location of the class.\n\n    In some locations, there is no recent carbon intensity data. `self.estimate` can be used to approximate the carbon intensity in such cases.\n    \"\"\"\n    try:\n        url = (\n            f\"https://api.electricitymap.org/v3/carbon-intensity/history?lat={self.lat}&amp;lon={self.long}\"\n            + f\"&amp;disableEstimations={not self.estimate}&amp;emissionFactorType={self.emission_factor_type}\"\n        )\n        resp = requests.get(url)\n    except requests.exceptions.RequestException as e:\n        raise ZeusCarbonIntensityHTTPError(f\"Failed to retrieve recent carbon intensity measurement: {e}\") from e\n\n    try:\n        recent_carbon_intensities: dict[datetime, float] = {\n            parser.parse(measurement[\"datetime\"]): measurement[\"carbonIntensity\"]\n            for measurement in resp.json()[\"history\"]\n        }\n        return recent_carbon_intensities\n    except KeyError as e:\n        # Raise exception when carbonIntensity does not exist in response\n        raise ZeusCarbonIntensityNotFoundError(\n            f\"Recent carbon intensity measurement not found at `({self.lat}, {self.long})` \"\n            f\"with estimate set to `{self.estimate}` and emission_factor_type set to `{self.emission_factor_type}`\\n\"\n            f\"JSON Response: {resp.text}\"\n        ) from e\n</code></pre>"},{"location":"reference/monitor/carbon/#zeus.monitor.carbon.CarbonEmissionMeasurement","title":"CarbonEmissionMeasurement  <code>dataclass</code>","text":"<p>Measurement result of one window.</p> <p>Attributes:</p> Name Type Description <code>time</code> <code>float</code> <p>Time elapsed (in seconds) during the measurement window.</p> <code>gpu_energy</code> <code>dict[int, float]</code> <p>Maps GPU indices to the energy consumed (in Joules) during the measurement window. GPU indices are from the DL framework's perspective after applying <code>CUDA_VISIBLE_DEVICES</code>.</p> <code>gpu_carbon_emission</code> <code>dict[int, float]</code> <p>Maps GPU indices to the carbon emission produced (in gCO2eq) during the measurement window. GPU indices are from the DL framework's perspective after applying <code>CUDA_VISIBLE_DEVICES</code>.</p> <code>cpu_energy</code> <code>dict[int, float] | None</code> <p>Maps CPU indices to the energy consumed (in Joules) during the measurement window. Each CPU index refers to one powerzone exposed by RAPL (intel-rapl:d). This can be 'None' if CPU measurement is not available.</p> <code>cpu_carbon_emission</code> <code>dict[int, float] | None</code> <p>Maps CPU indices to the carbon emission produced (in gCO2eq) during the measurement window. Each CPU index refers to one powerzone exposed by RAPL (intel-rapl:d). This can be 'None' if CPU measurement is not available.</p> <code>dram_energy</code> <code>dict[int, float] | None</code> <p>Maps CPU indices to the energy consumed (in Joules) during the measurement window. Each CPU index refers to one powerzone exposed by RAPL (intel-rapl:d) and DRAM measurements are taken from sub-packages within each powerzone. This can be 'None' if CPU measurement is not available or DRAM measurement is not available.</p> <code>dram_carbon_emission</code> <code>dict[int, float] | None</code> <p>Maps CPU indices to the carbon emission produced (in gCO2eq) during the measurement window. Each CPU index refers to one powerzone exposed by RAPL (intel-rapl:d). This can be 'None' if CPU measurement is not available or DRAM measurement is not available.</p> Source code in <code>zeus/monitor/carbon.py</code> <pre><code>@dataclass\nclass CarbonEmissionMeasurement:\n    \"\"\"Measurement result of one window.\n\n    Attributes:\n        time: Time elapsed (in seconds) during the measurement window.\n        gpu_energy: Maps GPU indices to the energy consumed (in Joules) during the\n            measurement window. GPU indices are from the DL framework's perspective\n            after applying `CUDA_VISIBLE_DEVICES`.\n        gpu_carbon_emission: Maps GPU indices to the carbon emission produced (in gCO2eq) during the\n            measurement window. GPU indices are from the DL framework's perspective\n            after applying `CUDA_VISIBLE_DEVICES`.\n        cpu_energy: Maps CPU indices to the energy consumed (in Joules) during the measurement\n            window. Each CPU index refers to one powerzone exposed by RAPL (intel-rapl:d). This can\n            be 'None' if CPU measurement is not available.\n        cpu_carbon_emission: Maps CPU indices to the carbon emission produced (in gCO2eq) during the measurement\n            window. Each CPU index refers to one powerzone exposed by RAPL (intel-rapl:d). This can\n            be 'None' if CPU measurement is not available.\n        dram_energy: Maps CPU indices to the energy consumed (in Joules) during the measurement\n            window. Each CPU index refers to one powerzone exposed by RAPL (intel-rapl:d) and DRAM\n            measurements are taken from sub-packages within each powerzone. This can be 'None' if\n            CPU measurement is not available or DRAM measurement is not available.\n        dram_carbon_emission: Maps CPU indices to the carbon emission produced (in gCO2eq) during the measurement\n            window. Each CPU index refers to one powerzone exposed by RAPL (intel-rapl:d). This can be 'None' if\n            CPU measurement is not available or DRAM measurement is not available.\n    \"\"\"\n\n    time: float\n    gpu_energy: dict[int, float]\n    gpu_carbon_emission: dict[int, float]\n    cpu_energy: dict[int, float] | None = None\n    cpu_carbon_emission: dict[int, float] | None = None\n    dram_energy: dict[int, float] | None = None\n    dram_carbon_emission: dict[int, float] | None = None\n</code></pre>"},{"location":"reference/monitor/carbon/#zeus.monitor.carbon.Op","title":"Op","text":"<p>               Bases: <code>Enum</code></p> <p>Enum used to communicate between CarbonEmissionMonitor and _polling_process.</p> Source code in <code>zeus/monitor/carbon.py</code> <pre><code>class Op(Enum):\n    \"\"\"Enum used to communicate between CarbonEmissionMonitor and _polling_process.\"\"\"\n\n    BEGIN = 0\n    END = 1\n    NEXTITER = 2\n</code></pre>"},{"location":"reference/monitor/carbon/#zeus.monitor.carbon.CarbonEmissionMonitor","title":"CarbonEmissionMonitor","text":"<p>Measure the carbon emission, GPU energy, and time consumption of a block of code.</p> <p>Works for multi-GPU and heterogeneous GPU types. Aware of <code>CUDA_VISIBLE_DEVICES</code>. For instance, if <code>CUDA_VISIBLE_DEVICES=2,3</code>, GPU index <code>1</code> passed into <code>gpu_indices</code> will be interpreted as CUDA device <code>3</code>.</p> <p>You can mark the beginning and end of a measurement window, during which the carbon emission, GPU energy, and time consumed will be recorded. Multiple concurrent measurement windows are supported.</p> <p>Note</p> <p><code>carbon_intensity_provider</code> must have <code>estimate</code> turned on because during some hours, ElectricityMaps does not have carbon intensity values available and has to rely on estimation.</p> <p>Warning</p> <p>This monitor uses multiprocessing with the spawn start method to measure energy and carbon emissions in a background process. Spawned processes re-import your main module, so keep heavy setup under <code>if __name__ == \"__main__\":</code> or inside functions. See also the \"Safe importing of main module\" section in the Python documentation.</p> Source code in <code>zeus/monitor/carbon.py</code> <pre><code>class CarbonEmissionMonitor:\n    \"\"\"Measure the carbon emission, GPU energy, and time consumption of a block of code.\n\n    Works for multi-GPU and heterogeneous GPU types. Aware of `CUDA_VISIBLE_DEVICES`.\n    For instance, if `CUDA_VISIBLE_DEVICES=2,3`, GPU index `1` passed into `gpu_indices`\n    will be interpreted as CUDA device `3`.\n\n    You can mark the beginning and end of a measurement window, during which the carbon\n    emission, GPU energy, and time consumed will be recorded. Multiple concurrent\n    measurement windows are supported.\n\n    !!! Note\n        `carbon_intensity_provider` must have `estimate` turned on because during some hours,\n        ElectricityMaps does not have carbon intensity values available and has to rely on\n        estimation.\n\n    !!! Warning\n        This monitor uses multiprocessing with the spawn start method to measure energy and carbon emissions\n        in a background process. Spawned processes re-import your main module, so keep\n        heavy setup under `if __name__ == \"__main__\":` or inside functions.\n        See also the \"Safe importing of main module\" section in the [Python documentation](\n        https://docs.python.org/3/library/multiprocessing.html#the-spawn-and-forkserver-start-methods).\n    \"\"\"\n\n    def __init__(\n        self,\n        carbon_intensity_provider: CarbonIntensityProvider,\n        gpu_indices: list[int] | None = None,\n        cpu_indices: list[int] | None = None,\n        sync_execution_with: Literal[\"torch\", \"jax\", \"cupy\"] = \"torch\",\n    ) -&gt; None:\n        \"\"\"Initializes Carbon Emission Monitor.\n\n        Args:\n            carbon_intensity_provider: provider for which carbon intensity values will be fetched from\n            gpu_indices: Indices of all the CUDA devices to monitor. Time/Energy measurements\n                will begin and end at the same time for these GPUs (i.e., synchronized).\n                If None, all the GPUs available will be used. `CUDA_VISIBLE_DEVICES`\n                is respected if set, e.g., GPU index `1` passed into `gpu_indices` when\n                `CUDA_VISIBLE_DEVICES=2,3` will be interpreted as CUDA device `3`.\n                `CUDA_VISIBLE_DEVICES`s formatted with comma-separated indices are supported.\n            cpu_indices: Indices of the CPU packages to monitor. If None, all CPU packages will\n                be used.\n            sync_execution_with: Deep learning framework to use to synchronize CPU/GPU computations.\n                Defaults to `\"torch\"`, in which case `torch.cuda.synchronize` will be used.\n                See [`sync_execution`][zeus.utils.framework.sync_execution] for more details.\n        \"\"\"\n        # Warn if instantiated as a global variable in a subprocess.\n        warn_if_global_in_subprocess(self)\n\n        self.zeus_monitor = ZeusMonitor(\n            gpu_indices=gpu_indices,\n            cpu_indices=cpu_indices,\n            sync_execution_with=sync_execution_with,\n        )\n        self.carbon_intensity_provider = carbon_intensity_provider\n        self.current_keys = set()\n\n        # set up process and shared queues\n        self.context = mp.get_context(\"spawn\")\n        self.command_q = self.context.Queue()\n        self.finished_q = self.context.Queue()\n\n    def begin_window(self, key: str, sync_execution: bool = True) -&gt; None:\n        \"\"\"Begin a new measurement window.\n\n        Args:\n            key: Unique name of the measurement window.\n            sync_execution: Whether to wait for asynchronously dispatched computations\n                to finish before starting the measurement window. For instance, PyTorch\n                and JAX will run GPU computations asynchronously, and waiting them to\n                finish is necessary to ensure that the measurement window captures all\n                and only the computations dispatched within the window.\n        \"\"\"\n        # check if key is already used\n        if key in self.current_keys:\n            raise ValueError(f\"Measurement window '{key}' already exists\")\n        self.current_keys.add(key)\n\n        # start window\n        self.zeus_monitor.begin_window(key, sync_execution=sync_execution)\n\n        # if there were previously no active windows, start polling process\n        if len(self.current_keys) == 1:\n            self.polling_process = self.context.Process(\n                target=_polling_process,\n                args=(\n                    self.command_q,\n                    self.finished_q,\n                    self.zeus_monitor.gpu_indices,\n                    self.zeus_monitor.cpu_indices,\n                    self.carbon_intensity_provider,\n                ),\n            )\n            self.polling_process.start()\n\n        # start subwindows\n        self.command_q.put((Op.BEGIN, key))\n\n    def end_window(self, key: str, sync_execution: bool = True) -&gt; CarbonEmissionMeasurement:\n        \"\"\"End a measurement window and return the time, energy consumption, and carbon emission.\n\n        Args:\n            key: Name of an active measurement window.\n            sync_execution: Whether to wait for asynchronously dispatched computations\n                to finish before starting the measurement window. For instance, PyTorch\n                and JAX will run GPU computations asynchronously, and waiting them to\n                finish is necessary to ensure that the measurement window captures all\n                and only the computations dispatched within the window.\n        \"\"\"\n        # check if begin_window has been called with key before\n        if key not in self.current_keys:\n            raise ValueError(f\"Measurement window '{key}' does not exist\")\n\n        # end window\n        self.command_q.put((Op.END, key))\n        (\n            gpu_carbon_emissions,\n            cpu_carbon_emissions,\n            dram_carbon_emissions,\n        ) = self.finished_q.get()\n        self.current_keys.remove(key)\n\n        overall_measurement = self.zeus_monitor.end_window(key, sync_execution=sync_execution)\n\n        measurement = CarbonEmissionMeasurement(\n            time=overall_measurement.time,\n            gpu_energy=overall_measurement.gpu_energy,\n            cpu_energy=overall_measurement.cpu_energy,\n            dram_energy=overall_measurement.dram_energy,\n            gpu_carbon_emission=gpu_carbon_emissions,\n            cpu_carbon_emission=cpu_carbon_emissions or None,\n            dram_carbon_emission=dram_carbon_emissions or None,\n        )\n\n        return measurement\n</code></pre>"},{"location":"reference/monitor/carbon/#zeus.monitor.carbon.CarbonEmissionMonitor.__init__","title":"__init__","text":"<pre><code>__init__(carbon_intensity_provider, gpu_indices=None, cpu_indices=None, sync_execution_with='torch')\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>carbon_intensity_provider</code> <code>CarbonIntensityProvider</code> <p>provider for which carbon intensity values will be fetched from</p> required <code>gpu_indices</code> <code>list[int] | None</code> <p>Indices of all the CUDA devices to monitor. Time/Energy measurements will begin and end at the same time for these GPUs (i.e., synchronized). If None, all the GPUs available will be used. <code>CUDA_VISIBLE_DEVICES</code> is respected if set, e.g., GPU index <code>1</code> passed into <code>gpu_indices</code> when <code>CUDA_VISIBLE_DEVICES=2,3</code> will be interpreted as CUDA device <code>3</code>. <code>CUDA_VISIBLE_DEVICES</code>s formatted with comma-separated indices are supported.</p> <code>None</code> <code>cpu_indices</code> <code>list[int] | None</code> <p>Indices of the CPU packages to monitor. If None, all CPU packages will be used.</p> <code>None</code> <code>sync_execution_with</code> <code>Literal['torch', 'jax', 'cupy']</code> <p>Deep learning framework to use to synchronize CPU/GPU computations. Defaults to <code>\"torch\"</code>, in which case <code>torch.cuda.synchronize</code> will be used. See <code>sync_execution</code> for more details.</p> <code>'torch'</code> Source code in <code>zeus/monitor/carbon.py</code> <pre><code>def __init__(\n    self,\n    carbon_intensity_provider: CarbonIntensityProvider,\n    gpu_indices: list[int] | None = None,\n    cpu_indices: list[int] | None = None,\n    sync_execution_with: Literal[\"torch\", \"jax\", \"cupy\"] = \"torch\",\n) -&gt; None:\n    \"\"\"Initializes Carbon Emission Monitor.\n\n    Args:\n        carbon_intensity_provider: provider for which carbon intensity values will be fetched from\n        gpu_indices: Indices of all the CUDA devices to monitor. Time/Energy measurements\n            will begin and end at the same time for these GPUs (i.e., synchronized).\n            If None, all the GPUs available will be used. `CUDA_VISIBLE_DEVICES`\n            is respected if set, e.g., GPU index `1` passed into `gpu_indices` when\n            `CUDA_VISIBLE_DEVICES=2,3` will be interpreted as CUDA device `3`.\n            `CUDA_VISIBLE_DEVICES`s formatted with comma-separated indices are supported.\n        cpu_indices: Indices of the CPU packages to monitor. If None, all CPU packages will\n            be used.\n        sync_execution_with: Deep learning framework to use to synchronize CPU/GPU computations.\n            Defaults to `\"torch\"`, in which case `torch.cuda.synchronize` will be used.\n            See [`sync_execution`][zeus.utils.framework.sync_execution] for more details.\n    \"\"\"\n    # Warn if instantiated as a global variable in a subprocess.\n    warn_if_global_in_subprocess(self)\n\n    self.zeus_monitor = ZeusMonitor(\n        gpu_indices=gpu_indices,\n        cpu_indices=cpu_indices,\n        sync_execution_with=sync_execution_with,\n    )\n    self.carbon_intensity_provider = carbon_intensity_provider\n    self.current_keys = set()\n\n    # set up process and shared queues\n    self.context = mp.get_context(\"spawn\")\n    self.command_q = self.context.Queue()\n    self.finished_q = self.context.Queue()\n</code></pre>"},{"location":"reference/monitor/carbon/#zeus.monitor.carbon.CarbonEmissionMonitor.begin_window","title":"begin_window","text":"<pre><code>begin_window(key, sync_execution=True)\n</code></pre> <p>Begin a new measurement window.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Unique name of the measurement window.</p> required <code>sync_execution</code> <code>bool</code> <p>Whether to wait for asynchronously dispatched computations to finish before starting the measurement window. For instance, PyTorch and JAX will run GPU computations asynchronously, and waiting them to finish is necessary to ensure that the measurement window captures all and only the computations dispatched within the window.</p> <code>True</code> Source code in <code>zeus/monitor/carbon.py</code> <pre><code>def begin_window(self, key: str, sync_execution: bool = True) -&gt; None:\n    \"\"\"Begin a new measurement window.\n\n    Args:\n        key: Unique name of the measurement window.\n        sync_execution: Whether to wait for asynchronously dispatched computations\n            to finish before starting the measurement window. For instance, PyTorch\n            and JAX will run GPU computations asynchronously, and waiting them to\n            finish is necessary to ensure that the measurement window captures all\n            and only the computations dispatched within the window.\n    \"\"\"\n    # check if key is already used\n    if key in self.current_keys:\n        raise ValueError(f\"Measurement window '{key}' already exists\")\n    self.current_keys.add(key)\n\n    # start window\n    self.zeus_monitor.begin_window(key, sync_execution=sync_execution)\n\n    # if there were previously no active windows, start polling process\n    if len(self.current_keys) == 1:\n        self.polling_process = self.context.Process(\n            target=_polling_process,\n            args=(\n                self.command_q,\n                self.finished_q,\n                self.zeus_monitor.gpu_indices,\n                self.zeus_monitor.cpu_indices,\n                self.carbon_intensity_provider,\n            ),\n        )\n        self.polling_process.start()\n\n    # start subwindows\n    self.command_q.put((Op.BEGIN, key))\n</code></pre>"},{"location":"reference/monitor/carbon/#zeus.monitor.carbon.CarbonEmissionMonitor.end_window","title":"end_window","text":"<pre><code>end_window(key, sync_execution=True)\n</code></pre> <p>End a measurement window and return the time, energy consumption, and carbon emission.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of an active measurement window.</p> required <code>sync_execution</code> <code>bool</code> <p>Whether to wait for asynchronously dispatched computations to finish before starting the measurement window. For instance, PyTorch and JAX will run GPU computations asynchronously, and waiting them to finish is necessary to ensure that the measurement window captures all and only the computations dispatched within the window.</p> <code>True</code> Source code in <code>zeus/monitor/carbon.py</code> <pre><code>def end_window(self, key: str, sync_execution: bool = True) -&gt; CarbonEmissionMeasurement:\n    \"\"\"End a measurement window and return the time, energy consumption, and carbon emission.\n\n    Args:\n        key: Name of an active measurement window.\n        sync_execution: Whether to wait for asynchronously dispatched computations\n            to finish before starting the measurement window. For instance, PyTorch\n            and JAX will run GPU computations asynchronously, and waiting them to\n            finish is necessary to ensure that the measurement window captures all\n            and only the computations dispatched within the window.\n    \"\"\"\n    # check if begin_window has been called with key before\n    if key not in self.current_keys:\n        raise ValueError(f\"Measurement window '{key}' does not exist\")\n\n    # end window\n    self.command_q.put((Op.END, key))\n    (\n        gpu_carbon_emissions,\n        cpu_carbon_emissions,\n        dram_carbon_emissions,\n    ) = self.finished_q.get()\n    self.current_keys.remove(key)\n\n    overall_measurement = self.zeus_monitor.end_window(key, sync_execution=sync_execution)\n\n    measurement = CarbonEmissionMeasurement(\n        time=overall_measurement.time,\n        gpu_energy=overall_measurement.gpu_energy,\n        cpu_energy=overall_measurement.cpu_energy,\n        dram_energy=overall_measurement.dram_energy,\n        gpu_carbon_emission=gpu_carbon_emissions,\n        cpu_carbon_emission=cpu_carbon_emissions or None,\n        dram_carbon_emission=dram_carbon_emissions or None,\n    )\n\n    return measurement\n</code></pre>"},{"location":"reference/monitor/energy/","title":"energy","text":""},{"location":"reference/monitor/energy/#zeus.monitor.energy","title":"zeus.monitor.energy","text":"<p>Measure the GPU time and energy consumption of a block of code.</p>"},{"location":"reference/monitor/energy/#zeus.monitor.energy.Measurement","title":"Measurement  <code>dataclass</code>","text":"<p>Measurement result of one window.</p> <p>Attributes:</p> Name Type Description <code>time</code> <code>float</code> <p>Time elapsed (in seconds) during the measurement window.</p> <code>gpu_energy</code> <code>dict[int, float]</code> <p>Maps GPU indices to the energy consumed (in Joules) during the measurement window. GPU indices are from the DL framework's perspective after applying <code>CUDA_VISIBLE_DEVICES</code>.</p> <code>cpu_energy</code> <code>dict[int, float] | None</code> <p>Maps CPU indices to the energy consumed (in Joules) during the measurement window. Each CPU index refers to one powerzone exposed by RAPL (intel-rapl:d). This can be 'None' if CPU measurement is not available.</p> <code>dram_energy</code> <code>dict[int, float] | None</code> <p>Maps CPU indices to the energy consumed (in Joules) during the measurement window. Each CPU index refers to one powerzone exposed by RAPL (intel-rapl:d)  and DRAM measurements are taken from sub-packages within each powerzone. This can be 'None' if CPU measurement is not available or DRAM measurement is not available.</p> <code>soc_energy</code> <code>SoCMeasurement | None</code> <p>If your machine contains an SoC (e.g., Apple silicon), metrics for various subsystems of the SoC (e.g., the on-chip CPU, the on-chip GPU) will all be included within this field.</p> Source code in <code>zeus/monitor/energy.py</code> <pre><code>@dataclass\nclass Measurement:\n    \"\"\"Measurement result of one window.\n\n    Attributes:\n        time: Time elapsed (in seconds) during the measurement window.\n        gpu_energy: Maps GPU indices to the energy consumed (in Joules) during the\n            measurement window. GPU indices are from the DL framework's perspective\n            after applying `CUDA_VISIBLE_DEVICES`.\n        cpu_energy: Maps CPU indices to the energy consumed (in Joules) during the measurement\n            window. Each CPU index refers to one powerzone exposed by RAPL (intel-rapl:d). This can\n            be 'None' if CPU measurement is not available.\n        dram_energy: Maps CPU indices to the energy consumed (in Joules) during the measurement\n            window. Each CPU index refers to one powerzone exposed by RAPL (intel-rapl:d)  and DRAM\n            measurements are taken from sub-packages within each powerzone. This can be 'None' if\n            CPU measurement is not available or DRAM measurement is not available.\n        soc_energy: If your machine contains an SoC (e.g., Apple silicon), metrics for various\n            subsystems of the SoC (e.g., the on-chip CPU, the on-chip GPU) will all be included\n            within this field.\n    \"\"\"\n\n    time: float\n    gpu_energy: dict[int, float]\n    cpu_energy: dict[int, float] | None = None\n    dram_energy: dict[int, float] | None = None\n\n    soc_energy: SoCMeasurement | None = None\n\n    @cached_property\n    def total_energy(self) -&gt; float:\n        \"\"\"Total energy consumed (in Joules) during the measurement window.\"\"\"\n        # TODO: Update method to total_gpu_energy, which may cause breaking changes in the examples/\n        return sum(self.gpu_energy.values())\n</code></pre>"},{"location":"reference/monitor/energy/#zeus.monitor.energy.Measurement.total_energy","title":"total_energy  <code>cached</code> <code>property</code>","text":"<pre><code>total_energy\n</code></pre> <p>Total energy consumed (in Joules) during the measurement window.</p>"},{"location":"reference/monitor/energy/#zeus.monitor.energy.MeasurementState","title":"MeasurementState  <code>dataclass</code>","text":"<p>Measurement state to keep track of measurements in start_window.</p> <p>Used in ZeusMonitor to map string keys of measurements to this dataclass.</p> <p>Attributes:</p> Name Type Description <code>time</code> <code>float</code> <p>The beginning timestamp of the measurement window.</p> <code>gpu_energy</code> <code>dict[int, float]</code> <p>Maps GPU indices to the energy consumed (in Joules) during the measurement window. GPU indices are from the DL framework's perspective after applying <code>CUDA_VISIBLE_DEVICES</code>.</p> <code>cpu_energy</code> <code>dict[int, float] | None</code> <p>Maps CPU indices to the energy consumed (in Joules) during the measurement window. Each CPU index refers to one powerzone exposed by RAPL (intel-rapl:d). This can be 'None' if CPU measurement is not available.</p> <code>dram_energy</code> <code>dict[int, float] | None</code> <p>Maps CPU indices to the energy consumed (in Joules) during the measurement window. Each CPU index refers to one powerzone exposed by RAPL (intel-rapl:d)  and DRAM measurements are taken from sub-packages within each powerzone. This can be 'None' if CPU measurement is not available or DRAM measurement is not available.</p> Source code in <code>zeus/monitor/energy.py</code> <pre><code>@dataclass\nclass MeasurementState:\n    \"\"\"Measurement state to keep track of measurements in start_window.\n\n    Used in ZeusMonitor to map string keys of measurements to this dataclass.\n\n    Attributes:\n        time: The beginning timestamp of the measurement window.\n        gpu_energy: Maps GPU indices to the energy consumed (in Joules) during the\n            measurement window. GPU indices are from the DL framework's perspective\n            after applying `CUDA_VISIBLE_DEVICES`.\n        cpu_energy: Maps CPU indices to the energy consumed (in Joules) during the measurement\n            window. Each CPU index refers to one powerzone exposed by RAPL (intel-rapl:d). This can\n            be 'None' if CPU measurement is not available.\n        dram_energy: Maps CPU indices to the energy consumed (in Joules) during the measurement\n            window. Each CPU index refers to one powerzone exposed by RAPL (intel-rapl:d)  and DRAM\n            measurements are taken from sub-packages within each powerzone. This can be 'None' if\n            CPU measurement is not available or DRAM measurement is not available.\n    \"\"\"\n\n    time: float\n    gpu_energy: dict[int, float]\n    cpu_energy: dict[int, float] | None = None\n    dram_energy: dict[int, float] | None = None\n\n    @cached_property\n    def total_energy(self) -&gt; float:\n        \"\"\"Total energy consumed (in Joules) during the measurement window.\"\"\"\n        return sum(self.gpu_energy.values())\n</code></pre>"},{"location":"reference/monitor/energy/#zeus.monitor.energy.MeasurementState.total_energy","title":"total_energy  <code>cached</code> <code>property</code>","text":"<pre><code>total_energy\n</code></pre> <p>Total energy consumed (in Joules) during the measurement window.</p>"},{"location":"reference/monitor/energy/#zeus.monitor.energy.ZeusMonitor","title":"ZeusMonitor","text":"<p>Measure the GPU energy and time consumption of a block of code.</p> <p>Works for multi-GPU and heterogeneous GPU types. Aware of <code>CUDA_VISIBLE_DEVICES</code>. For instance, if <code>CUDA_VISIBLE_DEVICES=2,3</code>, GPU index <code>1</code> passed into <code>gpu_indices</code> will be interpreted as CUDA device <code>3</code>.</p> <p>You can mark the beginning and end of a measurement window, during which the GPU energy and time consumed will be recorded. Multiple concurrent measurement windows are supported.</p> <p>For Volta or newer GPUs, energy consumption is measured very cheaply with the <code>nvmlDeviceGetTotalEnergyConsumption</code> API. On older architectures, this API is not supported, so a separate Python process is used to poll <code>nvmlDeviceGetPowerUsage</code> to get power samples over time, which are integrated to compute energy consumption.</p> <p>Warning</p> <p>This monitor may start helper processes for power polling on some hardware using the spawn start method. Spawned processes re-import your main module, so keep heavy initialization (for example, model loading) under <code>if __name__ == \"__main__\":</code> or inside functions. See also the \"Safe importing of main module\" section in the Python documentation.</p>"},{"location":"reference/monitor/energy/#zeus.monitor.energy.ZeusMonitor--integration-example","title":"Integration Example","text":"<pre><code>from zeus.monitor import ZeusMonitor\n\ndef training():\n    \"\"\"A dummy training function.\"\"\"\n    import time\n    time.sleep(5)\n\n# Make sure to protect the entry point of the program to avoid monitoring\n# subprocesses from re-executing the main module.\nif __name__ == \"__main__\":\n    # Time/Energy measurements for four GPUs will begin and end at the same time.\n    gpu_indices = [0, 1, 2, 3]\n    monitor = ZeusMonitor(gpu_indices)\n\n    # Mark the beginning of a measurement window. You can use any string\n    # as the window name, but make sure it's unique.\n    monitor.begin_window(\"entire_training\")\n\n    # Actual work\n    training()\n\n    # Mark the end of a measurement window and retrieve the measurment result.\n    result = monitor.end_window(\"entire_training\")\n\n    # Print the measurement result.\n    print(f\"Training consumed {result.total_energy} Joules.\")\n    for gpu_idx, gpu_energy in result.gpu_energy.items():\n        print(f\"GPU {gpu_idx} consumed {gpu_energy} Joules.\")\n</code></pre> <p>Attributes:</p> Name Type Description <code>gpu_indices</code> <code>`list[int]`</code> <p>Indices of all the CUDA devices to monitor, from the DL framework's perspective after applying <code>CUDA_VISIBLE_DEVICES</code>.</p> Source code in <code>zeus/monitor/energy.py</code> <pre><code>class ZeusMonitor:\n    \"\"\"Measure the GPU energy and time consumption of a block of code.\n\n    Works for multi-GPU and heterogeneous GPU types. Aware of `CUDA_VISIBLE_DEVICES`.\n    For instance, if `CUDA_VISIBLE_DEVICES=2,3`, GPU index `1` passed into `gpu_indices`\n    will be interpreted as CUDA device `3`.\n\n    You can mark the beginning and end of a measurement window, during which the GPU\n    energy and time consumed will be recorded. Multiple concurrent measurement windows\n    are supported.\n\n    For Volta or newer GPUs, energy consumption is measured very cheaply with the\n    `nvmlDeviceGetTotalEnergyConsumption` API. On older architectures, this API is\n    not supported, so a separate Python process is used to poll `nvmlDeviceGetPowerUsage`\n    to get power samples over time, which are integrated to compute energy consumption.\n\n    !!! Warning\n        This monitor may start helper processes for power polling on some hardware using\n        the spawn start method. Spawned processes re-import your main module, so keep heavy\n        initialization (for example, model loading) under `if __name__ == \"__main__\":` or\n        inside functions.\n        See also the \"Safe importing of main module\" section in the [Python documentation](\n        https://docs.python.org/3/library/multiprocessing.html#the-spawn-and-forkserver-start-methods).\n\n    ## Integration Example\n\n    ```python\n    from zeus.monitor import ZeusMonitor\n\n    def training():\n        \\\"\\\"\\\"A dummy training function.\\\"\\\"\\\"\n        import time\n        time.sleep(5)\n\n    # Make sure to protect the entry point of the program to avoid monitoring\n    # subprocesses from re-executing the main module.\n    if __name__ == \"__main__\":\n        # Time/Energy measurements for four GPUs will begin and end at the same time.\n        gpu_indices = [0, 1, 2, 3]\n        monitor = ZeusMonitor(gpu_indices)\n\n        # Mark the beginning of a measurement window. You can use any string\n        # as the window name, but make sure it's unique.\n        monitor.begin_window(\"entire_training\")\n\n        # Actual work\n        training()\n\n        # Mark the end of a measurement window and retrieve the measurment result.\n        result = monitor.end_window(\"entire_training\")\n\n        # Print the measurement result.\n        print(f\"Training consumed {result.total_energy} Joules.\")\n        for gpu_idx, gpu_energy in result.gpu_energy.items():\n            print(f\"GPU {gpu_idx} consumed {gpu_energy} Joules.\")\n    ```\n\n    Attributes:\n        gpu_indices (`list[int]`): Indices of all the CUDA devices to monitor, from the\n            DL framework's perspective after applying `CUDA_VISIBLE_DEVICES`.\n    \"\"\"\n\n    def __init__(\n        self,\n        gpu_indices: list[int] | None = None,\n        cpu_indices: list[int] | None = None,\n        approx_instant_energy: bool = False,\n        log_file: str | Path | None = None,\n        sync_execution_with: Literal[\"torch\", \"jax\", \"cupy\"] = \"torch\",\n    ) -&gt; None:\n        \"\"\"Instantiate the monitor.\n\n        Args:\n            gpu_indices: Indices of all the CUDA devices to monitor. Time/Energy measurements\n                will begin and end at the same time for these GPUs (i.e., synchronized).\n                If None, all the GPUs available will be used. `CUDA_VISIBLE_DEVICES`\n                is respected if set, e.g., GPU index `1` passed into `gpu_indices` when\n                `CUDA_VISIBLE_DEVICES=2,3` will be interpreted as CUDA device `3`.\n                `CUDA_VISIBLE_DEVICES`s formatted with comma-separated indices are supported.\n            cpu_indices: Indices of the CPU packages to monitor. If None, all CPU packages will\n                be used.\n            approx_instant_energy: When the execution time of a measurement window is\n                shorter than the NVML energy counter's update period, energy consumption may\n                be observed as zero. In this case, if `approx_instant_energy` is True, the\n                window's energy consumption will be approximated by multiplying the current\n                instantaneous power consumption with the window's execution time. This should\n                be a better estimate than zero, but it's still an approximation.\n            log_file: Path to the log CSV file. If `None`, logging will be disabled.\n            sync_execution_with: Deep learning framework to use to synchronize CPU/GPU computations.\n                Defaults to `\"torch\"`, in which case `torch.cuda.synchronize` will be used.\n                See [`sync_execution`][zeus.utils.framework.sync_execution] for more details.\n        \"\"\"\n        # Warn if instantiated as a global variable in a subprocess.\n        warn_if_global_in_subprocess(self)\n\n        # Save arguments.\n        self.approx_instant_energy = approx_instant_energy\n        self.sync_with: Literal[\"torch\", \"jax\", \"cupy\"] = sync_execution_with\n\n        # Get GPU instances.\n        try:\n            self.gpus = get_gpus()\n        except ZeusGPUInitError:\n            self.gpus = EmptyGPUs()\n\n        # Get CPU instance.\n        try:\n            self.cpus = get_cpus()\n        except ZeusCPUInitError:\n            self.cpus = EmptyCPUs()\n        except ZeusCPUNoPermissionError as err:\n            if cpu_indices:\n                raise RuntimeError(\n                    \"Root privilege is required to read RAPL metrics. See \"\n                    \"https://ml.energy/zeus/getting_started/#system-privileges \"\n                    \"for more information or disable CPU measurement by passing cpu_indices=[] to \"\n                    \"ZeusMonitor\"\n                ) from err\n            self.cpus = EmptyCPUs()\n\n        # Get an SoC instance, if an SoC is present on the host device.\n        self.soc_is_present = False\n        try:\n            self.soc = get_soc()\n            self.soc_is_present = True\n        except ZeusSoCInitError:\n            self.soc = EmptySoC()\n\n        # Resolve GPU indices. If the user did not specify `gpu_indices`, use all available GPUs.\n        self.gpu_indices = gpu_indices if gpu_indices is not None else list(range(len(self.gpus)))\n\n        # Resolve CPU indices. If the user did not specify `cpu_indices`, use all available CPUs.\n        self.cpu_indices = cpu_indices if cpu_indices is not None else list(range(len(self.cpus)))\n\n        logger.info(\"Monitoring GPU indices %s.\", self.gpu_indices)\n        logger.info(\"Monitoring CPU indices %s\", self.cpu_indices)\n\n        # Initialize loggers.\n        if log_file is None:\n            self.log_file = None\n        else:\n            if dir := os.path.dirname(log_file):\n                os.makedirs(dir, exist_ok=True)\n            self.log_file = open(log_file, \"w\")\n            logger.info(\"Writing measurement logs to %s.\", log_file)\n            self.log_file.write(\n                f\"start_time,window_name,elapsed_time,{','.join(map(lambda i: f'gpu{i}_energy', self.gpu_indices))}\\n\",\n            )\n            self.log_file.flush()\n\n        # A dictionary that maps the string keys of active measurement windows to\n        # the state of the measurement window. Each element in the dictionary is a Measurement State\n        # object with:\n        #     1) Time elapsed at the beginning of this window.\n        #     2) Total energy consumed by each &gt;= Volta GPU at the beginning of\n        #        this window (`None` for older GPUs).\n        #     3) Total energy consumed by each CPU powerzone at the beginning of this window.\n        #        ('None' if CPU measurement is not supported)\n        #     4) Total energy consumed by each DRAM in powerzones at the beginning of this window.\n        #        ('None' if DRAM measurement is not supported)\n        self.measurement_states: dict[str, MeasurementState] = {}\n\n        # Initialize power monitors for older architecture GPUs.\n        old_gpu_indices = [\n            gpu_index\n            for gpu_index in self.gpu_indices\n            if not self.gpus.supports_get_total_energy_consumption(gpu_index)\n        ]\n        if old_gpu_indices:\n            self.power_monitor = PowerMonitor(gpu_indices=old_gpu_indices, update_period=None)\n        else:\n            self.power_monitor = None\n\n    def _get_instant_power(self) -&gt; tuple[dict[int, float], float]:\n        \"\"\"Measure the power consumption of all GPUs at the current time.\"\"\"\n        power_measurement_start_time: float = time()\n        power = {i: self.gpus.get_instant_power_usage(i) / 1000.0 for i in self.gpu_indices}\n        power_measurement_time = time() - power_measurement_start_time\n        return power, power_measurement_time\n\n    def begin_window(self, key: str, sync_execution: bool = True) -&gt; None:\n        \"\"\"Begin a new measurement window.\n\n        Args:\n            key: Unique name of the measurement window.\n            sync_execution: Whether to wait for asynchronously dispatched computations\n                to finish before starting the measurement window. For instance, PyTorch\n                and JAX will run GPU computations asynchronously, and waiting them to\n                finish is necessary to ensure that the measurement window captures all\n                and only the computations dispatched within the window.\n        \"\"\"\n        # Make sure the key is unique.\n        if key in self.measurement_states:\n            raise ValueError(f\"Measurement window '{key}' already exists\")\n\n        # Synchronize execution (e.g., cudaSynchronize) to freeze at the right time.\n        if sync_execution and self.gpu_indices:\n            sync_execution_fn(self.gpu_indices, sync_with=self.sync_with)\n\n        # Freeze the start time of the profiling window.\n        timestamp: float = time()\n        gpu_energy_state: dict[int, float] = {}\n        for gpu_index in self.gpu_indices:\n            # Query energy directly if the GPU has newer architecture.\n            # Otherwise, the Zeus power monitor is running in the background to\n            # collect power consumption, so we just need to read the log file later.\n            if self.gpus.supports_get_total_energy_consumption(gpu_index):\n                gpu_energy_state[gpu_index] = self.gpus.get_total_energy_consumption(gpu_index) / 1000.0\n\n        cpu_energy_state: dict[int, float] = {}\n        dram_energy_state: dict[int, float] = {}\n        for cpu_index in self.cpu_indices:\n            cpu_measurement = self.cpus.get_total_energy_consumption(cpu_index) / 1000.0\n            cpu_energy_state[cpu_index] = cpu_measurement.cpu_mj\n            if cpu_measurement.dram_mj is not None:\n                dram_energy_state[cpu_index] = cpu_measurement.dram_mj\n\n        if self.soc_is_present:\n            self.soc.begin_window(key)\n\n        # Add measurement state to dictionary.\n        self.measurement_states[key] = MeasurementState(\n            time=timestamp,\n            gpu_energy=gpu_energy_state,\n            cpu_energy=cpu_energy_state or None,\n            dram_energy=dram_energy_state or None,\n        )\n        logger.debug(\"Measurement window '%s' started.\", key)\n\n    def end_window(self, key: str, sync_execution: bool = True, cancel: bool = False) -&gt; Measurement:\n        \"\"\"End a measurement window and return the time and energy consumption.\n\n        Args:\n            key: Name of an active measurement window.\n            sync_execution: Whether to wait for asynchronously dispatched computations\n                to finish before starting the measurement window. For instance, PyTorch\n                and JAX will run GPU computations asynchronously, and waiting them to\n                finish is necessary to ensure that the measurement window captures all\n                and only the computations dispatched within the window.\n            cancel: Whether to cancel the measurement window. If `True`, the measurement\n                window is assumed to be cancelled and discarded. Thus, an empty Measurement\n                object will be returned and the measurement window will not be recorded in\n                the log file either. `sync_execution` is still respected.\n        \"\"\"\n        # Retrieve the start time and energy consumption of this window.\n        try:\n            measurement_state = self.measurement_states.pop(key)\n        except KeyError:\n            raise ValueError(f\"Measurement window '{key}' does not exist\") from None\n\n        # If we're also tracking an SoC, end its window.\n        soc_energy_consumption: SoCMeasurement | None = None\n        if self.soc_is_present:\n            soc_energy_consumption = self.soc.end_window(key)\n\n        # Take instant power consumption measurements.\n        # This, in theory, is introducing extra NVMLs call in the critical path\n        # even if computation time is not so short. However, it is reasonable to\n        # expect that computation time would be short if the user explicitly\n        # turned on the `approx_instant_energy` option. Calling this function\n        # as early as possible will lead to more accurate energy approximation.\n        power, power_measurement_time = self._get_instant_power() if self.approx_instant_energy else ({}, 0.0)\n\n        # Synchronize execution (e.g., cudaSynchronize) to freeze at the right time.\n        if sync_execution and self.gpu_indices:\n            sync_execution_fn(self.gpu_indices, sync_with=self.sync_with)\n\n        # If the measurement window is cancelled, return an empty Measurement object.\n        if cancel:\n            logger.debug(\"Measurement window '%s' cancelled.\", key)\n\n            # If we had a non-None SoC measurement object to report, empty its fields.\n            if soc_energy_consumption is not None:\n                soc_energy_consumption.zero_all_fields()\n\n            return Measurement(\n                time=0.0,\n                gpu_energy={gpu: 0.0 for gpu in self.gpu_indices},\n                cpu_energy={cpu: 0.0 for cpu in self.cpu_indices},\n                soc_energy=soc_energy_consumption,\n            )\n\n        end_time: float = time()\n        start_time = measurement_state.time\n        gpu_start_energy = measurement_state.gpu_energy\n        cpu_start_energy = measurement_state.cpu_energy\n        dram_start_energy = measurement_state.dram_energy\n\n        time_consumption: float = end_time - start_time\n        gpu_energy_consumption: dict[int, float] = {}\n        for gpu_index in self.gpu_indices:\n            # Query energy directly if the GPU has newer architecture.\n            if self.gpus.supports_get_total_energy_consumption(gpu_index):\n                end_energy = self.gpus.get_total_energy_consumption(gpu_index) / 1000.0\n                gpu_energy_consumption[gpu_index] = end_energy - gpu_start_energy[gpu_index]\n\n        cpu_energy_consumption: dict[int, float] = {}\n        dram_energy_consumption: dict[int, float] = {}\n        for cpu_index in self.cpu_indices:\n            cpu_measurement = self.cpus.get_total_energy_consumption(cpu_index) / 1000.0\n            if cpu_start_energy is not None:\n                cpu_energy_consumption[cpu_index] = cpu_measurement.cpu_mj - cpu_start_energy[cpu_index]\n            if dram_start_energy is not None and cpu_measurement.dram_mj is not None:\n                dram_energy_consumption[cpu_index] = cpu_measurement.dram_mj - dram_start_energy[cpu_index]\n\n        # If there are older GPU architectures, the PowerMonitor will take care of those.\n        if self.power_monitor is not None:\n            energy = self.power_monitor.get_energy(start_time, end_time)\n            # Fallback to the instant power measurement if the PowerMonitor does not\n            # have the power samples.\n            if energy is None:\n                energy = {}\n                for gpu_index in self.power_monitor.gpu_indices:\n                    energy[gpu_index] = power[gpu_index] * (time_consumption - power_measurement_time)\n            gpu_energy_consumption |= energy\n\n        # Approximate energy consumption if the measurement window is too short.\n        if self.approx_instant_energy:\n            for gpu_index in self.gpu_indices:\n                if gpu_energy_consumption[gpu_index] == 0.0:\n                    gpu_energy_consumption[gpu_index] = power[gpu_index] * (time_consumption - power_measurement_time)\n\n        # Trigger a warning if energy consumption is zero and approx_instant_energy is not enabled.\n        if not self.approx_instant_energy and any(energy == 0.0 for energy in gpu_energy_consumption.values()):\n            warnings.warn(\n                \"The energy consumption of one or more GPUs was measured as zero. This means that the time duration of the measurement window was shorter than the GPU's energy counter update period. Consider turning on the `approx_instant_energy` option in `ZeusMonitor`, which approximates the energy consumption of a short time window as instant power draw x window duration.\",\n                stacklevel=1,\n            )\n\n        logger.debug(\"Measurement window '%s' ended.\", key)\n\n        # Add to log file.\n        if self.log_file is not None:\n            self.log_file.write(\n                f\"{start_time},{key},{time_consumption},\"\n                + \",\".join(str(gpu_energy_consumption[gpu]) for gpu in self.gpu_indices)\n                + \"\\n\"\n            )\n            self.log_file.flush()\n\n        return Measurement(\n            time=time_consumption,\n            gpu_energy=gpu_energy_consumption,\n            cpu_energy=cpu_energy_consumption or None,\n            dram_energy=dram_energy_consumption or None,\n            soc_energy=soc_energy_consumption,\n        )\n\n    def reset_windows(self) -&gt; None:\n        \"\"\"Reset the monitor by removing all active measurement windows.\n\n        Any ongoing measurements will be cancelled and their data will be lost.\n        \"\"\"\n        # Get list of active measurement windows\n        active_windows = list(self.measurement_states.keys())\n\n        # Cancel all active SoC windows first (if SoC is present)\n        if self.soc_is_present:\n            for window_key in active_windows:\n                try:\n                    # End the SoC window and zero its fields to cancel it\n                    soc_measurement = self.soc.end_window(window_key)\n                    if soc_measurement is not None:\n                        soc_measurement.zero_all_fields()\n                except Exception as e:\n                    logger.warning(\n                        \"Failed to cancel SoC window '%s' during reset: %s\",\n                        window_key,\n                        e,\n                    )\n\n        # Clear all measurement states\n        self.measurement_states.clear()\n\n        if active_windows:\n            logger.info(\"ZeusMonitor reset dropped active windows: %s\", active_windows)\n        else:\n            logger.info(\"ZeusMonitor reset with no active windows to drop.\")\n</code></pre>"},{"location":"reference/monitor/energy/#zeus.monitor.energy.ZeusMonitor.__init__","title":"__init__","text":"<pre><code>__init__(gpu_indices=None, cpu_indices=None, approx_instant_energy=False, log_file=None, sync_execution_with='torch')\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>gpu_indices</code> <code>list[int] | None</code> <p>Indices of all the CUDA devices to monitor. Time/Energy measurements will begin and end at the same time for these GPUs (i.e., synchronized). If None, all the GPUs available will be used. <code>CUDA_VISIBLE_DEVICES</code> is respected if set, e.g., GPU index <code>1</code> passed into <code>gpu_indices</code> when <code>CUDA_VISIBLE_DEVICES=2,3</code> will be interpreted as CUDA device <code>3</code>. <code>CUDA_VISIBLE_DEVICES</code>s formatted with comma-separated indices are supported.</p> <code>None</code> <code>cpu_indices</code> <code>list[int] | None</code> <p>Indices of the CPU packages to monitor. If None, all CPU packages will be used.</p> <code>None</code> <code>approx_instant_energy</code> <code>bool</code> <p>When the execution time of a measurement window is shorter than the NVML energy counter's update period, energy consumption may be observed as zero. In this case, if <code>approx_instant_energy</code> is True, the window's energy consumption will be approximated by multiplying the current instantaneous power consumption with the window's execution time. This should be a better estimate than zero, but it's still an approximation.</p> <code>False</code> <code>log_file</code> <code>str | Path | None</code> <p>Path to the log CSV file. If <code>None</code>, logging will be disabled.</p> <code>None</code> <code>sync_execution_with</code> <code>Literal['torch', 'jax', 'cupy']</code> <p>Deep learning framework to use to synchronize CPU/GPU computations. Defaults to <code>\"torch\"</code>, in which case <code>torch.cuda.synchronize</code> will be used. See <code>sync_execution</code> for more details.</p> <code>'torch'</code> Source code in <code>zeus/monitor/energy.py</code> <pre><code>def __init__(\n    self,\n    gpu_indices: list[int] | None = None,\n    cpu_indices: list[int] | None = None,\n    approx_instant_energy: bool = False,\n    log_file: str | Path | None = None,\n    sync_execution_with: Literal[\"torch\", \"jax\", \"cupy\"] = \"torch\",\n) -&gt; None:\n    \"\"\"Instantiate the monitor.\n\n    Args:\n        gpu_indices: Indices of all the CUDA devices to monitor. Time/Energy measurements\n            will begin and end at the same time for these GPUs (i.e., synchronized).\n            If None, all the GPUs available will be used. `CUDA_VISIBLE_DEVICES`\n            is respected if set, e.g., GPU index `1` passed into `gpu_indices` when\n            `CUDA_VISIBLE_DEVICES=2,3` will be interpreted as CUDA device `3`.\n            `CUDA_VISIBLE_DEVICES`s formatted with comma-separated indices are supported.\n        cpu_indices: Indices of the CPU packages to monitor. If None, all CPU packages will\n            be used.\n        approx_instant_energy: When the execution time of a measurement window is\n            shorter than the NVML energy counter's update period, energy consumption may\n            be observed as zero. In this case, if `approx_instant_energy` is True, the\n            window's energy consumption will be approximated by multiplying the current\n            instantaneous power consumption with the window's execution time. This should\n            be a better estimate than zero, but it's still an approximation.\n        log_file: Path to the log CSV file. If `None`, logging will be disabled.\n        sync_execution_with: Deep learning framework to use to synchronize CPU/GPU computations.\n            Defaults to `\"torch\"`, in which case `torch.cuda.synchronize` will be used.\n            See [`sync_execution`][zeus.utils.framework.sync_execution] for more details.\n    \"\"\"\n    # Warn if instantiated as a global variable in a subprocess.\n    warn_if_global_in_subprocess(self)\n\n    # Save arguments.\n    self.approx_instant_energy = approx_instant_energy\n    self.sync_with: Literal[\"torch\", \"jax\", \"cupy\"] = sync_execution_with\n\n    # Get GPU instances.\n    try:\n        self.gpus = get_gpus()\n    except ZeusGPUInitError:\n        self.gpus = EmptyGPUs()\n\n    # Get CPU instance.\n    try:\n        self.cpus = get_cpus()\n    except ZeusCPUInitError:\n        self.cpus = EmptyCPUs()\n    except ZeusCPUNoPermissionError as err:\n        if cpu_indices:\n            raise RuntimeError(\n                \"Root privilege is required to read RAPL metrics. See \"\n                \"https://ml.energy/zeus/getting_started/#system-privileges \"\n                \"for more information or disable CPU measurement by passing cpu_indices=[] to \"\n                \"ZeusMonitor\"\n            ) from err\n        self.cpus = EmptyCPUs()\n\n    # Get an SoC instance, if an SoC is present on the host device.\n    self.soc_is_present = False\n    try:\n        self.soc = get_soc()\n        self.soc_is_present = True\n    except ZeusSoCInitError:\n        self.soc = EmptySoC()\n\n    # Resolve GPU indices. If the user did not specify `gpu_indices`, use all available GPUs.\n    self.gpu_indices = gpu_indices if gpu_indices is not None else list(range(len(self.gpus)))\n\n    # Resolve CPU indices. If the user did not specify `cpu_indices`, use all available CPUs.\n    self.cpu_indices = cpu_indices if cpu_indices is not None else list(range(len(self.cpus)))\n\n    logger.info(\"Monitoring GPU indices %s.\", self.gpu_indices)\n    logger.info(\"Monitoring CPU indices %s\", self.cpu_indices)\n\n    # Initialize loggers.\n    if log_file is None:\n        self.log_file = None\n    else:\n        if dir := os.path.dirname(log_file):\n            os.makedirs(dir, exist_ok=True)\n        self.log_file = open(log_file, \"w\")\n        logger.info(\"Writing measurement logs to %s.\", log_file)\n        self.log_file.write(\n            f\"start_time,window_name,elapsed_time,{','.join(map(lambda i: f'gpu{i}_energy', self.gpu_indices))}\\n\",\n        )\n        self.log_file.flush()\n\n    # A dictionary that maps the string keys of active measurement windows to\n    # the state of the measurement window. Each element in the dictionary is a Measurement State\n    # object with:\n    #     1) Time elapsed at the beginning of this window.\n    #     2) Total energy consumed by each &gt;= Volta GPU at the beginning of\n    #        this window (`None` for older GPUs).\n    #     3) Total energy consumed by each CPU powerzone at the beginning of this window.\n    #        ('None' if CPU measurement is not supported)\n    #     4) Total energy consumed by each DRAM in powerzones at the beginning of this window.\n    #        ('None' if DRAM measurement is not supported)\n    self.measurement_states: dict[str, MeasurementState] = {}\n\n    # Initialize power monitors for older architecture GPUs.\n    old_gpu_indices = [\n        gpu_index\n        for gpu_index in self.gpu_indices\n        if not self.gpus.supports_get_total_energy_consumption(gpu_index)\n    ]\n    if old_gpu_indices:\n        self.power_monitor = PowerMonitor(gpu_indices=old_gpu_indices, update_period=None)\n    else:\n        self.power_monitor = None\n</code></pre>"},{"location":"reference/monitor/energy/#zeus.monitor.energy.ZeusMonitor._get_instant_power","title":"_get_instant_power","text":"<pre><code>_get_instant_power()\n</code></pre> <p>Measure the power consumption of all GPUs at the current time.</p> Source code in <code>zeus/monitor/energy.py</code> <pre><code>def _get_instant_power(self) -&gt; tuple[dict[int, float], float]:\n    \"\"\"Measure the power consumption of all GPUs at the current time.\"\"\"\n    power_measurement_start_time: float = time()\n    power = {i: self.gpus.get_instant_power_usage(i) / 1000.0 for i in self.gpu_indices}\n    power_measurement_time = time() - power_measurement_start_time\n    return power, power_measurement_time\n</code></pre>"},{"location":"reference/monitor/energy/#zeus.monitor.energy.ZeusMonitor.begin_window","title":"begin_window","text":"<pre><code>begin_window(key, sync_execution=True)\n</code></pre> <p>Begin a new measurement window.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Unique name of the measurement window.</p> required <code>sync_execution</code> <code>bool</code> <p>Whether to wait for asynchronously dispatched computations to finish before starting the measurement window. For instance, PyTorch and JAX will run GPU computations asynchronously, and waiting them to finish is necessary to ensure that the measurement window captures all and only the computations dispatched within the window.</p> <code>True</code> Source code in <code>zeus/monitor/energy.py</code> <pre><code>def begin_window(self, key: str, sync_execution: bool = True) -&gt; None:\n    \"\"\"Begin a new measurement window.\n\n    Args:\n        key: Unique name of the measurement window.\n        sync_execution: Whether to wait for asynchronously dispatched computations\n            to finish before starting the measurement window. For instance, PyTorch\n            and JAX will run GPU computations asynchronously, and waiting them to\n            finish is necessary to ensure that the measurement window captures all\n            and only the computations dispatched within the window.\n    \"\"\"\n    # Make sure the key is unique.\n    if key in self.measurement_states:\n        raise ValueError(f\"Measurement window '{key}' already exists\")\n\n    # Synchronize execution (e.g., cudaSynchronize) to freeze at the right time.\n    if sync_execution and self.gpu_indices:\n        sync_execution_fn(self.gpu_indices, sync_with=self.sync_with)\n\n    # Freeze the start time of the profiling window.\n    timestamp: float = time()\n    gpu_energy_state: dict[int, float] = {}\n    for gpu_index in self.gpu_indices:\n        # Query energy directly if the GPU has newer architecture.\n        # Otherwise, the Zeus power monitor is running in the background to\n        # collect power consumption, so we just need to read the log file later.\n        if self.gpus.supports_get_total_energy_consumption(gpu_index):\n            gpu_energy_state[gpu_index] = self.gpus.get_total_energy_consumption(gpu_index) / 1000.0\n\n    cpu_energy_state: dict[int, float] = {}\n    dram_energy_state: dict[int, float] = {}\n    for cpu_index in self.cpu_indices:\n        cpu_measurement = self.cpus.get_total_energy_consumption(cpu_index) / 1000.0\n        cpu_energy_state[cpu_index] = cpu_measurement.cpu_mj\n        if cpu_measurement.dram_mj is not None:\n            dram_energy_state[cpu_index] = cpu_measurement.dram_mj\n\n    if self.soc_is_present:\n        self.soc.begin_window(key)\n\n    # Add measurement state to dictionary.\n    self.measurement_states[key] = MeasurementState(\n        time=timestamp,\n        gpu_energy=gpu_energy_state,\n        cpu_energy=cpu_energy_state or None,\n        dram_energy=dram_energy_state or None,\n    )\n    logger.debug(\"Measurement window '%s' started.\", key)\n</code></pre>"},{"location":"reference/monitor/energy/#zeus.monitor.energy.ZeusMonitor.end_window","title":"end_window","text":"<pre><code>end_window(key, sync_execution=True, cancel=False)\n</code></pre> <p>End a measurement window and return the time and energy consumption.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of an active measurement window.</p> required <code>sync_execution</code> <code>bool</code> <p>Whether to wait for asynchronously dispatched computations to finish before starting the measurement window. For instance, PyTorch and JAX will run GPU computations asynchronously, and waiting them to finish is necessary to ensure that the measurement window captures all and only the computations dispatched within the window.</p> <code>True</code> <code>cancel</code> <code>bool</code> <p>Whether to cancel the measurement window. If <code>True</code>, the measurement window is assumed to be cancelled and discarded. Thus, an empty Measurement object will be returned and the measurement window will not be recorded in the log file either. <code>sync_execution</code> is still respected.</p> <code>False</code> Source code in <code>zeus/monitor/energy.py</code> <pre><code>def end_window(self, key: str, sync_execution: bool = True, cancel: bool = False) -&gt; Measurement:\n    \"\"\"End a measurement window and return the time and energy consumption.\n\n    Args:\n        key: Name of an active measurement window.\n        sync_execution: Whether to wait for asynchronously dispatched computations\n            to finish before starting the measurement window. For instance, PyTorch\n            and JAX will run GPU computations asynchronously, and waiting them to\n            finish is necessary to ensure that the measurement window captures all\n            and only the computations dispatched within the window.\n        cancel: Whether to cancel the measurement window. If `True`, the measurement\n            window is assumed to be cancelled and discarded. Thus, an empty Measurement\n            object will be returned and the measurement window will not be recorded in\n            the log file either. `sync_execution` is still respected.\n    \"\"\"\n    # Retrieve the start time and energy consumption of this window.\n    try:\n        measurement_state = self.measurement_states.pop(key)\n    except KeyError:\n        raise ValueError(f\"Measurement window '{key}' does not exist\") from None\n\n    # If we're also tracking an SoC, end its window.\n    soc_energy_consumption: SoCMeasurement | None = None\n    if self.soc_is_present:\n        soc_energy_consumption = self.soc.end_window(key)\n\n    # Take instant power consumption measurements.\n    # This, in theory, is introducing extra NVMLs call in the critical path\n    # even if computation time is not so short. However, it is reasonable to\n    # expect that computation time would be short if the user explicitly\n    # turned on the `approx_instant_energy` option. Calling this function\n    # as early as possible will lead to more accurate energy approximation.\n    power, power_measurement_time = self._get_instant_power() if self.approx_instant_energy else ({}, 0.0)\n\n    # Synchronize execution (e.g., cudaSynchronize) to freeze at the right time.\n    if sync_execution and self.gpu_indices:\n        sync_execution_fn(self.gpu_indices, sync_with=self.sync_with)\n\n    # If the measurement window is cancelled, return an empty Measurement object.\n    if cancel:\n        logger.debug(\"Measurement window '%s' cancelled.\", key)\n\n        # If we had a non-None SoC measurement object to report, empty its fields.\n        if soc_energy_consumption is not None:\n            soc_energy_consumption.zero_all_fields()\n\n        return Measurement(\n            time=0.0,\n            gpu_energy={gpu: 0.0 for gpu in self.gpu_indices},\n            cpu_energy={cpu: 0.0 for cpu in self.cpu_indices},\n            soc_energy=soc_energy_consumption,\n        )\n\n    end_time: float = time()\n    start_time = measurement_state.time\n    gpu_start_energy = measurement_state.gpu_energy\n    cpu_start_energy = measurement_state.cpu_energy\n    dram_start_energy = measurement_state.dram_energy\n\n    time_consumption: float = end_time - start_time\n    gpu_energy_consumption: dict[int, float] = {}\n    for gpu_index in self.gpu_indices:\n        # Query energy directly if the GPU has newer architecture.\n        if self.gpus.supports_get_total_energy_consumption(gpu_index):\n            end_energy = self.gpus.get_total_energy_consumption(gpu_index) / 1000.0\n            gpu_energy_consumption[gpu_index] = end_energy - gpu_start_energy[gpu_index]\n\n    cpu_energy_consumption: dict[int, float] = {}\n    dram_energy_consumption: dict[int, float] = {}\n    for cpu_index in self.cpu_indices:\n        cpu_measurement = self.cpus.get_total_energy_consumption(cpu_index) / 1000.0\n        if cpu_start_energy is not None:\n            cpu_energy_consumption[cpu_index] = cpu_measurement.cpu_mj - cpu_start_energy[cpu_index]\n        if dram_start_energy is not None and cpu_measurement.dram_mj is not None:\n            dram_energy_consumption[cpu_index] = cpu_measurement.dram_mj - dram_start_energy[cpu_index]\n\n    # If there are older GPU architectures, the PowerMonitor will take care of those.\n    if self.power_monitor is not None:\n        energy = self.power_monitor.get_energy(start_time, end_time)\n        # Fallback to the instant power measurement if the PowerMonitor does not\n        # have the power samples.\n        if energy is None:\n            energy = {}\n            for gpu_index in self.power_monitor.gpu_indices:\n                energy[gpu_index] = power[gpu_index] * (time_consumption - power_measurement_time)\n        gpu_energy_consumption |= energy\n\n    # Approximate energy consumption if the measurement window is too short.\n    if self.approx_instant_energy:\n        for gpu_index in self.gpu_indices:\n            if gpu_energy_consumption[gpu_index] == 0.0:\n                gpu_energy_consumption[gpu_index] = power[gpu_index] * (time_consumption - power_measurement_time)\n\n    # Trigger a warning if energy consumption is zero and approx_instant_energy is not enabled.\n    if not self.approx_instant_energy and any(energy == 0.0 for energy in gpu_energy_consumption.values()):\n        warnings.warn(\n            \"The energy consumption of one or more GPUs was measured as zero. This means that the time duration of the measurement window was shorter than the GPU's energy counter update period. Consider turning on the `approx_instant_energy` option in `ZeusMonitor`, which approximates the energy consumption of a short time window as instant power draw x window duration.\",\n            stacklevel=1,\n        )\n\n    logger.debug(\"Measurement window '%s' ended.\", key)\n\n    # Add to log file.\n    if self.log_file is not None:\n        self.log_file.write(\n            f\"{start_time},{key},{time_consumption},\"\n            + \",\".join(str(gpu_energy_consumption[gpu]) for gpu in self.gpu_indices)\n            + \"\\n\"\n        )\n        self.log_file.flush()\n\n    return Measurement(\n        time=time_consumption,\n        gpu_energy=gpu_energy_consumption,\n        cpu_energy=cpu_energy_consumption or None,\n        dram_energy=dram_energy_consumption or None,\n        soc_energy=soc_energy_consumption,\n    )\n</code></pre>"},{"location":"reference/monitor/energy/#zeus.monitor.energy.ZeusMonitor.reset_windows","title":"reset_windows","text":"<pre><code>reset_windows()\n</code></pre> <p>Reset the monitor by removing all active measurement windows.</p> <p>Any ongoing measurements will be cancelled and their data will be lost.</p> Source code in <code>zeus/monitor/energy.py</code> <pre><code>def reset_windows(self) -&gt; None:\n    \"\"\"Reset the monitor by removing all active measurement windows.\n\n    Any ongoing measurements will be cancelled and their data will be lost.\n    \"\"\"\n    # Get list of active measurement windows\n    active_windows = list(self.measurement_states.keys())\n\n    # Cancel all active SoC windows first (if SoC is present)\n    if self.soc_is_present:\n        for window_key in active_windows:\n            try:\n                # End the SoC window and zero its fields to cancel it\n                soc_measurement = self.soc.end_window(window_key)\n                if soc_measurement is not None:\n                    soc_measurement.zero_all_fields()\n            except Exception as e:\n                logger.warning(\n                    \"Failed to cancel SoC window '%s' during reset: %s\",\n                    window_key,\n                    e,\n                )\n\n    # Clear all measurement states\n    self.measurement_states.clear()\n\n    if active_windows:\n        logger.info(\"ZeusMonitor reset dropped active windows: %s\", active_windows)\n    else:\n        logger.info(\"ZeusMonitor reset with no active windows to drop.\")\n</code></pre>"},{"location":"reference/monitor/power/","title":"power","text":""},{"location":"reference/monitor/power/#zeus.monitor.power","title":"zeus.monitor.power","text":"<p>Monitor the power usage of GPUs.</p>"},{"location":"reference/monitor/power/#zeus.monitor.power.PowerDomain","title":"PowerDomain","text":"<p>               Bases: <code>Enum</code></p> <p>Power measurement domains with different update characteristics.</p> Source code in <code>zeus/monitor/power.py</code> <pre><code>class PowerDomain(Enum):\n    \"\"\"Power measurement domains with different update characteristics.\"\"\"\n\n    DEVICE_INSTANT = \"device_instant\"\n    DEVICE_AVERAGE = \"device_average\"\n    MEMORY_AVERAGE = \"memory_average\"\n</code></pre>"},{"location":"reference/monitor/power/#zeus.monitor.power.PowerSample","title":"PowerSample  <code>dataclass</code>","text":"<p>A single power measurement sample.</p> Source code in <code>zeus/monitor/power.py</code> <pre><code>@dataclass\nclass PowerSample:\n    \"\"\"A single power measurement sample.\"\"\"\n\n    timestamp: float\n    gpu_index: int\n    power_mw: float\n</code></pre>"},{"location":"reference/monitor/power/#zeus.monitor.power.PowerMonitor","title":"PowerMonitor","text":"<p>Enhanced PowerMonitor with multiple power domains and timeline export.</p> <p>This class provides:</p> <ol> <li>Multiple power domains: device instant, device average, and memory average</li> <li>Timeline export with independent deduplication per domain</li> <li>Separate processes for each power domain (2-3 processes depending on GPU support)</li> <li>Backward compatibility with existing PowerMonitor interface</li> </ol> <p>Note</p> <p>The current implementation only supports cases where all GPUs are homegeneous (i.e., the same model).</p> <p>Warning</p> <p>This monitor uses multiprocessing with the spawn start method to poll power in background processes. Spawned processes re-import your main module, so keep heavy initialization (for example, model loading) under <code>if __name__ == \"__main__\":</code> or inside functions. See also the \"Safe importing of main module\" section in the Python documentation.</p> Source code in <code>zeus/monitor/power.py</code> <pre><code>class PowerMonitor:\n    \"\"\"Enhanced PowerMonitor with multiple power domains and timeline export.\n\n    This class provides:\n\n    1. Multiple power domains: device instant, device average, and memory average\n    2. Timeline export with independent deduplication per domain\n    3. Separate processes for each power domain (2-3 processes depending on GPU support)\n    4. Backward compatibility with existing PowerMonitor interface\n\n    !!! Note\n        The current implementation only supports cases where all GPUs are homegeneous\n        (i.e., the same model).\n\n    !!! Warning\n        This monitor uses multiprocessing with the spawn start method to poll power in\n        background processes. Spawned processes re-import your main module, so keep heavy\n        initialization (for example, model loading) under `if __name__ == \"__main__\":` or\n        inside functions.\n        See also the \"Safe importing of main module\" section in the [Python documentation](\n        https://docs.python.org/3/library/multiprocessing.html#the-spawn-and-forkserver-start-methods).\n    \"\"\"\n\n    def __init__(\n        self,\n        gpu_indices: list[int] | None = None,\n        update_period: float | None = None,\n        max_samples_per_gpu: int | None = None,\n        power_domains: list[PowerDomain | Literal[\"device_instant\", \"device_average\", \"memory_average\"]] | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the enhanced power monitor.\n\n        Args:\n            gpu_indices: Indices of the GPUs to monitor. If None, monitor all GPUs.\n            update_period: Update period of the power monitor in seconds. If None,\n                infer the update period by max speed polling the power counter for\n                each GPU model.\n            max_samples_per_gpu: Maximum number of power samples to keep per GPU per domain\n                in memory. If None (default), unlimited samples are kept.\n            power_domains: Power domains to monitor. If None, monitor all supported domains.\n        \"\"\"\n        # Warn if instantiated as a global variable in a subprocess.\n        warn_if_global_in_subprocess(self)\n\n        if gpu_indices is not None and not gpu_indices:\n            raise ValueError(\"`gpu_indices` must be either `None` or non-empty\")\n\n        if power_domains is not None and not power_domains:\n            raise ValueError(\"`power_domains` must be either `None` or non-empty\")\n\n        # Get GPUs\n        gpus = get_gpus(ensure_homogeneous=True)\n\n        # Configure GPU indices\n        self.gpu_indices = gpu_indices if gpu_indices is not None else list(range(len(gpus)))\n        if not self.gpu_indices:\n            raise ValueError(\"At least one GPU index must be specified\")\n        logger.info(\"Monitoring power usage of GPUs %s\", self.gpu_indices)\n\n        # Infer update period from GPU instant power, if necessary\n        if update_period is None:\n            update_period = infer_counter_update_period(self.gpu_indices)\n        elif update_period &lt; 0.05:\n            logger.warning(\n                \"An update period of %g might be too fast, which may lead to unexpected \"\n                \"errors (e.g., NotSupported) and/or zero values being returned. \"\n                \"If you see these, consider increasing to &gt;= 0.05.\",\n                update_period,\n            )\n        self.update_period = update_period\n\n        # Inter-process communication - separate unbounded queue per domain\n        self.data_queues: dict[PowerDomain, mp.Queue] = {}\n        self.ready_events: dict[PowerDomain, EventClass] = {}\n        self.stop_events: dict[PowerDomain, EventClass] = {}\n        self.processes: dict[PowerDomain, SpawnProcess] = {}\n\n        # Determine which domains are supported\n        supported_domains = self._determine_supported_domains()\n        logger.info(\"Supported power domains: %s\", [d.value for d in supported_domains])\n\n        # Configure requested power domains\n        self.measurement_domains: list[PowerDomain] = []\n        if power_domains is None:\n            self.measurement_domains = supported_domains\n        else:\n            for requested_domain in power_domains:\n                domain = PowerDomain(requested_domain)\n                if domain not in supported_domains:\n                    raise ValueError(\n                        f\"Requested power domain {domain.value} is not supported by the current GPUs. \"\n                        f\"Supported domains are: {[d.value for d in supported_domains]}\",\n                    )\n                self.measurement_domains.append(domain)\n        self.measurement_domains = list(set(self.measurement_domains))\n\n        if PowerDomain.DEVICE_INSTANT not in self.measurement_domains:\n            logger.warning(\n                \"PowerDomain.DEVICE_INSTANT is not being monitored. The `get_power` method will not be available.\",\n            )\n\n        # Power samples are collected for each power domain and device index.\n        self.samples: dict[PowerDomain, dict[int, collections.deque[PowerSample]]] = {}\n        for domain in self.measurement_domains:\n            self.samples[domain] = {}\n            for gpu_idx in self.gpu_indices:\n                self.samples[domain][gpu_idx] = collections.deque(maxlen=max_samples_per_gpu)\n\n        # Spawn collector processes for each supported domain\n        ctx = mp.get_context(\"spawn\")\n        for domain in self.measurement_domains:\n            self.data_queues[domain] = ctx.Queue()\n            self.ready_events[domain] = ctx.Event()\n            self.stop_events[domain] = ctx.Event()\n            self.processes[domain] = ctx.Process(\n                target=_domain_polling_process,\n                kwargs=dict(\n                    power_domain=domain,\n                    gpu_indices=self.gpu_indices,\n                    data_queue=self.data_queues[domain],\n                    ready_event=self.ready_events[domain],\n                    stop_event=self.stop_events[domain],\n                    update_period=update_period,\n                ),\n                daemon=True,\n                name=f\"zeus-power-monitor-{domain.value}\",\n            )\n        for process in self.processes.values():\n            process.start()\n\n        # Cleanup functions\n        self._finalizer = weakref.finalize(self, _cleanup_processes, self.stop_events, self.processes)\n\n        # Wait for all subprocesses to signal they're ready\n        logger.info(\"Waiting for all power monitoring subprocesses to be ready...\")\n        for domain in self.measurement_domains:\n            if not self.ready_events[domain].wait(timeout=10.0):\n                logger.warning(\n                    \"Power monitor subprocess for %s did not signal ready within 10 seconds\",\n                    domain.value,\n                )\n                raise RuntimeError(\n                    f\"Power monitor subprocess for {domain.value} failed to start within 10 seconds\",\n                )\n        logger.info(\"All power monitoring subprocesses are ready\")\n\n    def _determine_supported_domains(self) -&gt; list[PowerDomain]:\n        \"\"\"Determine which power domains are supported by the current GPUs.\"\"\"\n        supported = []\n        gpus = get_gpus(ensure_homogeneous=True)\n        methods = {\n            PowerDomain.DEVICE_INSTANT: gpus.get_instant_power_usage,\n            PowerDomain.DEVICE_AVERAGE: gpus.get_average_power_usage,\n            PowerDomain.MEMORY_AVERAGE: gpus.get_average_memory_power_usage,\n        }\n\n        # Just check the first GPU for support, since all GPUs are homogeneous.\n        for domain, method in methods.items():\n            try:\n                _ = method(0)\n                supported.append(domain)\n                logger.info(\"Power domain %s is supported\", domain.value)\n            except ZeusGPUNotSupportedError:\n                logger.info(\"Power domain %s is not supported\", domain.value)\n            except Exception as e:\n                logger.warning(\n                    \"Unexpected error while checking for %s support on GPU %d: %s\",\n                    domain.value,\n                    self.gpu_indices[0],\n                    e,\n                )\n\n        return supported\n\n    def stop(self) -&gt; None:\n        \"\"\"Stop all monitoring processes.\"\"\"\n        if self._finalizer.alive:\n            self._finalizer()\n\n    def _process_queue_data(self, domain: PowerDomain) -&gt; None:\n        \"\"\"Process all pending samples from a specific domain's queue.\"\"\"\n        if domain not in self.data_queues:\n            return\n\n        while True:\n            try:\n                sample = self.data_queues[domain].get_nowait()\n                if sample == \"STOP\":\n                    break\n                assert isinstance(sample, PowerSample)\n                self.samples[domain][sample.gpu_index].append(sample)\n            except Empty:\n                break\n\n    def _process_all_queue_data(self) -&gt; None:\n        \"\"\"Process all pending samples from all domain queues.\"\"\"\n        for domain in self.measurement_domains:\n            self._process_queue_data(domain)\n\n    def get_power_timeline(\n        self,\n        power_domain: PowerDomain | Literal[\"device_instant\", \"device_average\", \"memory_average\"],\n        gpu_index: int | None = None,\n        start_time: float | None = None,\n        end_time: float | None = None,\n    ) -&gt; dict[int, list[tuple[float, float]]]:\n        \"\"\"Get power timeline for specific power domain and GPU(s).\n\n        Args:\n            power_domain: Power domain to query\n            gpu_index: Specific GPU index, or None for all GPUs\n            start_time: Start time filter (unix timestamp from time.time() or similar)\n            end_time: End time filter (unix timestamp from time.time() or similar)\n\n        Returns:\n            Dictionary mapping GPU indices to timeline data with deduplication.\n            Timeline data is list of (timestamp, power_watts) tuples.\n        \"\"\"\n        if isinstance(power_domain, str):\n            power_domain = PowerDomain(power_domain)\n\n        if power_domain not in self.measurement_domains:\n            raise ValueError(\n                f\"Power domain {power_domain.value} is not being monitored. \"\n                f\"Monitored domains: {[d.value for d in self.measurement_domains]}\",\n            )\n\n        # Process any pending queue data for this domain\n        self._process_queue_data(power_domain)\n\n        # Determine which GPUs to query\n        target_gpus = [gpu_index] if gpu_index is not None else self.gpu_indices\n\n        result = {}\n        for gpu_idx in target_gpus:\n            if gpu_idx not in self.samples[power_domain]:\n                continue\n\n            # Extract timeline from samples\n            timeline = []\n            for sample in self.samples[power_domain][gpu_idx]:\n                # Apply time filters\n                if start_time is not None and sample.timestamp &lt; start_time:\n                    continue\n                if end_time is not None and sample.timestamp &gt; end_time:\n                    continue\n\n                timeline.append((sample.timestamp, sample.power_mw / 1000.0))  # Convert to watts\n\n            # Sort by timestamp\n            timeline.sort(key=lambda x: x[0])\n            result[gpu_idx] = timeline\n\n        return result\n\n    def get_all_power_timelines(\n        self,\n        gpu_index: int | None = None,\n        start_time: float | None = None,\n        end_time: float | None = None,\n    ) -&gt; dict[str, dict[int, list[tuple[float, float]]]]:\n        \"\"\"Get all power timelines organized by power domain.\n\n        Args:\n            gpu_index: Specific GPU index, or None for all GPUs\n            start_time: Start time filter (unix timestamp from time.time() or similar)\n            end_time: End time filter (unix timestamp from time.time() or similar)\n\n        Returns:\n            Dictionary with power domain names as keys and each value is a dict\n            mapping GPU indices to timeline data.\n        \"\"\"\n        result = {}\n        for domain in self.measurement_domains:\n            result[domain.value] = self.get_power_timeline(domain, gpu_index, start_time, end_time)\n        return result\n\n    def get_energy(self, start_time: float, end_time: float) -&gt; dict[int, float] | None:\n        \"\"\"Get the energy used by the GPUs between two times (backward compatibility).\n\n        Uses device instant power for energy calculation.\n\n        Args:\n            start_time: Start time of the interval, from time.time().\n            end_time: End time of the interval, from time.time().\n\n        Returns:\n            A dictionary mapping GPU indices to the energy used by the GPU between the\n            two times. If there are no power readings, return None.\n        \"\"\"\n        if PowerDomain.DEVICE_INSTANT in self.measurement_domains:\n            domain = PowerDomain.DEVICE_INSTANT\n        elif PowerDomain.DEVICE_AVERAGE in self.measurement_domains:\n            domain = PowerDomain.DEVICE_AVERAGE\n        else:\n            raise ValueError(\n                \"Neither PowerDomain.DEVICE_INSTANT nor PowerDomain.DEVICE_AVERAGE is being monitored. \"\n                \"Cannot compute energy usage.\",\n            )\n\n        timelines = self.get_power_timeline(domain, start_time=start_time, end_time=end_time)\n\n        if not timelines:\n            return None\n\n        energy_result = {}\n        for gpu_idx, timeline in timelines.items():\n            if not timeline or len(timeline) &lt; 2:\n                energy_result[gpu_idx] = 0.0\n                continue\n\n            timestamps = [t[0] for t in timeline]\n            powers = [t[1] for t in timeline]\n\n            try:\n                energy_result[gpu_idx] = float(auc(timestamps, powers))\n            except ValueError:\n                energy_result[gpu_idx] = 0.0\n\n        return energy_result\n\n    def get_power(self, time: float | None = None) -&gt; dict[int, float] | None:\n        \"\"\"Get the instant power usage of the GPUs at a specific time point.\n\n        Uses device instant power for compatibility.\n\n        Args:\n            time: Time point to get the power usage at. If None, get the power usage\n                at the last recorded time point.\n\n        Returns:\n            A dictionary mapping GPU indices to the power usage of the GPU at the\n            specified time point. If there are no power readings, return None.\n        \"\"\"\n        if PowerDomain.DEVICE_INSTANT not in self.measurement_domains:\n            raise ValueError(\n                f\"PowerDomain.DEVICE_INSTANT is not being monitored. Currently monitored domains: {[d.value for d in self.measurement_domains]}\",\n            )\n\n        # Process any pending queue data\n        self._process_all_queue_data()\n\n        result = {}\n        for gpu_idx in self.gpu_indices:\n            samples = self.samples[PowerDomain.DEVICE_INSTANT][gpu_idx]\n            if not samples:\n                return None\n\n            if time is None:\n                # Get the most recent sample\n                latest_sample = samples[-1]\n                result[gpu_idx] = latest_sample.power_mw / 1000.0  # Convert to watts\n            else:\n                # Find the closest sample to the requested time using bisect\n                timestamps = [sample.timestamp for sample in samples]\n                pos = bisect.bisect_left(timestamps, time)\n\n                if pos == 0:\n                    closest_sample = samples[0]\n                elif pos == len(samples):\n                    closest_sample = samples[-1]\n                else:\n                    # Check the closest sample before and after the requested time\n                    before = samples[pos - 1]\n                    after = samples[pos]\n                    closest_sample = before if time - before.timestamp &lt;= after.timestamp - time else after\n                result[gpu_idx] = closest_sample.power_mw / 1000.0  # To Watts\n\n        return result\n</code></pre>"},{"location":"reference/monitor/power/#zeus.monitor.power.PowerMonitor.__init__","title":"__init__","text":"<pre><code>__init__(gpu_indices=None, update_period=None, max_samples_per_gpu=None, power_domains=None)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>gpu_indices</code> <code>list[int] | None</code> <p>Indices of the GPUs to monitor. If None, monitor all GPUs.</p> <code>None</code> <code>update_period</code> <code>float | None</code> <p>Update period of the power monitor in seconds. If None, infer the update period by max speed polling the power counter for each GPU model.</p> <code>None</code> <code>max_samples_per_gpu</code> <code>int | None</code> <p>Maximum number of power samples to keep per GPU per domain in memory. If None (default), unlimited samples are kept.</p> <code>None</code> <code>power_domains</code> <code>list[PowerDomain | Literal['device_instant', 'device_average', 'memory_average']] | None</code> <p>Power domains to monitor. If None, monitor all supported domains.</p> <code>None</code> Source code in <code>zeus/monitor/power.py</code> <pre><code>def __init__(\n    self,\n    gpu_indices: list[int] | None = None,\n    update_period: float | None = None,\n    max_samples_per_gpu: int | None = None,\n    power_domains: list[PowerDomain | Literal[\"device_instant\", \"device_average\", \"memory_average\"]] | None = None,\n) -&gt; None:\n    \"\"\"Initialize the enhanced power monitor.\n\n    Args:\n        gpu_indices: Indices of the GPUs to monitor. If None, monitor all GPUs.\n        update_period: Update period of the power monitor in seconds. If None,\n            infer the update period by max speed polling the power counter for\n            each GPU model.\n        max_samples_per_gpu: Maximum number of power samples to keep per GPU per domain\n            in memory. If None (default), unlimited samples are kept.\n        power_domains: Power domains to monitor. If None, monitor all supported domains.\n    \"\"\"\n    # Warn if instantiated as a global variable in a subprocess.\n    warn_if_global_in_subprocess(self)\n\n    if gpu_indices is not None and not gpu_indices:\n        raise ValueError(\"`gpu_indices` must be either `None` or non-empty\")\n\n    if power_domains is not None and not power_domains:\n        raise ValueError(\"`power_domains` must be either `None` or non-empty\")\n\n    # Get GPUs\n    gpus = get_gpus(ensure_homogeneous=True)\n\n    # Configure GPU indices\n    self.gpu_indices = gpu_indices if gpu_indices is not None else list(range(len(gpus)))\n    if not self.gpu_indices:\n        raise ValueError(\"At least one GPU index must be specified\")\n    logger.info(\"Monitoring power usage of GPUs %s\", self.gpu_indices)\n\n    # Infer update period from GPU instant power, if necessary\n    if update_period is None:\n        update_period = infer_counter_update_period(self.gpu_indices)\n    elif update_period &lt; 0.05:\n        logger.warning(\n            \"An update period of %g might be too fast, which may lead to unexpected \"\n            \"errors (e.g., NotSupported) and/or zero values being returned. \"\n            \"If you see these, consider increasing to &gt;= 0.05.\",\n            update_period,\n        )\n    self.update_period = update_period\n\n    # Inter-process communication - separate unbounded queue per domain\n    self.data_queues: dict[PowerDomain, mp.Queue] = {}\n    self.ready_events: dict[PowerDomain, EventClass] = {}\n    self.stop_events: dict[PowerDomain, EventClass] = {}\n    self.processes: dict[PowerDomain, SpawnProcess] = {}\n\n    # Determine which domains are supported\n    supported_domains = self._determine_supported_domains()\n    logger.info(\"Supported power domains: %s\", [d.value for d in supported_domains])\n\n    # Configure requested power domains\n    self.measurement_domains: list[PowerDomain] = []\n    if power_domains is None:\n        self.measurement_domains = supported_domains\n    else:\n        for requested_domain in power_domains:\n            domain = PowerDomain(requested_domain)\n            if domain not in supported_domains:\n                raise ValueError(\n                    f\"Requested power domain {domain.value} is not supported by the current GPUs. \"\n                    f\"Supported domains are: {[d.value for d in supported_domains]}\",\n                )\n            self.measurement_domains.append(domain)\n    self.measurement_domains = list(set(self.measurement_domains))\n\n    if PowerDomain.DEVICE_INSTANT not in self.measurement_domains:\n        logger.warning(\n            \"PowerDomain.DEVICE_INSTANT is not being monitored. The `get_power` method will not be available.\",\n        )\n\n    # Power samples are collected for each power domain and device index.\n    self.samples: dict[PowerDomain, dict[int, collections.deque[PowerSample]]] = {}\n    for domain in self.measurement_domains:\n        self.samples[domain] = {}\n        for gpu_idx in self.gpu_indices:\n            self.samples[domain][gpu_idx] = collections.deque(maxlen=max_samples_per_gpu)\n\n    # Spawn collector processes for each supported domain\n    ctx = mp.get_context(\"spawn\")\n    for domain in self.measurement_domains:\n        self.data_queues[domain] = ctx.Queue()\n        self.ready_events[domain] = ctx.Event()\n        self.stop_events[domain] = ctx.Event()\n        self.processes[domain] = ctx.Process(\n            target=_domain_polling_process,\n            kwargs=dict(\n                power_domain=domain,\n                gpu_indices=self.gpu_indices,\n                data_queue=self.data_queues[domain],\n                ready_event=self.ready_events[domain],\n                stop_event=self.stop_events[domain],\n                update_period=update_period,\n            ),\n            daemon=True,\n            name=f\"zeus-power-monitor-{domain.value}\",\n        )\n    for process in self.processes.values():\n        process.start()\n\n    # Cleanup functions\n    self._finalizer = weakref.finalize(self, _cleanup_processes, self.stop_events, self.processes)\n\n    # Wait for all subprocesses to signal they're ready\n    logger.info(\"Waiting for all power monitoring subprocesses to be ready...\")\n    for domain in self.measurement_domains:\n        if not self.ready_events[domain].wait(timeout=10.0):\n            logger.warning(\n                \"Power monitor subprocess for %s did not signal ready within 10 seconds\",\n                domain.value,\n            )\n            raise RuntimeError(\n                f\"Power monitor subprocess for {domain.value} failed to start within 10 seconds\",\n            )\n    logger.info(\"All power monitoring subprocesses are ready\")\n</code></pre>"},{"location":"reference/monitor/power/#zeus.monitor.power.PowerMonitor._determine_supported_domains","title":"_determine_supported_domains","text":"<pre><code>_determine_supported_domains()\n</code></pre> <p>Determine which power domains are supported by the current GPUs.</p> Source code in <code>zeus/monitor/power.py</code> <pre><code>def _determine_supported_domains(self) -&gt; list[PowerDomain]:\n    \"\"\"Determine which power domains are supported by the current GPUs.\"\"\"\n    supported = []\n    gpus = get_gpus(ensure_homogeneous=True)\n    methods = {\n        PowerDomain.DEVICE_INSTANT: gpus.get_instant_power_usage,\n        PowerDomain.DEVICE_AVERAGE: gpus.get_average_power_usage,\n        PowerDomain.MEMORY_AVERAGE: gpus.get_average_memory_power_usage,\n    }\n\n    # Just check the first GPU for support, since all GPUs are homogeneous.\n    for domain, method in methods.items():\n        try:\n            _ = method(0)\n            supported.append(domain)\n            logger.info(\"Power domain %s is supported\", domain.value)\n        except ZeusGPUNotSupportedError:\n            logger.info(\"Power domain %s is not supported\", domain.value)\n        except Exception as e:\n            logger.warning(\n                \"Unexpected error while checking for %s support on GPU %d: %s\",\n                domain.value,\n                self.gpu_indices[0],\n                e,\n            )\n\n    return supported\n</code></pre>"},{"location":"reference/monitor/power/#zeus.monitor.power.PowerMonitor.stop","title":"stop","text":"<pre><code>stop()\n</code></pre> <p>Stop all monitoring processes.</p> Source code in <code>zeus/monitor/power.py</code> <pre><code>def stop(self) -&gt; None:\n    \"\"\"Stop all monitoring processes.\"\"\"\n    if self._finalizer.alive:\n        self._finalizer()\n</code></pre>"},{"location":"reference/monitor/power/#zeus.monitor.power.PowerMonitor._process_queue_data","title":"_process_queue_data","text":"<pre><code>_process_queue_data(domain)\n</code></pre> <p>Process all pending samples from a specific domain's queue.</p> Source code in <code>zeus/monitor/power.py</code> <pre><code>def _process_queue_data(self, domain: PowerDomain) -&gt; None:\n    \"\"\"Process all pending samples from a specific domain's queue.\"\"\"\n    if domain not in self.data_queues:\n        return\n\n    while True:\n        try:\n            sample = self.data_queues[domain].get_nowait()\n            if sample == \"STOP\":\n                break\n            assert isinstance(sample, PowerSample)\n            self.samples[domain][sample.gpu_index].append(sample)\n        except Empty:\n            break\n</code></pre>"},{"location":"reference/monitor/power/#zeus.monitor.power.PowerMonitor._process_all_queue_data","title":"_process_all_queue_data","text":"<pre><code>_process_all_queue_data()\n</code></pre> <p>Process all pending samples from all domain queues.</p> Source code in <code>zeus/monitor/power.py</code> <pre><code>def _process_all_queue_data(self) -&gt; None:\n    \"\"\"Process all pending samples from all domain queues.\"\"\"\n    for domain in self.measurement_domains:\n        self._process_queue_data(domain)\n</code></pre>"},{"location":"reference/monitor/power/#zeus.monitor.power.PowerMonitor.get_power_timeline","title":"get_power_timeline","text":"<pre><code>get_power_timeline(power_domain, gpu_index=None, start_time=None, end_time=None)\n</code></pre> <p>Get power timeline for specific power domain and GPU(s).</p> <p>Parameters:</p> Name Type Description Default <code>power_domain</code> <code>PowerDomain | Literal['device_instant', 'device_average', 'memory_average']</code> <p>Power domain to query</p> required <code>gpu_index</code> <code>int | None</code> <p>Specific GPU index, or None for all GPUs</p> <code>None</code> <code>start_time</code> <code>float | None</code> <p>Start time filter (unix timestamp from time.time() or similar)</p> <code>None</code> <code>end_time</code> <code>float | None</code> <p>End time filter (unix timestamp from time.time() or similar)</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[int, list[tuple[float, float]]]</code> <p>Dictionary mapping GPU indices to timeline data with deduplication.</p> <code>dict[int, list[tuple[float, float]]]</code> <p>Timeline data is list of (timestamp, power_watts) tuples.</p> Source code in <code>zeus/monitor/power.py</code> <pre><code>def get_power_timeline(\n    self,\n    power_domain: PowerDomain | Literal[\"device_instant\", \"device_average\", \"memory_average\"],\n    gpu_index: int | None = None,\n    start_time: float | None = None,\n    end_time: float | None = None,\n) -&gt; dict[int, list[tuple[float, float]]]:\n    \"\"\"Get power timeline for specific power domain and GPU(s).\n\n    Args:\n        power_domain: Power domain to query\n        gpu_index: Specific GPU index, or None for all GPUs\n        start_time: Start time filter (unix timestamp from time.time() or similar)\n        end_time: End time filter (unix timestamp from time.time() or similar)\n\n    Returns:\n        Dictionary mapping GPU indices to timeline data with deduplication.\n        Timeline data is list of (timestamp, power_watts) tuples.\n    \"\"\"\n    if isinstance(power_domain, str):\n        power_domain = PowerDomain(power_domain)\n\n    if power_domain not in self.measurement_domains:\n        raise ValueError(\n            f\"Power domain {power_domain.value} is not being monitored. \"\n            f\"Monitored domains: {[d.value for d in self.measurement_domains]}\",\n        )\n\n    # Process any pending queue data for this domain\n    self._process_queue_data(power_domain)\n\n    # Determine which GPUs to query\n    target_gpus = [gpu_index] if gpu_index is not None else self.gpu_indices\n\n    result = {}\n    for gpu_idx in target_gpus:\n        if gpu_idx not in self.samples[power_domain]:\n            continue\n\n        # Extract timeline from samples\n        timeline = []\n        for sample in self.samples[power_domain][gpu_idx]:\n            # Apply time filters\n            if start_time is not None and sample.timestamp &lt; start_time:\n                continue\n            if end_time is not None and sample.timestamp &gt; end_time:\n                continue\n\n            timeline.append((sample.timestamp, sample.power_mw / 1000.0))  # Convert to watts\n\n        # Sort by timestamp\n        timeline.sort(key=lambda x: x[0])\n        result[gpu_idx] = timeline\n\n    return result\n</code></pre>"},{"location":"reference/monitor/power/#zeus.monitor.power.PowerMonitor.get_all_power_timelines","title":"get_all_power_timelines","text":"<pre><code>get_all_power_timelines(gpu_index=None, start_time=None, end_time=None)\n</code></pre> <p>Get all power timelines organized by power domain.</p> <p>Parameters:</p> Name Type Description Default <code>gpu_index</code> <code>int | None</code> <p>Specific GPU index, or None for all GPUs</p> <code>None</code> <code>start_time</code> <code>float | None</code> <p>Start time filter (unix timestamp from time.time() or similar)</p> <code>None</code> <code>end_time</code> <code>float | None</code> <p>End time filter (unix timestamp from time.time() or similar)</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, dict[int, list[tuple[float, float]]]]</code> <p>Dictionary with power domain names as keys and each value is a dict</p> <code>dict[str, dict[int, list[tuple[float, float]]]]</code> <p>mapping GPU indices to timeline data.</p> Source code in <code>zeus/monitor/power.py</code> <pre><code>def get_all_power_timelines(\n    self,\n    gpu_index: int | None = None,\n    start_time: float | None = None,\n    end_time: float | None = None,\n) -&gt; dict[str, dict[int, list[tuple[float, float]]]]:\n    \"\"\"Get all power timelines organized by power domain.\n\n    Args:\n        gpu_index: Specific GPU index, or None for all GPUs\n        start_time: Start time filter (unix timestamp from time.time() or similar)\n        end_time: End time filter (unix timestamp from time.time() or similar)\n\n    Returns:\n        Dictionary with power domain names as keys and each value is a dict\n        mapping GPU indices to timeline data.\n    \"\"\"\n    result = {}\n    for domain in self.measurement_domains:\n        result[domain.value] = self.get_power_timeline(domain, gpu_index, start_time, end_time)\n    return result\n</code></pre>"},{"location":"reference/monitor/power/#zeus.monitor.power.PowerMonitor.get_energy","title":"get_energy","text":"<pre><code>get_energy(start_time, end_time)\n</code></pre> <p>Get the energy used by the GPUs between two times (backward compatibility).</p> <p>Uses device instant power for energy calculation.</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>float</code> <p>Start time of the interval, from time.time().</p> required <code>end_time</code> <code>float</code> <p>End time of the interval, from time.time().</p> required <p>Returns:</p> Type Description <code>dict[int, float] | None</code> <p>A dictionary mapping GPU indices to the energy used by the GPU between the</p> <code>dict[int, float] | None</code> <p>two times. If there are no power readings, return None.</p> Source code in <code>zeus/monitor/power.py</code> <pre><code>def get_energy(self, start_time: float, end_time: float) -&gt; dict[int, float] | None:\n    \"\"\"Get the energy used by the GPUs between two times (backward compatibility).\n\n    Uses device instant power for energy calculation.\n\n    Args:\n        start_time: Start time of the interval, from time.time().\n        end_time: End time of the interval, from time.time().\n\n    Returns:\n        A dictionary mapping GPU indices to the energy used by the GPU between the\n        two times. If there are no power readings, return None.\n    \"\"\"\n    if PowerDomain.DEVICE_INSTANT in self.measurement_domains:\n        domain = PowerDomain.DEVICE_INSTANT\n    elif PowerDomain.DEVICE_AVERAGE in self.measurement_domains:\n        domain = PowerDomain.DEVICE_AVERAGE\n    else:\n        raise ValueError(\n            \"Neither PowerDomain.DEVICE_INSTANT nor PowerDomain.DEVICE_AVERAGE is being monitored. \"\n            \"Cannot compute energy usage.\",\n        )\n\n    timelines = self.get_power_timeline(domain, start_time=start_time, end_time=end_time)\n\n    if not timelines:\n        return None\n\n    energy_result = {}\n    for gpu_idx, timeline in timelines.items():\n        if not timeline or len(timeline) &lt; 2:\n            energy_result[gpu_idx] = 0.0\n            continue\n\n        timestamps = [t[0] for t in timeline]\n        powers = [t[1] for t in timeline]\n\n        try:\n            energy_result[gpu_idx] = float(auc(timestamps, powers))\n        except ValueError:\n            energy_result[gpu_idx] = 0.0\n\n    return energy_result\n</code></pre>"},{"location":"reference/monitor/power/#zeus.monitor.power.PowerMonitor.get_power","title":"get_power","text":"<pre><code>get_power(time=None)\n</code></pre> <p>Get the instant power usage of the GPUs at a specific time point.</p> <p>Uses device instant power for compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>time</code> <code>float | None</code> <p>Time point to get the power usage at. If None, get the power usage at the last recorded time point.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[int, float] | None</code> <p>A dictionary mapping GPU indices to the power usage of the GPU at the</p> <code>dict[int, float] | None</code> <p>specified time point. If there are no power readings, return None.</p> Source code in <code>zeus/monitor/power.py</code> <pre><code>def get_power(self, time: float | None = None) -&gt; dict[int, float] | None:\n    \"\"\"Get the instant power usage of the GPUs at a specific time point.\n\n    Uses device instant power for compatibility.\n\n    Args:\n        time: Time point to get the power usage at. If None, get the power usage\n            at the last recorded time point.\n\n    Returns:\n        A dictionary mapping GPU indices to the power usage of the GPU at the\n        specified time point. If there are no power readings, return None.\n    \"\"\"\n    if PowerDomain.DEVICE_INSTANT not in self.measurement_domains:\n        raise ValueError(\n            f\"PowerDomain.DEVICE_INSTANT is not being monitored. Currently monitored domains: {[d.value for d in self.measurement_domains]}\",\n        )\n\n    # Process any pending queue data\n    self._process_all_queue_data()\n\n    result = {}\n    for gpu_idx in self.gpu_indices:\n        samples = self.samples[PowerDomain.DEVICE_INSTANT][gpu_idx]\n        if not samples:\n            return None\n\n        if time is None:\n            # Get the most recent sample\n            latest_sample = samples[-1]\n            result[gpu_idx] = latest_sample.power_mw / 1000.0  # Convert to watts\n        else:\n            # Find the closest sample to the requested time using bisect\n            timestamps = [sample.timestamp for sample in samples]\n            pos = bisect.bisect_left(timestamps, time)\n\n            if pos == 0:\n                closest_sample = samples[0]\n            elif pos == len(samples):\n                closest_sample = samples[-1]\n            else:\n                # Check the closest sample before and after the requested time\n                before = samples[pos - 1]\n                after = samples[pos]\n                closest_sample = before if time - before.timestamp &lt;= after.timestamp - time else after\n            result[gpu_idx] = closest_sample.power_mw / 1000.0  # To Watts\n\n    return result\n</code></pre>"},{"location":"reference/monitor/power/#zeus.monitor.power.infer_counter_update_period","title":"infer_counter_update_period","text":"<pre><code>infer_counter_update_period(gpu_indicies)\n</code></pre> <p>Infer the update period of the GPU power counter.</p> <p>GPU power counters can update as slow as 10 Hz depending on the GPU model, so there's no need to poll them too faster than that. This function infers the update period for each unique GPU model and selects the fastest-updating period detected. Then, it returns half the period to ensure that the counter is polled at least twice per update period.</p> Source code in <code>zeus/monitor/power.py</code> <pre><code>def infer_counter_update_period(gpu_indicies: list[int]) -&gt; float:\n    \"\"\"Infer the update period of the GPU power counter.\n\n    GPU power counters can update as slow as 10 Hz depending on the GPU model, so\n    there's no need to poll them too faster than that. This function infers the\n    update period for each unique GPU model and selects the fastest-updating\n    period detected. Then, it returns half the period to ensure that the\n    counter is polled at least twice per update period.\n    \"\"\"\n    gpus = get_gpus()\n\n    # For each unique GPU model, infer the update period.\n    update_period = 0.0\n    gpu_models_covered = set()\n    for index in gpu_indicies:\n        if (model := gpus.get_name(index)) not in gpu_models_covered:\n            logger.info(\"Detected %s, inferring GPU power counter update period.\", model)\n            gpu_models_covered.add(model)\n            detected_period = _infer_counter_update_period_single(index)\n            logger.info(\n                \"Counter update period for %s is %.2f s\",\n                model,\n                detected_period,\n            )\n            update_period = min(update_period, detected_period)\n\n    # Target half the update period to ensure that the counter is enough.\n    update_period /= 2.0\n\n    # Anything less than ten times a second is probably too slow.\n    if update_period &gt; 0.1:\n        logger.warning(\n            \"Inferred update period (%.2f s) is too long. Using 0.1 s instead.\",\n            update_period,\n        )\n        update_period = 0.1\n    return update_period\n</code></pre>"},{"location":"reference/monitor/power/#zeus.monitor.power._infer_counter_update_period_single","title":"_infer_counter_update_period_single","text":"<pre><code>_infer_counter_update_period_single(gpu_index)\n</code></pre> <p>Infer the update period of the GPU power counter for a single GPU.</p> Source code in <code>zeus/monitor/power.py</code> <pre><code>def _infer_counter_update_period_single(gpu_index: int) -&gt; float:\n    \"\"\"Infer the update period of the GPU power counter for a single GPU.\"\"\"\n    gpus = get_gpus()\n\n    # Determine which power measurement method to use\n    # Try instant power first, fall back to average power\n    power_method: Callable[[int], int] | None = None\n    try:\n        # Test if instant power is available\n        _ = gpus.get_instant_power_usage(gpu_index)\n        power_method = gpus.get_instant_power_usage\n    except ZeusGPUNotSupportedError:\n        try:\n            # Fall back to average power\n            _ = gpus.get_average_power_usage(gpu_index)\n            power_method = gpus.get_average_power_usage\n            logger.info(\n                \"Instant power not available for GPU %d, using average power for counter period inference\",\n                gpu_index,\n            )\n        except ZeusGPUNotSupportedError:\n            # Neither method available, return conservative default\n            logger.warning(\n                \"Neither instant nor average power available for GPU %d. \"\n                \"Using conservative default update period of 0.1s\",\n                gpu_index,\n            )\n            return 0.2  # Will be halved later to 0.1s\n\n    # Collect 1000 samples of the power counter with timestamps.\n    time_power_samples: list[tuple[float, int]] = [(0.0, 0) for _ in range(1000)]\n    for i in range(len(time_power_samples)):\n        time_power_samples[i] = (\n            time(),\n            power_method(gpu_index),\n        )\n\n    # Find the timestamps when the power readings changed.\n    time_power_samples = time_power_samples[10:]\n    changed_times = []\n    prev_power = time_power_samples[0][1]\n    for t, p in time_power_samples:\n        if p != prev_power:\n            changed_times.append(t)\n            prev_power = p\n\n    # Compute the minimum time difference between power change timestamps.\n    intervals = [time2 - time1 for time1, time2 in zip(changed_times, changed_times[1:])]\n    if len(intervals) == 0:\n        return 0.1\n    return min(intervals)\n</code></pre>"},{"location":"reference/monitor/power/#zeus.monitor.power._cleanup_processes","title":"_cleanup_processes","text":"<pre><code>_cleanup_processes(stop_events, processes)\n</code></pre> <p>Idempotent cleanup function for power monitoring processes.</p> Source code in <code>zeus/monitor/power.py</code> <pre><code>def _cleanup_processes(\n    stop_events: dict[PowerDomain, EventClass],\n    processes: dict[PowerDomain, SpawnProcess],\n) -&gt; None:\n    \"\"\"Idempotent cleanup function for power monitoring processes.\"\"\"\n    # Signal all processes to stop\n    for event in stop_events.values():\n        event.set()\n\n    # Wait for each process to complete\n    for process in processes.values():\n        if process.is_alive():\n            process.join(timeout=2.0)\n            if process.is_alive():\n                process.terminate()\n                process.join(timeout=1.0)\n                if process.is_alive():\n                    process.kill()\n                    process.join(timeout=1.0)\n\n    # Clean up dictionaries\n    stop_events.clear()\n    processes.clear()\n</code></pre>"},{"location":"reference/monitor/power/#zeus.monitor.power._domain_polling_process","title":"_domain_polling_process","text":"<pre><code>_domain_polling_process(power_domain, gpu_indices, data_queue, ready_event, stop_event, update_period)\n</code></pre> <p>Polling process for a specific power domain with deduplication.</p> Source code in <code>zeus/monitor/power.py</code> <pre><code>def _domain_polling_process(\n    power_domain: PowerDomain,\n    gpu_indices: list[int],\n    data_queue: mp.Queue,\n    ready_event: EventClass,\n    stop_event: EventClass,\n    update_period: float,\n) -&gt; None:\n    \"\"\"Polling process for a specific power domain with deduplication.\"\"\"\n    try:\n        # Get GPUs\n        gpus = get_gpus(ensure_homogeneous=True)\n\n        # Determine the GPU method to call based on domain\n        power_methods = {\n            PowerDomain.DEVICE_INSTANT: gpus.get_instant_power_usage,\n            PowerDomain.DEVICE_AVERAGE: gpus.get_average_power_usage,\n            PowerDomain.MEMORY_AVERAGE: gpus.get_average_memory_power_usage,\n        }\n        try:\n            power_method = power_methods[power_domain]\n        except KeyError:\n            raise ValueError(f\"Unknown power domain: {power_domain}\") from None\n\n        # Track previous power values for deduplication\n        prev_power: dict[int, float] = {}\n\n        # Signal that this process is ready to start monitoring\n        ready_event.set()\n\n        # Start polling loop\n        num_not_supported_encounter = 0\n        while not stop_event.is_set():\n            timestamp = time()\n\n            for gpu_index in gpu_indices:\n                try:\n                    power_mw = power_method(gpu_index)\n\n                    # Sometimes, if we poll too fast, power can return 0. Skip.\n                    if power_mw &lt;= 0:\n                        logger.warning(\n                            \"GPU %d power domain %s encountered %g mW measurement. \"\n                            \"Skipping. Polling frequency may be too high.\",\n                            gpu_index,\n                            power_domain.value,\n                            power_mw,\n                        )\n                        continue\n\n                    # Deduplication: only send if power changed\n                    if gpu_index in prev_power and prev_power[gpu_index] == power_mw:\n                        continue\n\n                    prev_power[gpu_index] = power_mw\n\n                    # Create and send power sample\n                    sample = PowerSample(\n                        timestamp=timestamp,\n                        gpu_index=gpu_index,\n                        power_mw=power_mw,\n                    )\n\n                    data_queue.put(sample)\n                except ZeusGPUNotSupportedError as e:\n                    # When polling at a high frequency, NVML sometimes raises\n                    # a NotSupported error.\n                    num_not_supported_encounter += 1\n                    if num_not_supported_encounter &gt; 10:\n                        num_not_supported_encounter = 0\n                        logger.warning(\n                            \"GPU %d domain %s encountered 10 NotSupported errors. \"\n                            \"This may indicate a polling frequency that is too high. \"\n                            \"Consider increasing the update period. \"\n                            \"Exception: '%s'\",\n                            gpu_index,\n                            power_domain.value,\n                            e,\n                        )\n                except Exception as e:\n                    logger.exception(\n                        \"Error polling power for GPU %d in domain %s: %s\",\n                        gpu_index,\n                        power_domain.value,\n                        e,\n                    )\n                    raise e\n\n            # Sleep for the remaining time\n            elapsed = time() - timestamp\n            sleep_time = update_period - elapsed\n            if sleep_time &gt; 0:\n                sleep(sleep_time)\n\n    except KeyboardInterrupt:\n        pass\n    except Exception as e:\n        logger.exception(\n            \"Exiting polling process for domain %s due to error: %s\",\n            power_domain.value,\n            e,\n        )\n        raise e\n    finally:\n        # Send stop signal\n        data_queue.put(\"STOP\")\n</code></pre>"},{"location":"reference/monitor/power_streaming/","title":"power_streaming","text":""},{"location":"reference/monitor/power_streaming/#zeus.monitor.power_streaming","title":"zeus.monitor.power_streaming","text":"<p>Stream GPU and CPU power readings from zeusd instances via SSE.</p> <p>This module provides <code>PowerStreamingClient</code>, a thread-based SSE client that connects to one or more zeusd endpoints (TCP or Unix domain socket) and provides the latest GPU and CPU power readings in a thread-safe manner.</p> <pre><code>from zeus.utils.zeusd import ZeusdConfig\nfrom zeus.monitor.power_streaming import PowerStreamingClient\n\nclient = PowerStreamingClient(\n    servers=[\n        ZeusdConfig.tcp(\"node1\", 4938, gpu_indices=[0, 1, 2, 3]),\n        ZeusdConfig.tcp(\"node2\", 4938),\n    ],\n)\n\n# Snapshot (latest readings at this instant):\nreadings = client.get_power()  # {\"node1:4938\": PowerReadings(...)}\n\n# Blocking iterator (yields on every SSE update):\nfor readings in client:\n    print(readings)\n\n# Async iterator:\nasync for readings in client:\n    print(readings)\n\nclient.stop()\n</code></pre>"},{"location":"reference/monitor/power_streaming/#zeus.monitor.power_streaming.CpuPowerReading","title":"CpuPowerReading  <code>dataclass</code>","text":"<p>Power reading for a single CPU package.</p> <p>Parameters:</p> Name Type Description Default <code>cpu_w</code> <code>float</code> <p>CPU package power in watts.</p> <code>0.0</code> <code>dram_w</code> <code>float | None</code> <p>DRAM power in watts, or None if not available.</p> <code>None</code> Source code in <code>zeus/monitor/power_streaming.py</code> <pre><code>@dataclass\nclass CpuPowerReading:\n    \"\"\"Power reading for a single CPU package.\n\n    Args:\n        cpu_w: CPU package power in watts.\n        dram_w: DRAM power in watts, or None if not available.\n    \"\"\"\n\n    cpu_w: float = 0.0\n    dram_w: float | None = None\n</code></pre>"},{"location":"reference/monitor/power_streaming/#zeus.monitor.power_streaming.PowerReadings","title":"PowerReadings  <code>dataclass</code>","text":"<p>Power readings from a single zeusd endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>timestamp_s</code> <code>float</code> <p>Unix timestamp (seconds) of the reading.</p> <code>0.0</code> <code>gpu_power_w</code> <code>dict[int, float]</code> <p>Per-GPU power in watts, keyed by GPU index.</p> <code>dict()</code> <code>cpu_power_w</code> <code>dict[int, CpuPowerReading]</code> <p>Per-CPU power readings, keyed by CPU index.</p> <code>dict()</code> Source code in <code>zeus/monitor/power_streaming.py</code> <pre><code>@dataclass\nclass PowerReadings:\n    \"\"\"Power readings from a single zeusd endpoint.\n\n    Args:\n        timestamp_s: Unix timestamp (seconds) of the reading.\n        gpu_power_w: Per-GPU power in watts, keyed by GPU index.\n        cpu_power_w: Per-CPU power readings, keyed by CPU index.\n    \"\"\"\n\n    timestamp_s: float = 0.0\n    gpu_power_w: dict[int, float] = field(default_factory=dict)\n    cpu_power_w: dict[int, CpuPowerReading] = field(default_factory=dict)\n</code></pre>"},{"location":"reference/monitor/power_streaming/#zeus.monitor.power_streaming.PowerStreamingClient","title":"PowerStreamingClient","text":"<p>Connect to multiple zeusd instances and stream GPU/CPU power readings.</p> <p>One background thread per device type per endpoint maintains an SSE connection to the zeusd streaming endpoints. The latest power readings are stored in a thread-safe dict, accessible via <code>get_power()</code>.</p> <p>The client supports three access patterns:</p> <ul> <li>Snapshot: Call <code>get_power()</code> to retrieve the latest readings at any time.</li> <li>Blocking iterator: Use <code>for readings in client</code> to block and yield a   snapshot each time new SSE data arrives. Iteration stops when <code>stop()</code>   is called.</li> <li>Async iterator: Use <code>async for readings in client</code> for the same   behavior without blocking the event loop.</li> </ul> <pre><code>client = PowerStreamingClient(servers=[...])\n\n# Snapshot\nreadings = client.get_power()\n\n# Blocking iterator\nfor readings in client:\n    print(readings)\n\n# Async iterator\nasync for readings in client:\n    print(readings)\n\nclient.stop()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>servers</code> <code>Sequence[ZeusdConfig]</code> <p>List of <code>ZeusdConfig</code> specifying zeusd endpoints and which GPUs/CPUs to stream from each.</p> required <code>reconnect_delay_s</code> <code>float</code> <p>Seconds to wait before reconnecting after a disconnect.</p> <code>1.0</code> Source code in <code>zeus/monitor/power_streaming.py</code> <pre><code>class PowerStreamingClient:\n    \"\"\"Connect to multiple zeusd instances and stream GPU/CPU power readings.\n\n    One background thread per device type per endpoint maintains an SSE\n    connection to the zeusd streaming endpoints. The latest power readings\n    are stored in a thread-safe dict, accessible via `get_power()`.\n\n    The client supports three access patterns:\n\n    - Snapshot: Call `get_power()` to retrieve the latest readings at any time.\n    - Blocking iterator: Use `for readings in client` to block and yield a\n      snapshot each time new SSE data arrives. Iteration stops when `stop()`\n      is called.\n    - Async iterator: Use `async for readings in client` for the same\n      behavior without blocking the event loop.\n\n    ```python\n    client = PowerStreamingClient(servers=[...])\n\n    # Snapshot\n    readings = client.get_power()\n\n    # Blocking iterator\n    for readings in client:\n        print(readings)\n\n    # Async iterator\n    async for readings in client:\n        print(readings)\n\n    client.stop()\n    ```\n\n    Args:\n        servers: List of `ZeusdConfig` specifying zeusd endpoints\n            and which GPUs/CPUs to stream from each.\n        reconnect_delay_s: Seconds to wait before reconnecting after a\n            disconnect.\n    \"\"\"\n\n    def __init__(\n        self,\n        servers: Sequence[ZeusdConfig],\n        reconnect_delay_s: float = 1.0,\n    ) -&gt; None:\n        \"\"\"Initialize the client and start background SSE connections.\"\"\"\n        self._servers = list(servers)\n        self._reconnect_delay = reconnect_delay_s\n\n        endpoints = [s.endpoint for s in self._servers]\n        duplicates = [e for e in endpoints if endpoints.count(e) &gt; 1]\n        if duplicates:\n            raise ValueError(f\"Duplicate server endpoints: {sorted(set(duplicates))}\")\n\n        self._lock = threading.Lock()\n        self._condition = threading.Condition(self._lock)\n        self._readings: dict[str, PowerReadings] = {}\n        self._clock_offsets: dict[str, float] = {}\n        self._daemon_clients: dict[str, ZeusdClient] = {}\n\n        self._threads: list[threading.Thread] = []\n        self._stop_event = threading.Event()\n\n        for server in self._servers:\n            stream_gpu, stream_cpu = self._init_server(server)\n            endpoint = server.endpoint\n            self._clock_offsets[endpoint] = self._estimate_clock_offset(endpoint)\n\n            if stream_gpu:\n                t = threading.Thread(\n                    target=self._gpu_stream_loop,\n                    args=(server, endpoint),\n                    name=f\"gpu-power-stream-{endpoint}\",\n                    daemon=True,\n                )\n                t.start()\n                self._threads.append(t)\n                logger.info(\"Started GPU power streaming thread for %s\", endpoint)\n\n            if stream_cpu:\n                t = threading.Thread(\n                    target=self._cpu_stream_loop,\n                    args=(server, endpoint),\n                    name=f\"cpu-power-stream-{endpoint}\",\n                    daemon=True,\n                )\n                t.start()\n                self._threads.append(t)\n                logger.info(\"Started CPU power streaming thread for %s\", endpoint)\n\n        if not self._threads:\n            logger.warning(\n                \"No GPU or CPU power streaming threads were started. \"\n                \"Check that the zeusd endpoints have the expected devices available.\"\n            )\n\n    def stop(self) -&gt; None:\n        \"\"\"Stop all background connections and wake any blocked iterators.\"\"\"\n        self._stop_event.set()\n        with self._condition:\n            self._condition.notify_all()\n        for t in self._threads:\n            t.join(timeout=5.0)\n        self._threads.clear()\n        logger.info(\"All power streaming threads stopped\")\n\n    def get_power(self) -&gt; dict[str, PowerReadings]:\n        \"\"\"Get the latest power readings from all endpoints.\n\n        Returns:\n            Mapping of endpoint identifier to `PowerReadings` containing\n            timestamp and per-GPU/CPU power in watts.\n        \"\"\"\n        with self._lock:\n            return {\n                k: PowerReadings(\n                    timestamp_s=v.timestamp_s,\n                    gpu_power_w=dict(v.gpu_power_w),\n                    cpu_power_w={\n                        idx: CpuPowerReading(cpu_w=r.cpu_w, dram_w=r.dram_w) for idx, r in v.cpu_power_w.items()\n                    },\n                )\n                for k, v in self._readings.items()\n            }\n\n    def __iter__(self) -&gt; Iterator[dict[str, PowerReadings]]:\n        \"\"\"Yield power reading snapshots as they arrive from SSE streams.\n\n        Blocks until new readings are available, then yields a snapshot\n        (same format as `get_power()`). Iteration stops when `stop()` is\n        called.\n\n        ```python\n        client = PowerStreamingClient(servers=[...])\n        for readings in client:\n            for endpoint, pr in readings.items():\n                print(f\"{endpoint}: {pr.gpu_power_w}\")\n        ```\n        \"\"\"\n        while not self._stop_event.is_set():\n            with self._condition:\n                notified = self._condition.wait(timeout=1.0)\n            if self._stop_event.is_set():\n                break\n            if notified:\n                yield self.get_power()\n\n    async def __aiter__(self) -&gt; AsyncIterator[dict[str, PowerReadings]]:\n        \"\"\"Async version of `__iter__`.\n\n        Yields power reading snapshots without blocking the event loop.\n        Iteration stops when `stop()` is called.\n\n        ```python\n        client = PowerStreamingClient(servers=[...])\n        async for readings in client:\n            for endpoint, pr in readings.items():\n                print(f\"{endpoint}: {pr.gpu_power_w}\")\n        ```\n        \"\"\"\n        import asyncio\n\n        loop = asyncio.get_running_loop()\n        while not self._stop_event.is_set():\n            notified = await loop.run_in_executor(None, self._wait_for_update)\n            if self._stop_event.is_set():\n                break\n            if notified:\n                yield self.get_power()\n\n    def _wait_for_update(self) -&gt; bool:\n        \"\"\"Block until readings are updated or timeout (1 s).\n\n        Used by `__aiter__` to avoid blocking the async event loop.\n        \"\"\"\n        with self._condition:\n            return self._condition.wait(timeout=1.0)\n\n    def _init_server(self, server: ZeusdConfig) -&gt; tuple[bool, bool]:\n        \"\"\"Initialize connection to a server and decide what to stream.\n\n        Creates a `ZeusdClient` for the server (handling discovery and auth),\n        validates requested indices, and checks scope permissions.\n\n        Returns:\n            A `(stream_gpu, stream_cpu)` pair of booleans.\n\n        Raises:\n            ZeusdConnectionError: If the server is not reachable.\n            ValueError: If explicitly requested indices are not available.\n            ZeusdCapabilityError: If explicitly requested streaming requires\n                a scope the token doesn't have.\n        \"\"\"\n        client = ZeusdClient(server)\n        endpoint = server.endpoint\n        self._daemon_clients[endpoint] = client\n\n        available_gpus = set(client.gpu_ids)\n        available_cpus = set(client.cpu_ids)\n\n        stream_gpu = self._resolve_streaming(\n            user_indices=server.gpu_indices,\n            available_ids=available_gpus,\n            has_permission=client.can_read_gpu,\n            scope_name=\"gpu-read\",\n            device_type=\"GPU\",\n            endpoint=endpoint,\n        )\n\n        stream_cpu = self._resolve_streaming(\n            user_indices=server.cpu_indices,\n            available_ids=available_cpus,\n            has_permission=client.can_read_cpu,\n            scope_name=\"cpu-read\",\n            device_type=\"CPU\",\n            endpoint=endpoint,\n        )\n\n        return stream_gpu, stream_cpu\n\n    @staticmethod\n    def _resolve_streaming(\n        user_indices: list[int] | None,\n        available_ids: set[int],\n        has_permission: bool,\n        scope_name: str,\n        device_type: str,\n        endpoint: str,\n    ) -&gt; bool:\n        \"\"\"Decide whether to stream a device type.\n\n        Semantics:\n        - `user_indices is None`: stream all available, silently skip if\n          none exist or if the token lacks the scope.\n        - `user_indices == []`: explicitly opt out, never stream.\n        - `user_indices` is a non-empty list: require all IDs to exist\n          and the scope to be granted; raise on mismatch.\n\n        Returns:\n            True if streaming should be started for this device type.\n\n        Raises:\n            ValueError: If explicit indices are not a subset of available.\n            ZeusdCapabilityError: If explicit indices are given but the\n                token lacks the required scope.\n        \"\"\"\n        if user_indices is not None:\n            if not user_indices:\n                return False\n            missing = set(user_indices) - available_ids\n            if missing:\n                raise ValueError(\n                    f\"{device_type} indices {sorted(missing)} requested for \"\n                    f\"{endpoint} but only {sorted(available_ids)} are available\"\n                )\n            if not has_permission:\n                raise ZeusdCapabilityError(\n                    f\"Token for {endpoint} lacks required scope '{scope_name}' \"\n                    f\"(explicitly requested {device_type.lower()}_indices={user_indices})\"\n                )\n            return True\n\n        if not available_ids:\n            logger.info(\n                \"No %ss available on %s; skipping %s power streaming\",\n                device_type,\n                endpoint,\n                device_type,\n            )\n            return False\n\n        if not has_permission:\n            logger.info(\n                \"Token for %s lacks '%s' scope; skipping %s streaming\",\n                endpoint,\n                scope_name,\n                device_type,\n            )\n            return False\n\n        return True\n\n    def _estimate_clock_offset(\n        self,\n        endpoint: str,\n        num_samples: int = 5,\n    ) -&gt; float:\n        \"\"\"Estimate the clock offset between this client and the daemon.\n\n        Performs `num_samples` round-trips to `GET /time` on the daemon,\n        computes `client_midpoint - daemon_time` for each, and returns the\n        median offset in seconds. A positive offset means the daemon clock\n        is behind the client clock.\n\n        Args:\n            endpoint: The endpoint identifier.\n            num_samples: Number of round-trips for robustness.\n\n        Returns:\n            Estimated clock offset in seconds. Add this to daemon\n            timestamps to align them with client time.\n        \"\"\"\n        client = self._daemon_clients[endpoint]\n        url = client.url(\"/time\")\n        offsets: list[float] = []\n        with client.make_client() as http:\n            for _ in range(num_samples):\n                t1 = time.time()\n                response = http.get(url)\n                t2 = time.time()\n                response.raise_for_status()\n                daemon_time_s = response.json()[\"timestamp_ms\"] / 1000.0\n                client_midpoint_s = (t1 + t2) / 2.0\n                offsets.append(client_midpoint_s - daemon_time_s)\n        offset = statistics.median(offsets)\n        logger.info(\n            \"Clock offset for %s: %.4f s (median of %d samples)\",\n            endpoint,\n            offset,\n            num_samples,\n        )\n        return offset\n\n    def _gpu_stream_loop(self, server: ZeusdConfig, endpoint: str) -&gt; None:\n        \"\"\"Background thread: stream GPU power from a single server.\"\"\"\n        client = self._daemon_clients[endpoint]\n        base_url = client.url(\"/gpu/stream_power\")\n        if server.gpu_indices is not None:\n            ids_param = \",\".join(str(i) for i in server.gpu_indices)\n            url = f\"{base_url}?gpu_ids={ids_param}\"\n        else:\n            url = base_url\n        self._stream_loop(url, endpoint, self._process_gpu_event, \"GPU\")\n\n    def _cpu_stream_loop(self, server: ZeusdConfig, endpoint: str) -&gt; None:\n        \"\"\"Background thread: stream CPU power from a single server.\"\"\"\n        client = self._daemon_clients[endpoint]\n        base_url = client.url(\"/cpu/stream_power\")\n        if server.cpu_indices is not None:\n            ids_param = \",\".join(str(i) for i in server.cpu_indices)\n            url = f\"{base_url}?cpu_ids={ids_param}\"\n        else:\n            url = base_url\n        self._stream_loop(url, endpoint, self._process_cpu_event, \"CPU\")\n\n    def _stream_loop(\n        self,\n        url: str,\n        endpoint: str,\n        process_fn: typing.Callable[[str, str], None],\n        label: str,\n    ) -&gt; None:\n        \"\"\"Shared reconnect loop for SSE streams.\"\"\"\n        while not self._stop_event.is_set():\n            try:\n                self._connect_and_stream(url, endpoint, process_fn)\n            except httpx.HTTPStatusError as e:\n                if e.response.status_code in (401, 403):\n                    logger.error(\n                        \"%s SSE connection to %s failed with HTTP %d: %s\",\n                        label,\n                        endpoint,\n                        e.response.status_code,\n                        e.response.text,\n                    )\n                    return\n                if not self._stop_event.is_set():\n                    logger.warning(\n                        \"%s SSE connection to %s rejected (HTTP %d), reconnecting in %.1fs\",\n                        label,\n                        endpoint,\n                        e.response.status_code,\n                        self._reconnect_delay,\n                        exc_info=True,\n                    )\n                    self._stop_event.wait(timeout=self._reconnect_delay)\n            except httpx.RequestError:\n                if not self._stop_event.is_set():\n                    logger.warning(\n                        \"%s SSE connection to %s lost, reconnecting in %.1fs\",\n                        label,\n                        endpoint,\n                        self._reconnect_delay,\n                        exc_info=True,\n                    )\n                    self._stop_event.wait(timeout=self._reconnect_delay)\n\n    def _connect_and_stream(\n        self,\n        url: str,\n        endpoint: str,\n        process_fn: typing.Callable[[str, str], None],\n    ) -&gt; None:\n        \"\"\"Open an SSE connection and process events until disconnected.\"\"\"\n        client = self._daemon_clients[endpoint]\n        logger.info(\"Connecting to SSE at %s\", url)\n        with client.make_client() as http, http.stream(\"GET\", url, timeout=None) as response:\n            response.raise_for_status()\n            logger.info(\"SSE connected to %s\", url)\n            buffer = \"\"\n            for chunk in response.iter_text():\n                if self._stop_event.is_set():\n                    return\n                buffer += chunk\n                while \"\\n\\n\" in buffer:\n                    event_text, buffer = buffer.split(\"\\n\\n\", 1)\n                    process_fn(event_text, endpoint)\n\n    def _process_gpu_event(self, event_text: str, endpoint: str) -&gt; None:\n        \"\"\"Parse a GPU SSE event and update readings.\"\"\"\n        for line in event_text.strip().split(\"\\n\"):\n            if line.startswith(\"data: \"):\n                data_str = line[6:]\n                try:\n                    data = json.loads(data_str)\n                except json.JSONDecodeError:\n                    logger.warning(\"Invalid JSON in GPU SSE event from %s: %s\", endpoint, data_str[:100])\n                    continue\n\n                power_mw = data.get(\"power_mw\", {})\n                timestamp_ms = data.get(\"timestamp_ms\", 0)\n                timestamp_s = timestamp_ms / 1000.0 + self._clock_offsets[endpoint]\n\n                gpu_power_w: dict[int, float] = {}\n                for gpu_id_str, mw in power_mw.items():\n                    gpu_power_w[int(gpu_id_str)] = float(mw) / 1000.0  # mW -&gt; W\n\n                with self._condition:\n                    existing = self._readings.get(endpoint)\n                    if existing is not None:\n                        existing.gpu_power_w = gpu_power_w\n                        existing.timestamp_s = max(existing.timestamp_s, timestamp_s)\n                    else:\n                        self._readings[endpoint] = PowerReadings(\n                            timestamp_s=timestamp_s,\n                            gpu_power_w=gpu_power_w,\n                        )\n                    self._condition.notify_all()\n\n    def _process_cpu_event(self, event_text: str, endpoint: str) -&gt; None:\n        \"\"\"Parse a CPU SSE event and update readings.\n\n        Expected JSON format: `{\"timestamp_ms\": N, \"power_mw\": {\"0\": {\"cpu_mw\": N, \"dram_mw\": N|null}}}`.\n        \"\"\"\n        for line in event_text.strip().split(\"\\n\"):\n            if line.startswith(\"data: \"):\n                data_str = line[6:]\n                try:\n                    data = json.loads(data_str)\n                except json.JSONDecodeError:\n                    logger.warning(\"Invalid JSON in CPU SSE event from %s: %s\", endpoint, data_str[:100])\n                    continue\n\n                power_mw = data.get(\"power_mw\", {})\n                timestamp_ms = data.get(\"timestamp_ms\", 0)\n                timestamp_s = timestamp_ms / 1000.0 + self._clock_offsets[endpoint]\n\n                cpu_power_w: dict[int, CpuPowerReading] = {}\n                for cpu_id_str, readings in power_mw.items():\n                    cpu_mw = readings.get(\"cpu_mw\", 0)\n                    dram_mw = readings.get(\"dram_mw\")\n                    cpu_power_w[int(cpu_id_str)] = CpuPowerReading(\n                        cpu_w=float(cpu_mw) / 1000.0,\n                        dram_w=float(dram_mw) / 1000.0 if dram_mw is not None else None,\n                    )\n\n                with self._condition:\n                    existing = self._readings.get(endpoint)\n                    if existing is not None:\n                        existing.cpu_power_w = cpu_power_w\n                        existing.timestamp_s = max(existing.timestamp_s, timestamp_s)\n                    else:\n                        self._readings[endpoint] = PowerReadings(\n                            timestamp_s=timestamp_s,\n                            cpu_power_w=cpu_power_w,\n                        )\n                    self._condition.notify_all()\n</code></pre>"},{"location":"reference/monitor/power_streaming/#zeus.monitor.power_streaming.PowerStreamingClient.__init__","title":"__init__","text":"<pre><code>__init__(servers, reconnect_delay_s=1.0)\n</code></pre> Source code in <code>zeus/monitor/power_streaming.py</code> <pre><code>def __init__(\n    self,\n    servers: Sequence[ZeusdConfig],\n    reconnect_delay_s: float = 1.0,\n) -&gt; None:\n    \"\"\"Initialize the client and start background SSE connections.\"\"\"\n    self._servers = list(servers)\n    self._reconnect_delay = reconnect_delay_s\n\n    endpoints = [s.endpoint for s in self._servers]\n    duplicates = [e for e in endpoints if endpoints.count(e) &gt; 1]\n    if duplicates:\n        raise ValueError(f\"Duplicate server endpoints: {sorted(set(duplicates))}\")\n\n    self._lock = threading.Lock()\n    self._condition = threading.Condition(self._lock)\n    self._readings: dict[str, PowerReadings] = {}\n    self._clock_offsets: dict[str, float] = {}\n    self._daemon_clients: dict[str, ZeusdClient] = {}\n\n    self._threads: list[threading.Thread] = []\n    self._stop_event = threading.Event()\n\n    for server in self._servers:\n        stream_gpu, stream_cpu = self._init_server(server)\n        endpoint = server.endpoint\n        self._clock_offsets[endpoint] = self._estimate_clock_offset(endpoint)\n\n        if stream_gpu:\n            t = threading.Thread(\n                target=self._gpu_stream_loop,\n                args=(server, endpoint),\n                name=f\"gpu-power-stream-{endpoint}\",\n                daemon=True,\n            )\n            t.start()\n            self._threads.append(t)\n            logger.info(\"Started GPU power streaming thread for %s\", endpoint)\n\n        if stream_cpu:\n            t = threading.Thread(\n                target=self._cpu_stream_loop,\n                args=(server, endpoint),\n                name=f\"cpu-power-stream-{endpoint}\",\n                daemon=True,\n            )\n            t.start()\n            self._threads.append(t)\n            logger.info(\"Started CPU power streaming thread for %s\", endpoint)\n\n    if not self._threads:\n        logger.warning(\n            \"No GPU or CPU power streaming threads were started. \"\n            \"Check that the zeusd endpoints have the expected devices available.\"\n        )\n</code></pre>"},{"location":"reference/monitor/power_streaming/#zeus.monitor.power_streaming.PowerStreamingClient.stop","title":"stop","text":"<pre><code>stop()\n</code></pre> <p>Stop all background connections and wake any blocked iterators.</p> Source code in <code>zeus/monitor/power_streaming.py</code> <pre><code>def stop(self) -&gt; None:\n    \"\"\"Stop all background connections and wake any blocked iterators.\"\"\"\n    self._stop_event.set()\n    with self._condition:\n        self._condition.notify_all()\n    for t in self._threads:\n        t.join(timeout=5.0)\n    self._threads.clear()\n    logger.info(\"All power streaming threads stopped\")\n</code></pre>"},{"location":"reference/monitor/power_streaming/#zeus.monitor.power_streaming.PowerStreamingClient.get_power","title":"get_power","text":"<pre><code>get_power()\n</code></pre> <p>Get the latest power readings from all endpoints.</p> <p>Returns:</p> Type Description <code>dict[str, PowerReadings]</code> <p>Mapping of endpoint identifier to <code>PowerReadings</code> containing</p> <code>dict[str, PowerReadings]</code> <p>timestamp and per-GPU/CPU power in watts.</p> Source code in <code>zeus/monitor/power_streaming.py</code> <pre><code>def get_power(self) -&gt; dict[str, PowerReadings]:\n    \"\"\"Get the latest power readings from all endpoints.\n\n    Returns:\n        Mapping of endpoint identifier to `PowerReadings` containing\n        timestamp and per-GPU/CPU power in watts.\n    \"\"\"\n    with self._lock:\n        return {\n            k: PowerReadings(\n                timestamp_s=v.timestamp_s,\n                gpu_power_w=dict(v.gpu_power_w),\n                cpu_power_w={\n                    idx: CpuPowerReading(cpu_w=r.cpu_w, dram_w=r.dram_w) for idx, r in v.cpu_power_w.items()\n                },\n            )\n            for k, v in self._readings.items()\n        }\n</code></pre>"},{"location":"reference/monitor/power_streaming/#zeus.monitor.power_streaming.PowerStreamingClient.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Yield power reading snapshots as they arrive from SSE streams.</p> <p>Blocks until new readings are available, then yields a snapshot (same format as <code>get_power()</code>). Iteration stops when <code>stop()</code> is called.</p> <pre><code>client = PowerStreamingClient(servers=[...])\nfor readings in client:\n    for endpoint, pr in readings.items():\n        print(f\"{endpoint}: {pr.gpu_power_w}\")\n</code></pre> Source code in <code>zeus/monitor/power_streaming.py</code> <pre><code>def __iter__(self) -&gt; Iterator[dict[str, PowerReadings]]:\n    \"\"\"Yield power reading snapshots as they arrive from SSE streams.\n\n    Blocks until new readings are available, then yields a snapshot\n    (same format as `get_power()`). Iteration stops when `stop()` is\n    called.\n\n    ```python\n    client = PowerStreamingClient(servers=[...])\n    for readings in client:\n        for endpoint, pr in readings.items():\n            print(f\"{endpoint}: {pr.gpu_power_w}\")\n    ```\n    \"\"\"\n    while not self._stop_event.is_set():\n        with self._condition:\n            notified = self._condition.wait(timeout=1.0)\n        if self._stop_event.is_set():\n            break\n        if notified:\n            yield self.get_power()\n</code></pre>"},{"location":"reference/monitor/power_streaming/#zeus.monitor.power_streaming.PowerStreamingClient.__aiter__","title":"__aiter__  <code>async</code>","text":"<pre><code>__aiter__()\n</code></pre> <p>Async version of <code>__iter__</code>.</p> <p>Yields power reading snapshots without blocking the event loop. Iteration stops when <code>stop()</code> is called.</p> <pre><code>client = PowerStreamingClient(servers=[...])\nasync for readings in client:\n    for endpoint, pr in readings.items():\n        print(f\"{endpoint}: {pr.gpu_power_w}\")\n</code></pre> Source code in <code>zeus/monitor/power_streaming.py</code> <pre><code>async def __aiter__(self) -&gt; AsyncIterator[dict[str, PowerReadings]]:\n    \"\"\"Async version of `__iter__`.\n\n    Yields power reading snapshots without blocking the event loop.\n    Iteration stops when `stop()` is called.\n\n    ```python\n    client = PowerStreamingClient(servers=[...])\n    async for readings in client:\n        for endpoint, pr in readings.items():\n            print(f\"{endpoint}: {pr.gpu_power_w}\")\n    ```\n    \"\"\"\n    import asyncio\n\n    loop = asyncio.get_running_loop()\n    while not self._stop_event.is_set():\n        notified = await loop.run_in_executor(None, self._wait_for_update)\n        if self._stop_event.is_set():\n            break\n        if notified:\n            yield self.get_power()\n</code></pre>"},{"location":"reference/monitor/power_streaming/#zeus.monitor.power_streaming.PowerStreamingClient._wait_for_update","title":"_wait_for_update","text":"<pre><code>_wait_for_update()\n</code></pre> <p>Block until readings are updated or timeout (1 s).</p> <p>Used by <code>__aiter__</code> to avoid blocking the async event loop.</p> Source code in <code>zeus/monitor/power_streaming.py</code> <pre><code>def _wait_for_update(self) -&gt; bool:\n    \"\"\"Block until readings are updated or timeout (1 s).\n\n    Used by `__aiter__` to avoid blocking the async event loop.\n    \"\"\"\n    with self._condition:\n        return self._condition.wait(timeout=1.0)\n</code></pre>"},{"location":"reference/monitor/power_streaming/#zeus.monitor.power_streaming.PowerStreamingClient._init_server","title":"_init_server","text":"<pre><code>_init_server(server)\n</code></pre> <p>Initialize connection to a server and decide what to stream.</p> <p>Creates a <code>ZeusdClient</code> for the server (handling discovery and auth), validates requested indices, and checks scope permissions.</p> <p>Returns:</p> Type Description <code>tuple[bool, bool]</code> <p>A <code>(stream_gpu, stream_cpu)</code> pair of booleans.</p> <p>Raises:</p> Type Description <code>ZeusdConnectionError</code> <p>If the server is not reachable.</p> <code>ValueError</code> <p>If explicitly requested indices are not available.</p> <code>ZeusdCapabilityError</code> <p>If explicitly requested streaming requires a scope the token doesn't have.</p> Source code in <code>zeus/monitor/power_streaming.py</code> <pre><code>def _init_server(self, server: ZeusdConfig) -&gt; tuple[bool, bool]:\n    \"\"\"Initialize connection to a server and decide what to stream.\n\n    Creates a `ZeusdClient` for the server (handling discovery and auth),\n    validates requested indices, and checks scope permissions.\n\n    Returns:\n        A `(stream_gpu, stream_cpu)` pair of booleans.\n\n    Raises:\n        ZeusdConnectionError: If the server is not reachable.\n        ValueError: If explicitly requested indices are not available.\n        ZeusdCapabilityError: If explicitly requested streaming requires\n            a scope the token doesn't have.\n    \"\"\"\n    client = ZeusdClient(server)\n    endpoint = server.endpoint\n    self._daemon_clients[endpoint] = client\n\n    available_gpus = set(client.gpu_ids)\n    available_cpus = set(client.cpu_ids)\n\n    stream_gpu = self._resolve_streaming(\n        user_indices=server.gpu_indices,\n        available_ids=available_gpus,\n        has_permission=client.can_read_gpu,\n        scope_name=\"gpu-read\",\n        device_type=\"GPU\",\n        endpoint=endpoint,\n    )\n\n    stream_cpu = self._resolve_streaming(\n        user_indices=server.cpu_indices,\n        available_ids=available_cpus,\n        has_permission=client.can_read_cpu,\n        scope_name=\"cpu-read\",\n        device_type=\"CPU\",\n        endpoint=endpoint,\n    )\n\n    return stream_gpu, stream_cpu\n</code></pre>"},{"location":"reference/monitor/power_streaming/#zeus.monitor.power_streaming.PowerStreamingClient._resolve_streaming","title":"_resolve_streaming  <code>staticmethod</code>","text":"<pre><code>_resolve_streaming(user_indices, available_ids, has_permission, scope_name, device_type, endpoint)\n</code></pre> <p>Decide whether to stream a device type.</p> <p>Semantics: - <code>user_indices is None</code>: stream all available, silently skip if   none exist or if the token lacks the scope. - <code>user_indices == []</code>: explicitly opt out, never stream. - <code>user_indices</code> is a non-empty list: require all IDs to exist   and the scope to be granted; raise on mismatch.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if streaming should be started for this device type.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If explicit indices are not a subset of available.</p> <code>ZeusdCapabilityError</code> <p>If explicit indices are given but the token lacks the required scope.</p> Source code in <code>zeus/monitor/power_streaming.py</code> <pre><code>@staticmethod\ndef _resolve_streaming(\n    user_indices: list[int] | None,\n    available_ids: set[int],\n    has_permission: bool,\n    scope_name: str,\n    device_type: str,\n    endpoint: str,\n) -&gt; bool:\n    \"\"\"Decide whether to stream a device type.\n\n    Semantics:\n    - `user_indices is None`: stream all available, silently skip if\n      none exist or if the token lacks the scope.\n    - `user_indices == []`: explicitly opt out, never stream.\n    - `user_indices` is a non-empty list: require all IDs to exist\n      and the scope to be granted; raise on mismatch.\n\n    Returns:\n        True if streaming should be started for this device type.\n\n    Raises:\n        ValueError: If explicit indices are not a subset of available.\n        ZeusdCapabilityError: If explicit indices are given but the\n            token lacks the required scope.\n    \"\"\"\n    if user_indices is not None:\n        if not user_indices:\n            return False\n        missing = set(user_indices) - available_ids\n        if missing:\n            raise ValueError(\n                f\"{device_type} indices {sorted(missing)} requested for \"\n                f\"{endpoint} but only {sorted(available_ids)} are available\"\n            )\n        if not has_permission:\n            raise ZeusdCapabilityError(\n                f\"Token for {endpoint} lacks required scope '{scope_name}' \"\n                f\"(explicitly requested {device_type.lower()}_indices={user_indices})\"\n            )\n        return True\n\n    if not available_ids:\n        logger.info(\n            \"No %ss available on %s; skipping %s power streaming\",\n            device_type,\n            endpoint,\n            device_type,\n        )\n        return False\n\n    if not has_permission:\n        logger.info(\n            \"Token for %s lacks '%s' scope; skipping %s streaming\",\n            endpoint,\n            scope_name,\n            device_type,\n        )\n        return False\n\n    return True\n</code></pre>"},{"location":"reference/monitor/power_streaming/#zeus.monitor.power_streaming.PowerStreamingClient._estimate_clock_offset","title":"_estimate_clock_offset","text":"<pre><code>_estimate_clock_offset(endpoint, num_samples=5)\n</code></pre> <p>Estimate the clock offset between this client and the daemon.</p> <p>Performs <code>num_samples</code> round-trips to <code>GET /time</code> on the daemon, computes <code>client_midpoint - daemon_time</code> for each, and returns the median offset in seconds. A positive offset means the daemon clock is behind the client clock.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>The endpoint identifier.</p> required <code>num_samples</code> <code>int</code> <p>Number of round-trips for robustness.</p> <code>5</code> <p>Returns:</p> Type Description <code>float</code> <p>Estimated clock offset in seconds. Add this to daemon</p> <code>float</code> <p>timestamps to align them with client time.</p> Source code in <code>zeus/monitor/power_streaming.py</code> <pre><code>def _estimate_clock_offset(\n    self,\n    endpoint: str,\n    num_samples: int = 5,\n) -&gt; float:\n    \"\"\"Estimate the clock offset between this client and the daemon.\n\n    Performs `num_samples` round-trips to `GET /time` on the daemon,\n    computes `client_midpoint - daemon_time` for each, and returns the\n    median offset in seconds. A positive offset means the daemon clock\n    is behind the client clock.\n\n    Args:\n        endpoint: The endpoint identifier.\n        num_samples: Number of round-trips for robustness.\n\n    Returns:\n        Estimated clock offset in seconds. Add this to daemon\n        timestamps to align them with client time.\n    \"\"\"\n    client = self._daemon_clients[endpoint]\n    url = client.url(\"/time\")\n    offsets: list[float] = []\n    with client.make_client() as http:\n        for _ in range(num_samples):\n            t1 = time.time()\n            response = http.get(url)\n            t2 = time.time()\n            response.raise_for_status()\n            daemon_time_s = response.json()[\"timestamp_ms\"] / 1000.0\n            client_midpoint_s = (t1 + t2) / 2.0\n            offsets.append(client_midpoint_s - daemon_time_s)\n    offset = statistics.median(offsets)\n    logger.info(\n        \"Clock offset for %s: %.4f s (median of %d samples)\",\n        endpoint,\n        offset,\n        num_samples,\n    )\n    return offset\n</code></pre>"},{"location":"reference/monitor/power_streaming/#zeus.monitor.power_streaming.PowerStreamingClient._gpu_stream_loop","title":"_gpu_stream_loop","text":"<pre><code>_gpu_stream_loop(server, endpoint)\n</code></pre> <p>Background thread: stream GPU power from a single server.</p> Source code in <code>zeus/monitor/power_streaming.py</code> <pre><code>def _gpu_stream_loop(self, server: ZeusdConfig, endpoint: str) -&gt; None:\n    \"\"\"Background thread: stream GPU power from a single server.\"\"\"\n    client = self._daemon_clients[endpoint]\n    base_url = client.url(\"/gpu/stream_power\")\n    if server.gpu_indices is not None:\n        ids_param = \",\".join(str(i) for i in server.gpu_indices)\n        url = f\"{base_url}?gpu_ids={ids_param}\"\n    else:\n        url = base_url\n    self._stream_loop(url, endpoint, self._process_gpu_event, \"GPU\")\n</code></pre>"},{"location":"reference/monitor/power_streaming/#zeus.monitor.power_streaming.PowerStreamingClient._cpu_stream_loop","title":"_cpu_stream_loop","text":"<pre><code>_cpu_stream_loop(server, endpoint)\n</code></pre> <p>Background thread: stream CPU power from a single server.</p> Source code in <code>zeus/monitor/power_streaming.py</code> <pre><code>def _cpu_stream_loop(self, server: ZeusdConfig, endpoint: str) -&gt; None:\n    \"\"\"Background thread: stream CPU power from a single server.\"\"\"\n    client = self._daemon_clients[endpoint]\n    base_url = client.url(\"/cpu/stream_power\")\n    if server.cpu_indices is not None:\n        ids_param = \",\".join(str(i) for i in server.cpu_indices)\n        url = f\"{base_url}?cpu_ids={ids_param}\"\n    else:\n        url = base_url\n    self._stream_loop(url, endpoint, self._process_cpu_event, \"CPU\")\n</code></pre>"},{"location":"reference/monitor/power_streaming/#zeus.monitor.power_streaming.PowerStreamingClient._stream_loop","title":"_stream_loop","text":"<pre><code>_stream_loop(url, endpoint, process_fn, label)\n</code></pre> <p>Shared reconnect loop for SSE streams.</p> Source code in <code>zeus/monitor/power_streaming.py</code> <pre><code>def _stream_loop(\n    self,\n    url: str,\n    endpoint: str,\n    process_fn: typing.Callable[[str, str], None],\n    label: str,\n) -&gt; None:\n    \"\"\"Shared reconnect loop for SSE streams.\"\"\"\n    while not self._stop_event.is_set():\n        try:\n            self._connect_and_stream(url, endpoint, process_fn)\n        except httpx.HTTPStatusError as e:\n            if e.response.status_code in (401, 403):\n                logger.error(\n                    \"%s SSE connection to %s failed with HTTP %d: %s\",\n                    label,\n                    endpoint,\n                    e.response.status_code,\n                    e.response.text,\n                )\n                return\n            if not self._stop_event.is_set():\n                logger.warning(\n                    \"%s SSE connection to %s rejected (HTTP %d), reconnecting in %.1fs\",\n                    label,\n                    endpoint,\n                    e.response.status_code,\n                    self._reconnect_delay,\n                    exc_info=True,\n                )\n                self._stop_event.wait(timeout=self._reconnect_delay)\n        except httpx.RequestError:\n            if not self._stop_event.is_set():\n                logger.warning(\n                    \"%s SSE connection to %s lost, reconnecting in %.1fs\",\n                    label,\n                    endpoint,\n                    self._reconnect_delay,\n                    exc_info=True,\n                )\n                self._stop_event.wait(timeout=self._reconnect_delay)\n</code></pre>"},{"location":"reference/monitor/power_streaming/#zeus.monitor.power_streaming.PowerStreamingClient._connect_and_stream","title":"_connect_and_stream","text":"<pre><code>_connect_and_stream(url, endpoint, process_fn)\n</code></pre> <p>Open an SSE connection and process events until disconnected.</p> Source code in <code>zeus/monitor/power_streaming.py</code> <pre><code>def _connect_and_stream(\n    self,\n    url: str,\n    endpoint: str,\n    process_fn: typing.Callable[[str, str], None],\n) -&gt; None:\n    \"\"\"Open an SSE connection and process events until disconnected.\"\"\"\n    client = self._daemon_clients[endpoint]\n    logger.info(\"Connecting to SSE at %s\", url)\n    with client.make_client() as http, http.stream(\"GET\", url, timeout=None) as response:\n        response.raise_for_status()\n        logger.info(\"SSE connected to %s\", url)\n        buffer = \"\"\n        for chunk in response.iter_text():\n            if self._stop_event.is_set():\n                return\n            buffer += chunk\n            while \"\\n\\n\" in buffer:\n                event_text, buffer = buffer.split(\"\\n\\n\", 1)\n                process_fn(event_text, endpoint)\n</code></pre>"},{"location":"reference/monitor/power_streaming/#zeus.monitor.power_streaming.PowerStreamingClient._process_gpu_event","title":"_process_gpu_event","text":"<pre><code>_process_gpu_event(event_text, endpoint)\n</code></pre> <p>Parse a GPU SSE event and update readings.</p> Source code in <code>zeus/monitor/power_streaming.py</code> <pre><code>def _process_gpu_event(self, event_text: str, endpoint: str) -&gt; None:\n    \"\"\"Parse a GPU SSE event and update readings.\"\"\"\n    for line in event_text.strip().split(\"\\n\"):\n        if line.startswith(\"data: \"):\n            data_str = line[6:]\n            try:\n                data = json.loads(data_str)\n            except json.JSONDecodeError:\n                logger.warning(\"Invalid JSON in GPU SSE event from %s: %s\", endpoint, data_str[:100])\n                continue\n\n            power_mw = data.get(\"power_mw\", {})\n            timestamp_ms = data.get(\"timestamp_ms\", 0)\n            timestamp_s = timestamp_ms / 1000.0 + self._clock_offsets[endpoint]\n\n            gpu_power_w: dict[int, float] = {}\n            for gpu_id_str, mw in power_mw.items():\n                gpu_power_w[int(gpu_id_str)] = float(mw) / 1000.0  # mW -&gt; W\n\n            with self._condition:\n                existing = self._readings.get(endpoint)\n                if existing is not None:\n                    existing.gpu_power_w = gpu_power_w\n                    existing.timestamp_s = max(existing.timestamp_s, timestamp_s)\n                else:\n                    self._readings[endpoint] = PowerReadings(\n                        timestamp_s=timestamp_s,\n                        gpu_power_w=gpu_power_w,\n                    )\n                self._condition.notify_all()\n</code></pre>"},{"location":"reference/monitor/power_streaming/#zeus.monitor.power_streaming.PowerStreamingClient._process_cpu_event","title":"_process_cpu_event","text":"<pre><code>_process_cpu_event(event_text, endpoint)\n</code></pre> <p>Parse a CPU SSE event and update readings.</p> <p>Expected JSON format: <code>{\"timestamp_ms\": N, \"power_mw\": {\"0\": {\"cpu_mw\": N, \"dram_mw\": N|null}}}</code>.</p> Source code in <code>zeus/monitor/power_streaming.py</code> <pre><code>def _process_cpu_event(self, event_text: str, endpoint: str) -&gt; None:\n    \"\"\"Parse a CPU SSE event and update readings.\n\n    Expected JSON format: `{\"timestamp_ms\": N, \"power_mw\": {\"0\": {\"cpu_mw\": N, \"dram_mw\": N|null}}}`.\n    \"\"\"\n    for line in event_text.strip().split(\"\\n\"):\n        if line.startswith(\"data: \"):\n            data_str = line[6:]\n            try:\n                data = json.loads(data_str)\n            except json.JSONDecodeError:\n                logger.warning(\"Invalid JSON in CPU SSE event from %s: %s\", endpoint, data_str[:100])\n                continue\n\n            power_mw = data.get(\"power_mw\", {})\n            timestamp_ms = data.get(\"timestamp_ms\", 0)\n            timestamp_s = timestamp_ms / 1000.0 + self._clock_offsets[endpoint]\n\n            cpu_power_w: dict[int, CpuPowerReading] = {}\n            for cpu_id_str, readings in power_mw.items():\n                cpu_mw = readings.get(\"cpu_mw\", 0)\n                dram_mw = readings.get(\"dram_mw\")\n                cpu_power_w[int(cpu_id_str)] = CpuPowerReading(\n                    cpu_w=float(cpu_mw) / 1000.0,\n                    dram_w=float(dram_mw) / 1000.0 if dram_mw is not None else None,\n                )\n\n            with self._condition:\n                existing = self._readings.get(endpoint)\n                if existing is not None:\n                    existing.cpu_power_w = cpu_power_w\n                    existing.timestamp_s = max(existing.timestamp_s, timestamp_s)\n                else:\n                    self._readings[endpoint] = PowerReadings(\n                        timestamp_s=timestamp_s,\n                        cpu_power_w=cpu_power_w,\n                    )\n                self._condition.notify_all()\n</code></pre>"},{"location":"reference/monitor/price/","title":"price","text":""},{"location":"reference/monitor/price/#zeus.monitor.price","title":"zeus.monitor.price","text":"<p>Electricity price providers used for price-aware optimizers.</p>"},{"location":"reference/monitor/price/#zeus.monitor.price.ZeusElectricityPriceHTTPError","title":"ZeusElectricityPriceHTTPError","text":"<p>               Bases: <code>ZeusBaseError</code></p> <p>Exception when HTTP request to electricity price provider fails.</p> Source code in <code>zeus/monitor/price.py</code> <pre><code>class ZeusElectricityPriceHTTPError(ZeusBaseError):\n    \"\"\"Exception when HTTP request to electricity price provider fails.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize HTTP request exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/monitor/price/#zeus.monitor.price.ZeusElectricityPriceHTTPError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/monitor/price.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize HTTP request exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/monitor/price/#zeus.monitor.price.ZeusElectricityPriceNotFoundError","title":"ZeusElectricityPriceNotFoundError","text":"<p>               Bases: <code>ZeusBaseError</code></p> <p>Exception when electricity price measurement could not be retrieved.</p> Source code in <code>zeus/monitor/price.py</code> <pre><code>class ZeusElectricityPriceNotFoundError(ZeusBaseError):\n    \"\"\"Exception when electricity price measurement could not be retrieved.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize price not found exception.\"\"\"\n        super().__init__(message)\n</code></pre>"},{"location":"reference/monitor/price/#zeus.monitor.price.ZeusElectricityPriceNotFoundError.__init__","title":"__init__","text":"<pre><code>__init__(message)\n</code></pre> Source code in <code>zeus/monitor/price.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize price not found exception.\"\"\"\n    super().__init__(message)\n</code></pre>"},{"location":"reference/monitor/price/#zeus.monitor.price.ElectricityPriceProvider","title":"ElectricityPriceProvider","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class for implementing ways to fetch electricity price.</p> Source code in <code>zeus/monitor/price.py</code> <pre><code>class ElectricityPriceProvider(abc.ABC):\n    \"\"\"Abstract class for implementing ways to fetch electricity price.\"\"\"\n\n    @abc.abstractmethod\n    def get_current_electricity_prices(self) -&gt; dict[str, list]:\n        \"\"\"Abstract method for fetching the current electricity price of the set location of the class.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/monitor/price/#zeus.monitor.price.ElectricityPriceProvider.get_current_electricity_prices","title":"get_current_electricity_prices  <code>abstractmethod</code>","text":"<pre><code>get_current_electricity_prices()\n</code></pre> <p>Abstract method for fetching the current electricity price of the set location of the class.</p> Source code in <code>zeus/monitor/price.py</code> <pre><code>@abc.abstractmethod\ndef get_current_electricity_prices(self) -&gt; dict[str, list]:\n    \"\"\"Abstract method for fetching the current electricity price of the set location of the class.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/monitor/price/#zeus.monitor.price.OpenEIClient","title":"OpenEIClient","text":"<p>               Bases: <code>ElectricityPriceProvider</code></p> <p>Electricity Price Provider with OpenEI API.</p> <p>Reference:</p> <ol> <li>OpenEI</li> <li>OpenEI Utility Rates API</li> </ol> Source code in <code>zeus/monitor/price.py</code> <pre><code>class OpenEIClient(ElectricityPriceProvider):\n    \"\"\"Electricity Price Provider with OpenEI API.\n\n    Reference:\n\n    1. [OpenEI](https://openei.org/wiki/Main_Page)\n    2. [OpenEI Utility Rates API](https://apps.openei.org/services/doc/rest/util_rates/?version=7)\n    \"\"\"\n\n    def __init__(\n        self,\n        location: tuple[float, float],\n        label: str,\n        sector: Literal[\"Residential\", \"Commercial\", \"Industrial\", \"Lighting\"] = \"Residential\",\n        radius: int = 0,\n    ) -&gt; None:\n        \"\"\"Initializes OpenEI Utility Rates Provider.\n\n        Args:\n            location: tuple of latitude and longitude (latitude, longitude)\n            label: unique identifier of a particular variant of a utility company's rate\n            sector: depends on which sector of electricity is relevant to you\n            radius: search radius for utility rates from the location\n        \"\"\"\n        self.lat, self.long = location\n        self.label = label\n        self.sector = sector\n        self.radius = radius\n\n    def search_json(self, data, key_name, target_value, return_value):\n        \"\"\"Recursively search for a key in a nested JSON and return the return_value field if found.\"\"\"\n        results = []\n\n        if isinstance(data, dict):\n            for key, val in data.items():\n                # Check if the current dictionary contains the matching key-value pair\n                if key == key_name and val == target_value:\n                    # If \"energyratestructure\" exists at the same level, add it to results\n                    if return_value in data:\n                        results.append(data[return_value])\n                    else:\n                        results.append(None)\n\n                # Recursively search deeper in nested dictionaries\n                results.extend(self.search_json(val, key_name, target_value, return_value))\n\n        elif isinstance(data, list):\n            for item in data:\n                results.extend(self.search_json(item, key_name, target_value, return_value))\n\n        return results\n\n    def get_current_electricity_prices(self) -&gt; dict[str, list]:\n        \"\"\"Fetches current carbon intensity of the location of the class.\"\"\"\n        try:\n            url = (\n                \"https://api.openei.org/utility_rates?version=latest&amp;format=json\"\n                + f\"&amp;api_key=tJASWWgPhBRpiZCwfhtKV2A3gyNxbDfvQvdI5Wa7&amp;lat={self.lat}\"\n                + f\"&amp;lon={self.long}&amp;radius={self.radius}\"\n                + f\"&amp;detail=full&amp;sector={self.sector}\"\n            )\n            resp = requests.get(url)\n            data = resp.json()\n\n        except requests.exceptions.RequestException as e:\n            raise ZeusElectricityPriceHTTPError(f\"Failed to retrieve current electricity price measurement: {e}\") from e\n\n        try:\n            if \"label\" not in json.dumps(data):\n                raise ZeusElectricityPriceNotFoundError(f\"No rates found for lat, lon: [{self.lat}, {self.long}].\")\n\n            energy_rate_structure = self.search_json(data, \"label\", self.label, \"energyratestructure\")\n            energy_weekday_schedule = self.search_json(data, \"label\", self.label, \"energyweekdayschedule\")\n            energy_weekend_schedule = self.search_json(data, \"label\", self.label, \"energyweekendschedule\")\n\n            if not energy_rate_structure or not energy_weekday_schedule or not energy_weekend_schedule:\n                raise ZeusElectricityPriceNotFoundError(f\"No rates found for the label: {self.label}.\")\n\n            rate_data = {\n                \"energy_rate_structure\": energy_rate_structure[0],\n                \"energy_weekday_schedule\": energy_weekday_schedule[0],\n                \"energy_weekend_schedule\": energy_weekend_schedule[0],\n            }\n            return rate_data\n\n        except (KeyError, ValueError) as e:\n            logger.error(\"Error occurred while processing electricity price data: %s\", e)\n            raise ZeusElectricityPriceNotFoundError(\"Failed to process electricity price data.\") from e\n</code></pre>"},{"location":"reference/monitor/price/#zeus.monitor.price.OpenEIClient.__init__","title":"__init__","text":"<pre><code>__init__(location, label, sector='Residential', radius=0)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>tuple[float, float]</code> <p>tuple of latitude and longitude (latitude, longitude)</p> required <code>label</code> <code>str</code> <p>unique identifier of a particular variant of a utility company's rate</p> required <code>sector</code> <code>Literal['Residential', 'Commercial', 'Industrial', 'Lighting']</code> <p>depends on which sector of electricity is relevant to you</p> <code>'Residential'</code> <code>radius</code> <code>int</code> <p>search radius for utility rates from the location</p> <code>0</code> Source code in <code>zeus/monitor/price.py</code> <pre><code>def __init__(\n    self,\n    location: tuple[float, float],\n    label: str,\n    sector: Literal[\"Residential\", \"Commercial\", \"Industrial\", \"Lighting\"] = \"Residential\",\n    radius: int = 0,\n) -&gt; None:\n    \"\"\"Initializes OpenEI Utility Rates Provider.\n\n    Args:\n        location: tuple of latitude and longitude (latitude, longitude)\n        label: unique identifier of a particular variant of a utility company's rate\n        sector: depends on which sector of electricity is relevant to you\n        radius: search radius for utility rates from the location\n    \"\"\"\n    self.lat, self.long = location\n    self.label = label\n    self.sector = sector\n    self.radius = radius\n</code></pre>"},{"location":"reference/monitor/price/#zeus.monitor.price.OpenEIClient.search_json","title":"search_json","text":"<pre><code>search_json(data, key_name, target_value, return_value)\n</code></pre> <p>Recursively search for a key in a nested JSON and return the return_value field if found.</p> Source code in <code>zeus/monitor/price.py</code> <pre><code>def search_json(self, data, key_name, target_value, return_value):\n    \"\"\"Recursively search for a key in a nested JSON and return the return_value field if found.\"\"\"\n    results = []\n\n    if isinstance(data, dict):\n        for key, val in data.items():\n            # Check if the current dictionary contains the matching key-value pair\n            if key == key_name and val == target_value:\n                # If \"energyratestructure\" exists at the same level, add it to results\n                if return_value in data:\n                    results.append(data[return_value])\n                else:\n                    results.append(None)\n\n            # Recursively search deeper in nested dictionaries\n            results.extend(self.search_json(val, key_name, target_value, return_value))\n\n    elif isinstance(data, list):\n        for item in data:\n            results.extend(self.search_json(item, key_name, target_value, return_value))\n\n    return results\n</code></pre>"},{"location":"reference/monitor/price/#zeus.monitor.price.OpenEIClient.get_current_electricity_prices","title":"get_current_electricity_prices","text":"<pre><code>get_current_electricity_prices()\n</code></pre> <p>Fetches current carbon intensity of the location of the class.</p> Source code in <code>zeus/monitor/price.py</code> <pre><code>def get_current_electricity_prices(self) -&gt; dict[str, list]:\n    \"\"\"Fetches current carbon intensity of the location of the class.\"\"\"\n    try:\n        url = (\n            \"https://api.openei.org/utility_rates?version=latest&amp;format=json\"\n            + f\"&amp;api_key=tJASWWgPhBRpiZCwfhtKV2A3gyNxbDfvQvdI5Wa7&amp;lat={self.lat}\"\n            + f\"&amp;lon={self.long}&amp;radius={self.radius}\"\n            + f\"&amp;detail=full&amp;sector={self.sector}\"\n        )\n        resp = requests.get(url)\n        data = resp.json()\n\n    except requests.exceptions.RequestException as e:\n        raise ZeusElectricityPriceHTTPError(f\"Failed to retrieve current electricity price measurement: {e}\") from e\n\n    try:\n        if \"label\" not in json.dumps(data):\n            raise ZeusElectricityPriceNotFoundError(f\"No rates found for lat, lon: [{self.lat}, {self.long}].\")\n\n        energy_rate_structure = self.search_json(data, \"label\", self.label, \"energyratestructure\")\n        energy_weekday_schedule = self.search_json(data, \"label\", self.label, \"energyweekdayschedule\")\n        energy_weekend_schedule = self.search_json(data, \"label\", self.label, \"energyweekendschedule\")\n\n        if not energy_rate_structure or not energy_weekday_schedule or not energy_weekend_schedule:\n            raise ZeusElectricityPriceNotFoundError(f\"No rates found for the label: {self.label}.\")\n\n        rate_data = {\n            \"energy_rate_structure\": energy_rate_structure[0],\n            \"energy_weekday_schedule\": energy_weekday_schedule[0],\n            \"energy_weekend_schedule\": energy_weekend_schedule[0],\n        }\n        return rate_data\n\n    except (KeyError, ValueError) as e:\n        logger.error(\"Error occurred while processing electricity price data: %s\", e)\n        raise ZeusElectricityPriceNotFoundError(\"Failed to process electricity price data.\") from e\n</code></pre>"},{"location":"reference/monitor/price/#zeus.monitor.price.EnergyCostMeasurement","title":"EnergyCostMeasurement  <code>dataclass</code>","text":"<p>Measurement result of one window.</p> <p>Attributes:</p> Name Type Description <code>time</code> <code>float</code> <p>Time elapsed (in seconds) during the measurement window.</p> <code>gpu_energy</code> <code>dict[int, float]</code> <p>Maps GPU indices to the energy consumed (in Joules) during the measurement window. GPU indices are from the DL framework's perspective after applying <code>CUDA_VISIBLE_DEVICES</code>.</p> <code>gpu_energy_cost</code> <code>dict[int, float]</code> <p>Maps GPU indices to the electricity cost (in $) during the measurement window. GPU indices are from the DL framework's perspective after applying <code>CUDA_VISIBLE_DEVICES</code>.</p> <code>cpu_energy</code> <code>dict[int, float] | None</code> <p>Maps CPU indices to the energy consumed (in Joules) during the measurement window. Each CPU index refers to one powerzone exposed by RAPL (intel-rapl:d). This can be 'None' if CPU measurement is not available.</p> <code>cpu_energy_cost</code> <code>dict[int, float] | None</code> <p>Maps CPU indices to the electricity cost (in $) during the measurement window. Each CPU index refers to one powerzone exposed by RAPL (intel-rapl:d). This can be 'None' if CPU measurement is not available.</p> <code>dram_energy</code> <code>dict[int, float] | None</code> <p>Maps CPU indices to the energy consumed (in Joules) during the measurement window. Each CPU index refers to one powerzone exposed by RAPL (intel-rapl:d) and DRAM measurements are taken from sub-packages within each powerzone. This can be 'None' if CPU measurement is not available or DRAM measurement is not available.</p> <code>dram_energy_cost</code> <code>dict[int, float] | None</code> <p>Maps CPU indices to the electricity cost (in $) during the measurement window. Each CPU index refers to one powerzone exposed by RAPL (intel-rapl:d). This can be 'None' if CPU measurement is not available or DRAM measurement is not available.</p> Source code in <code>zeus/monitor/price.py</code> <pre><code>@dataclass\nclass EnergyCostMeasurement:\n    \"\"\"Measurement result of one window.\n\n    Attributes:\n        time: Time elapsed (in seconds) during the measurement window.\n        gpu_energy: Maps GPU indices to the energy consumed (in Joules) during the\n            measurement window. GPU indices are from the DL framework's perspective\n            after applying `CUDA_VISIBLE_DEVICES`.\n        gpu_energy_cost: Maps GPU indices to the electricity cost (in $) during the\n            measurement window. GPU indices are from the DL framework's perspective\n            after applying `CUDA_VISIBLE_DEVICES`.\n        cpu_energy: Maps CPU indices to the energy consumed (in Joules) during the measurement\n            window. Each CPU index refers to one powerzone exposed by RAPL (intel-rapl:d). This can\n            be 'None' if CPU measurement is not available.\n        cpu_energy_cost: Maps CPU indices to the electricity cost (in $) during the measurement\n            window. Each CPU index refers to one powerzone exposed by RAPL (intel-rapl:d). This can\n            be 'None' if CPU measurement is not available.\n        dram_energy: Maps CPU indices to the energy consumed (in Joules) during the measurement\n            window. Each CPU index refers to one powerzone exposed by RAPL (intel-rapl:d) and DRAM\n            measurements are taken from sub-packages within each powerzone. This can be 'None' if\n            CPU measurement is not available or DRAM measurement is not available.\n        dram_energy_cost: Maps CPU indices to the electricity cost (in $) during the measurement\n            window. Each CPU index refers to one powerzone exposed by RAPL (intel-rapl:d). This can be 'None' if\n            CPU measurement is not available or DRAM measurement is not available.\n    \"\"\"\n\n    time: float\n    gpu_energy: dict[int, float]\n    gpu_energy_cost: dict[int, float]\n    cpu_energy: dict[int, float] | None = None\n    cpu_energy_cost: dict[int, float] | None = None\n    dram_energy: dict[int, float] | None = None\n    dram_energy_cost: dict[int, float] | None = None\n</code></pre>"},{"location":"reference/monitor/price/#zeus.monitor.price.Op","title":"Op","text":"<p>               Bases: <code>Enum</code></p> <p>Enum used to communicate between EnergyCostMonitor and _polling_process.</p> Source code in <code>zeus/monitor/price.py</code> <pre><code>class Op(Enum):\n    \"\"\"Enum used to communicate between EnergyCostMonitor and _polling_process.\"\"\"\n\n    BEGIN = 0\n    END = 1\n    NEXTITER = 2\n</code></pre>"},{"location":"reference/monitor/price/#zeus.monitor.price.EnergyCostMonitor","title":"EnergyCostMonitor","text":"<p>Measure the energy, energy cost, and time consumption of a block of code.</p> <p>Works for multi-GPU and heterogeneous GPU types. Aware of <code>CUDA_VISIBLE_DEVICES</code>. For instance, if <code>CUDA_VISIBLE_DEVICES=2,3</code>, GPU index <code>1</code> passed into <code>gpu_indices</code> will be interpreted as CUDA device <code>3</code>.</p> <p>You can mark the beginning and end of a measurement window, during which the energy cost, GPU energy, and time consumed will be recorded. Multiple concurrent measurement windows are supported.</p> <p>Warning</p> <p>This monitor uses multiprocessing with the spawn start method to measure energy and electricity cost in a background process. Spawned processes re-import your main module, so keep heavy setup under <code>if __name__ == \"__main__\":</code> or inside functions. See also the \"Safe importing of main module\" section in the Python documentation.</p> Source code in <code>zeus/monitor/price.py</code> <pre><code>class EnergyCostMonitor:\n    \"\"\"Measure the energy, energy cost, and time consumption of a block of code.\n\n    Works for multi-GPU and heterogeneous GPU types. Aware of `CUDA_VISIBLE_DEVICES`.\n    For instance, if `CUDA_VISIBLE_DEVICES=2,3`, GPU index `1` passed into `gpu_indices`\n    will be interpreted as CUDA device `3`.\n\n    You can mark the beginning and end of a measurement window, during which the energy cost,\n    GPU energy, and time consumed will be recorded. Multiple concurrent measurement windows\n    are supported.\n\n    !!! Warning\n        This monitor uses multiprocessing with the spawn start method to measure energy and electricity cost\n        in a background process. Spawned processes re-import your main module, so keep\n        heavy setup under `if __name__ == \"__main__\":` or inside functions.\n        See also the \"Safe importing of main module\" section in the [Python documentation](\n        https://docs.python.org/3/library/multiprocessing.html#the-spawn-and-forkserver-start-methods).\n    \"\"\"\n\n    def __init__(\n        self,\n        electricity_price_provider: ElectricityPriceProvider,\n        gpu_indices: list[int] | None = None,\n        cpu_indices: list[int] | None = None,\n        sync_execution_with: Literal[\"torch\", \"jax\", \"cupy\"] = \"torch\",\n    ) -&gt; None:\n        \"\"\"Initializes Energy Cost Monitor.\n\n        Args:\n            electricity_price_provider: provider for which electricity price values will be fetched from\n            gpu_indices: Indices of all the CUDA devices to monitor. Time/Energy measurements\n                will begin and end at the same time for these GPUs (i.e., synchronized).\n                If None, all the GPUs available will be used. `CUDA_VISIBLE_DEVICES`\n                is respected if set, e.g., GPU index `1` passed into `gpu_indices` when\n                `CUDA_VISIBLE_DEVICES=2,3` will be interpreted as CUDA device `3`.\n                `CUDA_VISIBLE_DEVICES`s formatted with comma-separated indices are supported.\n            cpu_indices: Indices of the CPU packages to monitor. If None, all CPU packages will\n                be used.\n            sync_execution_with: Deep learning framework to use to synchronize CPU/GPU computations.\n                Defaults to `\"torch\"`, in which case `torch.cuda.synchronize` will be used.\n                See [`sync_execution`][zeus.utils.framework.sync_execution] for more details.\n        \"\"\"\n        # Warn if instantiated as a global variable in a subprocess.\n        warn_if_global_in_subprocess(self)\n\n        self.zeus_monitor = ZeusMonitor(\n            gpu_indices=gpu_indices,\n            cpu_indices=cpu_indices,\n            sync_execution_with=sync_execution_with,\n            approx_instant_energy=True,\n        )\n        self.electricity_price_provider = electricity_price_provider\n        self.current_keys = set()\n\n        # set up process and shared queues\n        self.context = mp.get_context(\"spawn\")\n        self.command_q = self.context.Queue()\n        self.finished_q = self.context.Queue()\n\n    def begin_window(self, key: str, sync_execution: bool = True) -&gt; None:\n        \"\"\"Begin a new measurement window.\n\n        Args:\n            key: Unique name of the measurement window.\n            sync_execution: Whether to wait for asynchronously dispatched computations\n                to finish before starting the measurement window. For instance, PyTorch\n                and JAX will run GPU computations asynchronously, and waiting them to\n                finish is necessary to ensure that the measurement window captures all\n                and only the computations dispatched within the window.\n        \"\"\"\n        # check if key is already used\n        if key in self.current_keys:\n            raise ValueError(f\"Measurement window '{key}' already exists\")\n        self.current_keys.add(key)\n\n        # start window\n        self.zeus_monitor.begin_window(key, sync_execution=sync_execution)\n\n        # if there were previously no active windows, start polling process\n        if len(self.current_keys) == 1:\n            self.polling_process = self.context.Process(\n                target=_polling_process,\n                args=(\n                    self.command_q,\n                    self.finished_q,\n                    self.zeus_monitor.gpu_indices,\n                    self.zeus_monitor.cpu_indices,\n                    self.electricity_price_provider,\n                ),\n            )\n            self.polling_process.start()\n\n        # start subwindows\n        self.command_q.put((Op.BEGIN, key))\n\n    def end_window(self, key: str, sync_execution: bool = True) -&gt; EnergyCostMeasurement:\n        \"\"\"End a measurement window and return the time, energy consumption, and energy cost.\n\n        Args:\n            key: Name of an active measurement window.\n            sync_execution: Whether to wait for asynchronously dispatched computations\n                to finish before starting the measurement window. For instance, PyTorch\n                and JAX will run GPU computations asynchronously, and waiting them to\n                finish is necessary to ensure that the measurement window captures all\n                and only the computations dispatched within the window.\n        \"\"\"\n        # check if begin_window has been called with key before\n        if key not in self.current_keys:\n            raise ValueError(f\"Measurement window '{key}' does not exist\")\n\n        # end window\n        self.command_q.put((Op.END, key))\n        (\n            gpu_energy_cost,\n            cpu_energy_cost,\n            dram_energy_cost,\n        ) = self.finished_q.get()\n        self.current_keys.remove(key)\n\n        overall_measurement = self.zeus_monitor.end_window(key, sync_execution=sync_execution)\n\n        measurement = EnergyCostMeasurement(\n            time=overall_measurement.time,\n            gpu_energy=overall_measurement.gpu_energy,\n            cpu_energy=overall_measurement.cpu_energy,\n            dram_energy=overall_measurement.dram_energy,\n            gpu_energy_cost=gpu_energy_cost,\n            cpu_energy_cost=cpu_energy_cost or None,\n            dram_energy_cost=dram_energy_cost or None,\n        )\n\n        return measurement\n</code></pre>"},{"location":"reference/monitor/price/#zeus.monitor.price.EnergyCostMonitor.__init__","title":"__init__","text":"<pre><code>__init__(electricity_price_provider, gpu_indices=None, cpu_indices=None, sync_execution_with='torch')\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>electricity_price_provider</code> <code>ElectricityPriceProvider</code> <p>provider for which electricity price values will be fetched from</p> required <code>gpu_indices</code> <code>list[int] | None</code> <p>Indices of all the CUDA devices to monitor. Time/Energy measurements will begin and end at the same time for these GPUs (i.e., synchronized). If None, all the GPUs available will be used. <code>CUDA_VISIBLE_DEVICES</code> is respected if set, e.g., GPU index <code>1</code> passed into <code>gpu_indices</code> when <code>CUDA_VISIBLE_DEVICES=2,3</code> will be interpreted as CUDA device <code>3</code>. <code>CUDA_VISIBLE_DEVICES</code>s formatted with comma-separated indices are supported.</p> <code>None</code> <code>cpu_indices</code> <code>list[int] | None</code> <p>Indices of the CPU packages to monitor. If None, all CPU packages will be used.</p> <code>None</code> <code>sync_execution_with</code> <code>Literal['torch', 'jax', 'cupy']</code> <p>Deep learning framework to use to synchronize CPU/GPU computations. Defaults to <code>\"torch\"</code>, in which case <code>torch.cuda.synchronize</code> will be used. See <code>sync_execution</code> for more details.</p> <code>'torch'</code> Source code in <code>zeus/monitor/price.py</code> <pre><code>def __init__(\n    self,\n    electricity_price_provider: ElectricityPriceProvider,\n    gpu_indices: list[int] | None = None,\n    cpu_indices: list[int] | None = None,\n    sync_execution_with: Literal[\"torch\", \"jax\", \"cupy\"] = \"torch\",\n) -&gt; None:\n    \"\"\"Initializes Energy Cost Monitor.\n\n    Args:\n        electricity_price_provider: provider for which electricity price values will be fetched from\n        gpu_indices: Indices of all the CUDA devices to monitor. Time/Energy measurements\n            will begin and end at the same time for these GPUs (i.e., synchronized).\n            If None, all the GPUs available will be used. `CUDA_VISIBLE_DEVICES`\n            is respected if set, e.g., GPU index `1` passed into `gpu_indices` when\n            `CUDA_VISIBLE_DEVICES=2,3` will be interpreted as CUDA device `3`.\n            `CUDA_VISIBLE_DEVICES`s formatted with comma-separated indices are supported.\n        cpu_indices: Indices of the CPU packages to monitor. If None, all CPU packages will\n            be used.\n        sync_execution_with: Deep learning framework to use to synchronize CPU/GPU computations.\n            Defaults to `\"torch\"`, in which case `torch.cuda.synchronize` will be used.\n            See [`sync_execution`][zeus.utils.framework.sync_execution] for more details.\n    \"\"\"\n    # Warn if instantiated as a global variable in a subprocess.\n    warn_if_global_in_subprocess(self)\n\n    self.zeus_monitor = ZeusMonitor(\n        gpu_indices=gpu_indices,\n        cpu_indices=cpu_indices,\n        sync_execution_with=sync_execution_with,\n        approx_instant_energy=True,\n    )\n    self.electricity_price_provider = electricity_price_provider\n    self.current_keys = set()\n\n    # set up process and shared queues\n    self.context = mp.get_context(\"spawn\")\n    self.command_q = self.context.Queue()\n    self.finished_q = self.context.Queue()\n</code></pre>"},{"location":"reference/monitor/price/#zeus.monitor.price.EnergyCostMonitor.begin_window","title":"begin_window","text":"<pre><code>begin_window(key, sync_execution=True)\n</code></pre> <p>Begin a new measurement window.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Unique name of the measurement window.</p> required <code>sync_execution</code> <code>bool</code> <p>Whether to wait for asynchronously dispatched computations to finish before starting the measurement window. For instance, PyTorch and JAX will run GPU computations asynchronously, and waiting them to finish is necessary to ensure that the measurement window captures all and only the computations dispatched within the window.</p> <code>True</code> Source code in <code>zeus/monitor/price.py</code> <pre><code>def begin_window(self, key: str, sync_execution: bool = True) -&gt; None:\n    \"\"\"Begin a new measurement window.\n\n    Args:\n        key: Unique name of the measurement window.\n        sync_execution: Whether to wait for asynchronously dispatched computations\n            to finish before starting the measurement window. For instance, PyTorch\n            and JAX will run GPU computations asynchronously, and waiting them to\n            finish is necessary to ensure that the measurement window captures all\n            and only the computations dispatched within the window.\n    \"\"\"\n    # check if key is already used\n    if key in self.current_keys:\n        raise ValueError(f\"Measurement window '{key}' already exists\")\n    self.current_keys.add(key)\n\n    # start window\n    self.zeus_monitor.begin_window(key, sync_execution=sync_execution)\n\n    # if there were previously no active windows, start polling process\n    if len(self.current_keys) == 1:\n        self.polling_process = self.context.Process(\n            target=_polling_process,\n            args=(\n                self.command_q,\n                self.finished_q,\n                self.zeus_monitor.gpu_indices,\n                self.zeus_monitor.cpu_indices,\n                self.electricity_price_provider,\n            ),\n        )\n        self.polling_process.start()\n\n    # start subwindows\n    self.command_q.put((Op.BEGIN, key))\n</code></pre>"},{"location":"reference/monitor/price/#zeus.monitor.price.EnergyCostMonitor.end_window","title":"end_window","text":"<pre><code>end_window(key, sync_execution=True)\n</code></pre> <p>End a measurement window and return the time, energy consumption, and energy cost.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of an active measurement window.</p> required <code>sync_execution</code> <code>bool</code> <p>Whether to wait for asynchronously dispatched computations to finish before starting the measurement window. For instance, PyTorch and JAX will run GPU computations asynchronously, and waiting them to finish is necessary to ensure that the measurement window captures all and only the computations dispatched within the window.</p> <code>True</code> Source code in <code>zeus/monitor/price.py</code> <pre><code>def end_window(self, key: str, sync_execution: bool = True) -&gt; EnergyCostMeasurement:\n    \"\"\"End a measurement window and return the time, energy consumption, and energy cost.\n\n    Args:\n        key: Name of an active measurement window.\n        sync_execution: Whether to wait for asynchronously dispatched computations\n            to finish before starting the measurement window. For instance, PyTorch\n            and JAX will run GPU computations asynchronously, and waiting them to\n            finish is necessary to ensure that the measurement window captures all\n            and only the computations dispatched within the window.\n    \"\"\"\n    # check if begin_window has been called with key before\n    if key not in self.current_keys:\n        raise ValueError(f\"Measurement window '{key}' does not exist\")\n\n    # end window\n    self.command_q.put((Op.END, key))\n    (\n        gpu_energy_cost,\n        cpu_energy_cost,\n        dram_energy_cost,\n    ) = self.finished_q.get()\n    self.current_keys.remove(key)\n\n    overall_measurement = self.zeus_monitor.end_window(key, sync_execution=sync_execution)\n\n    measurement = EnergyCostMeasurement(\n        time=overall_measurement.time,\n        gpu_energy=overall_measurement.gpu_energy,\n        cpu_energy=overall_measurement.cpu_energy,\n        dram_energy=overall_measurement.dram_energy,\n        gpu_energy_cost=gpu_energy_cost,\n        cpu_energy_cost=cpu_energy_cost or None,\n        dram_energy_cost=dram_energy_cost or None,\n    )\n\n    return measurement\n</code></pre>"},{"location":"reference/monitor/price/#zeus.monitor.price.get_time_info","title":"get_time_info","text":"<pre><code>get_time_info()\n</code></pre> <p>Retrieve the month, day_type (weekend or weekday), and hour.</p> Source code in <code>zeus/monitor/price.py</code> <pre><code>def get_time_info() -&gt; tuple[str, str, int]:\n    \"\"\"Retrieve the month, day_type (weekend or weekday), and hour.\"\"\"\n    now = datetime.now()\n\n    month = now.strftime(\"%B\")\n\n    day_type = \"Weekend\" if now.weekday() &gt;= 5 else \"Weekday\"\n\n    hour = now.hour\n\n    return month, day_type, hour\n</code></pre>"},{"location":"reference/monitor/temperature/","title":"temperature","text":""},{"location":"reference/monitor/temperature/#zeus.monitor.temperature","title":"zeus.monitor.temperature","text":"<p>Monitor the temperature of GPUs.</p>"},{"location":"reference/monitor/temperature/#zeus.monitor.temperature.TemperatureSample","title":"TemperatureSample  <code>dataclass</code>","text":"<p>A single temperature measurement sample.</p> Source code in <code>zeus/monitor/temperature.py</code> <pre><code>@dataclass\nclass TemperatureSample:\n    \"\"\"A single temperature measurement sample.\"\"\"\n\n    timestamp: float\n    gpu_index: int\n    temperature_c: int\n</code></pre>"},{"location":"reference/monitor/temperature/#zeus.monitor.temperature.TemperatureMonitor","title":"TemperatureMonitor","text":"<p>Monitor GPU temperature over time.</p> <p>This class provides:</p> <ol> <li>Continuous temperature monitoring in a background process</li> <li>Timeline export with deduplication</li> <li>Point-in-time temperature queries</li> </ol> <p>Note</p> <p>The current implementation only supports cases where all GPUs are homogeneous (i.e., the same model).</p> <p>Warning</p> <p>This monitor uses multiprocessing with the spawn start method to poll temperature in a background process. Spawned processes re-import your main module, so keep heavy setup under <code>if __name__ == \"__main__\":</code> or inside functions. See also the \"Safe importing of main module\" section in the Python documentation.</p> Source code in <code>zeus/monitor/temperature.py</code> <pre><code>class TemperatureMonitor:\n    \"\"\"Monitor GPU temperature over time.\n\n    This class provides:\n\n    1. Continuous temperature monitoring in a background process\n    2. Timeline export with deduplication\n    3. Point-in-time temperature queries\n\n    !!! Note\n        The current implementation only supports cases where all GPUs are homogeneous\n        (i.e., the same model).\n\n    !!! Warning\n        This monitor uses multiprocessing with the spawn start method to poll temperature\n        in a background process. Spawned processes re-import your main module, so keep\n        heavy setup under `if __name__ == \"__main__\":` or inside functions.\n        See also the \"Safe importing of main module\" section in the [Python documentation](\n        https://docs.python.org/3/library/multiprocessing.html#the-spawn-and-forkserver-start-methods).\n    \"\"\"\n\n    def __init__(\n        self,\n        gpu_indices: list[int] | None = None,\n        update_period: float = 1.0,\n        max_samples_per_gpu: int | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the temperature monitor.\n\n        Args:\n            gpu_indices: Indices of the GPUs to monitor. If None, monitor all GPUs.\n            update_period: Update period of the temperature monitor in seconds.\n                Defaults to 1.0 second. Temperature typically doesn't change as\n                rapidly as power, so a longer update period is reasonable.\n            max_samples_per_gpu: Maximum number of temperature samples to keep per GPU\n                in memory. If None (default), unlimited samples are kept.\n        \"\"\"\n        # Warn if instantiated as a global variable in a subprocess.\n        warn_if_global_in_subprocess(self)\n\n        if gpu_indices is not None and not gpu_indices:\n            raise ValueError(\"`gpu_indices` must be either `None` or non-empty\")\n\n        # Get GPUs\n        gpus = get_gpus(ensure_homogeneous=True)\n\n        # Configure GPU indices\n        self.gpu_indices = gpu_indices if gpu_indices is not None else list(range(len(gpus)))\n        if not self.gpu_indices:\n            raise ValueError(\"At least one GPU index must be specified\")\n        logger.info(\"Monitoring temperature of GPUs %s\", self.gpu_indices)\n\n        self.update_period = update_period\n\n        # Temperature samples are collected for each device index.\n        self.temperature_samples: dict[int, collections.deque[TemperatureSample]] = {}\n        for gpu_idx in self.gpu_indices:\n            self.temperature_samples[gpu_idx] = collections.deque(maxlen=max_samples_per_gpu)\n\n        # Spawn temperature collector process\n        ctx = mp.get_context(\"spawn\")\n        self.temperature_queue = ctx.Queue()\n        self.temperature_ready_event = ctx.Event()\n        self.temperature_stop_event = ctx.Event()\n        self.temperature_process = ctx.Process(\n            target=_temperature_polling_process,\n            kwargs=dict(\n                gpu_indices=self.gpu_indices,\n                data_queue=self.temperature_queue,\n                ready_event=self.temperature_ready_event,\n                stop_event=self.temperature_stop_event,\n                update_period=update_period,\n            ),\n            daemon=True,\n            name=\"zeus-temperature-monitor\",\n        )\n        self.temperature_process.start()\n\n        # Cleanup function\n        self._finalizer = weakref.finalize(\n            self,\n            _cleanup_temperature_process,\n            self.temperature_stop_event,\n            self.temperature_process,\n        )\n\n        # Wait for subprocess to signal it's ready\n        logger.info(\"Waiting for temperature monitoring subprocess to be ready...\")\n        if not self.temperature_ready_event.wait(timeout=10.0):\n            logger.warning(\"Temperature monitor subprocess did not signal ready within timeout\")\n        logger.info(\"Temperature monitoring subprocess is ready\")\n\n    def stop(self) -&gt; None:\n        \"\"\"Stop the monitoring process.\"\"\"\n        if self._finalizer.alive:\n            self._finalizer()\n\n    def _process_temperature_queue_data(self) -&gt; None:\n        \"\"\"Process all pending temperature samples from the queue.\"\"\"\n        if not hasattr(self, \"temperature_queue\"):\n            return\n\n        while True:\n            try:\n                sample = self.temperature_queue.get_nowait()\n                if sample == \"STOP\":\n                    break\n                assert isinstance(sample, TemperatureSample)\n                self.temperature_samples[sample.gpu_index].append(sample)\n            except Empty:\n                break\n\n    def get_temperature_timeline(\n        self,\n        gpu_index: int | None = None,\n        start_time: float | None = None,\n        end_time: float | None = None,\n    ) -&gt; dict[int, list[tuple[float, int]]]:\n        \"\"\"Get temperature timeline for specific GPU(s).\n\n        Args:\n            gpu_index: Specific GPU index, or None for all GPUs\n            start_time: Start time filter (unix timestamp)\n            end_time: End time filter (unix timestamp)\n\n        Returns:\n            Dictionary mapping GPU indices to timeline data.\n            Timeline data is list of (timestamp, temperature_celsius) tuples.\n        \"\"\"\n        # Process any pending queue data\n        self._process_temperature_queue_data()\n\n        # Determine which GPUs to query\n        target_gpus = [gpu_index] if gpu_index is not None else self.gpu_indices\n\n        result = {}\n        for gpu_idx in target_gpus:\n            if gpu_idx not in self.temperature_samples:\n                continue\n\n            # Extract timeline from samples\n            timeline = []\n            for sample in self.temperature_samples[gpu_idx]:\n                # Apply time filters\n                if start_time is not None and sample.timestamp &lt; start_time:\n                    continue\n                if end_time is not None and sample.timestamp &gt; end_time:\n                    continue\n\n                timeline.append((sample.timestamp, sample.temperature_c))\n\n            # Sort by timestamp\n            timeline.sort(key=lambda x: x[0])\n            result[gpu_idx] = timeline\n\n        return result\n\n    def get_temperature(self, time: float | None = None) -&gt; dict[int, int] | None:\n        \"\"\"Get the GPU temperature at a specific time point.\n\n        Args:\n            time: Time point to get the temperature at. If None, get the temperature\n                at the last recorded time point.\n\n        Returns:\n            A dictionary mapping GPU indices to the temperature of the GPU at the\n            specified time point. If there are no temperature readings, return None.\n        \"\"\"\n        # Process any pending queue data\n        self._process_temperature_queue_data()\n\n        result = {}\n        for gpu_idx in self.gpu_indices:\n            samples = self.temperature_samples[gpu_idx]\n            if not samples:\n                return None\n\n            if time is None:\n                # Get the most recent sample\n                latest_sample = samples[-1]\n                result[gpu_idx] = latest_sample.temperature_c\n            else:\n                # Find the closest sample to the requested time using bisect\n                timestamps = [sample.timestamp for sample in samples]\n                pos = bisect.bisect_left(timestamps, time)\n\n                if pos == 0:\n                    closest_sample = samples[0]\n                elif pos == len(samples):\n                    closest_sample = samples[-1]\n                else:\n                    # Check the closest sample before and after the requested time\n                    before = samples[pos - 1]\n                    after = samples[pos]\n                    closest_sample = before if time - before.timestamp &lt;= after.timestamp - time else after\n                result[gpu_idx] = closest_sample.temperature_c\n\n        return result\n</code></pre>"},{"location":"reference/monitor/temperature/#zeus.monitor.temperature.TemperatureMonitor.__init__","title":"__init__","text":"<pre><code>__init__(gpu_indices=None, update_period=1.0, max_samples_per_gpu=None)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>gpu_indices</code> <code>list[int] | None</code> <p>Indices of the GPUs to monitor. If None, monitor all GPUs.</p> <code>None</code> <code>update_period</code> <code>float</code> <p>Update period of the temperature monitor in seconds. Defaults to 1.0 second. Temperature typically doesn't change as rapidly as power, so a longer update period is reasonable.</p> <code>1.0</code> <code>max_samples_per_gpu</code> <code>int | None</code> <p>Maximum number of temperature samples to keep per GPU in memory. If None (default), unlimited samples are kept.</p> <code>None</code> Source code in <code>zeus/monitor/temperature.py</code> <pre><code>def __init__(\n    self,\n    gpu_indices: list[int] | None = None,\n    update_period: float = 1.0,\n    max_samples_per_gpu: int | None = None,\n) -&gt; None:\n    \"\"\"Initialize the temperature monitor.\n\n    Args:\n        gpu_indices: Indices of the GPUs to monitor. If None, monitor all GPUs.\n        update_period: Update period of the temperature monitor in seconds.\n            Defaults to 1.0 second. Temperature typically doesn't change as\n            rapidly as power, so a longer update period is reasonable.\n        max_samples_per_gpu: Maximum number of temperature samples to keep per GPU\n            in memory. If None (default), unlimited samples are kept.\n    \"\"\"\n    # Warn if instantiated as a global variable in a subprocess.\n    warn_if_global_in_subprocess(self)\n\n    if gpu_indices is not None and not gpu_indices:\n        raise ValueError(\"`gpu_indices` must be either `None` or non-empty\")\n\n    # Get GPUs\n    gpus = get_gpus(ensure_homogeneous=True)\n\n    # Configure GPU indices\n    self.gpu_indices = gpu_indices if gpu_indices is not None else list(range(len(gpus)))\n    if not self.gpu_indices:\n        raise ValueError(\"At least one GPU index must be specified\")\n    logger.info(\"Monitoring temperature of GPUs %s\", self.gpu_indices)\n\n    self.update_period = update_period\n\n    # Temperature samples are collected for each device index.\n    self.temperature_samples: dict[int, collections.deque[TemperatureSample]] = {}\n    for gpu_idx in self.gpu_indices:\n        self.temperature_samples[gpu_idx] = collections.deque(maxlen=max_samples_per_gpu)\n\n    # Spawn temperature collector process\n    ctx = mp.get_context(\"spawn\")\n    self.temperature_queue = ctx.Queue()\n    self.temperature_ready_event = ctx.Event()\n    self.temperature_stop_event = ctx.Event()\n    self.temperature_process = ctx.Process(\n        target=_temperature_polling_process,\n        kwargs=dict(\n            gpu_indices=self.gpu_indices,\n            data_queue=self.temperature_queue,\n            ready_event=self.temperature_ready_event,\n            stop_event=self.temperature_stop_event,\n            update_period=update_period,\n        ),\n        daemon=True,\n        name=\"zeus-temperature-monitor\",\n    )\n    self.temperature_process.start()\n\n    # Cleanup function\n    self._finalizer = weakref.finalize(\n        self,\n        _cleanup_temperature_process,\n        self.temperature_stop_event,\n        self.temperature_process,\n    )\n\n    # Wait for subprocess to signal it's ready\n    logger.info(\"Waiting for temperature monitoring subprocess to be ready...\")\n    if not self.temperature_ready_event.wait(timeout=10.0):\n        logger.warning(\"Temperature monitor subprocess did not signal ready within timeout\")\n    logger.info(\"Temperature monitoring subprocess is ready\")\n</code></pre>"},{"location":"reference/monitor/temperature/#zeus.monitor.temperature.TemperatureMonitor.stop","title":"stop","text":"<pre><code>stop()\n</code></pre> <p>Stop the monitoring process.</p> Source code in <code>zeus/monitor/temperature.py</code> <pre><code>def stop(self) -&gt; None:\n    \"\"\"Stop the monitoring process.\"\"\"\n    if self._finalizer.alive:\n        self._finalizer()\n</code></pre>"},{"location":"reference/monitor/temperature/#zeus.monitor.temperature.TemperatureMonitor._process_temperature_queue_data","title":"_process_temperature_queue_data","text":"<pre><code>_process_temperature_queue_data()\n</code></pre> <p>Process all pending temperature samples from the queue.</p> Source code in <code>zeus/monitor/temperature.py</code> <pre><code>def _process_temperature_queue_data(self) -&gt; None:\n    \"\"\"Process all pending temperature samples from the queue.\"\"\"\n    if not hasattr(self, \"temperature_queue\"):\n        return\n\n    while True:\n        try:\n            sample = self.temperature_queue.get_nowait()\n            if sample == \"STOP\":\n                break\n            assert isinstance(sample, TemperatureSample)\n            self.temperature_samples[sample.gpu_index].append(sample)\n        except Empty:\n            break\n</code></pre>"},{"location":"reference/monitor/temperature/#zeus.monitor.temperature.TemperatureMonitor.get_temperature_timeline","title":"get_temperature_timeline","text":"<pre><code>get_temperature_timeline(gpu_index=None, start_time=None, end_time=None)\n</code></pre> <p>Get temperature timeline for specific GPU(s).</p> <p>Parameters:</p> Name Type Description Default <code>gpu_index</code> <code>int | None</code> <p>Specific GPU index, or None for all GPUs</p> <code>None</code> <code>start_time</code> <code>float | None</code> <p>Start time filter (unix timestamp)</p> <code>None</code> <code>end_time</code> <code>float | None</code> <p>End time filter (unix timestamp)</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[int, list[tuple[float, int]]]</code> <p>Dictionary mapping GPU indices to timeline data.</p> <code>dict[int, list[tuple[float, int]]]</code> <p>Timeline data is list of (timestamp, temperature_celsius) tuples.</p> Source code in <code>zeus/monitor/temperature.py</code> <pre><code>def get_temperature_timeline(\n    self,\n    gpu_index: int | None = None,\n    start_time: float | None = None,\n    end_time: float | None = None,\n) -&gt; dict[int, list[tuple[float, int]]]:\n    \"\"\"Get temperature timeline for specific GPU(s).\n\n    Args:\n        gpu_index: Specific GPU index, or None for all GPUs\n        start_time: Start time filter (unix timestamp)\n        end_time: End time filter (unix timestamp)\n\n    Returns:\n        Dictionary mapping GPU indices to timeline data.\n        Timeline data is list of (timestamp, temperature_celsius) tuples.\n    \"\"\"\n    # Process any pending queue data\n    self._process_temperature_queue_data()\n\n    # Determine which GPUs to query\n    target_gpus = [gpu_index] if gpu_index is not None else self.gpu_indices\n\n    result = {}\n    for gpu_idx in target_gpus:\n        if gpu_idx not in self.temperature_samples:\n            continue\n\n        # Extract timeline from samples\n        timeline = []\n        for sample in self.temperature_samples[gpu_idx]:\n            # Apply time filters\n            if start_time is not None and sample.timestamp &lt; start_time:\n                continue\n            if end_time is not None and sample.timestamp &gt; end_time:\n                continue\n\n            timeline.append((sample.timestamp, sample.temperature_c))\n\n        # Sort by timestamp\n        timeline.sort(key=lambda x: x[0])\n        result[gpu_idx] = timeline\n\n    return result\n</code></pre>"},{"location":"reference/monitor/temperature/#zeus.monitor.temperature.TemperatureMonitor.get_temperature","title":"get_temperature","text":"<pre><code>get_temperature(time=None)\n</code></pre> <p>Get the GPU temperature at a specific time point.</p> <p>Parameters:</p> Name Type Description Default <code>time</code> <code>float | None</code> <p>Time point to get the temperature at. If None, get the temperature at the last recorded time point.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[int, int] | None</code> <p>A dictionary mapping GPU indices to the temperature of the GPU at the</p> <code>dict[int, int] | None</code> <p>specified time point. If there are no temperature readings, return None.</p> Source code in <code>zeus/monitor/temperature.py</code> <pre><code>def get_temperature(self, time: float | None = None) -&gt; dict[int, int] | None:\n    \"\"\"Get the GPU temperature at a specific time point.\n\n    Args:\n        time: Time point to get the temperature at. If None, get the temperature\n            at the last recorded time point.\n\n    Returns:\n        A dictionary mapping GPU indices to the temperature of the GPU at the\n        specified time point. If there are no temperature readings, return None.\n    \"\"\"\n    # Process any pending queue data\n    self._process_temperature_queue_data()\n\n    result = {}\n    for gpu_idx in self.gpu_indices:\n        samples = self.temperature_samples[gpu_idx]\n        if not samples:\n            return None\n\n        if time is None:\n            # Get the most recent sample\n            latest_sample = samples[-1]\n            result[gpu_idx] = latest_sample.temperature_c\n        else:\n            # Find the closest sample to the requested time using bisect\n            timestamps = [sample.timestamp for sample in samples]\n            pos = bisect.bisect_left(timestamps, time)\n\n            if pos == 0:\n                closest_sample = samples[0]\n            elif pos == len(samples):\n                closest_sample = samples[-1]\n            else:\n                # Check the closest sample before and after the requested time\n                before = samples[pos - 1]\n                after = samples[pos]\n                closest_sample = before if time - before.timestamp &lt;= after.timestamp - time else after\n            result[gpu_idx] = closest_sample.temperature_c\n\n    return result\n</code></pre>"},{"location":"reference/monitor/temperature/#zeus.monitor.temperature._cleanup_temperature_process","title":"_cleanup_temperature_process","text":"<pre><code>_cleanup_temperature_process(stop_event, process)\n</code></pre> <p>Idempotent cleanup function for temperature monitoring process.</p> Source code in <code>zeus/monitor/temperature.py</code> <pre><code>def _cleanup_temperature_process(\n    stop_event: EventClass,\n    process: SpawnProcess,\n) -&gt; None:\n    \"\"\"Idempotent cleanup function for temperature monitoring process.\"\"\"\n    # Signal the process to stop\n    stop_event.set()\n\n    # Wait for the process to complete\n    if process.is_alive():\n        process.join(timeout=2.0)\n        if process.is_alive():\n            process.terminate()\n            process.join(timeout=1.0)\n            if process.is_alive():\n                process.kill()\n                process.join(timeout=1.0)\n</code></pre>"},{"location":"reference/monitor/temperature/#zeus.monitor.temperature._temperature_polling_process","title":"_temperature_polling_process","text":"<pre><code>_temperature_polling_process(gpu_indices, data_queue, ready_event, stop_event, update_period)\n</code></pre> <p>Polling process for GPU temperature with deduplication.</p> Source code in <code>zeus/monitor/temperature.py</code> <pre><code>def _temperature_polling_process(\n    gpu_indices: list[int],\n    data_queue: mp.Queue,\n    ready_event: EventClass,\n    stop_event: EventClass,\n    update_period: float,\n) -&gt; None:\n    \"\"\"Polling process for GPU temperature with deduplication.\"\"\"\n    try:\n        # Get GPUs\n        gpus = get_gpus()\n\n        # Track previous temperature values for deduplication\n        prev_temperature: dict[int, int] = {}\n\n        # Signal that this process is ready to start monitoring\n        ready_event.set()\n\n        # Start polling loop\n        while not stop_event.is_set():\n            timestamp = time()\n\n            for gpu_index in gpu_indices:\n                try:\n                    temperature_c = gpus.get_gpu_temperature(gpu_index)\n\n                    # Deduplication: only send if temperature changed\n                    if gpu_index in prev_temperature and prev_temperature[gpu_index] == temperature_c:\n                        continue\n\n                    prev_temperature[gpu_index] = temperature_c\n\n                    # Create and send temperature sample\n                    sample = TemperatureSample(\n                        timestamp=timestamp,\n                        gpu_index=gpu_index,\n                        temperature_c=temperature_c,\n                    )\n\n                    data_queue.put(sample)\n                except ZeusGPUNotSupportedError as e:\n                    logger.warning(\n                        \"GPU %d temperature reading not supported: %s\",\n                        gpu_index,\n                        e,\n                    )\n                    # Don't keep trying if it's not supported\n                    break\n                except Exception as e:\n                    logger.exception(\n                        \"Error polling temperature for GPU %d: %s\",\n                        gpu_index,\n                        e,\n                    )\n                    raise\n\n            # Sleep for the remaining time\n            elapsed = time() - timestamp\n            sleep_time = update_period - elapsed\n            if sleep_time &gt; 0:\n                sleep(sleep_time)\n\n    except KeyboardInterrupt:\n        pass\n    except Exception as e:\n        logger.exception(\n            \"Exiting temperature polling process due to error: %s\",\n            e,\n        )\n        raise e\n    finally:\n        # Send stop signal\n        data_queue.put(\"STOP\")\n</code></pre>"},{"location":"reference/optimizer/","title":"optimizer","text":""},{"location":"reference/optimizer/#zeus.optimizer","title":"zeus.optimizer","text":"<p>Zeus energy optimizers.</p>"},{"location":"reference/optimizer/power_limit/","title":"power_limit","text":""},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit","title":"zeus.optimizer.power_limit","text":"<p>Optimizers that select the optimum power limit.</p> <p>This module contains the following pieces:</p> <ul> <li><code>GlobalPowerLimitOptimizer</code>   is the main class that implements the state machine   and the logic for profiling power limits and selecting   the optimum power limit.</li> <li><code>PowerLimitMeasurement</code> and various   state classes are helpers that support the state machine.</li> <li><code>OptimumSelector</code>   is an abstract base class for selecting the optimum power limit   from a list of power limit profiling results. There are concrete classes   that implement different selection strategies, like   minimizing energy,   minimizing time,   minimizing the Zeus time-energy cost,   or selecting the lowest power limit that meets the given maximum training time slowdown factor.</li> <li><code>HFGlobalPowerLimitOptimizer</code>   is a wrapper for the Hugging Face <code>TrainerCallback</code> class that uses <code>GlobalPowerLimitOptimizer</code>.</li> </ul>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.OptimumSelector","title":"OptimumSelector","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for optimum power limit selectors.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>class OptimumSelector(ABC):\n    \"\"\"Base class for optimum power limit selectors.\"\"\"\n\n    @abstractmethod\n    def select(self, measurements: list[PowerLimitMeasurement]) -&gt; int:\n        \"\"\"Select the optimal power limit (W) from measurements.\"\"\"\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.OptimumSelector.select","title":"select  <code>abstractmethod</code>","text":"<pre><code>select(measurements)\n</code></pre> <p>Select the optimal power limit (W) from measurements.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>@abstractmethod\ndef select(self, measurements: list[PowerLimitMeasurement]) -&gt; int:\n    \"\"\"Select the optimal power limit (W) from measurements.\"\"\"\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.Energy","title":"Energy","text":"<p>               Bases: <code>OptimumSelector</code></p> <p>Selects the power limit that minimizes energy consumption.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>class Energy(OptimumSelector):\n    \"\"\"Selects the power limit that minimizes energy consumption.\"\"\"\n\n    def select(self, measurements: list[PowerLimitMeasurement]) -&gt; int:\n        \"\"\"Select the optimal power limit (W) from measurements.\"\"\"\n        return min(measurements, key=lambda x: x.energy).power_limit\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.Energy.select","title":"select","text":"<pre><code>select(measurements)\n</code></pre> <p>Select the optimal power limit (W) from measurements.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def select(self, measurements: list[PowerLimitMeasurement]) -&gt; int:\n    \"\"\"Select the optimal power limit (W) from measurements.\"\"\"\n    return min(measurements, key=lambda x: x.energy).power_limit\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.Time","title":"Time","text":"<p>               Bases: <code>OptimumSelector</code></p> <p>Selects the power limit that minimizes training time.</p> <p>This may not necessarily choose the maximum power limit, as time profiling results can be slightly noisy. However, we believe that's actually better because it means that training time is very similar among higher power limits, but lower power limit will consume less power.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>class Time(OptimumSelector):\n    \"\"\"Selects the power limit that minimizes training time.\n\n    This may not necessarily choose the maximum power limit, as time profiling\n    results can be slightly noisy. However, we believe that's actually better\n    because it means that training time is very similar among higher power limits,\n    but lower power limit will consume less power.\n    \"\"\"\n\n    def select(self, measurements: list[PowerLimitMeasurement]) -&gt; int:\n        \"\"\"Select the optimal power limit (W) from measurements.\"\"\"\n        return min(measurements, key=lambda x: x.time).power_limit\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.Time.select","title":"select","text":"<pre><code>select(measurements)\n</code></pre> <p>Select the optimal power limit (W) from measurements.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def select(self, measurements: list[PowerLimitMeasurement]) -&gt; int:\n    \"\"\"Select the optimal power limit (W) from measurements.\"\"\"\n    return min(measurements, key=lambda x: x.time).power_limit\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.ZeusCost","title":"ZeusCost","text":"<p>               Bases: <code>OptimumSelector</code></p> <p>Selects the power limit that minimizes a linear Zeus time-energy cost function.</p> <p>Cost function is \\(\\eta \\cdot \\text{Energy} + (1 - \\eta) \\cdot \\text{MaxPower} \\cdot \\text{Time}\\).</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>class ZeusCost(OptimumSelector):\n    r\"\"\"Selects the power limit that minimizes a linear Zeus time-energy cost function.\n\n    Cost function is $\\eta \\cdot \\text{Energy} + (1 - \\eta) \\cdot \\text{MaxPower} \\cdot \\text{Time}$.\n    \"\"\"\n\n    def __init__(self, eta_knob: float, world_size: int = 1) -&gt; None:\n        r\"\"\"Initialize the selector.\n\n        Args:\n            eta_knob: The $0 \\le \\eta \\le 1$ knob for the Zeus time-energy cost function.\n            world_size: The number of GPUs in the training job. Defaults to 1.\n        \"\"\"\n        if eta_knob &lt; 0 or eta_knob &gt; 1:\n            raise ValueError(\"eta_knob must be between 0 and 1, inclusive both sides.\")\n        if world_size &lt; 1:\n            raise ValueError(\"world_size must be greater than or equal to 1.\")\n\n        self.eta_knob = eta_knob\n        self.world_size = world_size\n\n    def select(self, measurements: list[PowerLimitMeasurement]) -&gt; int:\n        \"\"\"Select the optimal power limit (W) from measurements.\"\"\"\n        max_power = max(measurement.power_limit for measurement in measurements) * self.world_size\n        zeus_cost_map = {\n            measurement.power_limit: zeus_cost(\n                energy=measurement.energy,\n                time=measurement.time,\n                eta_knob=self.eta_knob,\n                max_power=max_power,\n            )\n            for measurement in measurements\n        }\n        return min(zeus_cost_map, key=lambda x: zeus_cost_map[x])\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.ZeusCost.__init__","title":"__init__","text":"<pre><code>__init__(eta_knob, world_size=1)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>eta_knob</code> <code>float</code> <p>The \\(0 \\le \\eta \\le 1\\) knob for the Zeus time-energy cost function.</p> required <code>world_size</code> <code>int</code> <p>The number of GPUs in the training job. Defaults to 1.</p> <code>1</code> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def __init__(self, eta_knob: float, world_size: int = 1) -&gt; None:\n    r\"\"\"Initialize the selector.\n\n    Args:\n        eta_knob: The $0 \\le \\eta \\le 1$ knob for the Zeus time-energy cost function.\n        world_size: The number of GPUs in the training job. Defaults to 1.\n    \"\"\"\n    if eta_knob &lt; 0 or eta_knob &gt; 1:\n        raise ValueError(\"eta_knob must be between 0 and 1, inclusive both sides.\")\n    if world_size &lt; 1:\n        raise ValueError(\"world_size must be greater than or equal to 1.\")\n\n    self.eta_knob = eta_knob\n    self.world_size = world_size\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.ZeusCost.select","title":"select","text":"<pre><code>select(measurements)\n</code></pre> <p>Select the optimal power limit (W) from measurements.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def select(self, measurements: list[PowerLimitMeasurement]) -&gt; int:\n    \"\"\"Select the optimal power limit (W) from measurements.\"\"\"\n    max_power = max(measurement.power_limit for measurement in measurements) * self.world_size\n    zeus_cost_map = {\n        measurement.power_limit: zeus_cost(\n            energy=measurement.energy,\n            time=measurement.time,\n            eta_knob=self.eta_knob,\n            max_power=max_power,\n        )\n        for measurement in measurements\n    }\n    return min(zeus_cost_map, key=lambda x: zeus_cost_map[x])\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.MaxSlowdownConstraint","title":"MaxSlowdownConstraint","text":"<p>               Bases: <code>OptimumSelector</code></p> <p>Selects the minumum power limit that does not slow down training by more than the given factor.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>class MaxSlowdownConstraint(OptimumSelector):\n    \"\"\"Selects the minumum power limit that does not slow down training by more than the given factor.\"\"\"\n\n    def __init__(self, factor: float) -&gt; None:\n        \"\"\"Initialize the selector.\n\n        Args:\n            factor: The maximum allowed slowdown factor. Greater than or equal to 1.0.\n        \"\"\"\n        if factor &lt; 1.0:\n            raise ValueError(\n                f\"max_slowdown_factor must be greater than or equal to 1.0. Got {factor}.\",\n            )\n\n        self.factor = factor\n\n    def select(self, measurements: list[PowerLimitMeasurement]) -&gt; int:\n        \"\"\"Select the optimal power limit (W) from measurements.\"\"\"\n        feasible_power_limits = []\n        max_power = max(measurement.power_limit for measurement in measurements)\n        shortest_time = next(measurement.time for measurement in measurements if measurement.power_limit == max_power)\n        for measurement in measurements:\n            if measurement.time &lt;= self.factor * shortest_time:\n                feasible_power_limits.append(measurement.power_limit)\n        return min(feasible_power_limits)\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.MaxSlowdownConstraint.__init__","title":"__init__","text":"<pre><code>__init__(factor)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>factor</code> <code>float</code> <p>The maximum allowed slowdown factor. Greater than or equal to 1.0.</p> required Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def __init__(self, factor: float) -&gt; None:\n    \"\"\"Initialize the selector.\n\n    Args:\n        factor: The maximum allowed slowdown factor. Greater than or equal to 1.0.\n    \"\"\"\n    if factor &lt; 1.0:\n        raise ValueError(\n            f\"max_slowdown_factor must be greater than or equal to 1.0. Got {factor}.\",\n        )\n\n    self.factor = factor\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.MaxSlowdownConstraint.select","title":"select","text":"<pre><code>select(measurements)\n</code></pre> <p>Select the optimal power limit (W) from measurements.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def select(self, measurements: list[PowerLimitMeasurement]) -&gt; int:\n    \"\"\"Select the optimal power limit (W) from measurements.\"\"\"\n    feasible_power_limits = []\n    max_power = max(measurement.power_limit for measurement in measurements)\n    shortest_time = next(measurement.time for measurement in measurements if measurement.power_limit == max_power)\n    for measurement in measurements:\n        if measurement.time &lt;= self.factor * shortest_time:\n            feasible_power_limits.append(measurement.power_limit)\n    return min(feasible_power_limits)\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.Ready","title":"Ready","text":"<p>               Bases: <code>BaseModel</code></p> <p>State for when we are ready to start measuring the next power limit.</p> <p>Initial state of the state machine if no previous profiling results were given. <code>Ready</code> -&gt; <code>Warmup</code> after <code>step</code>'th <code>on_step_begin</code>.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>class Ready(BaseModel):\n    \"\"\"State for when we are ready to start measuring the next power limit.\n\n    Initial state of the state machine if no previous profiling results were given.\n    `Ready` -&gt; `Warmup` after `step`'th `on_step_begin`.\n    \"\"\"\n\n    next_power_limit: PositiveInt\n    steps: PositiveInt\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.Warmup","title":"Warmup","text":"<p>               Bases: <code>BaseModel</code></p> <p>State for when we are warming up for a power limit.</p> <p><code>Warmup</code> -&gt; <code>Profiling</code> on the <code>steps</code>'th <code>on_step_begin</code>. <code>Warmup</code> -&gt; <code>Ready</code> on <code>on_epoch_end</code> before <code>steps</code>'th <code>on_step_begin</code>.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>class Warmup(BaseModel):\n    \"\"\"State for when we are warming up for a power limit.\n\n    `Warmup` -&gt; `Profiling` on the `steps`'th `on_step_begin`.\n    `Warmup` -&gt; `Ready` on `on_epoch_end` before `steps`'th `on_step_begin`.\n    \"\"\"\n\n    current_power_limit: PositiveInt\n    steps: PositiveInt\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.Profiling","title":"Profiling","text":"<p>               Bases: <code>BaseModel</code></p> <p>State for when we are profiling a power limit.</p> <p><code>Profiling</code> -&gt; <code>Warmup</code> after <code>steps</code>'th <code>on_step_begin</code> and     there are still power limits left to profile. <code>Profiling</code> -&gt; <code>Done</code> after <code>steps</code>'th <code>on_step_begin</code> and     there are no more power limits left to profile. <code>Profiling</code> -&gt; <code>Ready</code> on <code>on_epoch_end</code> before <code>steps</code>'th <code>on_step_begin</code>.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>class Profiling(BaseModel):\n    \"\"\"State for when we are profiling a power limit.\n\n    `Profiling` -&gt; `Warmup` after `steps`'th `on_step_begin` and\n        there are still power limits left to profile.\n    `Profiling` -&gt; `Done` after `steps`'th `on_step_begin` and\n        there are no more power limits left to profile.\n    `Profiling` -&gt; `Ready` on `on_epoch_end` before `steps`'th `on_step_begin`.\n    \"\"\"\n\n    current_power_limit: PositiveInt\n    steps: PositiveInt\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.Done","title":"Done","text":"<p>               Bases: <code>BaseModel</code></p> <p>State for when we are done profiling all power limits.</p> <p>Initial state of the state machine if previous profiling results were given. Final state of the state machine in any case.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>class Done(BaseModel):\n    \"\"\"State for when we are done profiling all power limits.\n\n    Initial state of the state machine if previous profiling results were given.\n    Final state of the state machine in any case.\n    \"\"\"\n\n    optimal_power_limit: PositiveInt\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.PowerLimitMeasurement","title":"PowerLimitMeasurement","text":"<p>               Bases: <code>BaseModel</code></p> <p>POD for GPU energy and time measurements for one power limit (W).</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>class PowerLimitMeasurement(BaseModel):\n    \"\"\"POD for GPU energy and time measurements for one power limit (W).\"\"\"\n\n    power_limit: PositiveInt  # In Watts.\n    energy: PositiveFloat\n    time: PositiveFloat\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit._PowerLimitMeasurementList","title":"_PowerLimitMeasurementList","text":"<p>               Bases: <code>BaseModel</code></p> <p>Proxy class to save and load a list of <code>PowerLimitMeasurement</code>s.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>class _PowerLimitMeasurementList(BaseModel):\n    \"\"\"Proxy class to save and load a list of `PowerLimitMeasurement`s.\"\"\"\n\n    measurements: list[PowerLimitMeasurement]\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.GlobalPowerLimitOptimizer","title":"GlobalPowerLimitOptimizer","text":"<p>               Bases: <code>Callback</code></p> <p>Optimizer for the power limit knob.</p> <p>This optimizer uses the JIT profiling log to determine the optimal power limit.</p>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.GlobalPowerLimitOptimizer--usage-with-distributed-data-parallelism","title":"Usage with distributed data parallelism","text":"<p>The global power limit optimizer expects one process to control each GPU used for training. For instance, <code>torchrun</code> will automatically spawn one process for each GPU on the node. Correspondingly, the <code>ZeusMonitor</code> instance passed in should be monitoring one GPU: the one being managed by the current process. The index of this GPU would typically match the local rank of the process. In the case of PyTorch, users would have called <code>torch.cuda.set_device</code> early on, so <code>torch.cuda.current_device</code> will give you the GPU index. <code>GlobalPowerLimitOptimizer</code> will internally do an AllReduce across all GPUs to aggregate time and energy measurements, and then select the globally optimal power limit.</p> <pre><code>monitor = ZeusMonitor(gpu_indices=[local_rank])  # pass in local rank to gpu_indices.\nplo = GlobalPowerLimitOptimizer(monitor)\n</code></pre> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>class GlobalPowerLimitOptimizer(Callback):\n    \"\"\"Optimizer for the power limit knob.\n\n    This optimizer uses the JIT profiling log to determine the optimal power limit.\n\n    ## Usage with distributed data parallelism\n\n    The global power limit optimizer expects one process to control each GPU used for training.\n    For instance, `torchrun` will automatically spawn one process for each GPU on the node.\n    Correspondingly, the [`ZeusMonitor`][zeus.monitor.energy.ZeusMonitor] instance passed in\n    should be monitoring **one GPU**: the one being managed by the current process. The index of\n    this GPU would typically match the local rank of the process. In the case of PyTorch, users would have\n    called `torch.cuda.set_device` early on, so `torch.cuda.current_device` will give you the GPU index.\n    `GlobalPowerLimitOptimizer` will internally do an AllReduce across all GPUs to aggregate\n    time and energy measurements, and then select the globally optimal power limit.\n\n\n    ```python\n    monitor = ZeusMonitor(gpu_indices=[local_rank])  # pass in local rank to gpu_indices.\n    plo = GlobalPowerLimitOptimizer(monitor)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        monitor: ZeusMonitor,\n        optimum_selector: OptimumSelector | None = None,\n        wait_steps: int = 1,\n        warmup_steps: int = 10,\n        profile_steps: int = 40,\n        pl_step: int = 25,\n        profile_path: str | Path | None = None,\n    ) -&gt; None:\n        r\"\"\"Initialize the optimizer.\n\n        GPU indices to profile and optimize for are taken from `monitor.gpu_indices`.\n\n        Args:\n            monitor: `ZeusMonitor` instance used to profile GPU time and energy consumption.\n            optimum_selector: The optimum selector to use. If not given, use `ZeusCost` with \\eta=0.5.\n            wait_steps: Number of steps to pass by before doing anything at the beginning.\n                Useful if you have something like `torch.backends.cudnn.benchmark=True`,\n                because the first iteration won't be representative of the rest of the iterations.\n            warmup_steps: Number of warmup iterations for each power limit.\n            profile_steps: Number of profie iterations for each power limit.\n            pl_step: The stride between power limits to explore, in unites of Watts.\n            profile_path: If the path points to an existing file, load the profile from the file\n                and do not run any profiling. If the path points to a non-existing file, profile\n                and save the profile to the file. If `None`, do not save or load any profile.\n        \"\"\"\n        # Sanity checks.\n        if wait_steps &lt; 0:\n            raise ValueError(\"wait_steps must be non-negative.\")\n        if warmup_steps &lt; 0:\n            raise ValueError(\"warmup_steps must be non-negative.\")\n        if profile_steps &lt;= 0:\n            raise ValueError(\"profile_steps must be positive.\")\n        if pl_step &lt;= 0:\n            raise ValueError(\"pl_step must be positive.\")\n\n        self.monitor = monitor\n        self.optimum_selector = optimum_selector or ZeusCost(\n            eta_knob=0.5,\n            world_size=len(monitor.gpu_indices),\n        )\n        self.warmup_steps = warmup_steps\n        self.profile_steps = profile_steps\n        self.pl_step = pl_step * 1000  # Internally, we use milliWatts.\n        self.profile_path = Path(profile_path) if isinstance(profile_path, str) else profile_path\n\n        gpus = get_gpus(ensure_homogeneous=True)\n\n        # Warn if distributed training is enabled with multiple GPUs monitored.\n        if is_distributed() and len(monitor.gpu_indices) &gt; 1:\n            logger.warning(\n                \"Distributed training is enabled with %d GPUs monitored. \"\n                \"For distributed training, it is recommended to monitor only one GPU per `ZeusMonitor` instance \"\n                \"since `GlobalPowerLimitOptimizer` performs an all-reduce operation internally over all devices.\",\n                len(monitor.gpu_indices),\n            )\n\n        # Set the range of power limits to explore.\n        # Assert that supported power limits ranges are uniform across GPUs.\n        pls = []\n        for index in monitor.gpu_indices:\n            pls.append(gpus.get_power_management_limit_constraints(index))\n        if not all(pls[0] == pl for pl in pls):\n            raise ValueError(\"Power limits ranges are not uniform across GPUs.\")\n        self.power_limits = list(range(pls[0][1], pls[0][0] - 1, -self.pl_step))\n\n        # Turn on persistence mode and set to the highest power limit.\n        try:\n            for index in monitor.gpu_indices:\n                gpus.set_persistence_mode(index, enabled=True, block=True)\n        except ZeusGPUNoPermissionError as ze:\n            raise RuntimeError(\n                \"SYS_ADMIN capability is required to modify GPU power limits. See \"\n                \"https://ml.energy/zeus/getting_started/#system-privileges \"\n                \"for more information.\"\n            ) from ze\n        self.current_power_limit = 0\n\n        # Store `Measurement` objects in a list, one for each power limit.\n        self.measurements: list[PowerLimitMeasurement] = []\n\n        # State for the profiler state machine.\n        self.state: Ready | Warmup | Profiling | Done\n\n        # Initialize JIT profiling states.\n        if self.profile_path is None:\n            logger.info(\"JIT profiling enabled.\")\n            logger.info(\"Will wait %d step(s) before profiling.\", wait_steps)\n            self.state = Ready(next_power_limit=self.power_limits[0], steps=wait_steps + 1)\n            logger.info(\"Set power limit to the maximum before starting.\")\n            self._set_power_limit(max(self.power_limits))\n        elif not self.profile_path.exists():\n            logger.info(\n                \"JIT Profiling enabled. Profile will be saved to '%s'.\",\n                str(self.profile_path),\n            )\n            logger.info(\"Will wait %d step(s) before profiling.\", wait_steps)\n            self.state = Ready(next_power_limit=self.power_limits[0], steps=wait_steps + 1)\n            logger.info(\"Set power limit to the maximum before starting.\")\n            self._set_power_limit(max(self.power_limits))\n        else:\n            self.measurements = _PowerLimitMeasurementList.parse_file(\n                self.profile_path,\n            ).measurements\n            # self.measurements = _PowerLimitMeasurementList.model_validate_json(\n            #     open(self.profile_path).read(),\n            #     strict=True,\n            # ).measurements\n            logger.info(\"Loaded previous profiling results from '%s'.\", str(self.profile_path))\n            optimal_power_limit = self._compute_optimal_power_limit()\n            logger.info(\"Optimal power limit is %d W.\", optimal_power_limit // 1000)\n            self.state = Done(optimal_power_limit=optimal_power_limit)\n            self._set_power_limit(self.state.optimal_power_limit)\n\n        # Restore all GPUs back to their maximum power limit on exit.\n        atexit.register(lambda: self._set_power_limit(max(self.power_limits)))\n\n    def on_epoch_end(self) -&gt; None:\n        \"\"\"Mark the end of a training epoch.\"\"\"\n        if isinstance(self.state, Ready):\n            pass\n\n        elif isinstance(self.state, (Warmup, Profiling)):\n            # Warmup/Profiling stage interrupted by the end of an epoch.\n            logger.info(\n                \"%s phase for %d W interrupted by the end of a training epoch.\",\n                type(self.state).__name__,\n                self.state.current_power_limit // 1000,\n            )\n            if isinstance(self.state, Profiling):\n                self.monitor.end_window(\n                    f\"__GlobalPowerLimitOptimizer_{self.state.current_power_limit // 1000}\",\n                    cancel=True,\n                )\n            self.state = Ready(next_power_limit=self.state.current_power_limit, steps=1)\n            self._set_power_limit(max(self.power_limits))\n\n        elif isinstance(self.state, Done):\n            pass\n\n    def on_step_begin(self) -&gt; None:\n        \"\"\"Mark the beginning of a training step.\"\"\"\n        if isinstance(self.state, Ready):\n            self.state.steps -= 1\n            if self.state.steps == 0:\n                logger.info(\n                    \"Starting warmup for power limit %d W.\",\n                    self.state.next_power_limit // 1000,\n                )\n                self._set_power_limit(self.state.next_power_limit)\n                self.state = Warmup(\n                    current_power_limit=self.state.next_power_limit,\n                    steps=self.warmup_steps,\n                )\n\n        elif isinstance(self.state, Warmup):\n            self.state.steps -= 1\n            if self.state.steps == 0:\n                logger.info(\n                    \"Starting actual profiling for power limit %d W.\",\n                    self.state.current_power_limit // 1000,\n                )\n                self.state = Profiling(\n                    current_power_limit=self.state.current_power_limit,\n                    steps=self.profile_steps,\n                )\n                self.monitor.begin_window(\n                    f\"__GlobalPowerLimitOptimizer_{self.state.current_power_limit // 1000}\",\n                )\n\n        elif isinstance(self.state, Profiling):\n            self.state.steps -= 1\n            if self.state.steps == 0:\n                measurement = self.monitor.end_window(\n                    f\"__GlobalPowerLimitOptimizer_{self.state.current_power_limit // 1000}\",\n                )\n                logger.info(\n                    \"Finished profiling for power limit %d W.\",\n                    self.state.current_power_limit // 1000,\n                )\n\n                self.measurements.append(\n                    PowerLimitMeasurement(\n                        power_limit=self.state.current_power_limit // 1000,\n                        energy=sum(all_reduce(list(measurement.gpu_energy.values()), operation=\"sum\")),\n                        time=max(all_reduce([measurement.time], operation=\"max\")),\n                    )\n                )\n                # If we're done profiling all power limits, compute the optimal\n                # power limit and transition to the Done state. Otherwise, move\n                # on to the Warmup phase for the next power limit.\n                current_power_limit_index = self.power_limits.index(self.state.current_power_limit)\n                if current_power_limit_index == len(self.power_limits) - 1:\n                    self.state = Done(\n                        optimal_power_limit=self._compute_optimal_power_limit(),\n                    )\n                    self._set_power_limit(self.state.optimal_power_limit)\n                    self._save_profile()\n                else:\n                    next_power_limit = self.power_limits[current_power_limit_index + 1]\n                    logger.info(\n                        \"Starting warmup for power limit %d W.\",\n                        next_power_limit // 1000,\n                    )\n                    self._set_power_limit(next_power_limit)\n                    self.state = Warmup(\n                        current_power_limit=next_power_limit,\n                        steps=self.warmup_steps,\n                    )\n\n        elif isinstance(self.state, Done):\n            pass\n\n    def _set_power_limit(self, power_limit: int) -&gt; None:\n        \"\"\"Set the power limit for all GPUs.\n\n        Args:\n            power_limit: The power limit to set, in milliWatts.\n        \"\"\"\n        gpus = get_gpus()\n        logger.info(\"Setting power limit to %d W.\", power_limit // 1000)\n        if self.current_power_limit == power_limit:\n            return\n        for index in self.monitor.gpu_indices:\n            gpus.set_power_management_limit(index, power_limit)\n        self.current_power_limit = power_limit\n\n    def _compute_optimal_power_limit(self) -&gt; int:\n        \"\"\"Compute the optimal power limit in milliWatts.\"\"\"\n        optimal_power_limit = self.optimum_selector.select(self.measurements) * 1000\n        logger.info(\"Optimal power limit is %d W.\", optimal_power_limit // 1000)\n        return optimal_power_limit\n\n    def _save_profile(self) -&gt; None:\n        \"\"\"Save JIT profiling results and the optimal power limit to a JSON file.\"\"\"\n        if self.profile_path is None:\n            return\n\n        assert isinstance(self.state, Done)\n        with self.profile_path.open(\"w\", encoding=\"utf-8\") as f:\n            f.write(\n                _PowerLimitMeasurementList(measurements=self.measurements).json(indent=4),\n            )\n        logger.info(\"JIT profiling results saved to '%s'.\", str(self.profile_path))\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.GlobalPowerLimitOptimizer.__init__","title":"__init__","text":"<pre><code>__init__(monitor, optimum_selector=None, wait_steps=1, warmup_steps=10, profile_steps=40, pl_step=25, profile_path=None)\n</code></pre> <p>GPU indices to profile and optimize for are taken from <code>monitor.gpu_indices</code>.</p> <p>Parameters:</p> Name Type Description Default <code>monitor</code> <code>ZeusMonitor</code> <p><code>ZeusMonitor</code> instance used to profile GPU time and energy consumption.</p> required <code>optimum_selector</code> <code>OptimumSelector | None</code> <p>The optimum selector to use. If not given, use <code>ZeusCost</code> with \\eta=0.5.</p> <code>None</code> <code>wait_steps</code> <code>int</code> <p>Number of steps to pass by before doing anything at the beginning. Useful if you have something like <code>torch.backends.cudnn.benchmark=True</code>, because the first iteration won't be representative of the rest of the iterations.</p> <code>1</code> <code>warmup_steps</code> <code>int</code> <p>Number of warmup iterations for each power limit.</p> <code>10</code> <code>profile_steps</code> <code>int</code> <p>Number of profie iterations for each power limit.</p> <code>40</code> <code>pl_step</code> <code>int</code> <p>The stride between power limits to explore, in unites of Watts.</p> <code>25</code> <code>profile_path</code> <code>str | Path | None</code> <p>If the path points to an existing file, load the profile from the file and do not run any profiling. If the path points to a non-existing file, profile and save the profile to the file. If <code>None</code>, do not save or load any profile.</p> <code>None</code> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def __init__(\n    self,\n    monitor: ZeusMonitor,\n    optimum_selector: OptimumSelector | None = None,\n    wait_steps: int = 1,\n    warmup_steps: int = 10,\n    profile_steps: int = 40,\n    pl_step: int = 25,\n    profile_path: str | Path | None = None,\n) -&gt; None:\n    r\"\"\"Initialize the optimizer.\n\n    GPU indices to profile and optimize for are taken from `monitor.gpu_indices`.\n\n    Args:\n        monitor: `ZeusMonitor` instance used to profile GPU time and energy consumption.\n        optimum_selector: The optimum selector to use. If not given, use `ZeusCost` with \\eta=0.5.\n        wait_steps: Number of steps to pass by before doing anything at the beginning.\n            Useful if you have something like `torch.backends.cudnn.benchmark=True`,\n            because the first iteration won't be representative of the rest of the iterations.\n        warmup_steps: Number of warmup iterations for each power limit.\n        profile_steps: Number of profie iterations for each power limit.\n        pl_step: The stride between power limits to explore, in unites of Watts.\n        profile_path: If the path points to an existing file, load the profile from the file\n            and do not run any profiling. If the path points to a non-existing file, profile\n            and save the profile to the file. If `None`, do not save or load any profile.\n    \"\"\"\n    # Sanity checks.\n    if wait_steps &lt; 0:\n        raise ValueError(\"wait_steps must be non-negative.\")\n    if warmup_steps &lt; 0:\n        raise ValueError(\"warmup_steps must be non-negative.\")\n    if profile_steps &lt;= 0:\n        raise ValueError(\"profile_steps must be positive.\")\n    if pl_step &lt;= 0:\n        raise ValueError(\"pl_step must be positive.\")\n\n    self.monitor = monitor\n    self.optimum_selector = optimum_selector or ZeusCost(\n        eta_knob=0.5,\n        world_size=len(monitor.gpu_indices),\n    )\n    self.warmup_steps = warmup_steps\n    self.profile_steps = profile_steps\n    self.pl_step = pl_step * 1000  # Internally, we use milliWatts.\n    self.profile_path = Path(profile_path) if isinstance(profile_path, str) else profile_path\n\n    gpus = get_gpus(ensure_homogeneous=True)\n\n    # Warn if distributed training is enabled with multiple GPUs monitored.\n    if is_distributed() and len(monitor.gpu_indices) &gt; 1:\n        logger.warning(\n            \"Distributed training is enabled with %d GPUs monitored. \"\n            \"For distributed training, it is recommended to monitor only one GPU per `ZeusMonitor` instance \"\n            \"since `GlobalPowerLimitOptimizer` performs an all-reduce operation internally over all devices.\",\n            len(monitor.gpu_indices),\n        )\n\n    # Set the range of power limits to explore.\n    # Assert that supported power limits ranges are uniform across GPUs.\n    pls = []\n    for index in monitor.gpu_indices:\n        pls.append(gpus.get_power_management_limit_constraints(index))\n    if not all(pls[0] == pl for pl in pls):\n        raise ValueError(\"Power limits ranges are not uniform across GPUs.\")\n    self.power_limits = list(range(pls[0][1], pls[0][0] - 1, -self.pl_step))\n\n    # Turn on persistence mode and set to the highest power limit.\n    try:\n        for index in monitor.gpu_indices:\n            gpus.set_persistence_mode(index, enabled=True, block=True)\n    except ZeusGPUNoPermissionError as ze:\n        raise RuntimeError(\n            \"SYS_ADMIN capability is required to modify GPU power limits. See \"\n            \"https://ml.energy/zeus/getting_started/#system-privileges \"\n            \"for more information.\"\n        ) from ze\n    self.current_power_limit = 0\n\n    # Store `Measurement` objects in a list, one for each power limit.\n    self.measurements: list[PowerLimitMeasurement] = []\n\n    # State for the profiler state machine.\n    self.state: Ready | Warmup | Profiling | Done\n\n    # Initialize JIT profiling states.\n    if self.profile_path is None:\n        logger.info(\"JIT profiling enabled.\")\n        logger.info(\"Will wait %d step(s) before profiling.\", wait_steps)\n        self.state = Ready(next_power_limit=self.power_limits[0], steps=wait_steps + 1)\n        logger.info(\"Set power limit to the maximum before starting.\")\n        self._set_power_limit(max(self.power_limits))\n    elif not self.profile_path.exists():\n        logger.info(\n            \"JIT Profiling enabled. Profile will be saved to '%s'.\",\n            str(self.profile_path),\n        )\n        logger.info(\"Will wait %d step(s) before profiling.\", wait_steps)\n        self.state = Ready(next_power_limit=self.power_limits[0], steps=wait_steps + 1)\n        logger.info(\"Set power limit to the maximum before starting.\")\n        self._set_power_limit(max(self.power_limits))\n    else:\n        self.measurements = _PowerLimitMeasurementList.parse_file(\n            self.profile_path,\n        ).measurements\n        # self.measurements = _PowerLimitMeasurementList.model_validate_json(\n        #     open(self.profile_path).read(),\n        #     strict=True,\n        # ).measurements\n        logger.info(\"Loaded previous profiling results from '%s'.\", str(self.profile_path))\n        optimal_power_limit = self._compute_optimal_power_limit()\n        logger.info(\"Optimal power limit is %d W.\", optimal_power_limit // 1000)\n        self.state = Done(optimal_power_limit=optimal_power_limit)\n        self._set_power_limit(self.state.optimal_power_limit)\n\n    # Restore all GPUs back to their maximum power limit on exit.\n    atexit.register(lambda: self._set_power_limit(max(self.power_limits)))\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.GlobalPowerLimitOptimizer.on_epoch_end","title":"on_epoch_end","text":"<pre><code>on_epoch_end()\n</code></pre> <p>Mark the end of a training epoch.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def on_epoch_end(self) -&gt; None:\n    \"\"\"Mark the end of a training epoch.\"\"\"\n    if isinstance(self.state, Ready):\n        pass\n\n    elif isinstance(self.state, (Warmup, Profiling)):\n        # Warmup/Profiling stage interrupted by the end of an epoch.\n        logger.info(\n            \"%s phase for %d W interrupted by the end of a training epoch.\",\n            type(self.state).__name__,\n            self.state.current_power_limit // 1000,\n        )\n        if isinstance(self.state, Profiling):\n            self.monitor.end_window(\n                f\"__GlobalPowerLimitOptimizer_{self.state.current_power_limit // 1000}\",\n                cancel=True,\n            )\n        self.state = Ready(next_power_limit=self.state.current_power_limit, steps=1)\n        self._set_power_limit(max(self.power_limits))\n\n    elif isinstance(self.state, Done):\n        pass\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.GlobalPowerLimitOptimizer.on_step_begin","title":"on_step_begin","text":"<pre><code>on_step_begin()\n</code></pre> <p>Mark the beginning of a training step.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def on_step_begin(self) -&gt; None:\n    \"\"\"Mark the beginning of a training step.\"\"\"\n    if isinstance(self.state, Ready):\n        self.state.steps -= 1\n        if self.state.steps == 0:\n            logger.info(\n                \"Starting warmup for power limit %d W.\",\n                self.state.next_power_limit // 1000,\n            )\n            self._set_power_limit(self.state.next_power_limit)\n            self.state = Warmup(\n                current_power_limit=self.state.next_power_limit,\n                steps=self.warmup_steps,\n            )\n\n    elif isinstance(self.state, Warmup):\n        self.state.steps -= 1\n        if self.state.steps == 0:\n            logger.info(\n                \"Starting actual profiling for power limit %d W.\",\n                self.state.current_power_limit // 1000,\n            )\n            self.state = Profiling(\n                current_power_limit=self.state.current_power_limit,\n                steps=self.profile_steps,\n            )\n            self.monitor.begin_window(\n                f\"__GlobalPowerLimitOptimizer_{self.state.current_power_limit // 1000}\",\n            )\n\n    elif isinstance(self.state, Profiling):\n        self.state.steps -= 1\n        if self.state.steps == 0:\n            measurement = self.monitor.end_window(\n                f\"__GlobalPowerLimitOptimizer_{self.state.current_power_limit // 1000}\",\n            )\n            logger.info(\n                \"Finished profiling for power limit %d W.\",\n                self.state.current_power_limit // 1000,\n            )\n\n            self.measurements.append(\n                PowerLimitMeasurement(\n                    power_limit=self.state.current_power_limit // 1000,\n                    energy=sum(all_reduce(list(measurement.gpu_energy.values()), operation=\"sum\")),\n                    time=max(all_reduce([measurement.time], operation=\"max\")),\n                )\n            )\n            # If we're done profiling all power limits, compute the optimal\n            # power limit and transition to the Done state. Otherwise, move\n            # on to the Warmup phase for the next power limit.\n            current_power_limit_index = self.power_limits.index(self.state.current_power_limit)\n            if current_power_limit_index == len(self.power_limits) - 1:\n                self.state = Done(\n                    optimal_power_limit=self._compute_optimal_power_limit(),\n                )\n                self._set_power_limit(self.state.optimal_power_limit)\n                self._save_profile()\n            else:\n                next_power_limit = self.power_limits[current_power_limit_index + 1]\n                logger.info(\n                    \"Starting warmup for power limit %d W.\",\n                    next_power_limit // 1000,\n                )\n                self._set_power_limit(next_power_limit)\n                self.state = Warmup(\n                    current_power_limit=next_power_limit,\n                    steps=self.warmup_steps,\n                )\n\n    elif isinstance(self.state, Done):\n        pass\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.GlobalPowerLimitOptimizer._set_power_limit","title":"_set_power_limit","text":"<pre><code>_set_power_limit(power_limit)\n</code></pre> <p>Set the power limit for all GPUs.</p> <p>Parameters:</p> Name Type Description Default <code>power_limit</code> <code>int</code> <p>The power limit to set, in milliWatts.</p> required Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def _set_power_limit(self, power_limit: int) -&gt; None:\n    \"\"\"Set the power limit for all GPUs.\n\n    Args:\n        power_limit: The power limit to set, in milliWatts.\n    \"\"\"\n    gpus = get_gpus()\n    logger.info(\"Setting power limit to %d W.\", power_limit // 1000)\n    if self.current_power_limit == power_limit:\n        return\n    for index in self.monitor.gpu_indices:\n        gpus.set_power_management_limit(index, power_limit)\n    self.current_power_limit = power_limit\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.GlobalPowerLimitOptimizer._compute_optimal_power_limit","title":"_compute_optimal_power_limit","text":"<pre><code>_compute_optimal_power_limit()\n</code></pre> <p>Compute the optimal power limit in milliWatts.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def _compute_optimal_power_limit(self) -&gt; int:\n    \"\"\"Compute the optimal power limit in milliWatts.\"\"\"\n    optimal_power_limit = self.optimum_selector.select(self.measurements) * 1000\n    logger.info(\"Optimal power limit is %d W.\", optimal_power_limit // 1000)\n    return optimal_power_limit\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.GlobalPowerLimitOptimizer._save_profile","title":"_save_profile","text":"<pre><code>_save_profile()\n</code></pre> <p>Save JIT profiling results and the optimal power limit to a JSON file.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def _save_profile(self) -&gt; None:\n    \"\"\"Save JIT profiling results and the optimal power limit to a JSON file.\"\"\"\n    if self.profile_path is None:\n        return\n\n    assert isinstance(self.state, Done)\n    with self.profile_path.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(\n            _PowerLimitMeasurementList(measurements=self.measurements).json(indent=4),\n        )\n    logger.info(\"JIT profiling results saved to '%s'.\", str(self.profile_path))\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.HFGlobalPowerLimitOptimizer","title":"HFGlobalPowerLimitOptimizer","text":"<p>               Bases: <code>TrainerCallback</code></p> <p>[Wrapped for Hugging Face Trainer Callback] Optimizer for the power limit knob.</p> <p>This optimizer uses the JIT profiling log to determine the optimal power limit. See <code>GlobalPowerLimitOptimizer</code> for the underlying optimizer implementation.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>class HFGlobalPowerLimitOptimizer(TrainerCallback):\n    \"\"\"[Wrapped for Hugging Face Trainer Callback] Optimizer for the power limit knob.\n\n    This optimizer uses the JIT profiling log to determine the optimal power limit.\n    See [`GlobalPowerLimitOptimizer`][zeus.optimizer.power_limit.GlobalPowerLimitOptimizer]\n    for the underlying optimizer implementation.\n    \"\"\"\n\n    def __init__(\n        self,\n        monitor: ZeusMonitor,\n        optimum_selector: OptimumSelector | None = None,\n        wait_steps: int = 1,\n        warmup_steps: int = 10,\n        profile_steps: int = 40,\n        pl_step: int = 25,\n        profile_path: str | Path | None = None,\n    ) -&gt; None:\n        r\"\"\"Initialize the optimizer.\n\n        GPU indices to profile and optimize for are taken from `monitor.gpu_indices`.\n\n        Args:\n            monitor: `ZeusMonitor` instance used to profile GPU time and energy consumption.\n            optimum_selector: The optimum selector to use. If not given, use `ZeusCost` with \\eta=0.5.\n            wait_steps: Number of steps to pass by before doing anything at the beginning.\n                Useful if you have something like `torch.backends.cudnn.benchmark=True`,\n                because the first iteration won't be representative of the rest of the iterations.\n            warmup_steps: Number of warmup iterations for each power limit.\n            profile_steps: Number of profie iterations for each power limit.\n            pl_step: The stride between power limits to explore, in unites of Watts.\n            profile_path: If the path points to an existing file, load the profile from the file\n                and do not run any profiling. If the path points to a non-existing file, profile\n                and save the profile to the file. If `None`, do not save or load any profile.\n        \"\"\"\n        if not transformers_available:\n            raise ImportError(\n                \"The transformers package is not installed. Please install it to use the HFGlobalPowerLimitOptimizer.\"\n            )\n\n        self.optimizer = GlobalPowerLimitOptimizer(\n            monitor=monitor,\n            optimum_selector=optimum_selector,\n            wait_steps=wait_steps,\n            warmup_steps=warmup_steps,\n            profile_steps=profile_steps,\n            pl_step=pl_step,\n            profile_path=profile_path,\n        )\n\n    def on_epoch_end(\n        self,\n        args: TrainingArguments,\n        state: TrainerState,\n        control: TrainerControl,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Mark the end of a training epoch.\"\"\"\n        self.optimizer.on_epoch_end()\n\n    def on_step_begin(\n        self,\n        args: TrainingArguments,\n        state: TrainerState,\n        control: TrainerControl,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Mark the beginning of a training step.\"\"\"\n        self.optimizer.on_step_begin()\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.HFGlobalPowerLimitOptimizer.__init__","title":"__init__","text":"<pre><code>__init__(monitor, optimum_selector=None, wait_steps=1, warmup_steps=10, profile_steps=40, pl_step=25, profile_path=None)\n</code></pre> <p>GPU indices to profile and optimize for are taken from <code>monitor.gpu_indices</code>.</p> <p>Parameters:</p> Name Type Description Default <code>monitor</code> <code>ZeusMonitor</code> <p><code>ZeusMonitor</code> instance used to profile GPU time and energy consumption.</p> required <code>optimum_selector</code> <code>OptimumSelector | None</code> <p>The optimum selector to use. If not given, use <code>ZeusCost</code> with \\eta=0.5.</p> <code>None</code> <code>wait_steps</code> <code>int</code> <p>Number of steps to pass by before doing anything at the beginning. Useful if you have something like <code>torch.backends.cudnn.benchmark=True</code>, because the first iteration won't be representative of the rest of the iterations.</p> <code>1</code> <code>warmup_steps</code> <code>int</code> <p>Number of warmup iterations for each power limit.</p> <code>10</code> <code>profile_steps</code> <code>int</code> <p>Number of profie iterations for each power limit.</p> <code>40</code> <code>pl_step</code> <code>int</code> <p>The stride between power limits to explore, in unites of Watts.</p> <code>25</code> <code>profile_path</code> <code>str | Path | None</code> <p>If the path points to an existing file, load the profile from the file and do not run any profiling. If the path points to a non-existing file, profile and save the profile to the file. If <code>None</code>, do not save or load any profile.</p> <code>None</code> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def __init__(\n    self,\n    monitor: ZeusMonitor,\n    optimum_selector: OptimumSelector | None = None,\n    wait_steps: int = 1,\n    warmup_steps: int = 10,\n    profile_steps: int = 40,\n    pl_step: int = 25,\n    profile_path: str | Path | None = None,\n) -&gt; None:\n    r\"\"\"Initialize the optimizer.\n\n    GPU indices to profile and optimize for are taken from `monitor.gpu_indices`.\n\n    Args:\n        monitor: `ZeusMonitor` instance used to profile GPU time and energy consumption.\n        optimum_selector: The optimum selector to use. If not given, use `ZeusCost` with \\eta=0.5.\n        wait_steps: Number of steps to pass by before doing anything at the beginning.\n            Useful if you have something like `torch.backends.cudnn.benchmark=True`,\n            because the first iteration won't be representative of the rest of the iterations.\n        warmup_steps: Number of warmup iterations for each power limit.\n        profile_steps: Number of profie iterations for each power limit.\n        pl_step: The stride between power limits to explore, in unites of Watts.\n        profile_path: If the path points to an existing file, load the profile from the file\n            and do not run any profiling. If the path points to a non-existing file, profile\n            and save the profile to the file. If `None`, do not save or load any profile.\n    \"\"\"\n    if not transformers_available:\n        raise ImportError(\n            \"The transformers package is not installed. Please install it to use the HFGlobalPowerLimitOptimizer.\"\n        )\n\n    self.optimizer = GlobalPowerLimitOptimizer(\n        monitor=monitor,\n        optimum_selector=optimum_selector,\n        wait_steps=wait_steps,\n        warmup_steps=warmup_steps,\n        profile_steps=profile_steps,\n        pl_step=pl_step,\n        profile_path=profile_path,\n    )\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.HFGlobalPowerLimitOptimizer.on_epoch_end","title":"on_epoch_end","text":"<pre><code>on_epoch_end(args, state, control, **kwargs)\n</code></pre> <p>Mark the end of a training epoch.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def on_epoch_end(\n    self,\n    args: TrainingArguments,\n    state: TrainerState,\n    control: TrainerControl,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Mark the end of a training epoch.\"\"\"\n    self.optimizer.on_epoch_end()\n</code></pre>"},{"location":"reference/optimizer/power_limit/#zeus.optimizer.power_limit.HFGlobalPowerLimitOptimizer.on_step_begin","title":"on_step_begin","text":"<pre><code>on_step_begin(args, state, control, **kwargs)\n</code></pre> <p>Mark the beginning of a training step.</p> Source code in <code>zeus/optimizer/power_limit.py</code> <pre><code>def on_step_begin(\n    self,\n    args: TrainingArguments,\n    state: TrainerState,\n    control: TrainerControl,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Mark the beginning of a training step.\"\"\"\n    self.optimizer.on_step_begin()\n</code></pre>"},{"location":"reference/optimizer/batch_size/","title":"batch_size","text":""},{"location":"reference/optimizer/batch_size/#zeus.optimizer.batch_size","title":"zeus.optimizer.batch_size","text":"<p>Batch size optimizer server and client.</p>"},{"location":"reference/optimizer/batch_size/client/","title":"client","text":""},{"location":"reference/optimizer/batch_size/client/#zeus.optimizer.batch_size.client","title":"zeus.optimizer.batch_size.client","text":"<p>Zeus batch size optimizer client that communicates with server.</p>"},{"location":"reference/optimizer/batch_size/client/#zeus.optimizer.batch_size.client.BatchSizeOptimizer","title":"BatchSizeOptimizer","text":"<p>               Bases: <code>Callback</code></p> <p>Batch size optimizer client that talks to server.</p> <p>The following methods must be called in order inside the training script:</p> <ul> <li><code>get_batch_size</code>: At the beginning of the script.</li> <li><code>on_train_begin</code>: Before running any epochs.</li> <li><code>on_evaluate</code>: After each epoch when the validation metric is available.</li> </ul> <p>One batch size optimizer per one training session of the job. The set of GPUs to be used for training should be homogeneous, and will be inferred from the <code>ZeusMonitor</code> instance passed into the constructor.</p> Source code in <code>zeus/optimizer/batch_size/client.py</code> <pre><code>class BatchSizeOptimizer(Callback):\n    \"\"\"Batch size optimizer client that talks to server.\n\n    The following methods must be called in order inside the training script:\n\n    - `get_batch_size`: At the beginning of the script.\n    - `on_train_begin`: Before running any epochs.\n    - `on_evaluate`: After each epoch when the validation metric is available.\n\n    One batch size optimizer per one training session of the job.\n    The set of GPUs to be used for training should be homogeneous, and will be inferred\n    from the `ZeusMonitor` instance passed into the constructor.\n    \"\"\"\n\n    def __init__(self, monitor: ZeusMonitor, server_url: str, job: JobSpec, rank: int = 0) -&gt; None:\n        \"\"\"Initialize the optimizer, and register the job to the server.\n\n        If job is already registered, check if the job configuration is identical with previously registered config.\n\n        Args:\n            monitor: `ZeusMonitor` instance configured to measure the energy of all and only the GPUs used for training.\n            server_url: url of batch size optimizer server\n            job: job specification. Refer to `JobSpec` for job specifcatio parameters.\n            rank: rank of gpu in the case of distributed training. We only let rank = 0 gpu to request for a batch size.\n        \"\"\"\n        self.monitor = monitor\n        self.server_url = server_url\n        self.cur_epoch = 0  # 0-indexed\n        self.running_time = 0.0\n        self.consumed_energy = 0.0\n        self.training_finished = False\n        self.trial_number = 0\n        self.rank = rank\n\n        # Currently, the BSO only supports homogeneous GPU training.\n        gpus = get_gpus(ensure_homogeneous=True)\n        if len(gpus) == 0:\n            raise ZeusBSOConfigError(\"No GPUs detected.\")\n\n        # set gpu configurations(max_power, number of gpus, and gpu model)\n        self.job_spec = JobSpecFromClient(\n            **job.dict(),\n            max_power=gpus.get_power_management_limit_constraints(0)[1] // 1000 * len(monitor.gpu_indices),\n            number_of_gpus=len(monitor.gpu_indices),\n            gpu_model=gpus.get_name(0),\n        )\n\n        # Track the batch size of current job\n        self.current_batch_size = 0\n\n        # Register job\n        res = httpx.post(self.server_url + REGISTER_JOB_URL, content=self.job_spec.json())\n        self._handle_response(res)\n\n        self.job = CreatedJob.parse_obj(res.json())\n\n        logger.critical(\"Job is registered with job_id: \\x1b[31;1m%s\\x1b[0m\", self.job.job_id)\n        logger.info(\"Job is registered: %s\", str(self.job))\n\n    def get_batch_size(self) -&gt; int:\n        \"\"\"Get batch size to use from the BSO server.\n\n        Returns:\n            return a batch size to use for current job\n\n        Raises:\n            `ZeusBSORuntimError`: if the batch size we receive is invalid\n        \"\"\"\n        if self.rank != 0:\n            raise ZeusBSOBadOperationError(\"Only rank 0 gpu can ask for a batch size.\")\n\n        if self.current_batch_size != 0:\n            # If we already got the batch size, return\n            return self.current_batch_size\n\n        self.cur_epoch = 0\n        res = httpx.get(\n            self.server_url + GET_NEXT_BATCH_SIZE_URL,\n            params={\"job_id\": self.job.job_id},\n        )\n        self._handle_response(res)\n        trial_id = TrialId.parse_obj(res.json())\n\n        if trial_id.batch_size not in self.job.batch_sizes:\n            raise ZeusBSORuntimError(f\"Zeus server returned a strange batch_size: {trial_id.batch_size}\")\n\n        self.current_batch_size = trial_id.batch_size\n        self.trial_number = trial_id.trial_number\n\n        logger.info(\"[BatchSizeOptimizer] Chosen batch size: %s\", trial_id.batch_size)\n\n        def report_end() -&gt; None:\n            httpx.patch(self.server_url + REPORT_END_URL, content=trial_id.json())\n\n        atexit.register(report_end)\n        return trial_id.batch_size\n\n    def on_train_begin(self) -&gt; None:\n        \"\"\"Start the monitor window and mark training is started.\"\"\"\n        self.training_finished = False\n        self.monitor.begin_window(\"BatciSizeOptimizerClient\")\n\n    def on_evaluate(\n        self,\n        metric: float,\n    ) -&gt; None:\n        \"\"\"Determine whether or not to stop training after evaluation.\n\n        Training stops when\n        - `max_epochs` was reached, or\n        - the target metric was reached. or\n        - Cost exceeded the early stop threshold\n\n        Args:\n            metric: Validation metric of this epoch. See also `higher_metric_is_better` in [`JobParams`][zeus.optimizer.batch_size.common.JobParams].\n\n        Raises:\n            `ZeusBSOOperationOrderError`: When `get_batch_size` was not called first.\n            `ZeusBSOTrainFailError`: When train failed for a chosen batch size and should be stopped.\n                                    This batch size will not be tried again. To proceed training, re-launch the training then bso will select another batch size\n            `ZeusBSORuntimError`: When the server returns an error\n        \"\"\"\n        if self.current_batch_size == 0:\n            raise ZeusBSOOperationOrderError(\"Call get_batch_size to set the batch size first\")\n\n        if self.training_finished:\n            return\n\n        self.cur_epoch += 1\n        measurement = self.monitor.end_window(\"BatciSizeOptimizerClient\")\n\n        # Accumulate time and energy\n        self.running_time += measurement.time\n        self.consumed_energy += measurement.total_energy\n\n        training_result = TrainingResult(\n            job_id=self.job.job_id,\n            batch_size=self.current_batch_size,\n            trial_number=self.trial_number,\n            time=self.running_time,\n            energy=self.consumed_energy,\n            metric=metric,\n            current_epoch=self.cur_epoch,\n        )\n\n        # report to the server about the result of this training\n        res = httpx.post(self.server_url + REPORT_RESULT_URL, content=training_result.json())\n        self._handle_response(res)\n\n        parsed_response = ReportResponse.parse_obj(res.json())\n\n        if not parsed_response.stop_train:\n            # Should keep training. Re-open the window\n            self.monitor.begin_window(\"BatciSizeOptimizerClient\")\n        else:\n            # Train is over. If not converged, raise an error\n            self.training_finished = True\n            if not parsed_response.converged:\n                raise ZeusBSOTrainFailError(\n                    f\"Train failed: {parsed_response.message} This batch size will not be selected again. Please re-launch the training\"\n                )\n\n    def _handle_response(self, res: httpx.Response) -&gt; None:\n        \"\"\"Check if the response is success. Otherwise raise an error with error message from the server.\n\n        Args:\n            res: response from the server\n        \"\"\"\n        if not (200 &lt;= (code := res.status_code) &lt; 300):\n            raise ZeusBSORuntimError(f\"Zeus server returned status code {code}: {res.text}\")\n</code></pre>"},{"location":"reference/optimizer/batch_size/client/#zeus.optimizer.batch_size.client.BatchSizeOptimizer.__init__","title":"__init__","text":"<pre><code>__init__(monitor, server_url, job, rank=0)\n</code></pre> <p>If job is already registered, check if the job configuration is identical with previously registered config.</p> <p>Parameters:</p> Name Type Description Default <code>monitor</code> <code>ZeusMonitor</code> <p><code>ZeusMonitor</code> instance configured to measure the energy of all and only the GPUs used for training.</p> required <code>server_url</code> <code>str</code> <p>url of batch size optimizer server</p> required <code>job</code> <code>JobSpec</code> <p>job specification. Refer to <code>JobSpec</code> for job specifcatio parameters.</p> required <code>rank</code> <code>int</code> <p>rank of gpu in the case of distributed training. We only let rank = 0 gpu to request for a batch size.</p> <code>0</code> Source code in <code>zeus/optimizer/batch_size/client.py</code> <pre><code>def __init__(self, monitor: ZeusMonitor, server_url: str, job: JobSpec, rank: int = 0) -&gt; None:\n    \"\"\"Initialize the optimizer, and register the job to the server.\n\n    If job is already registered, check if the job configuration is identical with previously registered config.\n\n    Args:\n        monitor: `ZeusMonitor` instance configured to measure the energy of all and only the GPUs used for training.\n        server_url: url of batch size optimizer server\n        job: job specification. Refer to `JobSpec` for job specifcatio parameters.\n        rank: rank of gpu in the case of distributed training. We only let rank = 0 gpu to request for a batch size.\n    \"\"\"\n    self.monitor = monitor\n    self.server_url = server_url\n    self.cur_epoch = 0  # 0-indexed\n    self.running_time = 0.0\n    self.consumed_energy = 0.0\n    self.training_finished = False\n    self.trial_number = 0\n    self.rank = rank\n\n    # Currently, the BSO only supports homogeneous GPU training.\n    gpus = get_gpus(ensure_homogeneous=True)\n    if len(gpus) == 0:\n        raise ZeusBSOConfigError(\"No GPUs detected.\")\n\n    # set gpu configurations(max_power, number of gpus, and gpu model)\n    self.job_spec = JobSpecFromClient(\n        **job.dict(),\n        max_power=gpus.get_power_management_limit_constraints(0)[1] // 1000 * len(monitor.gpu_indices),\n        number_of_gpus=len(monitor.gpu_indices),\n        gpu_model=gpus.get_name(0),\n    )\n\n    # Track the batch size of current job\n    self.current_batch_size = 0\n\n    # Register job\n    res = httpx.post(self.server_url + REGISTER_JOB_URL, content=self.job_spec.json())\n    self._handle_response(res)\n\n    self.job = CreatedJob.parse_obj(res.json())\n\n    logger.critical(\"Job is registered with job_id: \\x1b[31;1m%s\\x1b[0m\", self.job.job_id)\n    logger.info(\"Job is registered: %s\", str(self.job))\n</code></pre>"},{"location":"reference/optimizer/batch_size/client/#zeus.optimizer.batch_size.client.BatchSizeOptimizer.get_batch_size","title":"get_batch_size","text":"<pre><code>get_batch_size()\n</code></pre> <p>Get batch size to use from the BSO server.</p> <p>Returns:</p> Type Description <code>int</code> <p>return a batch size to use for current job</p> <p>Raises:</p> Type Description <code>`ZeusBSORuntimError`</code> <p>if the batch size we receive is invalid</p> Source code in <code>zeus/optimizer/batch_size/client.py</code> <pre><code>def get_batch_size(self) -&gt; int:\n    \"\"\"Get batch size to use from the BSO server.\n\n    Returns:\n        return a batch size to use for current job\n\n    Raises:\n        `ZeusBSORuntimError`: if the batch size we receive is invalid\n    \"\"\"\n    if self.rank != 0:\n        raise ZeusBSOBadOperationError(\"Only rank 0 gpu can ask for a batch size.\")\n\n    if self.current_batch_size != 0:\n        # If we already got the batch size, return\n        return self.current_batch_size\n\n    self.cur_epoch = 0\n    res = httpx.get(\n        self.server_url + GET_NEXT_BATCH_SIZE_URL,\n        params={\"job_id\": self.job.job_id},\n    )\n    self._handle_response(res)\n    trial_id = TrialId.parse_obj(res.json())\n\n    if trial_id.batch_size not in self.job.batch_sizes:\n        raise ZeusBSORuntimError(f\"Zeus server returned a strange batch_size: {trial_id.batch_size}\")\n\n    self.current_batch_size = trial_id.batch_size\n    self.trial_number = trial_id.trial_number\n\n    logger.info(\"[BatchSizeOptimizer] Chosen batch size: %s\", trial_id.batch_size)\n\n    def report_end() -&gt; None:\n        httpx.patch(self.server_url + REPORT_END_URL, content=trial_id.json())\n\n    atexit.register(report_end)\n    return trial_id.batch_size\n</code></pre>"},{"location":"reference/optimizer/batch_size/client/#zeus.optimizer.batch_size.client.BatchSizeOptimizer.on_train_begin","title":"on_train_begin","text":"<pre><code>on_train_begin()\n</code></pre> <p>Start the monitor window and mark training is started.</p> Source code in <code>zeus/optimizer/batch_size/client.py</code> <pre><code>def on_train_begin(self) -&gt; None:\n    \"\"\"Start the monitor window and mark training is started.\"\"\"\n    self.training_finished = False\n    self.monitor.begin_window(\"BatciSizeOptimizerClient\")\n</code></pre>"},{"location":"reference/optimizer/batch_size/client/#zeus.optimizer.batch_size.client.BatchSizeOptimizer.on_evaluate","title":"on_evaluate","text":"<pre><code>on_evaluate(metric)\n</code></pre> <p>Determine whether or not to stop training after evaluation.</p> <p>Training stops when - <code>max_epochs</code> was reached, or - the target metric was reached. or - Cost exceeded the early stop threshold</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>float</code> <p>Validation metric of this epoch. See also <code>higher_metric_is_better</code> in <code>JobParams</code>.</p> required <p>Raises:</p> Type Description <code>`ZeusBSOOperationOrderError`</code> <p>When <code>get_batch_size</code> was not called first.</p> <code>`ZeusBSOTrainFailError`</code> <p>When train failed for a chosen batch size and should be stopped.                     This batch size will not be tried again. To proceed training, re-launch the training then bso will select another batch size</p> <code>`ZeusBSORuntimError`</code> <p>When the server returns an error</p> Source code in <code>zeus/optimizer/batch_size/client.py</code> <pre><code>def on_evaluate(\n    self,\n    metric: float,\n) -&gt; None:\n    \"\"\"Determine whether or not to stop training after evaluation.\n\n    Training stops when\n    - `max_epochs` was reached, or\n    - the target metric was reached. or\n    - Cost exceeded the early stop threshold\n\n    Args:\n        metric: Validation metric of this epoch. See also `higher_metric_is_better` in [`JobParams`][zeus.optimizer.batch_size.common.JobParams].\n\n    Raises:\n        `ZeusBSOOperationOrderError`: When `get_batch_size` was not called first.\n        `ZeusBSOTrainFailError`: When train failed for a chosen batch size and should be stopped.\n                                This batch size will not be tried again. To proceed training, re-launch the training then bso will select another batch size\n        `ZeusBSORuntimError`: When the server returns an error\n    \"\"\"\n    if self.current_batch_size == 0:\n        raise ZeusBSOOperationOrderError(\"Call get_batch_size to set the batch size first\")\n\n    if self.training_finished:\n        return\n\n    self.cur_epoch += 1\n    measurement = self.monitor.end_window(\"BatciSizeOptimizerClient\")\n\n    # Accumulate time and energy\n    self.running_time += measurement.time\n    self.consumed_energy += measurement.total_energy\n\n    training_result = TrainingResult(\n        job_id=self.job.job_id,\n        batch_size=self.current_batch_size,\n        trial_number=self.trial_number,\n        time=self.running_time,\n        energy=self.consumed_energy,\n        metric=metric,\n        current_epoch=self.cur_epoch,\n    )\n\n    # report to the server about the result of this training\n    res = httpx.post(self.server_url + REPORT_RESULT_URL, content=training_result.json())\n    self._handle_response(res)\n\n    parsed_response = ReportResponse.parse_obj(res.json())\n\n    if not parsed_response.stop_train:\n        # Should keep training. Re-open the window\n        self.monitor.begin_window(\"BatciSizeOptimizerClient\")\n    else:\n        # Train is over. If not converged, raise an error\n        self.training_finished = True\n        if not parsed_response.converged:\n            raise ZeusBSOTrainFailError(\n                f\"Train failed: {parsed_response.message} This batch size will not be selected again. Please re-launch the training\"\n            )\n</code></pre>"},{"location":"reference/optimizer/batch_size/client/#zeus.optimizer.batch_size.client.BatchSizeOptimizer._handle_response","title":"_handle_response","text":"<pre><code>_handle_response(res)\n</code></pre> <p>Check if the response is success. Otherwise raise an error with error message from the server.</p> <p>Parameters:</p> Name Type Description Default <code>res</code> <code>Response</code> <p>response from the server</p> required Source code in <code>zeus/optimizer/batch_size/client.py</code> <pre><code>def _handle_response(self, res: httpx.Response) -&gt; None:\n    \"\"\"Check if the response is success. Otherwise raise an error with error message from the server.\n\n    Args:\n        res: response from the server\n    \"\"\"\n    if not (200 &lt;= (code := res.status_code) &lt; 300):\n        raise ZeusBSORuntimError(f\"Zeus server returned status code {code}: {res.text}\")\n</code></pre>"},{"location":"reference/optimizer/batch_size/common/","title":"common","text":""},{"location":"reference/optimizer/batch_size/common/#zeus.optimizer.batch_size.common","title":"zeus.optimizer.batch_size.common","text":"<p>Shared model definitions for the server and client.</p>"},{"location":"reference/optimizer/batch_size/common/#zeus.optimizer.batch_size.common.JobParams","title":"JobParams","text":"<p>               Bases: <code>BaseModel</code></p> <p>Job parameters.</p> <p>Attributes:</p> Name Type Description <code>job_id</code> <code>str</code> <p>unique ID for the job</p> <code>batch_sizes</code> <code>list[int]</code> <p>list of batch sizes to try</p> <code>default_batch_size</code> <code>int</code> <p>first batch size to try</p> <code>eta_knob</code> <code>float</code> <p>\\(\\eta\\) parameter for computing <code>zeus_cost</code></p> <code>beta_knob</code> <code>Optional[float]</code> <p>beta for early stopping. If min_cost*beta_knob &lt; current_cost, job will be stopped by bso server.         To disable, set it to None.</p> <code>target_metric</code> <code>float</code> <p>target metric to achieve for training.</p> <code>higher_is_better_metric</code> <code>bool</code> <p>if the goal of training is achieving higher metric than <code>target_metric</code></p> <code>max_epochs</code> <code>int</code> <p>Maximum number of epochs for a training run.</p> <code>num_pruning_rounds</code> <code>int</code> <p>Number of rounds we are trying for pruning stage</p> <code>window_size</code> <code>int</code> <p>For MAB, how many recent measurements to fetch for computing the arm states. If set to 0, fetch all measurements.</p> <code>mab_prior_mean</code> <code>float</code> <p>Mean of the belief prior distribution.</p> <code>mab_prior_precision</code> <code>float</code> <p>Precision of the belief prior distribution.</p> <code>mab_num_explorations</code> <code>int</code> <p>How many static explorations to run when no observations are available.</p> <code>mab_seed</code> <code>Optional[int]</code> <p>The random seed to use.</p> Source code in <code>zeus/optimizer/batch_size/common.py</code> <pre><code>class JobParams(BaseModel):\n    r\"\"\"Job parameters.\n\n    Attributes:\n        job_id: unique ID for the job\n        batch_sizes: list of batch sizes to try\n        default_batch_size: first batch size to try\n        eta_knob: $\\eta$ parameter for computing `zeus_cost`\n        beta_knob: beta for early stopping. If min_cost*beta_knob &lt; current_cost, job will be stopped by bso server.\n                    To disable, set it to None.\n        target_metric: target metric to achieve for training.\n        higher_is_better_metric: if the goal of training is achieving higher metric than `target_metric`\n        max_epochs: Maximum number of epochs for a training run.\n        num_pruning_rounds: Number of rounds we are trying for pruning stage\n        window_size: For MAB, how many recent measurements to fetch for computing the arm states. If set to 0, fetch all measurements.\n\n        mab_prior_mean: Mean of the belief prior distribution.\n        mab_prior_precision: Precision of the belief prior distribution.\n        mab_num_explorations: How many static explorations to run when no observations are available.\n        mab_seed: The random seed to use.\n    \"\"\"\n\n    job_id: str\n    job_id_prefix: str\n    batch_sizes: list[int]\n    default_batch_size: int = Field(gt=0)\n    eta_knob: float = 0.5\n    beta_knob: Optional[float] = 2.0\n    target_metric: float = 0.50\n    higher_is_better_metric: bool = True\n    max_epochs: int = Field(default=100, gt=0)\n    num_pruning_rounds: int = Field(default=2, ge=0)\n    window_size: int = 0\n\n    mab_prior_mean: float = 0.0\n    mab_prior_precision: float = 0.0\n    mab_num_explorations: int = Field(default=2, ge=0)\n    mab_seed: Optional[int] = None\n\n    @validator(\"batch_sizes\")\n    def _validate_batch_sizes(cls, bs: list[int]) -&gt; list[int]:\n        if bs is not None and len(bs) &gt; 0:\n            bs.sort()\n            return bs\n        else:\n            raise ValueError(f\"Batch Sizes = {bs} is empty\")\n\n    @validator(\"eta_knob\")\n    def _validate_eta_knob(cls, v: float) -&gt; float:\n        if v &lt; 0 or v &gt; 1:\n            raise ValueError(\"eta_knob should be in range [0,1]\")\n        return v\n\n    @validator(\"beta_knob\")\n    def _validate_beta_knob(cls, v: float) -&gt; float:\n        if v is None or v &gt; 0:\n            return v\n        else:\n            raise ValueError(\n                f\"Invalid beta_knob({v}). To disable early stop, set beta_knob = None to disable or positive value.\"\n            )\n\n    @root_validator(skip_on_failure=True)\n    def _check_default_batch_size(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n        bs = values[\"default_batch_size\"]\n        bss = values[\"batch_sizes\"]\n        if bs not in bss:\n            raise ValueError(f\"Default BS({bs}) not in batch_sizes({bss}).\")\n        return values\n</code></pre>"},{"location":"reference/optimizer/batch_size/common/#zeus.optimizer.batch_size.common.GpuConfig","title":"GpuConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Gpu configuration of current training.</p> Source code in <code>zeus/optimizer/batch_size/common.py</code> <pre><code>class GpuConfig(BaseModel):\n    \"\"\"Gpu configuration of current training.\"\"\"\n\n    max_power: float = Field(gt=0)\n    number_of_gpus: int = Field(gt=0)\n    gpu_model: str\n\n    @validator(\"gpu_model\")\n    def _validate_gpu_model(cls, v: str) -&gt; str:\n        if v is None or v == \"\":\n            raise ValueError(f\"Invalid gpu_model({v}). Shouldn't be empty.\")\n        else:\n            return v\n</code></pre>"},{"location":"reference/optimizer/batch_size/common/#zeus.optimizer.batch_size.common.JobSpec","title":"JobSpec","text":"<p>               Bases: <code>JobParams</code></p> <p>Job specification that user inputs.</p> <p>Attributes:</p> Name Type Description <code>job_id</code> <code>Optional[str]</code> <p>ID of job. If none is provided, will be created by server.</p> <p>Refer to <code>JobParams</code> for other attributes.</p> Source code in <code>zeus/optimizer/batch_size/common.py</code> <pre><code>class JobSpec(JobParams):\n    \"\"\"Job specification that user inputs.\n\n    Attributes:\n        job_id: ID of job. If none is provided, will be created by server.\n\n    Refer to [`JobParams`][zeus.optimizer.batch_size.common.JobParams] for other attributes.\n    \"\"\"\n\n    job_id: Optional[str]\n\n    @root_validator(skip_on_failure=True)\n    def _check_job_id(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n        job_id: str | None = values.get(\"job_id\")\n        prefix: str = values[\"job_id_prefix\"]\n\n        if job_id is not None and not job_id.startswith(prefix):\n            raise ValueError(f\"Job_id({job_id}) does not start with prefix({prefix}).\")\n        return values\n</code></pre>"},{"location":"reference/optimizer/batch_size/common/#zeus.optimizer.batch_size.common.JobSpecFromClient","title":"JobSpecFromClient","text":"<p>               Bases: <code>JobSpec</code>, <code>GpuConfig</code></p> <p>Internal job configuration including gpu settings. Job Id is optional here.</p> Source code in <code>zeus/optimizer/batch_size/common.py</code> <pre><code>class JobSpecFromClient(JobSpec, GpuConfig):\n    \"\"\"Internal job configuration including gpu settings. Job Id is optional here.\"\"\"\n</code></pre>"},{"location":"reference/optimizer/batch_size/common/#zeus.optimizer.batch_size.common.CreatedJob","title":"CreatedJob","text":"<p>               Bases: <code>JobParams</code>, <code>GpuConfig</code></p> <p>Job configuration from the server. Job Id is required.</p> Source code in <code>zeus/optimizer/batch_size/common.py</code> <pre><code>class CreatedJob(JobParams, GpuConfig):\n    \"\"\"Job configuration from the server. Job Id is required.\"\"\"\n</code></pre>"},{"location":"reference/optimizer/batch_size/common/#zeus.optimizer.batch_size.common.TrialId","title":"TrialId","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response format from the server for getting a batch size to use, which is an unique idnetifier of trial.</p> <p>Attributes:</p> Name Type Description <code>job_id</code> <code>str</code> <p>ID of job</p> <code>batch_size</code> <code>int</code> <p>batch size to use.</p> <code>trial_number</code> <code>int</code> <p>trial number of current training.</p> Source code in <code>zeus/optimizer/batch_size/common.py</code> <pre><code>class TrialId(BaseModel):\n    \"\"\"Response format from the server for getting a batch size to use, which is an unique idnetifier of trial.\n\n    Attributes:\n        job_id: ID of job\n        batch_size: batch size to use.\n        trial_number: trial number of current training.\n    \"\"\"\n\n    job_id: str\n    batch_size: int\n    trial_number: int\n</code></pre>"},{"location":"reference/optimizer/batch_size/common/#zeus.optimizer.batch_size.common.TrainingResult","title":"TrainingResult","text":"<p>               Bases: <code>TrialId</code></p> <p>Result of training for that job &amp; batch size.</p> <p>Attributes:</p> Name Type Description <code>time</code> <code>float</code> <p>total time consumption so far</p> <code>energy</code> <code>float</code> <p>total energy consumption so far</p> <code>metric</code> <code>float</code> <p>current metric value after <code>current_epoch</code></p> <code>current_epoch</code> <code>int</code> <p>current epoch of training. Server can check if the train reached the <code>max_epochs</code></p> Source code in <code>zeus/optimizer/batch_size/common.py</code> <pre><code>class TrainingResult(TrialId):\n    \"\"\"Result of training for that job &amp; batch size.\n\n    Attributes:\n        time: total time consumption so far\n        energy: total energy consumption so far\n        metric: current metric value after `current_epoch`\n        current_epoch: current epoch of training. Server can check if the train reached the `max_epochs`\n    \"\"\"\n\n    time: float\n    energy: float\n    metric: float\n    current_epoch: int\n</code></pre>"},{"location":"reference/optimizer/batch_size/common/#zeus.optimizer.batch_size.common.ReportResponse","title":"ReportResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response format from the server for client's training result report.</p> <p>Attributes:</p> Name Type Description <code>stop_train</code> <code>bool</code> <p>Whether we should stop training or not.</p> <code>converged</code> <code>bool</code> <p>Whether the target metric has been reached.</p> <code>message</code> <code>str</code> <p>message from the server regarding training. ex) why train should be stopped.</p> Source code in <code>zeus/optimizer/batch_size/common.py</code> <pre><code>class ReportResponse(BaseModel):\n    \"\"\"Response format from the server for client's training result report.\n\n    Attributes:\n        stop_train: Whether we should stop training or not.\n        converged: Whether the target metric has been reached.\n        message: message from the server regarding training. ex) why train should be stopped.\n    \"\"\"\n\n    stop_train: bool\n    converged: bool\n    message: str\n</code></pre>"},{"location":"reference/optimizer/batch_size/exceptions/","title":"exceptions","text":""},{"location":"reference/optimizer/batch_size/exceptions/#zeus.optimizer.batch_size.exceptions","title":"zeus.optimizer.batch_size.exceptions","text":"<p>Zeus batch size optimizer client exceptions.</p>"},{"location":"reference/optimizer/batch_size/exceptions/#zeus.optimizer.batch_size.exceptions.ZeusBSORuntimError","title":"ZeusBSORuntimError","text":"<p>               Bases: <code>ZeusBaseError</code></p> <p>Bso server failed to process the request correctly.</p> Source code in <code>zeus/optimizer/batch_size/exceptions.py</code> <pre><code>class ZeusBSORuntimError(ZeusBaseError):\n    \"\"\"Bso server failed to process the request correctly.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/optimizer/batch_size/exceptions/#zeus.optimizer.batch_size.exceptions.ZeusBSOTrainFailError","title":"ZeusBSOTrainFailError","text":"<p>               Bases: <code>ZeusBaseError</code></p> <p>Training failed for the chosen batch_size.</p> Source code in <code>zeus/optimizer/batch_size/exceptions.py</code> <pre><code>class ZeusBSOTrainFailError(ZeusBaseError):\n    \"\"\"Training failed for the chosen batch_size.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/optimizer/batch_size/exceptions/#zeus.optimizer.batch_size.exceptions.ZeusBSOConfigError","title":"ZeusBSOConfigError","text":"<p>               Bases: <code>ZeusBaseError</code></p> <p>Configuration of training doesn't meet the requirements. ex) heterogeneous GPU.</p> Source code in <code>zeus/optimizer/batch_size/exceptions.py</code> <pre><code>class ZeusBSOConfigError(ZeusBaseError):\n    \"\"\"Configuration of training doesn't meet the requirements. ex) heterogeneous GPU.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/optimizer/batch_size/exceptions/#zeus.optimizer.batch_size.exceptions.ZeusBSOOperationOrderError","title":"ZeusBSOOperationOrderError","text":"<p>               Bases: <code>ZeusBaseError</code></p> <p>Order of calling methods of BatchSizeOptimizer is wrong.</p> Source code in <code>zeus/optimizer/batch_size/exceptions.py</code> <pre><code>class ZeusBSOOperationOrderError(ZeusBaseError):\n    \"\"\"Order of calling methods of BatchSizeOptimizer is wrong.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/optimizer/batch_size/exceptions/#zeus.optimizer.batch_size.exceptions.ZeusBSOBadOperationError","title":"ZeusBSOBadOperationError","text":"<p>               Bases: <code>ZeusBaseError</code></p> <p>The usage of operations is wrong.</p> Source code in <code>zeus/optimizer/batch_size/exceptions.py</code> <pre><code>class ZeusBSOBadOperationError(ZeusBaseError):\n    \"\"\"The usage of operations is wrong.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/","title":"server","text":""},{"location":"reference/optimizer/batch_size/server/#zeus.optimizer.batch_size.server","title":"zeus.optimizer.batch_size.server","text":"<p>Zeus batch size optimizer server.</p> <p>[Description]</p> <p>Batch size optimizer is in composed of server and client. The reason for server-client architecture is that we need to maintain the states accross all trainings. Therefore, we need a central place that is not limited by a scope of one training.</p> <p>Server's role is maintaining and updating the state of the job based on client's report. There are three types of states.</p> <ul> <li>Job related states: <code>JobState</code></li> <li>Trial related states: <code>Trial</code></li> <li>MAB related states: <code>GaussianTsArmState</code></li> </ul> <p>Client's role is letting the server know that the user started a new training and report the result of training.</p> <p>[Structure of code]</p> <p>Each domain (batch_size_state and job) is composed of repository, commands, and models.</p> <ul> <li>Repository is the lowest layer that modifies the DB. It provides CRUD operations, and performs corresponding sql operation for a given request.</li> <li>Commands are the command (collection of arguments) to use each method in repository. It is mainly used to validate the request.</li> <li>Models are used to safely perform operations on objects. All ORM objects can be converted into these models and we also have some helper models.</li> </ul> <p>In services directory, we have a single service, <code>ZeusService</code> which performs one or more operations of repositories. It performs business logics, and provides more complicated operations to application layer. It also has commands that validates requests of using service's method.</p> <p>[Hierarchy of program]</p> <pre><code>                            | Application layer     | Business logic | DB operation               | Storage\n                            |                       |                |                            |\nClient request -&gt; Router -&gt; | Optimizer -&gt; Explorer | -&gt; ZeusService | -&gt; JobStateRepository      | &lt;-&gt; DB\n                            |           -&gt; Mab      |                | -&gt; BatchSizeStateRepository|\n                            |                       |                |                            |\n</code></pre> <p>[Database Transaction]</p> <p>Each session represent a single transaction. When Fastapi receives the request, it creates a single session. Then, at the end of the request, it commits every operations to Database.</p>"},{"location":"reference/optimizer/batch_size/server/config/","title":"config","text":""},{"location":"reference/optimizer/batch_size/server/config/#zeus.optimizer.batch_size.server.config","title":"zeus.optimizer.batch_size.server.config","text":"<p>Server global configurations.</p>"},{"location":"reference/optimizer/batch_size/server/config/#zeus.optimizer.batch_size.server.config.ZeusBsoSettings","title":"ZeusBsoSettings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>App setting.</p> <p>Attributes:</p> Name Type Description <code>database_url</code> <code>str</code> <p>url of database for the server</p> <code>echo_sql</code> <code>Union[bool, str]</code> <p>log sql statements it executes</p> <code>log_level</code> <code>str</code> <p>level of log</p> Source code in <code>zeus/optimizer/batch_size/server/config.py</code> <pre><code>class ZeusBsoSettings(BaseSettings):\n    \"\"\"App setting.\n\n    Attributes:\n        database_url: url of database for the server\n        echo_sql: log sql statements it executes\n        log_level: level of log\n    \"\"\"\n\n    database_url: str\n    echo_sql: Union[bool, str] = False  # To prevent conversion error for empty string\n    log_level: str = \"INFO\"\n\n    class Config:\n        \"\"\"Model configuration.\n\n        Set how to find the env variables and how to parse it.\n        \"\"\"\n\n        env_prefix = \"ZEUS_BSO_\"\n        env_file = find_dotenv(filename=\".env\")\n        env_file_encoding = \"utf-8\"\n\n    @validator(\"echo_sql\")\n    def _validate_echo_sql(cls, v) -&gt; bool:\n        if v is not None and isinstance(v, bool):\n            return v\n        elif v is not None and isinstance(v, str):\n            if v.lower() == \"false\":\n                return False\n            elif v.lower() == \"true\":\n                return True\n        return False\n\n    @validator(\"log_level\")\n    def _validate_log_level(cls, v) -&gt; str:\n        if v is None or v not in {\n            \"NOTSET\",\n            \"DEBUG\",\n            \"INFO\",\n            \"WARN\",\n            \"ERROR\",\n            \"CRITICAL\",\n        }:\n            # Default log level\n            return \"INFO\"\n        return v\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/config/#zeus.optimizer.batch_size.server.config.ZeusBsoSettings.Config","title":"Config","text":"<p>Model configuration.</p> <p>Set how to find the env variables and how to parse it.</p> Source code in <code>zeus/optimizer/batch_size/server/config.py</code> <pre><code>class Config:\n    \"\"\"Model configuration.\n\n    Set how to find the env variables and how to parse it.\n    \"\"\"\n\n    env_prefix = \"ZEUS_BSO_\"\n    env_file = find_dotenv(filename=\".env\")\n    env_file_encoding = \"utf-8\"\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/exceptions/","title":"exceptions","text":""},{"location":"reference/optimizer/batch_size/server/exceptions/#zeus.optimizer.batch_size.server.exceptions","title":"zeus.optimizer.batch_size.server.exceptions","text":"<p>Zeus server exceptions.</p>"},{"location":"reference/optimizer/batch_size/server/exceptions/#zeus.optimizer.batch_size.server.exceptions.ZeusBSOServerBaseError","title":"ZeusBSOServerBaseError","text":"<p>               Bases: <code>ZeusBaseError</code></p> <p>Base error class for BSO server.</p> Source code in <code>zeus/optimizer/batch_size/server/exceptions.py</code> <pre><code>class ZeusBSOServerBaseError(ZeusBaseError):\n    \"\"\"Base error class for BSO server.\"\"\"\n\n    def __init__(self, msg: str):\n        \"\"\"Set status code.\"\"\"\n        super().__init__(msg)\n        self.status_code = 500\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/exceptions/#zeus.optimizer.batch_size.server.exceptions.ZeusBSOServerBaseError.__init__","title":"__init__","text":"<pre><code>__init__(msg)\n</code></pre> Source code in <code>zeus/optimizer/batch_size/server/exceptions.py</code> <pre><code>def __init__(self, msg: str):\n    \"\"\"Set status code.\"\"\"\n    super().__init__(msg)\n    self.status_code = 500\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/exceptions/#zeus.optimizer.batch_size.server.exceptions.ZeusBSOJobConfigMismatchError","title":"ZeusBSOJobConfigMismatchError","text":"<p>               Bases: <code>ZeusBSOServerBaseError</code></p> <p>When the job configuration doesn't align for the same job_id.</p> Source code in <code>zeus/optimizer/batch_size/server/exceptions.py</code> <pre><code>class ZeusBSOJobConfigMismatchError(ZeusBSOServerBaseError):\n    \"\"\"When the job configuration doesn't align for the same job_id.\"\"\"\n\n    def __init__(self, msg: str):\n        \"\"\"Set status code.\"\"\"\n        super().__init__(msg)\n        self.status_code = 409\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/exceptions/#zeus.optimizer.batch_size.server.exceptions.ZeusBSOJobConfigMismatchError.__init__","title":"__init__","text":"<pre><code>__init__(msg)\n</code></pre> Source code in <code>zeus/optimizer/batch_size/server/exceptions.py</code> <pre><code>def __init__(self, msg: str):\n    \"\"\"Set status code.\"\"\"\n    super().__init__(msg)\n    self.status_code = 409\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/exceptions/#zeus.optimizer.batch_size.server.exceptions.ZeusBSOValueError","title":"ZeusBSOValueError","text":"<p>               Bases: <code>ZeusBSOServerBaseError</code></p> <p>When the certain value is invalid.</p> Source code in <code>zeus/optimizer/batch_size/server/exceptions.py</code> <pre><code>class ZeusBSOValueError(ZeusBSOServerBaseError):\n    \"\"\"When the certain value is invalid.\"\"\"\n\n    def __init__(self, msg: str):\n        \"\"\"Set status code.\"\"\"\n        super().__init__(msg)\n        self.status_code = 400\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/exceptions/#zeus.optimizer.batch_size.server.exceptions.ZeusBSOValueError.__init__","title":"__init__","text":"<pre><code>__init__(msg)\n</code></pre> Source code in <code>zeus/optimizer/batch_size/server/exceptions.py</code> <pre><code>def __init__(self, msg: str):\n    \"\"\"Set status code.\"\"\"\n    super().__init__(msg)\n    self.status_code = 400\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/exceptions/#zeus.optimizer.batch_size.server.exceptions.ZeusBSOServerNotFoundError","title":"ZeusBSOServerNotFoundError","text":"<p>               Bases: <code>ZeusBSOServerBaseError</code></p> <p>Resource we are looking for is not found.</p> Source code in <code>zeus/optimizer/batch_size/server/exceptions.py</code> <pre><code>class ZeusBSOServerNotFoundError(ZeusBSOServerBaseError):\n    \"\"\"Resource we are looking for is not found.\"\"\"\n\n    def __init__(self, msg: str):\n        \"\"\"Set status code.\"\"\"\n        super().__init__(msg)\n        self.status_code = 404\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/exceptions/#zeus.optimizer.batch_size.server.exceptions.ZeusBSOServerNotFoundError.__init__","title":"__init__","text":"<pre><code>__init__(msg)\n</code></pre> Source code in <code>zeus/optimizer/batch_size/server/exceptions.py</code> <pre><code>def __init__(self, msg: str):\n    \"\"\"Set status code.\"\"\"\n    super().__init__(msg)\n    self.status_code = 404\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/exceptions/#zeus.optimizer.batch_size.server.exceptions.ZeusBSOServiceBadOperationError","title":"ZeusBSOServiceBadOperationError","text":"<p>               Bases: <code>ZeusBSOServerBaseError</code></p> <p>When the operation doesn't meet requirements. ex) fetching measurements before fetching a job.</p> Source code in <code>zeus/optimizer/batch_size/server/exceptions.py</code> <pre><code>class ZeusBSOServiceBadOperationError(ZeusBSOServerBaseError):\n    \"\"\"When the operation doesn't meet requirements. ex) fetching measurements before fetching a job.\"\"\"\n\n    def __init__(self, msg: str):\n        \"\"\"Set status code.\"\"\"\n        super().__init__(msg)\n        self.status_code = 400\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/exceptions/#zeus.optimizer.batch_size.server.exceptions.ZeusBSOServiceBadOperationError.__init__","title":"__init__","text":"<pre><code>__init__(msg)\n</code></pre> Source code in <code>zeus/optimizer/batch_size/server/exceptions.py</code> <pre><code>def __init__(self, msg: str):\n    \"\"\"Set status code.\"\"\"\n    super().__init__(msg)\n    self.status_code = 400\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/exceptions/#zeus.optimizer.batch_size.server.exceptions.ZeusBSOServerRuntimeError","title":"ZeusBSOServerRuntimeError","text":"<p>               Bases: <code>ZeusBSOServerBaseError</code></p> <p>Initialization or other errors during runtime.</p> Source code in <code>zeus/optimizer/batch_size/server/exceptions.py</code> <pre><code>class ZeusBSOServerRuntimeError(ZeusBSOServerBaseError):\n    \"\"\"Initialization or other errors during runtime.\"\"\"\n\n    def __init__(self, msg: str):\n        \"\"\"Set status code.\"\"\"\n        super().__init__(msg)\n        self.status_code = 500\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/exceptions/#zeus.optimizer.batch_size.server.exceptions.ZeusBSOServerRuntimeError.__init__","title":"__init__","text":"<pre><code>__init__(msg)\n</code></pre> Source code in <code>zeus/optimizer/batch_size/server/exceptions.py</code> <pre><code>def __init__(self, msg: str):\n    \"\"\"Set status code.\"\"\"\n    super().__init__(msg)\n    self.status_code = 500\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/explorer/","title":"explorer","text":""},{"location":"reference/optimizer/batch_size/server/explorer/#zeus.optimizer.batch_size.server.explorer","title":"zeus.optimizer.batch_size.server.explorer","text":"<p>Provides report/next_batch_size during pruning stage.</p>"},{"location":"reference/optimizer/batch_size/server/explorer/#zeus.optimizer.batch_size.server.explorer.PruningExploreManager","title":"PruningExploreManager","text":"<p>Pruning manager that manges the batch size states in pruning stage.</p> Source code in <code>zeus/optimizer/batch_size/server/explorer.py</code> <pre><code>class PruningExploreManager:\n    \"\"\"Pruning manager that manges the batch size states in pruning stage.\"\"\"\n\n    def __init__(self, service: ZeusService):\n        \"\"\"Set up zeus service.\"\"\"\n        self.service = service\n\n    async def next_batch_size(\n        self,\n        job: JobState,\n        exploration_history: ExplorationsPerJob,\n    ) -&gt; ReadTrial | list[int]:\n        \"\"\"Find the next batch size to explore.\n\n        Three cases possible.\n        1. Pruninig Stage : There is a batch size that has not explored during the round.\n        2. Concurrent job : There is an exploration with \"Dispatched\" state.\n        3. Mab stage : All batch sizes have been explored and round is over.\n\n        Args:\n            job: state of the job\n            exploration_history: all \"succeeded\" explorations that we have done for that job\n\n        Returns:\n            Return the batch size to use during Pruning stage.\n            If Pruning stage was over, return None.\n\n        Raises:\n            `ZeusBSOValueError`: If the value is invalid. EX) default batch size is not in the converged batch size list.\n        \"\"\"\n        batch_sizes = job.batch_sizes\n        exp_default_bs = job.default_batch_size\n\n        for round in range(job.num_pruning_rounds):\n            converged_bs_list = []\n\n            min_cost_of_round = float(\"inf\")\n            min_batch_size_of_round = 0\n\n            batch_sizes.sort()\n            idx = batch_sizes.index(exp_default_bs)\n            down = sorted(batch_sizes[: idx + 1], reverse=True)\n            up = sorted(batch_sizes[idx + 1 :])\n\n            for bs_list in [down, up]:\n                for bs in bs_list:\n                    if (\n                        bs in exploration_history.explorations_per_bs\n                        and len(exploration_history.explorations_per_bs[bs]) &gt; round\n                    ):\n                        # Already explored at this round\n                        if exploration_history.explorations_per_bs[bs][round].status == TrialStatus.Dispatched:\n                            # We are waiting for the result of this exploration -&gt; Concurrent job!\n                            return await self.service.create_trial(\n                                CreateConcurrentTrial(\n                                    job_id=job.job_id,\n                                    batch_size=job.min_cost_batch_size,\n                                )\n                            )\n\n                        if not exploration_history.explorations_per_bs[bs][round].converged:\n                            # Failed to converge -&gt; Go to next list or round\n                            break\n                        else:\n                            # Training converged.\n                            converged_bs_list.append(bs)\n\n                            m = exploration_history.explorations_per_bs[bs][round]\n                            if m.energy is None or m.time is None:\n                                raise ZeusBSOValueError(\"Energy or time is not available for the exploration.\")\n                            cost = zeus_cost(m.energy, m.time, job.eta_knob, job.max_power)\n                            if cost &lt; min_cost_of_round:\n                                min_cost_of_round = cost\n                                min_batch_size_of_round = bs\n\n                    else:\n                        # Did not explore this round. Should explore!\n                        return await self.service.create_trial(\n                            CreateExplorationTrial(\n                                job_id=job.job_id,\n                                batch_size=bs,\n                            )\n                        )\n\n            # We should go to next round. Update exp_default_bs and batch sizes!\n            exp_default_bs = min_batch_size_of_round\n            batch_sizes = converged_bs_list\n\n            logger.info(\n                \"[PruningExploreManager] go to next round(%d) new default bs = %d converged bs list = %s\",\n                round,\n                exp_default_bs,\n                batch_sizes,\n            )\n\n            if len(batch_sizes) == 0:\n                raise ZeusBSOServerRuntimeError(\n                    \"No converged batch sizes has observed. Reconfigure batch_sizes and re-launch the job.\"\n                )\n        # After going through pruning rounds, we couldn't find the bs. Should go to MAB stage, so return good batch_sizes.\n        return sorted(batch_sizes)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/explorer/#zeus.optimizer.batch_size.server.explorer.PruningExploreManager.__init__","title":"__init__","text":"<pre><code>__init__(service)\n</code></pre> Source code in <code>zeus/optimizer/batch_size/server/explorer.py</code> <pre><code>def __init__(self, service: ZeusService):\n    \"\"\"Set up zeus service.\"\"\"\n    self.service = service\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/explorer/#zeus.optimizer.batch_size.server.explorer.PruningExploreManager.next_batch_size","title":"next_batch_size  <code>async</code>","text":"<pre><code>next_batch_size(job, exploration_history)\n</code></pre> <p>Find the next batch size to explore.</p> <p>Three cases possible. 1. Pruninig Stage : There is a batch size that has not explored during the round. 2. Concurrent job : There is an exploration with \"Dispatched\" state. 3. Mab stage : All batch sizes have been explored and round is over.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>JobState</code> <p>state of the job</p> required <code>exploration_history</code> <code>ExplorationsPerJob</code> <p>all \"succeeded\" explorations that we have done for that job</p> required <p>Returns:</p> Type Description <code>ReadTrial | list[int]</code> <p>Return the batch size to use during Pruning stage.</p> <code>ReadTrial | list[int]</code> <p>If Pruning stage was over, return None.</p> <p>Raises:</p> Type Description <code>`ZeusBSOValueError`</code> <p>If the value is invalid. EX) default batch size is not in the converged batch size list.</p> Source code in <code>zeus/optimizer/batch_size/server/explorer.py</code> <pre><code>async def next_batch_size(\n    self,\n    job: JobState,\n    exploration_history: ExplorationsPerJob,\n) -&gt; ReadTrial | list[int]:\n    \"\"\"Find the next batch size to explore.\n\n    Three cases possible.\n    1. Pruninig Stage : There is a batch size that has not explored during the round.\n    2. Concurrent job : There is an exploration with \"Dispatched\" state.\n    3. Mab stage : All batch sizes have been explored and round is over.\n\n    Args:\n        job: state of the job\n        exploration_history: all \"succeeded\" explorations that we have done for that job\n\n    Returns:\n        Return the batch size to use during Pruning stage.\n        If Pruning stage was over, return None.\n\n    Raises:\n        `ZeusBSOValueError`: If the value is invalid. EX) default batch size is not in the converged batch size list.\n    \"\"\"\n    batch_sizes = job.batch_sizes\n    exp_default_bs = job.default_batch_size\n\n    for round in range(job.num_pruning_rounds):\n        converged_bs_list = []\n\n        min_cost_of_round = float(\"inf\")\n        min_batch_size_of_round = 0\n\n        batch_sizes.sort()\n        idx = batch_sizes.index(exp_default_bs)\n        down = sorted(batch_sizes[: idx + 1], reverse=True)\n        up = sorted(batch_sizes[idx + 1 :])\n\n        for bs_list in [down, up]:\n            for bs in bs_list:\n                if (\n                    bs in exploration_history.explorations_per_bs\n                    and len(exploration_history.explorations_per_bs[bs]) &gt; round\n                ):\n                    # Already explored at this round\n                    if exploration_history.explorations_per_bs[bs][round].status == TrialStatus.Dispatched:\n                        # We are waiting for the result of this exploration -&gt; Concurrent job!\n                        return await self.service.create_trial(\n                            CreateConcurrentTrial(\n                                job_id=job.job_id,\n                                batch_size=job.min_cost_batch_size,\n                            )\n                        )\n\n                    if not exploration_history.explorations_per_bs[bs][round].converged:\n                        # Failed to converge -&gt; Go to next list or round\n                        break\n                    else:\n                        # Training converged.\n                        converged_bs_list.append(bs)\n\n                        m = exploration_history.explorations_per_bs[bs][round]\n                        if m.energy is None or m.time is None:\n                            raise ZeusBSOValueError(\"Energy or time is not available for the exploration.\")\n                        cost = zeus_cost(m.energy, m.time, job.eta_knob, job.max_power)\n                        if cost &lt; min_cost_of_round:\n                            min_cost_of_round = cost\n                            min_batch_size_of_round = bs\n\n                else:\n                    # Did not explore this round. Should explore!\n                    return await self.service.create_trial(\n                        CreateExplorationTrial(\n                            job_id=job.job_id,\n                            batch_size=bs,\n                        )\n                    )\n\n        # We should go to next round. Update exp_default_bs and batch sizes!\n        exp_default_bs = min_batch_size_of_round\n        batch_sizes = converged_bs_list\n\n        logger.info(\n            \"[PruningExploreManager] go to next round(%d) new default bs = %d converged bs list = %s\",\n            round,\n            exp_default_bs,\n            batch_sizes,\n        )\n\n        if len(batch_sizes) == 0:\n            raise ZeusBSOServerRuntimeError(\n                \"No converged batch sizes has observed. Reconfigure batch_sizes and re-launch the job.\"\n            )\n    # After going through pruning rounds, we couldn't find the bs. Should go to MAB stage, so return good batch_sizes.\n    return sorted(batch_sizes)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/mab/","title":"mab","text":""},{"location":"reference/optimizer/batch_size/server/mab/#zeus.optimizer.batch_size.server.mab","title":"zeus.optimizer.batch_size.server.mab","text":"<p>Thompson Sampling policy for Gaussian bandits. MAB related logic is implented here.</p>"},{"location":"reference/optimizer/batch_size/server/mab/#zeus.optimizer.batch_size.server.mab.GaussianTS","title":"GaussianTS","text":"<p>Thompson Sampling policy for Gaussian bandits.</p> <p>For each arm, the reward is modeled as a Gaussian distribution with known precision. The conjugate priors are also Gaussian distributions.</p> Source code in <code>zeus/optimizer/batch_size/server/mab.py</code> <pre><code>class GaussianTS:\n    \"\"\"Thompson Sampling policy for Gaussian bandits.\n\n    For each arm, the reward is modeled as a Gaussian distribution with\n    known precision. The conjugate priors are also Gaussian distributions.\n    \"\"\"\n\n    def __init__(self, service: ZeusService):\n        \"\"\"Set up zeus service to interact with database.\"\"\"\n        self.service = service\n        self.name = \"GaussianTS\"\n\n    def _fit_arm(\n        self,\n        bs_base: BatchSizeBase,\n        prior_mean: float,\n        prior_precision: float,\n        rewards: np.ndarray,\n    ) -&gt; GaussianTsArmState:\n        \"\"\"Update the parameter distribution for one arm.\n\n        Reference: &lt;https://en.wikipedia.org/wiki/Conjugate_prior&gt;\n\n        Args:\n            bs_base: job id and batch size tha represents this arm\n            prior_mean: Mean of the belief prior distribution.\n            prior_precision: Precision of the belief prior distribution.\n            rewards: Array of rewards observed by pulling that arm.\n\n        Returns:\n            Updated arm state\n        \"\"\"\n        if len(rewards) == 0:\n            raise ZeusBSOValueError(\"No rewards to fit the arm.\")\n\n        variance = np.var(rewards)\n        reward_prec = np.inf if variance == 0.0 else np.reciprocal(variance)\n\n        # Reset to priors\n        mean = prior_mean\n        prec = prior_precision\n\n        # Compute the parameters of the posterior distribution.\n        # The reward distribution's precision is given as infinite only when we\n        # have exactly one observation for the arm, s.t. sampling yields that\n        # exact observation.\n        if reward_prec == np.inf:\n            new_prec = np.inf\n            new_mean = rewards.mean()\n        else:\n            new_prec = prec + len(rewards) * reward_prec\n            new_mean = (prec * mean + reward_prec * rewards.sum()) / new_prec\n\n        # Updated state.\n        return GaussianTsArmState(\n            job_id=bs_base.job_id,\n            batch_size=bs_base.batch_size,\n            param_mean=new_mean,\n            param_precision=new_prec,\n            reward_precision=reward_prec,\n            num_observations=len(rewards),\n        )\n\n    def predict(\n        self,\n        job_id: str,\n        prior_precision: float,\n        num_exploration: int,\n        arms: list[GaussianTsArmState],\n    ) -&gt; int:\n        \"\"\"Return the arm with the largest sampled expected reward.\n\n        Args:\n            job_id: job id\n            prior_precision: Precision of the belief prior distribution.\n            num_exploration: How many static explorations to run when no observations are available.\n            arms: list of arms\n\n        Returns:\n            batch size to use\n        \"\"\"\n        arm_dict = {arm.batch_size: arm for arm in arms}\n\n        # Exploration-only phase.\n        # Order is random considering concurrent bandit scenarios.\n        choices = self.service.get_random_choices(\n            GetRandomChoices(job_id=job_id, choices=[arm.batch_size for arm in arms])\n        )\n\n        for arm in choices:\n            if arm_dict[arm].num_observations &lt; num_exploration:\n                logger.info(\"[%s] Explore arm %s.\", self.name, str(arm))\n                return arm\n\n        # Thomopson Sampling phase.\n        # Sample the expected reward for each arm.\n        # Assumes that each arm has been explored at least once. Otherwise,\n        # a value will be sampled from the prior.\n\n        expectations = {}  # A mapping from every arm to their sampled expected reward.\n        for arm in arms:\n            if arm.param_precision == prior_precision:\n                logger.warning(\n                    \"predict_expectations called when arm '%d' is cold.\",\n                    arm.batch_size,\n                    stacklevel=1,\n                )\n            expectations[arm.batch_size] = self.service.get_normal(\n                GetNormal(\n                    job_id=job_id,\n                    loc=arm.param_mean,\n                    scale=np.sqrt(np.reciprocal(arm.param_precision)),\n                )\n            )\n\n        logger.info(\"[%s] Sampled mean rewards:\", self.name)\n        for arm, sample in expectations.items():\n            logger.info(\n                \"[%s] Arm %d: mu ~ N(%.2f, %.2f) -&gt; %.2f\",\n                self.name,\n                arm,\n                arm_dict[arm].param_mean,\n                1 / arm_dict[arm].param_precision,\n                sample,\n            )\n\n        bs = max(expectations, key=expectations.get)  # type: ignore\n        logger.info(\"%s in Thompson Sampling stage -&gt; BS = %d\", job_id, bs)\n        return bs\n\n    async def construct_mab(\n        self, job: JobState, evidence: ExplorationsPerJob, good_bs: list[int]\n    ) -&gt; list[GaussianTsArmState]:\n        \"\"\"Construct arms and initialize them.\n\n        Args:\n            job: state of job.\n            evidence: Completed explorations. We create arms based on the explorations we have done during pruning stage.\n            good_bs: Converged batch size list.\n\n        Returns:\n            list of arms that we created\n\n        Raises:\n            `ValueError`: If exploration states is invalid (ex. number of pruning rounds doesn't corresponds)\n            `ZeusBSOValueError`: No converged batch sizes from pruning stage.\n        \"\"\"\n        if job.job_id != evidence.job_id:\n            raise ZeusBSOServiceBadOperationError(\n                f\"Job Id is not consistent: job({job.job_id}) != explorations({evidence.job_id})\"\n            )\n\n        if len(good_bs) == 0:\n            raise ZeusBSOValueError(\"While creating arms, no batch size is selected\")\n\n        logger.info(\n            \"Construct MAB for %s with arms %s\",\n            job.job_id,\n            str(good_bs),\n        )\n\n        new_arms: list[GaussianTsArmState] = []\n\n        # Fit the arm for each good batch size.\n        for _, bs in enumerate(good_bs):\n            rewards = []\n            # Collect rewards starting from the most recent ones and backwards.\n            for trial in evidence.explorations_per_bs[bs]:\n                if trial.energy is None or trial.time is None:\n                    raise ZeusBSOValueError(f\"Trial {trial.trial_number} has no energy or time set.\")\n                rewards.append(-zeus_cost(trial.energy, trial.time, job.eta_knob, job.max_power))\n\n            new_arms.append(\n                # create an arm\n                self._fit_arm(\n                    BatchSizeBase(job_id=job.job_id, batch_size=bs),\n                    job.mab_prior_mean,\n                    job.mab_prior_precision,\n                    np.array(rewards),\n                )\n            )\n\n        # submit new arms to db\n        self.service.create_arms(new_arms)\n        # update job stage from pruning to mab since we created arms\n        self.service.update_job_stage(UpdateJobStage(job_id=job.job_id, stage=Stage.MAB))\n        return new_arms\n\n    async def report(self, job: JobState, trial_result: UpdateTrial) -&gt; None:\n        \"\"\"Based on the measurement, update the arm state.\n\n        Args:\n            job: state of the job\n            trial_result: result of training (job id, batch_size, trial_number)\n\n        Raises:\n            `ZeusBSOValueError`: When the arm (job id, batch_size) doesn't exist\n        \"\"\"\n        if trial_result.energy is None or trial_result.time is None:\n            raise ZeusBSOValueError(f\"Trial {trial_result.trial_number} has no energy or time set.\")\n\n        # Since we're learning the reward precision, we need to\n        # 1. re-compute the precision of this arm based on the reward history,\n        # 2. update the arm's reward precision\n        # 3. and `fit` the new MAB instance on all the reward history.\n        # Note that `arm_rewards` always has more than one entry (and hence a\n        # non-zero variance) because we've been through pruning exploration.\n        batch_size_key = BatchSizeBase(job_id=job.job_id, batch_size=trial_result.batch_size)\n\n        # Get measurements of this bs in descending order. At most window_size length\n        history = await self.service.get_trial_results_of_bs(batch_size_key)\n\n        if len(history.results) &gt;= job.window_size and job.window_size &gt; 0:\n            # if the history is already above the window size, pop the last one to leave the spot for the current measurement.\n            history.results.pop()\n            history.results.reverse()  # Now ascending order.\n\n        costs = [-zeus_cost(m.energy, m.time, job.eta_knob, job.max_power) for m in history.results]\n        # Add current measurement to the costs\n        costs.append(-zeus_cost(trial_result.energy, trial_result.time, job.eta_knob, job.max_power))\n        arm_rewards = np.array(costs)\n\n        logger.info(\"Arm_rewards: %s\", str(arm_rewards))\n\n        # Get current arm.\n        arm = await self.service.get_arm(batch_size_key)\n\n        if arm is None:\n            raise ZeusBSOValueError(f\"MAB stage but Arm for batch size({trial_result.batch_size}) is not found.\")\n\n        # Get a new arm state based on observation\n        new_arm = self._fit_arm(batch_size_key, job.mab_prior_mean, job.mab_prior_precision, arm_rewards)\n\n        # update the new arm state in db\n        self.service.update_arm_state(\n            UpdateArm(\n                trial=ReadTrial(\n                    job_id=trial_result.job_id,\n                    batch_size=trial_result.batch_size,\n                    trial_number=trial_result.trial_number,\n                ),\n                updated_arm=new_arm,\n            )\n        )\n        # update corresponding trial\n        self.service.update_trial(trial_result)\n\n        arm_rewards_repr = \", \".join([f\"{r:.2f}\" for r in arm_rewards])\n        logger.info(\n            \"%s @ %d: arm_rewards = [%s], reward_prec = %.2f\",\n            job.job_id,\n            trial_result.batch_size,\n            arm_rewards_repr,\n            new_arm.reward_precision,\n        )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/mab/#zeus.optimizer.batch_size.server.mab.GaussianTS.__init__","title":"__init__","text":"<pre><code>__init__(service)\n</code></pre> Source code in <code>zeus/optimizer/batch_size/server/mab.py</code> <pre><code>def __init__(self, service: ZeusService):\n    \"\"\"Set up zeus service to interact with database.\"\"\"\n    self.service = service\n    self.name = \"GaussianTS\"\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/mab/#zeus.optimizer.batch_size.server.mab.GaussianTS._fit_arm","title":"_fit_arm","text":"<pre><code>_fit_arm(bs_base, prior_mean, prior_precision, rewards)\n</code></pre> <p>Update the parameter distribution for one arm.</p> <p>Reference: https://en.wikipedia.org/wiki/Conjugate_prior</p> <p>Parameters:</p> Name Type Description Default <code>bs_base</code> <code>BatchSizeBase</code> <p>job id and batch size tha represents this arm</p> required <code>prior_mean</code> <code>float</code> <p>Mean of the belief prior distribution.</p> required <code>prior_precision</code> <code>float</code> <p>Precision of the belief prior distribution.</p> required <code>rewards</code> <code>ndarray</code> <p>Array of rewards observed by pulling that arm.</p> required <p>Returns:</p> Type Description <code>GaussianTsArmState</code> <p>Updated arm state</p> Source code in <code>zeus/optimizer/batch_size/server/mab.py</code> <pre><code>def _fit_arm(\n    self,\n    bs_base: BatchSizeBase,\n    prior_mean: float,\n    prior_precision: float,\n    rewards: np.ndarray,\n) -&gt; GaussianTsArmState:\n    \"\"\"Update the parameter distribution for one arm.\n\n    Reference: &lt;https://en.wikipedia.org/wiki/Conjugate_prior&gt;\n\n    Args:\n        bs_base: job id and batch size tha represents this arm\n        prior_mean: Mean of the belief prior distribution.\n        prior_precision: Precision of the belief prior distribution.\n        rewards: Array of rewards observed by pulling that arm.\n\n    Returns:\n        Updated arm state\n    \"\"\"\n    if len(rewards) == 0:\n        raise ZeusBSOValueError(\"No rewards to fit the arm.\")\n\n    variance = np.var(rewards)\n    reward_prec = np.inf if variance == 0.0 else np.reciprocal(variance)\n\n    # Reset to priors\n    mean = prior_mean\n    prec = prior_precision\n\n    # Compute the parameters of the posterior distribution.\n    # The reward distribution's precision is given as infinite only when we\n    # have exactly one observation for the arm, s.t. sampling yields that\n    # exact observation.\n    if reward_prec == np.inf:\n        new_prec = np.inf\n        new_mean = rewards.mean()\n    else:\n        new_prec = prec + len(rewards) * reward_prec\n        new_mean = (prec * mean + reward_prec * rewards.sum()) / new_prec\n\n    # Updated state.\n    return GaussianTsArmState(\n        job_id=bs_base.job_id,\n        batch_size=bs_base.batch_size,\n        param_mean=new_mean,\n        param_precision=new_prec,\n        reward_precision=reward_prec,\n        num_observations=len(rewards),\n    )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/mab/#zeus.optimizer.batch_size.server.mab.GaussianTS.predict","title":"predict","text":"<pre><code>predict(job_id, prior_precision, num_exploration, arms)\n</code></pre> <p>Return the arm with the largest sampled expected reward.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>job id</p> required <code>prior_precision</code> <code>float</code> <p>Precision of the belief prior distribution.</p> required <code>num_exploration</code> <code>int</code> <p>How many static explorations to run when no observations are available.</p> required <code>arms</code> <code>list[GaussianTsArmState]</code> <p>list of arms</p> required <p>Returns:</p> Type Description <code>int</code> <p>batch size to use</p> Source code in <code>zeus/optimizer/batch_size/server/mab.py</code> <pre><code>def predict(\n    self,\n    job_id: str,\n    prior_precision: float,\n    num_exploration: int,\n    arms: list[GaussianTsArmState],\n) -&gt; int:\n    \"\"\"Return the arm with the largest sampled expected reward.\n\n    Args:\n        job_id: job id\n        prior_precision: Precision of the belief prior distribution.\n        num_exploration: How many static explorations to run when no observations are available.\n        arms: list of arms\n\n    Returns:\n        batch size to use\n    \"\"\"\n    arm_dict = {arm.batch_size: arm for arm in arms}\n\n    # Exploration-only phase.\n    # Order is random considering concurrent bandit scenarios.\n    choices = self.service.get_random_choices(\n        GetRandomChoices(job_id=job_id, choices=[arm.batch_size for arm in arms])\n    )\n\n    for arm in choices:\n        if arm_dict[arm].num_observations &lt; num_exploration:\n            logger.info(\"[%s] Explore arm %s.\", self.name, str(arm))\n            return arm\n\n    # Thomopson Sampling phase.\n    # Sample the expected reward for each arm.\n    # Assumes that each arm has been explored at least once. Otherwise,\n    # a value will be sampled from the prior.\n\n    expectations = {}  # A mapping from every arm to their sampled expected reward.\n    for arm in arms:\n        if arm.param_precision == prior_precision:\n            logger.warning(\n                \"predict_expectations called when arm '%d' is cold.\",\n                arm.batch_size,\n                stacklevel=1,\n            )\n        expectations[arm.batch_size] = self.service.get_normal(\n            GetNormal(\n                job_id=job_id,\n                loc=arm.param_mean,\n                scale=np.sqrt(np.reciprocal(arm.param_precision)),\n            )\n        )\n\n    logger.info(\"[%s] Sampled mean rewards:\", self.name)\n    for arm, sample in expectations.items():\n        logger.info(\n            \"[%s] Arm %d: mu ~ N(%.2f, %.2f) -&gt; %.2f\",\n            self.name,\n            arm,\n            arm_dict[arm].param_mean,\n            1 / arm_dict[arm].param_precision,\n            sample,\n        )\n\n    bs = max(expectations, key=expectations.get)  # type: ignore\n    logger.info(\"%s in Thompson Sampling stage -&gt; BS = %d\", job_id, bs)\n    return bs\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/mab/#zeus.optimizer.batch_size.server.mab.GaussianTS.construct_mab","title":"construct_mab  <code>async</code>","text":"<pre><code>construct_mab(job, evidence, good_bs)\n</code></pre> <p>Construct arms and initialize them.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>JobState</code> <p>state of job.</p> required <code>evidence</code> <code>ExplorationsPerJob</code> <p>Completed explorations. We create arms based on the explorations we have done during pruning stage.</p> required <code>good_bs</code> <code>list[int]</code> <p>Converged batch size list.</p> required <p>Returns:</p> Type Description <code>list[GaussianTsArmState]</code> <p>list of arms that we created</p> <p>Raises:</p> Type Description <code>`ValueError`</code> <p>If exploration states is invalid (ex. number of pruning rounds doesn't corresponds)</p> <code>`ZeusBSOValueError`</code> <p>No converged batch sizes from pruning stage.</p> Source code in <code>zeus/optimizer/batch_size/server/mab.py</code> <pre><code>async def construct_mab(\n    self, job: JobState, evidence: ExplorationsPerJob, good_bs: list[int]\n) -&gt; list[GaussianTsArmState]:\n    \"\"\"Construct arms and initialize them.\n\n    Args:\n        job: state of job.\n        evidence: Completed explorations. We create arms based on the explorations we have done during pruning stage.\n        good_bs: Converged batch size list.\n\n    Returns:\n        list of arms that we created\n\n    Raises:\n        `ValueError`: If exploration states is invalid (ex. number of pruning rounds doesn't corresponds)\n        `ZeusBSOValueError`: No converged batch sizes from pruning stage.\n    \"\"\"\n    if job.job_id != evidence.job_id:\n        raise ZeusBSOServiceBadOperationError(\n            f\"Job Id is not consistent: job({job.job_id}) != explorations({evidence.job_id})\"\n        )\n\n    if len(good_bs) == 0:\n        raise ZeusBSOValueError(\"While creating arms, no batch size is selected\")\n\n    logger.info(\n        \"Construct MAB for %s with arms %s\",\n        job.job_id,\n        str(good_bs),\n    )\n\n    new_arms: list[GaussianTsArmState] = []\n\n    # Fit the arm for each good batch size.\n    for _, bs in enumerate(good_bs):\n        rewards = []\n        # Collect rewards starting from the most recent ones and backwards.\n        for trial in evidence.explorations_per_bs[bs]:\n            if trial.energy is None or trial.time is None:\n                raise ZeusBSOValueError(f\"Trial {trial.trial_number} has no energy or time set.\")\n            rewards.append(-zeus_cost(trial.energy, trial.time, job.eta_knob, job.max_power))\n\n        new_arms.append(\n            # create an arm\n            self._fit_arm(\n                BatchSizeBase(job_id=job.job_id, batch_size=bs),\n                job.mab_prior_mean,\n                job.mab_prior_precision,\n                np.array(rewards),\n            )\n        )\n\n    # submit new arms to db\n    self.service.create_arms(new_arms)\n    # update job stage from pruning to mab since we created arms\n    self.service.update_job_stage(UpdateJobStage(job_id=job.job_id, stage=Stage.MAB))\n    return new_arms\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/mab/#zeus.optimizer.batch_size.server.mab.GaussianTS.report","title":"report  <code>async</code>","text":"<pre><code>report(job, trial_result)\n</code></pre> <p>Based on the measurement, update the arm state.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>JobState</code> <p>state of the job</p> required <code>trial_result</code> <code>UpdateTrial</code> <p>result of training (job id, batch_size, trial_number)</p> required <p>Raises:</p> Type Description <code>`ZeusBSOValueError`</code> <p>When the arm (job id, batch_size) doesn't exist</p> Source code in <code>zeus/optimizer/batch_size/server/mab.py</code> <pre><code>async def report(self, job: JobState, trial_result: UpdateTrial) -&gt; None:\n    \"\"\"Based on the measurement, update the arm state.\n\n    Args:\n        job: state of the job\n        trial_result: result of training (job id, batch_size, trial_number)\n\n    Raises:\n        `ZeusBSOValueError`: When the arm (job id, batch_size) doesn't exist\n    \"\"\"\n    if trial_result.energy is None or trial_result.time is None:\n        raise ZeusBSOValueError(f\"Trial {trial_result.trial_number} has no energy or time set.\")\n\n    # Since we're learning the reward precision, we need to\n    # 1. re-compute the precision of this arm based on the reward history,\n    # 2. update the arm's reward precision\n    # 3. and `fit` the new MAB instance on all the reward history.\n    # Note that `arm_rewards` always has more than one entry (and hence a\n    # non-zero variance) because we've been through pruning exploration.\n    batch_size_key = BatchSizeBase(job_id=job.job_id, batch_size=trial_result.batch_size)\n\n    # Get measurements of this bs in descending order. At most window_size length\n    history = await self.service.get_trial_results_of_bs(batch_size_key)\n\n    if len(history.results) &gt;= job.window_size and job.window_size &gt; 0:\n        # if the history is already above the window size, pop the last one to leave the spot for the current measurement.\n        history.results.pop()\n        history.results.reverse()  # Now ascending order.\n\n    costs = [-zeus_cost(m.energy, m.time, job.eta_knob, job.max_power) for m in history.results]\n    # Add current measurement to the costs\n    costs.append(-zeus_cost(trial_result.energy, trial_result.time, job.eta_knob, job.max_power))\n    arm_rewards = np.array(costs)\n\n    logger.info(\"Arm_rewards: %s\", str(arm_rewards))\n\n    # Get current arm.\n    arm = await self.service.get_arm(batch_size_key)\n\n    if arm is None:\n        raise ZeusBSOValueError(f\"MAB stage but Arm for batch size({trial_result.batch_size}) is not found.\")\n\n    # Get a new arm state based on observation\n    new_arm = self._fit_arm(batch_size_key, job.mab_prior_mean, job.mab_prior_precision, arm_rewards)\n\n    # update the new arm state in db\n    self.service.update_arm_state(\n        UpdateArm(\n            trial=ReadTrial(\n                job_id=trial_result.job_id,\n                batch_size=trial_result.batch_size,\n                trial_number=trial_result.trial_number,\n            ),\n            updated_arm=new_arm,\n        )\n    )\n    # update corresponding trial\n    self.service.update_trial(trial_result)\n\n    arm_rewards_repr = \", \".join([f\"{r:.2f}\" for r in arm_rewards])\n    logger.info(\n        \"%s @ %d: arm_rewards = [%s], reward_prec = %.2f\",\n        job.job_id,\n        trial_result.batch_size,\n        arm_rewards_repr,\n        new_arm.reward_precision,\n    )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/optimizer/","title":"optimizer","text":""},{"location":"reference/optimizer/batch_size/server/optimizer/#zeus.optimizer.batch_size.server.optimizer","title":"zeus.optimizer.batch_size.server.optimizer","text":"<p>Batch size optimizer top-most layer that provides register/report/predict.</p>"},{"location":"reference/optimizer/batch_size/server/optimizer/#zeus.optimizer.batch_size.server.optimizer.ZeusBatchSizeOptimizer","title":"ZeusBatchSizeOptimizer","text":"<p>Batch size optimizer server. Manages which stage the job is in and call corresponding manager (pruning or mab).</p> Source code in <code>zeus/optimizer/batch_size/server/optimizer.py</code> <pre><code>class ZeusBatchSizeOptimizer:\n    \"\"\"Batch size optimizer server. Manages which stage the job is in and call corresponding manager (pruning or mab).\"\"\"\n\n    def __init__(self, service: ZeusService) -&gt; None:\n        \"\"\"Initialize the server. Set the service, pruning manager, and mab.\n\n        Args:\n            service: ZeusService for interacting with database\n        \"\"\"\n        self.service = service\n        self.pruning_manager = PruningExploreManager(service)\n        self.mab = GaussianTS(service)\n\n    async def register_job(self, job: JobSpecFromClient) -&gt; bool:\n        \"\"\"Register a job that user submitted. If the job id already exists, check if it is identical with previously registered configuration.\n\n        Args:\n            job: job configuration\n\n        Returns:\n            True if a job is regiested, False if a job already exists and identical with previous configuration\n\n        Raises:\n            [`ZeusBSOJobConfigMismatchError`][zeus.optimizer.batch_size.server.exceptions.ZeusBSOJobConfigMismatchError]: In the case of existing job, if job configuration doesn't match with previously registered config\n        \"\"\"\n        registered_job = None\n\n        if job.job_id is None:\n            while True:\n                job.job_id = f\"{job.job_id_prefix}-{hashlib.sha256(str(time.time()).encode()).hexdigest()[:8]}\"\n                if (await self.service.get_job(job.job_id)) is None:\n                    break\n        else:\n            registered_job = await self.service.get_job(job.job_id)\n\n        if registered_job is not None:\n            # Job exists\n            logger.info(\"Job(%s) already exists\", job.job_id)\n            registerd_job_config = JobSpecFromClient.parse_obj(registered_job.dict())\n\n            # check if it is identical\n            if registerd_job_config != job:\n                raise ZeusBSOJobConfigMismatchError(\n                    \"JobSpec doesn't match with existing jobSpec. Use a new job_id for different configuration\"\n                )\n            return False\n\n        self.service.create_job(CreateJob.from_job_config(job))\n        logger.info(\"Registered %s\", job.job_id)\n\n        return True\n\n    async def predict(self, job_id: str) -&gt; TrialId:\n        \"\"\"Return a batch size to use.\n\n        Args:\n            job_id: Id of job\n\n        Returns:\n            batch size to use\n\n        Raises:\n            [`ZeusBSOValueError`][zeus.optimizer.batch_size.server.exceptions.ZeusBSOValueError]: If the job id is unknown, or creating a mab failed due to no converged batch size\n        \"\"\"\n        job = await self.service.get_job(job_id)\n\n        if job is None:\n            raise ZeusBSOValueError(f\"Unknown job({job_id}). Please register the job first\")\n\n        if job.stage == Stage.MAB:\n            # If we are in MAB stage, use mab to get the next batch size\n            arms = await self.service.get_arms(job_id)\n            next_trial = await self.service.create_trial(\n                CreateMabTrial(\n                    job_id=job_id,\n                    batch_size=self.mab.predict(job_id, job.mab_prior_precision, job.mab_num_explorations, arms),\n                )\n            )\n        else:\n            # Pruning stage\n            explorations = await self.service.get_explorations_of_job(job_id)\n            # First check if pruning explorer can give us any batch size. Returns batch_size or MAB to indicate going to MAB stage\n            res = await self.pruning_manager.next_batch_size(job, explorations)\n\n            if isinstance(res, list):\n                # MAB stage: construct MAB and update the job stage to MAB. Return the batch size from MAB\n                logger.info(\"Constructing a MAB\")\n                arms = await self.mab.construct_mab(job, explorations, res)\n                next_trial = await self.service.create_trial(\n                    CreateMabTrial(\n                        job_id=job_id,\n                        batch_size=self.mab.predict(\n                            job_id,\n                            job.mab_prior_precision,\n                            job.mab_num_explorations,\n                            arms,\n                        ),\n                    )\n                )\n            else:\n                next_trial = res\n\n        return TrialId(\n            job_id=next_trial.job_id,\n            batch_size=next_trial.batch_size,\n            trial_number=next_trial.trial_number,\n        )\n\n    async def report(self, result: TrainingResult) -&gt; ReportResponse:\n        \"\"\"Report the training result. Stop train if the train is converged or reached max epochs or reached early stop threshold. Otherwise, keep training.\n\n        Args:\n            result: result of training [`TrainingResult`][zeus.optimizer.batch_size.common.TrainingResult].\n\n        Returns:\n            Decision on training [`ReportResponse`][zeus.optimizer.batch_size.common.ReportResponse].\n        \"\"\"\n        cost_ub = np.inf\n        job = await self.service.get_job(result.job_id)\n        if job is None:\n            raise ZeusBSOServiceBadOperationError(f\"Unknown job {result.job_id}\")\n\n        trial = await self.service.get_trial(\n            ReadTrial(\n                job_id=result.job_id,\n                batch_size=result.batch_size,\n                trial_number=result.trial_number,\n            )\n        )\n        if trial is None:\n            raise ZeusBSOServiceBadOperationError(f\"Unknown trial {result}\")\n\n        if trial.status != TrialStatus.Dispatched:\n            # result is already reported\n            if trial.converged is None:\n                raise ZeusBSOValueError(f\"Trial({trial.trial_number}) is already reported but converged is not set.\")\n            return ReportResponse(\n                stop_train=True,\n                converged=trial.converged,\n                message=f\"Result for this trial({trial.trial_number}) is already reported.\",\n            )\n\n        if job.beta_knob is not None and job.min_cost is not None:  # Early stop enabled\n            cost_ub = job.beta_knob * job.min_cost\n\n        reported_cost = zeus_cost(\n            result.energy,\n            result.time,\n            job.eta_knob,\n            job.max_power,\n        )\n\n        within_cost_range = cost_ub &gt;= reported_cost\n        converged = (job.higher_is_better_metric and job.target_metric &lt;= result.metric) or (\n            not job.higher_is_better_metric and job.target_metric &gt;= result.metric\n        )\n\n        if within_cost_range and result.current_epoch &lt; job.max_epochs and not converged:\n            # If it's not converged but below cost upper bound and haven't reached max_epochs, keep training\n\n            return ReportResponse(\n                stop_train=False,\n                converged=False,\n                message=\"Stop condition not met, keep training\",\n            )\n\n        # Two cases below here (training ended)\n        # 1. Converged == true\n        # 2. reached max_epoch OR excceded upper bound cost (error case)\n        if converged and within_cost_range:\n            message = \"Train succeeded\"\n        elif not within_cost_range:\n            message = f\"\"\"Batch Size({result.batch_size}) exceeded the cost upper bound: current cost({reported_cost}) &gt;\n                    beta_knob({job.beta_knob})*min_cost({job.min_cost})\"\"\"\n        else:\n            # not converged\n            message = f\"Train failed to converge within max_epoch({job.max_epochs})\"\n\n        trial_result = UpdateTrial(\n            job_id=result.job_id,\n            batch_size=result.batch_size,\n            status=TrialStatus.Succeeded,\n            trial_number=result.trial_number,\n            time=result.time,\n            energy=result.energy,\n            converged=converged and within_cost_range,\n        )\n\n        if job.stage == Stage.MAB:\n            await self.mab.report(job, trial_result)\n        else:\n            # Pruning stage\n            logger.info(\n                \"%s in pruning stage, Current BS %s that did %s converge.\",\n                result.job_id,\n                result.batch_size,\n                \"not\" * (not converged),\n            )\n            # update trial\n            self.service.update_trial(trial_result)\n\n        assert trial_result.converged is not None, \"This just set to boolean above.\"\n        return ReportResponse(stop_train=True, converged=trial_result.converged, message=message)\n\n    async def end_trial(self, trial_id: TrialId) -&gt; None:\n        \"\"\"Mark the trial as finished. If status is still `Dispatched` make the trial as `Failed`.\n\n        Args:\n            trial_id: Unique identifier of trial\n\n        Raises:\n            [`ZeusBSOServerNotFound`][zeus.optimizer.batch_size.server.exceptions.ZeusBSOServerNotFound]: If there is no corresponding trial.\n        \"\"\"\n        trial = await self.service.get_trial(ReadTrial(**trial_id.dict()))\n\n        if trial is not None:\n            if trial.status == TrialStatus.Dispatched:\n                self.service.update_trial(\n                    UpdateTrial(\n                        job_id=trial_id.job_id,\n                        batch_size=trial_id.batch_size,\n                        trial_number=trial_id.trial_number,\n                        status=TrialStatus.Failed,\n                    )\n                )\n        else:\n            raise ZeusBSOServerNotFoundError(f\"Could not find the trial: {trial_id}\")\n\n    async def delete_job(self, job_id: str) -&gt; None:\n        \"\"\"Delete a job.\n\n        Args:\n            job_id: ID of a job.\n\n        Returns:\n            True if the job is deleted. False if none was deleted\n        \"\"\"\n        if not (await self.service.delete_job(job_id)):\n            raise ZeusBSOServerNotFoundError(\"No job was deleted.\")\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/optimizer/#zeus.optimizer.batch_size.server.optimizer.ZeusBatchSizeOptimizer.__init__","title":"__init__","text":"<pre><code>__init__(service)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>service</code> <code>ZeusService</code> <p>ZeusService for interacting with database</p> required Source code in <code>zeus/optimizer/batch_size/server/optimizer.py</code> <pre><code>def __init__(self, service: ZeusService) -&gt; None:\n    \"\"\"Initialize the server. Set the service, pruning manager, and mab.\n\n    Args:\n        service: ZeusService for interacting with database\n    \"\"\"\n    self.service = service\n    self.pruning_manager = PruningExploreManager(service)\n    self.mab = GaussianTS(service)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/optimizer/#zeus.optimizer.batch_size.server.optimizer.ZeusBatchSizeOptimizer.register_job","title":"register_job  <code>async</code>","text":"<pre><code>register_job(job)\n</code></pre> <p>Register a job that user submitted. If the job id already exists, check if it is identical with previously registered configuration.</p> <p>Parameters:</p> Name Type Description Default <code>job</code> <code>JobSpecFromClient</code> <p>job configuration</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if a job is regiested, False if a job already exists and identical with previous configuration</p> <p>Raises:</p> Type Description <code>[`ZeusBSOJobConfigMismatchError`][zeus.optimizer.batch_size.server.exceptions.ZeusBSOJobConfigMismatchError]</code> <p>In the case of existing job, if job configuration doesn't match with previously registered config</p> Source code in <code>zeus/optimizer/batch_size/server/optimizer.py</code> <pre><code>async def register_job(self, job: JobSpecFromClient) -&gt; bool:\n    \"\"\"Register a job that user submitted. If the job id already exists, check if it is identical with previously registered configuration.\n\n    Args:\n        job: job configuration\n\n    Returns:\n        True if a job is regiested, False if a job already exists and identical with previous configuration\n\n    Raises:\n        [`ZeusBSOJobConfigMismatchError`][zeus.optimizer.batch_size.server.exceptions.ZeusBSOJobConfigMismatchError]: In the case of existing job, if job configuration doesn't match with previously registered config\n    \"\"\"\n    registered_job = None\n\n    if job.job_id is None:\n        while True:\n            job.job_id = f\"{job.job_id_prefix}-{hashlib.sha256(str(time.time()).encode()).hexdigest()[:8]}\"\n            if (await self.service.get_job(job.job_id)) is None:\n                break\n    else:\n        registered_job = await self.service.get_job(job.job_id)\n\n    if registered_job is not None:\n        # Job exists\n        logger.info(\"Job(%s) already exists\", job.job_id)\n        registerd_job_config = JobSpecFromClient.parse_obj(registered_job.dict())\n\n        # check if it is identical\n        if registerd_job_config != job:\n            raise ZeusBSOJobConfigMismatchError(\n                \"JobSpec doesn't match with existing jobSpec. Use a new job_id for different configuration\"\n            )\n        return False\n\n    self.service.create_job(CreateJob.from_job_config(job))\n    logger.info(\"Registered %s\", job.job_id)\n\n    return True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/optimizer/#zeus.optimizer.batch_size.server.optimizer.ZeusBatchSizeOptimizer.predict","title":"predict  <code>async</code>","text":"<pre><code>predict(job_id)\n</code></pre> <p>Return a batch size to use.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>Id of job</p> required <p>Returns:</p> Type Description <code>TrialId</code> <p>batch size to use</p> <p>Raises:</p> Type Description <code>[`ZeusBSOValueError`][zeus.optimizer.batch_size.server.exceptions.ZeusBSOValueError]</code> <p>If the job id is unknown, or creating a mab failed due to no converged batch size</p> Source code in <code>zeus/optimizer/batch_size/server/optimizer.py</code> <pre><code>async def predict(self, job_id: str) -&gt; TrialId:\n    \"\"\"Return a batch size to use.\n\n    Args:\n        job_id: Id of job\n\n    Returns:\n        batch size to use\n\n    Raises:\n        [`ZeusBSOValueError`][zeus.optimizer.batch_size.server.exceptions.ZeusBSOValueError]: If the job id is unknown, or creating a mab failed due to no converged batch size\n    \"\"\"\n    job = await self.service.get_job(job_id)\n\n    if job is None:\n        raise ZeusBSOValueError(f\"Unknown job({job_id}). Please register the job first\")\n\n    if job.stage == Stage.MAB:\n        # If we are in MAB stage, use mab to get the next batch size\n        arms = await self.service.get_arms(job_id)\n        next_trial = await self.service.create_trial(\n            CreateMabTrial(\n                job_id=job_id,\n                batch_size=self.mab.predict(job_id, job.mab_prior_precision, job.mab_num_explorations, arms),\n            )\n        )\n    else:\n        # Pruning stage\n        explorations = await self.service.get_explorations_of_job(job_id)\n        # First check if pruning explorer can give us any batch size. Returns batch_size or MAB to indicate going to MAB stage\n        res = await self.pruning_manager.next_batch_size(job, explorations)\n\n        if isinstance(res, list):\n            # MAB stage: construct MAB and update the job stage to MAB. Return the batch size from MAB\n            logger.info(\"Constructing a MAB\")\n            arms = await self.mab.construct_mab(job, explorations, res)\n            next_trial = await self.service.create_trial(\n                CreateMabTrial(\n                    job_id=job_id,\n                    batch_size=self.mab.predict(\n                        job_id,\n                        job.mab_prior_precision,\n                        job.mab_num_explorations,\n                        arms,\n                    ),\n                )\n            )\n        else:\n            next_trial = res\n\n    return TrialId(\n        job_id=next_trial.job_id,\n        batch_size=next_trial.batch_size,\n        trial_number=next_trial.trial_number,\n    )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/optimizer/#zeus.optimizer.batch_size.server.optimizer.ZeusBatchSizeOptimizer.report","title":"report  <code>async</code>","text":"<pre><code>report(result)\n</code></pre> <p>Report the training result. Stop train if the train is converged or reached max epochs or reached early stop threshold. Otherwise, keep training.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>TrainingResult</code> <p>result of training <code>TrainingResult</code>.</p> required <p>Returns:</p> Type Description <code>ReportResponse</code> <p>Decision on training <code>ReportResponse</code>.</p> Source code in <code>zeus/optimizer/batch_size/server/optimizer.py</code> <pre><code>async def report(self, result: TrainingResult) -&gt; ReportResponse:\n    \"\"\"Report the training result. Stop train if the train is converged or reached max epochs or reached early stop threshold. Otherwise, keep training.\n\n    Args:\n        result: result of training [`TrainingResult`][zeus.optimizer.batch_size.common.TrainingResult].\n\n    Returns:\n        Decision on training [`ReportResponse`][zeus.optimizer.batch_size.common.ReportResponse].\n    \"\"\"\n    cost_ub = np.inf\n    job = await self.service.get_job(result.job_id)\n    if job is None:\n        raise ZeusBSOServiceBadOperationError(f\"Unknown job {result.job_id}\")\n\n    trial = await self.service.get_trial(\n        ReadTrial(\n            job_id=result.job_id,\n            batch_size=result.batch_size,\n            trial_number=result.trial_number,\n        )\n    )\n    if trial is None:\n        raise ZeusBSOServiceBadOperationError(f\"Unknown trial {result}\")\n\n    if trial.status != TrialStatus.Dispatched:\n        # result is already reported\n        if trial.converged is None:\n            raise ZeusBSOValueError(f\"Trial({trial.trial_number}) is already reported but converged is not set.\")\n        return ReportResponse(\n            stop_train=True,\n            converged=trial.converged,\n            message=f\"Result for this trial({trial.trial_number}) is already reported.\",\n        )\n\n    if job.beta_knob is not None and job.min_cost is not None:  # Early stop enabled\n        cost_ub = job.beta_knob * job.min_cost\n\n    reported_cost = zeus_cost(\n        result.energy,\n        result.time,\n        job.eta_knob,\n        job.max_power,\n    )\n\n    within_cost_range = cost_ub &gt;= reported_cost\n    converged = (job.higher_is_better_metric and job.target_metric &lt;= result.metric) or (\n        not job.higher_is_better_metric and job.target_metric &gt;= result.metric\n    )\n\n    if within_cost_range and result.current_epoch &lt; job.max_epochs and not converged:\n        # If it's not converged but below cost upper bound and haven't reached max_epochs, keep training\n\n        return ReportResponse(\n            stop_train=False,\n            converged=False,\n            message=\"Stop condition not met, keep training\",\n        )\n\n    # Two cases below here (training ended)\n    # 1. Converged == true\n    # 2. reached max_epoch OR excceded upper bound cost (error case)\n    if converged and within_cost_range:\n        message = \"Train succeeded\"\n    elif not within_cost_range:\n        message = f\"\"\"Batch Size({result.batch_size}) exceeded the cost upper bound: current cost({reported_cost}) &gt;\n                beta_knob({job.beta_knob})*min_cost({job.min_cost})\"\"\"\n    else:\n        # not converged\n        message = f\"Train failed to converge within max_epoch({job.max_epochs})\"\n\n    trial_result = UpdateTrial(\n        job_id=result.job_id,\n        batch_size=result.batch_size,\n        status=TrialStatus.Succeeded,\n        trial_number=result.trial_number,\n        time=result.time,\n        energy=result.energy,\n        converged=converged and within_cost_range,\n    )\n\n    if job.stage == Stage.MAB:\n        await self.mab.report(job, trial_result)\n    else:\n        # Pruning stage\n        logger.info(\n            \"%s in pruning stage, Current BS %s that did %s converge.\",\n            result.job_id,\n            result.batch_size,\n            \"not\" * (not converged),\n        )\n        # update trial\n        self.service.update_trial(trial_result)\n\n    assert trial_result.converged is not None, \"This just set to boolean above.\"\n    return ReportResponse(stop_train=True, converged=trial_result.converged, message=message)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/optimizer/#zeus.optimizer.batch_size.server.optimizer.ZeusBatchSizeOptimizer.end_trial","title":"end_trial  <code>async</code>","text":"<pre><code>end_trial(trial_id)\n</code></pre> <p>Mark the trial as finished. If status is still <code>Dispatched</code> make the trial as <code>Failed</code>.</p> <p>Parameters:</p> Name Type Description Default <code>trial_id</code> <code>TrialId</code> <p>Unique identifier of trial</p> required <p>Raises:</p> Type Description <code>[`ZeusBSOServerNotFound`][zeus.optimizer.batch_size.server.exceptions.ZeusBSOServerNotFound]</code> <p>If there is no corresponding trial.</p> Source code in <code>zeus/optimizer/batch_size/server/optimizer.py</code> <pre><code>async def end_trial(self, trial_id: TrialId) -&gt; None:\n    \"\"\"Mark the trial as finished. If status is still `Dispatched` make the trial as `Failed`.\n\n    Args:\n        trial_id: Unique identifier of trial\n\n    Raises:\n        [`ZeusBSOServerNotFound`][zeus.optimizer.batch_size.server.exceptions.ZeusBSOServerNotFound]: If there is no corresponding trial.\n    \"\"\"\n    trial = await self.service.get_trial(ReadTrial(**trial_id.dict()))\n\n    if trial is not None:\n        if trial.status == TrialStatus.Dispatched:\n            self.service.update_trial(\n                UpdateTrial(\n                    job_id=trial_id.job_id,\n                    batch_size=trial_id.batch_size,\n                    trial_number=trial_id.trial_number,\n                    status=TrialStatus.Failed,\n                )\n            )\n    else:\n        raise ZeusBSOServerNotFoundError(f\"Could not find the trial: {trial_id}\")\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/optimizer/#zeus.optimizer.batch_size.server.optimizer.ZeusBatchSizeOptimizer.delete_job","title":"delete_job  <code>async</code>","text":"<pre><code>delete_job(job_id)\n</code></pre> <p>Delete a job.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>ID of a job.</p> required <p>Returns:</p> Type Description <code>None</code> <p>True if the job is deleted. False if none was deleted</p> Source code in <code>zeus/optimizer/batch_size/server/optimizer.py</code> <pre><code>async def delete_job(self, job_id: str) -&gt; None:\n    \"\"\"Delete a job.\n\n    Args:\n        job_id: ID of a job.\n\n    Returns:\n        True if the job is deleted. False if none was deleted\n    \"\"\"\n    if not (await self.service.delete_job(job_id)):\n        raise ZeusBSOServerNotFoundError(\"No job was deleted.\")\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/router/","title":"router","text":""},{"location":"reference/optimizer/batch_size/server/router/#zeus.optimizer.batch_size.server.router","title":"zeus.optimizer.batch_size.server.router","text":"<p>Zeus batch size optimizer server FAST API router.</p>"},{"location":"reference/optimizer/batch_size/server/router/#zeus.optimizer.batch_size.server.router.get_job_locks","title":"get_job_locks","text":"<pre><code>get_job_locks()\n</code></pre> <p>Get global job locks.</p> Source code in <code>zeus/optimizer/batch_size/server/router.py</code> <pre><code>def get_job_locks() -&gt; defaultdict[str, asyncio.Lock]:\n    \"\"\"Get global job locks.\"\"\"\n    return JOB_LOCKS\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/router/#zeus.optimizer.batch_size.server.router.get_prefix_locks","title":"get_prefix_locks","text":"<pre><code>get_prefix_locks()\n</code></pre> <p>Get global job Id prefix locks.</p> Source code in <code>zeus/optimizer/batch_size/server/router.py</code> <pre><code>def get_prefix_locks() -&gt; defaultdict[str, asyncio.Lock]:\n    \"\"\"Get global job Id prefix locks.\"\"\"\n    return PREFIX_LOCKS\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/router/#zeus.optimizer.batch_size.server.router.register_job","title":"register_job  <code>async</code>","text":"<pre><code>register_job(job, response, db_session=Depends(get_db_session), prefix_locks=Depends(get_prefix_locks))\n</code></pre> <p>Endpoint for users to register a job or check if the job is registered and configuration is identical.</p> Source code in <code>zeus/optimizer/batch_size/server/router.py</code> <pre><code>@app.post(\n    REGISTER_JOB_URL,\n    responses={\n        200: {\"description\": \"Job is already registered\"},\n        201: {\"description\": \"Job is successfully registered\"},\n    },\n    response_model=JobSpecFromClient,\n)\nasync def register_job(\n    job: JobSpecFromClient,\n    response: Response,\n    db_session: AsyncSession = Depends(get_db_session),\n    prefix_locks: defaultdict[str, asyncio.Lock] = Depends(get_prefix_locks),\n):\n    \"\"\"Endpoint for users to register a job or check if the job is registered and configuration is identical.\"\"\"\n    async with prefix_locks[job.job_id_prefix]:\n        # One lock for registering a job. To prevent getting a same lock\n        optimizer = ZeusBatchSizeOptimizer(ZeusService(db_session))\n        try:\n            created = await optimizer.register_job(job)\n            await db_session.commit()\n            if created:\n                # new job is created\n                response.status_code = status.HTTP_201_CREATED\n            else:\n                # job already exists\n                response.status_code = status.HTTP_200_OK\n            return job\n        except ZeusBSOServerBaseError as err:\n            await db_session.rollback()\n            return JSONResponse(\n                status_code=err.status_code,\n                content={\"message\": err.message},\n            )\n        except Exception as err:\n            await db_session.rollback()\n            logger.error(\"Commit Failed: %s\", str(err))\n            return JSONResponse(\n                status_code=500,\n                content={\"message\": str(err)},\n            )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/router/#zeus.optimizer.batch_size.server.router.delete_job","title":"delete_job  <code>async</code>","text":"<pre><code>delete_job(job_id, db_session=Depends(get_db_session), job_locks=Depends(get_job_locks))\n</code></pre> <p>Endpoint for users to delete a job.</p> Source code in <code>zeus/optimizer/batch_size/server/router.py</code> <pre><code>@app.delete(DELETE_JOB_URL)\nasync def delete_job(\n    job_id: str,\n    db_session: AsyncSession = Depends(get_db_session),\n    job_locks: defaultdict[str, asyncio.Lock] = Depends(get_job_locks),\n):\n    \"\"\"Endpoint for users to delete a job.\"\"\"\n    async with job_locks[job_id]:\n        try:\n            optimizer = ZeusBatchSizeOptimizer(ZeusService(db_session))\n            await optimizer.delete_job(job_id)\n            await db_session.commit()\n        except ZeusBSOServerBaseError as err:\n            await db_session.rollback()\n            return JSONResponse(\n                status_code=err.status_code,\n                content={\"message\": err.message},\n            )\n        except Exception as err:\n            await db_session.rollback()\n            logger.error(\"Commit Failed: %s\", str(err))\n            return JSONResponse(\n                status_code=500,\n                content={\"message\": str(err)},\n            )\n        finally:\n            job_locks.pop(job_id)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/router/#zeus.optimizer.batch_size.server.router.end_trial","title":"end_trial  <code>async</code>","text":"<pre><code>end_trial(trial, db_session=Depends(get_db_session), job_locks=Depends(get_job_locks))\n</code></pre> <p>Endpoint for users to end the trial.</p> Source code in <code>zeus/optimizer/batch_size/server/router.py</code> <pre><code>@app.patch(REPORT_END_URL)\nasync def end_trial(\n    trial: TrialId,\n    db_session: AsyncSession = Depends(get_db_session),\n    job_locks: defaultdict[str, asyncio.Lock] = Depends(get_job_locks),\n):\n    \"\"\"Endpoint for users to end the trial.\"\"\"\n    async with job_locks[trial.job_id]:\n        optimizer = ZeusBatchSizeOptimizer(ZeusService(db_session))\n        try:\n            await optimizer.end_trial(trial)\n            await db_session.commit()\n        except ZeusBSOServerBaseError as err:\n            await db_session.rollback()\n            return JSONResponse(\n                status_code=err.status_code,\n                content={\"message\": err.message},\n            )\n        except Exception as err:\n            await db_session.rollback()\n            logger.error(\"Commit Failed: %s\", str(err))\n            return JSONResponse(\n                status_code=500,\n                content={\"message\": str(err)},\n            )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/router/#zeus.optimizer.batch_size.server.router.predict","title":"predict  <code>async</code>","text":"<pre><code>predict(job_id, db_session=Depends(get_db_session), job_locks=Depends(get_job_locks))\n</code></pre> <p>Endpoint for users to receive a batch size.</p> Source code in <code>zeus/optimizer/batch_size/server/router.py</code> <pre><code>@app.get(GET_NEXT_BATCH_SIZE_URL, response_model=TrialId)\nasync def predict(\n    job_id: str,\n    db_session: AsyncSession = Depends(get_db_session),\n    job_locks: defaultdict[str, asyncio.Lock] = Depends(get_job_locks),\n):\n    \"\"\"Endpoint for users to receive a batch size.\"\"\"\n    async with job_locks[job_id]:\n        optimizer = ZeusBatchSizeOptimizer(ZeusService(db_session))\n        try:\n            res = await optimizer.predict(job_id)\n            await db_session.commit()\n            return res\n        except ZeusBSOServerBaseError as err:\n            await db_session.rollback()\n            return JSONResponse(\n                status_code=err.status_code,\n                content={\"message\": err.message},\n            )\n        except Exception as err:\n            await db_session.rollback()\n            logger.error(\"Commit Failed: %s\", str(err))\n            return JSONResponse(\n                status_code=500,\n                content={\"message\": str(err)},\n            )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/router/#zeus.optimizer.batch_size.server.router.report","title":"report  <code>async</code>","text":"<pre><code>report(result, db_session=Depends(get_db_session), job_locks=Depends(get_job_locks))\n</code></pre> <p>Endpoint for users to report the training result.</p> Source code in <code>zeus/optimizer/batch_size/server/router.py</code> <pre><code>@app.post(REPORT_RESULT_URL, response_model=ReportResponse)\nasync def report(\n    result: TrainingResult,\n    db_session: AsyncSession = Depends(get_db_session),\n    job_locks: defaultdict[str, asyncio.Lock] = Depends(get_job_locks),\n):\n    \"\"\"Endpoint for users to report the training result.\"\"\"\n    async with job_locks[result.job_id]:\n        optimizer = ZeusBatchSizeOptimizer(ZeusService(db_session))\n        try:\n            logger.info(\"Report with result %s\", str(result))\n            res = await optimizer.report(result)\n            await db_session.commit()\n            return res\n        except ZeusBSOServerBaseError as err:\n            await db_session.rollback()\n            return JSONResponse(\n                status_code=err.status_code,\n                content={\"message\": err.message},\n            )\n        except Exception as err:\n            await db_session.rollback()\n            logger.error(\"Commit Failed: %s\", str(err))\n            return JSONResponse(\n                status_code=500,\n                content={\"message\": str(err)},\n            )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/","title":"batch_size_state","text":""},{"location":"reference/optimizer/batch_size/server/batch_size_state/#zeus.optimizer.batch_size.server.batch_size_state","title":"zeus.optimizer.batch_size.server.batch_size_state","text":"<p>Batch size state models, repository, and commands.</p> <p>Batch size state includes trials and GaussianTs states.</p>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/","title":"commands","text":""},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands","title":"zeus.optimizer.batch_size.server.batch_size_state.commands","text":"<p>Commands to use <code>BatchSizeStateRepository</code>.</p>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.ReadTrial","title":"ReadTrial","text":"<p>               Bases: <code>BatchSizeBase</code></p> <p>Command to read a trial.</p> <p>Equivalent to primary key of Trial.</p> <p>Attributes:</p> Name Type Description <code>job_id</code> <code>str</code> <p>ID of job</p> <code>batch_size</code> <code>int</code> <p>batch size of a given trial</p> <code>trial_number</code> <code>int</code> <p>number of trial</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>class ReadTrial(BatchSizeBase):\n    \"\"\"Command to read a trial.\n\n    Equivalent to primary key of Trial.\n\n    Attributes:\n        job_id: ID of job\n        batch_size: batch size of a given trial\n        trial_number: number of trial\n    \"\"\"\n\n    trial_number: int = Field(gt=0)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.CreateTrialBase","title":"CreateTrialBase","text":"<p>               Bases: <code>BatchSizeBase</code></p> <p>Base command to create trial.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>class CreateTrialBase(BatchSizeBase):\n    \"\"\"Base command to create trial.\"\"\"\n\n    type: TrialType\n    start_timestamp: datetime = Field(default_factory=datetime.now)\n    status: TrialStatus = Field(default=TrialStatus.Dispatched, const=True)\n\n    class Config:\n        \"\"\"Model configuration.\n\n        Make it immutable after it's created.\n        \"\"\"\n\n        frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.CreateTrialBase.Config","title":"Config","text":"<p>Model configuration.</p> <p>Make it immutable after it's created.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>class Config:\n    \"\"\"Model configuration.\n\n    Make it immutable after it's created.\n    \"\"\"\n\n    frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.CreateTrial","title":"CreateTrial","text":"<p>               Bases: <code>CreateTrialBase</code></p> <p>Internal command to create trial.</p> <p>trial_number is populate within ZeusService.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>class CreateTrial(CreateTrialBase):\n    \"\"\"Internal command to create trial.\n\n    trial_number is populate within ZeusService.\n    \"\"\"\n\n    trial_number: int = Field(gt=0)\n\n    class Config:\n        \"\"\"Model configuration.\n\n        Make it immutable after it's created.\n        \"\"\"\n\n        frozen = True\n\n    def to_orm(self) -&gt; TrialTable:\n        \"\"\"Create an ORM object from pydantic model.\n\n        Returns:\n            `TrialTable`: ORM object representing the trial.\n        \"\"\"\n        d = self.dict()\n        t = TrialTable()\n        for k, v in d.items():\n            setattr(t, k, v)\n        return t\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.CreateTrial.Config","title":"Config","text":"<p>Model configuration.</p> <p>Make it immutable after it's created.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>class Config:\n    \"\"\"Model configuration.\n\n    Make it immutable after it's created.\n    \"\"\"\n\n    frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.CreateTrial.to_orm","title":"to_orm","text":"<pre><code>to_orm()\n</code></pre> <p>Create an ORM object from pydantic model.</p> <p>Returns:</p> Type Description <code>TrialTable</code> <p><code>TrialTable</code>: ORM object representing the trial.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>def to_orm(self) -&gt; TrialTable:\n    \"\"\"Create an ORM object from pydantic model.\n\n    Returns:\n        `TrialTable`: ORM object representing the trial.\n    \"\"\"\n    d = self.dict()\n    t = TrialTable()\n    for k, v in d.items():\n        setattr(t, k, v)\n    return t\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.CreateExplorationTrial","title":"CreateExplorationTrial","text":"<p>               Bases: <code>CreateTrialBase</code></p> <p>Create a exploration.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>class CreateExplorationTrial(CreateTrialBase):\n    \"\"\"Create a exploration.\"\"\"\n\n    type: TrialType = Field(default=TrialType.Exploration, const=True)\n\n    class Config:\n        \"\"\"Model configuration.\n\n        Make it immutable after it's created.\n        \"\"\"\n\n        frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.CreateExplorationTrial.Config","title":"Config","text":"<p>Model configuration.</p> <p>Make it immutable after it's created.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>class Config:\n    \"\"\"Model configuration.\n\n    Make it immutable after it's created.\n    \"\"\"\n\n    frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.CreateMabTrial","title":"CreateMabTrial","text":"<p>               Bases: <code>CreateTrialBase</code></p> <p>Create a MAB trial.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>class CreateMabTrial(CreateTrialBase):\n    \"\"\"Create a MAB trial.\"\"\"\n\n    type: TrialType = Field(default=TrialType.MAB, const=True)\n\n    class Config:\n        \"\"\"Model configuration.\n\n        Make it immutable after it's created.\n        \"\"\"\n\n        frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.CreateMabTrial.Config","title":"Config","text":"<p>Model configuration.</p> <p>Make it immutable after it's created.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>class Config:\n    \"\"\"Model configuration.\n\n    Make it immutable after it's created.\n    \"\"\"\n\n    frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.CreateConcurrentTrial","title":"CreateConcurrentTrial","text":"<p>               Bases: <code>CreateTrialBase</code></p> <p>Create a exploration.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>class CreateConcurrentTrial(CreateTrialBase):\n    \"\"\"Create a exploration.\"\"\"\n\n    type: TrialType = Field(default=TrialType.Concurrent, const=True)\n\n    class Config:\n        \"\"\"Model configuration.\n\n        Make it immutable after it's created.\n        \"\"\"\n\n        frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.CreateConcurrentTrial.Config","title":"Config","text":"<p>Model configuration.</p> <p>Make it immutable after it's created.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>class Config:\n    \"\"\"Model configuration.\n\n    Make it immutable after it's created.\n    \"\"\"\n\n    frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.UpdateTrial","title":"UpdateTrial","text":"<p>               Bases: <code>BatchSizeBase</code></p> <p>Report the result of trial.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>class UpdateTrial(BatchSizeBase):\n    \"\"\"Report the result of trial.\"\"\"\n\n    trial_number: int = Field(gt=0)\n    end_timestamp: datetime = Field(default_factory=datetime.now)\n    status: TrialStatus\n    time: Optional[float] = Field(default=None, ge=0)\n    energy: Optional[float] = Field(default=None, ge=0)\n    converged: Optional[bool] = None\n\n    class Config:\n        \"\"\"Model configuration.\n\n        Make it immutable after it's created.\n        \"\"\"\n\n        frozen = True\n\n    @validator(\"status\")\n    def _check_status(cls, s: TrialStatus) -&gt; TrialStatus:\n        \"\"\"Check if status is equal to Dispatched.\"\"\"\n        if s != TrialStatus.Dispatched:\n            return s\n        else:\n            raise ValueError(f\"{s} shouldn't be Dispatched since this is reporting the result.\")\n\n    @root_validator(skip_on_failure=True)\n    def _validate_sanity(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Validate result.\n\n        We are checking\n            - if status == Failed, time/energy/converged == None.\n                else, time/energy/converged != None.\n        \"\"\"\n        status: TrialStatus = values[\"status\"]\n\n        time: float | None = values[\"time\"]\n        energy: float | None = values[\"energy\"]\n        converged: bool | None = values[\"converged\"]\n\n        if status != TrialStatus.Failed and (time is None or energy is None or converged is None):\n            raise ValueError(f\"Result is incomplete: time({time}), energy({energy}), converged({converged})\")\n\n        return values\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.UpdateTrial.Config","title":"Config","text":"<p>Model configuration.</p> <p>Make it immutable after it's created.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>class Config:\n    \"\"\"Model configuration.\n\n    Make it immutable after it's created.\n    \"\"\"\n\n    frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.UpdateTrial._check_status","title":"_check_status","text":"<pre><code>_check_status(s)\n</code></pre> <p>Check if status is equal to Dispatched.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>@validator(\"status\")\ndef _check_status(cls, s: TrialStatus) -&gt; TrialStatus:\n    \"\"\"Check if status is equal to Dispatched.\"\"\"\n    if s != TrialStatus.Dispatched:\n        return s\n    else:\n        raise ValueError(f\"{s} shouldn't be Dispatched since this is reporting the result.\")\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/commands/#zeus.optimizer.batch_size.server.batch_size_state.commands.UpdateTrial._validate_sanity","title":"_validate_sanity","text":"<pre><code>_validate_sanity(values)\n</code></pre> <p>Validate result.</p> <p>We are checking     - if status == Failed, time/energy/converged == None.         else, time/energy/converged != None.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/commands.py</code> <pre><code>@root_validator(skip_on_failure=True)\ndef _validate_sanity(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Validate result.\n\n    We are checking\n        - if status == Failed, time/energy/converged == None.\n            else, time/energy/converged != None.\n    \"\"\"\n    status: TrialStatus = values[\"status\"]\n\n    time: float | None = values[\"time\"]\n    energy: float | None = values[\"energy\"]\n    converged: bool | None = values[\"converged\"]\n\n    if status != TrialStatus.Failed and (time is None or energy is None or converged is None):\n        raise ValueError(f\"Result is incomplete: time({time}), energy({energy}), converged({converged})\")\n\n    return values\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/","title":"models","text":""},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models","title":"zeus.optimizer.batch_size.server.batch_size_state.models","text":"<p>Pydantic models for Batch size/Trials/GaussianTsArmState.</p>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.BatchSizeBase","title":"BatchSizeBase","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base model for representing batch size.</p> <p>Attributes:</p> Name Type Description <code>job_id</code> <code>str</code> <p>The ID of the job.</p> <code>batch_size</code> <code>int</code> <p>The size of the batch (greater than 0).</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>class BatchSizeBase(BaseModel):\n    \"\"\"Base model for representing batch size.\n\n    Attributes:\n        job_id (str): The ID of the job.\n        batch_size (int): The size of the batch (greater than 0).\n    \"\"\"\n\n    job_id: str\n    batch_size: int = Field(gt=0)\n\n    class Config:\n        \"\"\"Model configuration.\n\n        Make it immutable after it's created.\n        \"\"\"\n\n        frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.BatchSizeBase.Config","title":"Config","text":"<p>Model configuration.</p> <p>Make it immutable after it's created.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>class Config:\n    \"\"\"Model configuration.\n\n    Make it immutable after it's created.\n    \"\"\"\n\n    frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.Trial","title":"Trial","text":"<p>               Bases: <code>BatchSizeBase</code></p> <p>Pydantic model that represents Trial.</p> <p>Attributes:</p> Name Type Description <code>job_id</code> <code>str</code> <p>The ID of the job.</p> <code>batch_size</code> <code>int</code> <p>The size of the batch (greater than 0).</p> <code>trial_number</code> <code>int</code> <p>Number of trial.</p> <code>start_timestamp</code> <code>datetime</code> <p>Start time of trial.</p> <code>end_timestamp</code> <code>datetime</code> <p>End time of trial.</p> <code>type</code> <code>TrialType</code> <p>Type of this trial, which means in which stage this trial was executed.</p> <code>status</code> <code>TrialStatus</code> <p>Status of trial</p> <code>time</code> <code>Optional[float]</code> <p>Total time consumption of this trial.</p> <code>energy</code> <code>Optional[float]</code> <p>Total energy consumption of this trial.</p> <code>converged</code> <code>Optional[bool]</code> <p>Whether this trial is converged or not.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>class Trial(BatchSizeBase):\n    \"\"\"Pydantic model that represents Trial.\n\n    Attributes:\n        job_id (str): The ID of the job.\n        batch_size (int): The size of the batch (greater than 0).\n        trial_number (int): Number of trial.\n        start_timestamp (datetime): Start time of trial.\n        end_timestamp (datetime): End time of trial.\n        type (TrialType): Type of this trial, which means in which stage this trial was executed.\n        status (TrialStatus): Status of trial\n        time (Optional[float]): Total time consumption of this trial.\n        energy (Optional[float]): Total energy consumption of this trial.\n        converged (Optional[bool]): Whether this trial is converged or not.\n    \"\"\"\n\n    trial_number: int = Field(gt=0)\n    start_timestamp: datetime\n    end_timestamp: Optional[datetime] = Field(None)\n    type: TrialType\n    status: TrialStatus\n    time: Optional[float] = Field(None, ge=0)\n    energy: Optional[float] = Field(None, ge=0)\n    converged: Optional[bool] = None\n\n    class Config:\n        \"\"\"Model configuration.\n\n        Enable instantiating model from an ORM object, and make it immutable after it's created.\n        \"\"\"\n\n        orm_mode = True\n        frozen = True\n\n    @root_validator(skip_on_failure=True)\n    def _validate_mab(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Validate trial.\n\n        We are checking\n            - start_timestamp &lt;= end_timestamp\n            - if status == dispatched | Failed, time/energy/converged = None\n                else time/energy/converged != None\n        \"\"\"\n        start_timestamp: datetime = values[\"start_timestamp\"]\n        end_timestamp: datetime | None = values[\"end_timestamp\"]\n        status: TrialStatus = values[\"status\"]\n        time: float | None = values[\"time\"]\n        energy: float | None = values[\"energy\"]\n        converged: bool | None = values[\"converged\"]\n\n        if end_timestamp is not None and start_timestamp &gt; end_timestamp:\n            raise ValueError(f\"start is earlier than end: {start_timestamp} &gt; {end_timestamp}\")\n        if status in (TrialStatus.Dispatched, TrialStatus.Failed):\n            if not (time is None and energy is None and converged is None):\n                raise ValueError(\"Trial status and result is not matching.\")\n            if status == TrialStatus.Failed and end_timestamp is None:\n                raise ValueError(\"Trial ended but end_timestamp is None.\")\n        elif time is None or energy is None or converged is None or end_timestamp is None:\n            raise ValueError(\n                f\"Trial ended but the result is incomplete: time({time}), energy({energy}), converged({converged}), end_timestamp({end_timestamp})\"\n            )\n\n        return values\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.Trial.Config","title":"Config","text":"<p>Model configuration.</p> <p>Enable instantiating model from an ORM object, and make it immutable after it's created.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>class Config:\n    \"\"\"Model configuration.\n\n    Enable instantiating model from an ORM object, and make it immutable after it's created.\n    \"\"\"\n\n    orm_mode = True\n    frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.Trial._validate_mab","title":"_validate_mab","text":"<pre><code>_validate_mab(values)\n</code></pre> <p>Validate trial.</p> <p>We are checking     - start_timestamp &lt;= end_timestamp     - if status == dispatched | Failed, time/energy/converged = None         else time/energy/converged != None</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>@root_validator(skip_on_failure=True)\ndef _validate_mab(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Validate trial.\n\n    We are checking\n        - start_timestamp &lt;= end_timestamp\n        - if status == dispatched | Failed, time/energy/converged = None\n            else time/energy/converged != None\n    \"\"\"\n    start_timestamp: datetime = values[\"start_timestamp\"]\n    end_timestamp: datetime | None = values[\"end_timestamp\"]\n    status: TrialStatus = values[\"status\"]\n    time: float | None = values[\"time\"]\n    energy: float | None = values[\"energy\"]\n    converged: bool | None = values[\"converged\"]\n\n    if end_timestamp is not None and start_timestamp &gt; end_timestamp:\n        raise ValueError(f\"start is earlier than end: {start_timestamp} &gt; {end_timestamp}\")\n    if status in (TrialStatus.Dispatched, TrialStatus.Failed):\n        if not (time is None and energy is None and converged is None):\n            raise ValueError(\"Trial status and result is not matching.\")\n        if status == TrialStatus.Failed and end_timestamp is None:\n            raise ValueError(\"Trial ended but end_timestamp is None.\")\n    elif time is None or energy is None or converged is None or end_timestamp is None:\n        raise ValueError(\n            f\"Trial ended but the result is incomplete: time({time}), energy({energy}), converged({converged}), end_timestamp({end_timestamp})\"\n        )\n\n    return values\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmState","title":"GaussianTsArmState","text":"<p>               Bases: <code>BatchSizeBase</code></p> <p>Model representing Gaussian Thompson Sampling arm state.</p> <p>Attributes:</p> Name Type Description <code>param_mean</code> <code>float</code> <p>Mean of the belief prior distribution.</p> <code>param_precision</code> <code>float</code> <p>Precision of the belief prior distribution.</p> <code>reward_precision</code> <code>float</code> <p>Precision (inverse variance) of the reward distribution.</p> <code>num_observations</code> <code>int</code> <p>How many observations we made.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>class GaussianTsArmState(BatchSizeBase):\n    \"\"\"Model representing Gaussian Thompson Sampling arm state.\n\n    Attributes:\n        param_mean (float): Mean of the belief prior distribution.\n        param_precision (float): Precision of the belief prior distribution.\n        reward_precision (float): Precision (inverse variance) of the reward distribution.\n        num_observations (int): How many observations we made.\n    \"\"\"\n\n    param_mean: float\n    param_precision: float\n    reward_precision: float\n    num_observations: int = Field(ge=0)\n\n    class Config:\n        \"\"\"Model configuration.\n\n        Enable instantiating model from an ORM object, and make it immutable after it's created.\n        \"\"\"\n\n        orm_mode = True\n        frozen = True\n\n    def to_orm(self) -&gt; GaussianTsArmStateTable:\n        \"\"\"Convert pydantic model to ORM object.\n\n        Returns:\n            GaussianTsArmState: The ORM object of Gaussian Arm State.\n        \"\"\"\n        d = self.dict()\n        g = GaussianTsArmStateTable()\n        for k, v in d.items():\n            setattr(g, k, v)\n        return g\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmState.Config","title":"Config","text":"<p>Model configuration.</p> <p>Enable instantiating model from an ORM object, and make it immutable after it's created.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>class Config:\n    \"\"\"Model configuration.\n\n    Enable instantiating model from an ORM object, and make it immutable after it's created.\n    \"\"\"\n\n    orm_mode = True\n    frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmState.to_orm","title":"to_orm","text":"<pre><code>to_orm()\n</code></pre> <p>Convert pydantic model to ORM object.</p> <p>Returns:</p> Name Type Description <code>GaussianTsArmState</code> <code>GaussianTsArmStateTable</code> <p>The ORM object of Gaussian Arm State.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>def to_orm(self) -&gt; GaussianTsArmStateTable:\n    \"\"\"Convert pydantic model to ORM object.\n\n    Returns:\n        GaussianTsArmState: The ORM object of Gaussian Arm State.\n    \"\"\"\n    d = self.dict()\n    g = GaussianTsArmStateTable()\n    for k, v in d.items():\n        setattr(g, k, v)\n    return g\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.TrialResult","title":"TrialResult","text":"<p>               Bases: <code>BatchSizeBase</code></p> <p>Model for reading the result of the trial.</p> <p>Refer to <code>Trial</code> for attributes.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>class TrialResult(BatchSizeBase):\n    \"\"\"Model for reading the result of the trial.\n\n    Refer to [`Trial`][zeus.optimizer.batch_size.server.batch_size_state.models.Trial] for attributes.\n    \"\"\"\n\n    trial_number: int = Field(gt=0)\n    status: TrialStatus\n    time: float = Field(ge=0)\n    energy: float = Field(ge=0)\n    converged: bool\n\n    class Config:\n        \"\"\"Model configuration.\n\n        Enable instantiating model from an ORM object, and make it immutable after it's created.\n        \"\"\"\n\n        orm_mode = True\n        frozen = True\n\n    @validator(\"status\")\n    def _check_state(cls, s: TrialStatus) -&gt; TrialStatus:\n        \"\"\"Check if status is equal to succeeded.\"\"\"\n        if s == TrialStatus.Succeeded:\n            return s\n        else:\n            raise ValueError(f\"{s} should be succeeded to have a valid result.\")\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.TrialResult.Config","title":"Config","text":"<p>Model configuration.</p> <p>Enable instantiating model from an ORM object, and make it immutable after it's created.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>class Config:\n    \"\"\"Model configuration.\n\n    Enable instantiating model from an ORM object, and make it immutable after it's created.\n    \"\"\"\n\n    orm_mode = True\n    frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.TrialResult._check_state","title":"_check_state","text":"<pre><code>_check_state(s)\n</code></pre> <p>Check if status is equal to succeeded.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>@validator(\"status\")\ndef _check_state(cls, s: TrialStatus) -&gt; TrialStatus:\n    \"\"\"Check if status is equal to succeeded.\"\"\"\n    if s == TrialStatus.Succeeded:\n        return s\n    else:\n        raise ValueError(f\"{s} should be succeeded to have a valid result.\")\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.TrialResultsPerBs","title":"TrialResultsPerBs","text":"<p>               Bases: <code>BatchSizeBase</code></p> <p>Model representing all succeeded results of trial for a given batch size.</p> <p>Attributes:</p> Name Type Description <code>results</code> <code>list[TrialResult]</code> <p>List of TrialResult per batch size.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>class TrialResultsPerBs(BatchSizeBase):\n    \"\"\"Model representing all succeeded results of trial for a given batch size.\n\n    Attributes:\n        results (list[TrialResult]): List of TrialResult per batch size.\n    \"\"\"\n\n    results: list[TrialResult]\n\n    @root_validator(skip_on_failure=True)\n    def _check_explorations(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Validate if job_id and bs are consistent across all items in results.\"\"\"\n        bs: int = values[\"batch_size\"]\n        job_id: str = values[\"job_id\"]\n        ms: list[TrialResult] = values[\"results\"]\n        ms.sort(key=lambda x: x.trial_number, reverse=True)\n\n        for m in ms:\n            if job_id != m.job_id:\n                raise ValueError(f\"job_id doesn't correspond with results: {job_id} != {m.job_id}\")\n            if bs != m.batch_size:\n                raise ValueError(f\"Batch size doesn't correspond with results: {bs} != {m.batch_size}\")\n            if m.status != TrialStatus.Succeeded:\n                raise ValueError(\n                    f\"This list should only contain succeeded trials. Encounted trial({m.trial_number}) of status = {m.status}\"\n                )\n\n        return values\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.TrialResultsPerBs._check_explorations","title":"_check_explorations","text":"<pre><code>_check_explorations(values)\n</code></pre> <p>Validate if job_id and bs are consistent across all items in results.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>@root_validator(skip_on_failure=True)\ndef _check_explorations(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Validate if job_id and bs are consistent across all items in results.\"\"\"\n    bs: int = values[\"batch_size\"]\n    job_id: str = values[\"job_id\"]\n    ms: list[TrialResult] = values[\"results\"]\n    ms.sort(key=lambda x: x.trial_number, reverse=True)\n\n    for m in ms:\n        if job_id != m.job_id:\n            raise ValueError(f\"job_id doesn't correspond with results: {job_id} != {m.job_id}\")\n        if bs != m.batch_size:\n            raise ValueError(f\"Batch size doesn't correspond with results: {bs} != {m.batch_size}\")\n        if m.status != TrialStatus.Succeeded:\n            raise ValueError(\n                f\"This list should only contain succeeded trials. Encounted trial({m.trial_number}) of status = {m.status}\"\n            )\n\n    return values\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.ExplorationsPerJob","title":"ExplorationsPerJob","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model representing all succeeded explorations we have done for a job. Immutable after it's created.</p> <p>Attributes:</p> Name Type Description <code>job_id</code> <code>str</code> <p>The ID of the job.</p> <code>explorations_per_bs</code> <code>dict[int, list[Trial]]</code> <p>Dictionary of \"succeeded\" explorations per batch size in trial_number ascending order.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>class ExplorationsPerJob(BaseModel):\n    \"\"\"Model representing all succeeded explorations we have done for a job. Immutable after it's created.\n\n    Attributes:\n        job_id (str): The ID of the job.\n        explorations_per_bs (dict[int, list[Trial]]): Dictionary of \"succeeded\" explorations per batch size in trial_number ascending order.\n    \"\"\"\n\n    job_id: str\n    explorations_per_bs: dict[int, list[Trial]]  # BS -&gt; Trials with exploration type\n\n    class Config:\n        \"\"\"Model configuration.\n\n        Make it immutable after it's created.\n        \"\"\"\n\n        frozen = True\n\n    @root_validator(skip_on_failure=True)\n    def _check_explorations(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Check bs and job_id corresponds to explorations_per_bs and batch size is consistent.\"\"\"\n        job_id: str = values[\"job_id\"]\n        exps_per_bs: dict[int, list[Trial]] = values[\"explorations_per_bs\"]\n\n        for bs, exps in exps_per_bs.items():\n            # Sort ascending just in case. Sql will return asc order anyways.\n            exps.sort(key=lambda x: x.trial_number)\n            for exp in exps:\n                if job_id != exp.job_id:\n                    raise ValueError(f\"job_id doesn't correspond with explorations: {job_id} != {exp.job_id}\")\n                if bs != exp.batch_size:\n                    raise ValueError(f\"Batch size doesn't correspond with explorations: {bs} != {exp.batch_size}\")\n                if exp.type != TrialType.Exploration:\n                    raise ValueError(\"Trial type is not equal to Exploration.\")\n                if exp.status == TrialStatus.Failed:\n                    raise ValueError(\"Should not include failed trial.\")\n\n        return values\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.ExplorationsPerJob.Config","title":"Config","text":"<p>Model configuration.</p> <p>Make it immutable after it's created.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>class Config:\n    \"\"\"Model configuration.\n\n    Make it immutable after it's created.\n    \"\"\"\n\n    frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/models/#zeus.optimizer.batch_size.server.batch_size_state.models.ExplorationsPerJob._check_explorations","title":"_check_explorations","text":"<pre><code>_check_explorations(values)\n</code></pre> <p>Check bs and job_id corresponds to explorations_per_bs and batch size is consistent.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/models.py</code> <pre><code>@root_validator(skip_on_failure=True)\ndef _check_explorations(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Check bs and job_id corresponds to explorations_per_bs and batch size is consistent.\"\"\"\n    job_id: str = values[\"job_id\"]\n    exps_per_bs: dict[int, list[Trial]] = values[\"explorations_per_bs\"]\n\n    for bs, exps in exps_per_bs.items():\n        # Sort ascending just in case. Sql will return asc order anyways.\n        exps.sort(key=lambda x: x.trial_number)\n        for exp in exps:\n            if job_id != exp.job_id:\n                raise ValueError(f\"job_id doesn't correspond with explorations: {job_id} != {exp.job_id}\")\n            if bs != exp.batch_size:\n                raise ValueError(f\"Batch size doesn't correspond with explorations: {bs} != {exp.batch_size}\")\n            if exp.type != TrialType.Exploration:\n                raise ValueError(\"Trial type is not equal to Exploration.\")\n            if exp.status == TrialStatus.Failed:\n                raise ValueError(\"Should not include failed trial.\")\n\n    return values\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/","title":"repository","text":""},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/#zeus.optimizer.batch_size.server.batch_size_state.repository","title":"zeus.optimizer.batch_size.server.batch_size_state.repository","text":"<p>Repository for batch size states(Trial, Gaussian Ts arm state).</p>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/#zeus.optimizer.batch_size.server.batch_size_state.repository.BatchSizeStateRepository","title":"BatchSizeStateRepository","text":"<p>               Bases: <code>DatabaseRepository</code></p> <p>Repository for handling batch size related operations.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/repository.py</code> <pre><code>class BatchSizeStateRepository(DatabaseRepository):\n    \"\"\"Repository for handling batch size related operations.\"\"\"\n\n    def __init__(self, session: AsyncSession):\n        \"\"\"Set db session and intialize fetched trial. We are only updating one trial per session.\"\"\"\n        super().__init__(session)\n        self.fetched_trial: TrialTable | None = None\n        self.fetched_arm: GaussianTsArmStateTable | None = None\n\n    async def get_next_trial_number(self, job_id: str) -&gt; int:\n        \"\"\"Get next trial number of a given job. Trial number starts from 1 and increase by 1 at a time.\"\"\"\n        stmt = select(func.max(TrialTable.trial_number)).where(\n            and_(\n                TrialTable.job_id == job_id,\n            )\n        )\n        res = await self.session.scalar(stmt)\n        if res is None:\n            return 1\n        return res + 1\n\n    async def get_trial_results_of_bs(self, batch_size: BatchSizeBase, window_size: int) -&gt; TrialResultsPerBs:\n        \"\"\"Load window size amount of results for a given batch size. If window size &lt;= 0, load all of them.\n\n        From all trials, we filter succeeded one since failed/dispatched ones doesn't have a valid result.\n\n        Args:\n            batch_size (BatchSizeBase): The batch size object.\n            window_size (int): The size of the measurement window.\n\n        Returns:\n            TrialResultsPerBs: trial results for the given batch size.\n        \"\"\"\n        stmt = (\n            select(TrialTable)\n            .where(\n                and_(\n                    TrialTable.job_id == batch_size.job_id,\n                    TrialTable.batch_size == batch_size.batch_size,\n                    TrialTable.status == TrialStatus.Succeeded,\n                )\n            )\n            .order_by(TrialTable.trial_number.desc())\n        )\n        if window_size &gt; 0:\n            stmt = stmt.limit(window_size)\n\n        res = (await self.session.scalars(stmt)).all()\n        return TrialResultsPerBs(\n            job_id=batch_size.job_id,\n            batch_size=batch_size.batch_size,\n            results=[TrialResult.from_orm(t) for t in res],\n        )\n\n    async def get_arms(self, job_id: str) -&gt; list[GaussianTsArmState]:\n        \"\"\"Retrieve Gaussian Thompson Sampling arms for a given job.\n\n        Args:\n            job_id (str): The ID of the job.\n\n        Returns:\n            List[GaussianTsArmStateModel]: List of Gaussian Thompson Sampling arms. These arms are all \"good\" arms (converged during pruning stage).\n            Refer to `GaussianTsArmStateModel`[zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmStateModel] for attributes.\n        \"\"\"\n        stmt = select(GaussianTsArmStateTable).where(GaussianTsArmStateTable.job_id == job_id)\n        res = (await self.session.scalars(stmt)).all()\n        return [GaussianTsArmState.from_orm(arm) for arm in res]\n\n    async def get_arm(self, bs: BatchSizeBase) -&gt; GaussianTsArmState | None:\n        \"\"\"Retrieve Gaussian Thompson Sampling arm for a given job id and batch size.\n\n        Args:\n            bs (BatchSizeBase): The batch size object.\n\n        Returns:\n            Optional[GaussianTsArmStateModel]: Gaussian Thompson Sampling arm if found, else None.\n            Refer to `GaussianTsArmStateModel`[zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmStateModel] for attributes.\n        \"\"\"\n        stmt = select(GaussianTsArmStateTable).where(\n            and_(\n                GaussianTsArmStateTable.job_id == bs.job_id,\n                GaussianTsArmStateTable.batch_size == bs.batch_size,\n            )\n        )\n        arm = await self.session.scalar(stmt)\n        if arm is None:\n            return None\n        self.fetched_arm = arm\n        return GaussianTsArmState.from_orm(arm)\n\n    async def get_trial(self, trial: ReadTrial) -&gt; Trial | None:\n        \"\"\"Get a corresponding trial.\n\n        Args:\n            trial: job_id, batch_size, trial_number.\n\n        Returns:\n            Found Trial. If none found, return None.\n        \"\"\"\n        stmt = select(TrialTable).where(\n            TrialTable.job_id == trial.job_id,\n            TrialTable.batch_size == trial.batch_size,\n            TrialTable.trial_number == trial.trial_number,\n        )\n        fetched_trial = await self.session.scalar(stmt)\n\n        if fetched_trial is None:\n            logger.info(\"get_trial: NoResultFound\")\n            return None\n\n        self.fetched_trial = fetched_trial\n        return Trial.from_orm(fetched_trial)\n\n    def get_trial_from_session(self, trial: ReadTrial) -&gt; Trial | None:\n        \"\"\"Fetch a trial from the session.\"\"\"\n        if (\n            self.fetched_trial is None\n            or self.fetched_trial.job_id != trial.job_id\n            or self.fetched_trial.batch_size != trial.batch_size\n            or self.fetched_trial.trial_number != trial.trial_number\n        ):\n            return None\n        return Trial.from_orm(self.fetched_trial)\n\n    def create_trial(self, trial: CreateTrial) -&gt; None:\n        \"\"\"Create a trial in db.\n\n        Refer to `CreateTrial`[zeus.optimizer.batch_size.server.batch_size_state.models.CreateTrial] for attributes.\n\n        Args:\n            trial (CreateTrial): The trial to add.\n        \"\"\"\n        self.session.add(trial.to_orm())\n\n    def updated_current_trial(self, updated_trial: UpdateTrial) -&gt; None:\n        \"\"\"Update trial in the database (report the result of trial).\n\n        Args:\n            updated_trial (UpdateTrial): The updated trial. Refer to `UpdateTrial`[zeus.optimizer.batch_size.server.batch_size_state.models.UpdateTrial] for attributes.\n        \"\"\"\n        if self.fetched_trial is None:\n            raise ZeusBSOValueError(\"No trial is fetched.\")\n\n        if (\n            self.fetched_trial.job_id != updated_trial.job_id\n            or self.fetched_trial.batch_size != updated_trial.batch_size\n            or self.fetched_trial.trial_number != updated_trial.trial_number\n        ):\n            raise ZeusBSOValueError(\"Trying to update invalid trial.\")\n\n        self.fetched_trial.end_timestamp = updated_trial.end_timestamp\n        self.fetched_trial.status = updated_trial.status\n        self.fetched_trial.time = updated_trial.time\n        self.fetched_trial.energy = updated_trial.energy\n        self.fetched_trial.converged = updated_trial.converged\n\n    def create_arms(self, new_arms: list[GaussianTsArmState]) -&gt; None:\n        \"\"\"Create Gaussian Thompson Sampling arms in the database.\n\n        Args:\n            new_arms (List[GaussianTsArmStateModel]): List of new arms to create.\n                Refer to `GaussianTsArmStateModel`[zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmStateModel] for attributes.\n        \"\"\"\n        self.session.add_all([arm.to_orm() for arm in new_arms])\n\n    def update_arm_state(self, updated_mab_state: GaussianTsArmState) -&gt; None:\n        \"\"\"Update Gaussian Thompson Sampling arm state in db.\n\n        Args:\n            updated_mab_state (GaussianTsArmStateModel): The updated arm state.\n                Refer to `GaussianTsArmStateModel`[zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmStateModel] for attributes.\n        \"\"\"\n        if self.fetched_arm is None:\n            raise ZeusBSOValueError(\"No arm is fetched.\")\n\n        if (\n            self.fetched_arm.job_id != updated_mab_state.job_id\n            or self.fetched_arm.batch_size != updated_mab_state.batch_size\n        ):\n            raise ZeusBSOValueError(\"Fetch arm does not correspond with the arm trying to update.\")\n\n        self.fetched_arm.param_mean = updated_mab_state.param_mean\n        self.fetched_arm.param_precision = updated_mab_state.param_precision\n        self.fetched_arm.reward_precision = updated_mab_state.reward_precision\n        self.fetched_arm.num_observations = updated_mab_state.num_observations\n\n    async def get_explorations_of_job(self, job_id: str) -&gt; ExplorationsPerJob:\n        \"\"\"Retrieve succeeded or ongoing explorations for a given job.\n\n        Args:\n            job_id: ID of the job\n\n        Returns:\n            ExplorationsPerJob: Explorations for the given batch size.\n            Refer to `ExplorationsPerJob`[zeus.optimizer.batch_size.server.batch_size_state.models.ExplorationsPerJob] for attributes.\n        \"\"\"\n        stmt = (\n            select(TrialTable)\n            .where(\n                and_(\n                    TrialTable.job_id == job_id,\n                    TrialTable.type == TrialType.Exploration,\n                    TrialTable.status != TrialStatus.Failed,\n                )\n            )\n            .order_by(TrialTable.trial_number.asc())\n        )\n\n        explorations = (await self.session.scalars(stmt)).all()\n        exps_per_bs: defaultdict[int, list[Trial]] = defaultdict(list)\n        for exp in explorations:\n            exps_per_bs[exp.batch_size].append(Trial.from_orm(exp))\n\n        return ExplorationsPerJob(job_id=job_id, explorations_per_bs=exps_per_bs)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/#zeus.optimizer.batch_size.server.batch_size_state.repository.BatchSizeStateRepository.__init__","title":"__init__","text":"<pre><code>__init__(session)\n</code></pre> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/repository.py</code> <pre><code>def __init__(self, session: AsyncSession):\n    \"\"\"Set db session and intialize fetched trial. We are only updating one trial per session.\"\"\"\n    super().__init__(session)\n    self.fetched_trial: TrialTable | None = None\n    self.fetched_arm: GaussianTsArmStateTable | None = None\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/#zeus.optimizer.batch_size.server.batch_size_state.repository.BatchSizeStateRepository.get_next_trial_number","title":"get_next_trial_number  <code>async</code>","text":"<pre><code>get_next_trial_number(job_id)\n</code></pre> <p>Get next trial number of a given job. Trial number starts from 1 and increase by 1 at a time.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/repository.py</code> <pre><code>async def get_next_trial_number(self, job_id: str) -&gt; int:\n    \"\"\"Get next trial number of a given job. Trial number starts from 1 and increase by 1 at a time.\"\"\"\n    stmt = select(func.max(TrialTable.trial_number)).where(\n        and_(\n            TrialTable.job_id == job_id,\n        )\n    )\n    res = await self.session.scalar(stmt)\n    if res is None:\n        return 1\n    return res + 1\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/#zeus.optimizer.batch_size.server.batch_size_state.repository.BatchSizeStateRepository.get_trial_results_of_bs","title":"get_trial_results_of_bs  <code>async</code>","text":"<pre><code>get_trial_results_of_bs(batch_size, window_size)\n</code></pre> <p>Load window size amount of results for a given batch size. If window size &lt;= 0, load all of them.</p> <p>From all trials, we filter succeeded one since failed/dispatched ones doesn't have a valid result.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>BatchSizeBase</code> <p>The batch size object.</p> required <code>window_size</code> <code>int</code> <p>The size of the measurement window.</p> required <p>Returns:</p> Name Type Description <code>TrialResultsPerBs</code> <code>TrialResultsPerBs</code> <p>trial results for the given batch size.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/repository.py</code> <pre><code>async def get_trial_results_of_bs(self, batch_size: BatchSizeBase, window_size: int) -&gt; TrialResultsPerBs:\n    \"\"\"Load window size amount of results for a given batch size. If window size &lt;= 0, load all of them.\n\n    From all trials, we filter succeeded one since failed/dispatched ones doesn't have a valid result.\n\n    Args:\n        batch_size (BatchSizeBase): The batch size object.\n        window_size (int): The size of the measurement window.\n\n    Returns:\n        TrialResultsPerBs: trial results for the given batch size.\n    \"\"\"\n    stmt = (\n        select(TrialTable)\n        .where(\n            and_(\n                TrialTable.job_id == batch_size.job_id,\n                TrialTable.batch_size == batch_size.batch_size,\n                TrialTable.status == TrialStatus.Succeeded,\n            )\n        )\n        .order_by(TrialTable.trial_number.desc())\n    )\n    if window_size &gt; 0:\n        stmt = stmt.limit(window_size)\n\n    res = (await self.session.scalars(stmt)).all()\n    return TrialResultsPerBs(\n        job_id=batch_size.job_id,\n        batch_size=batch_size.batch_size,\n        results=[TrialResult.from_orm(t) for t in res],\n    )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/#zeus.optimizer.batch_size.server.batch_size_state.repository.BatchSizeStateRepository.get_arms","title":"get_arms  <code>async</code>","text":"<pre><code>get_arms(job_id)\n</code></pre> <p>Retrieve Gaussian Thompson Sampling arms for a given job.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>The ID of the job.</p> required <p>Returns:</p> Type Description <code>list[GaussianTsArmState]</code> <p>List[GaussianTsArmStateModel]: List of Gaussian Thompson Sampling arms. These arms are all \"good\" arms (converged during pruning stage).</p> <code>list[GaussianTsArmState]</code> <p>Refer to <code>GaussianTsArmStateModel</code>[zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmStateModel] for attributes.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/repository.py</code> <pre><code>async def get_arms(self, job_id: str) -&gt; list[GaussianTsArmState]:\n    \"\"\"Retrieve Gaussian Thompson Sampling arms for a given job.\n\n    Args:\n        job_id (str): The ID of the job.\n\n    Returns:\n        List[GaussianTsArmStateModel]: List of Gaussian Thompson Sampling arms. These arms are all \"good\" arms (converged during pruning stage).\n        Refer to `GaussianTsArmStateModel`[zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmStateModel] for attributes.\n    \"\"\"\n    stmt = select(GaussianTsArmStateTable).where(GaussianTsArmStateTable.job_id == job_id)\n    res = (await self.session.scalars(stmt)).all()\n    return [GaussianTsArmState.from_orm(arm) for arm in res]\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/#zeus.optimizer.batch_size.server.batch_size_state.repository.BatchSizeStateRepository.get_arm","title":"get_arm  <code>async</code>","text":"<pre><code>get_arm(bs)\n</code></pre> <p>Retrieve Gaussian Thompson Sampling arm for a given job id and batch size.</p> <p>Parameters:</p> Name Type Description Default <code>bs</code> <code>BatchSizeBase</code> <p>The batch size object.</p> required <p>Returns:</p> Type Description <code>GaussianTsArmState | None</code> <p>Optional[GaussianTsArmStateModel]: Gaussian Thompson Sampling arm if found, else None.</p> <code>GaussianTsArmState | None</code> <p>Refer to <code>GaussianTsArmStateModel</code>[zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmStateModel] for attributes.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/repository.py</code> <pre><code>async def get_arm(self, bs: BatchSizeBase) -&gt; GaussianTsArmState | None:\n    \"\"\"Retrieve Gaussian Thompson Sampling arm for a given job id and batch size.\n\n    Args:\n        bs (BatchSizeBase): The batch size object.\n\n    Returns:\n        Optional[GaussianTsArmStateModel]: Gaussian Thompson Sampling arm if found, else None.\n        Refer to `GaussianTsArmStateModel`[zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmStateModel] for attributes.\n    \"\"\"\n    stmt = select(GaussianTsArmStateTable).where(\n        and_(\n            GaussianTsArmStateTable.job_id == bs.job_id,\n            GaussianTsArmStateTable.batch_size == bs.batch_size,\n        )\n    )\n    arm = await self.session.scalar(stmt)\n    if arm is None:\n        return None\n    self.fetched_arm = arm\n    return GaussianTsArmState.from_orm(arm)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/#zeus.optimizer.batch_size.server.batch_size_state.repository.BatchSizeStateRepository.get_trial","title":"get_trial  <code>async</code>","text":"<pre><code>get_trial(trial)\n</code></pre> <p>Get a corresponding trial.</p> <p>Parameters:</p> Name Type Description Default <code>trial</code> <code>ReadTrial</code> <p>job_id, batch_size, trial_number.</p> required <p>Returns:</p> Type Description <code>Trial | None</code> <p>Found Trial. If none found, return None.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/repository.py</code> <pre><code>async def get_trial(self, trial: ReadTrial) -&gt; Trial | None:\n    \"\"\"Get a corresponding trial.\n\n    Args:\n        trial: job_id, batch_size, trial_number.\n\n    Returns:\n        Found Trial. If none found, return None.\n    \"\"\"\n    stmt = select(TrialTable).where(\n        TrialTable.job_id == trial.job_id,\n        TrialTable.batch_size == trial.batch_size,\n        TrialTable.trial_number == trial.trial_number,\n    )\n    fetched_trial = await self.session.scalar(stmt)\n\n    if fetched_trial is None:\n        logger.info(\"get_trial: NoResultFound\")\n        return None\n\n    self.fetched_trial = fetched_trial\n    return Trial.from_orm(fetched_trial)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/#zeus.optimizer.batch_size.server.batch_size_state.repository.BatchSizeStateRepository.get_trial_from_session","title":"get_trial_from_session","text":"<pre><code>get_trial_from_session(trial)\n</code></pre> <p>Fetch a trial from the session.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/repository.py</code> <pre><code>def get_trial_from_session(self, trial: ReadTrial) -&gt; Trial | None:\n    \"\"\"Fetch a trial from the session.\"\"\"\n    if (\n        self.fetched_trial is None\n        or self.fetched_trial.job_id != trial.job_id\n        or self.fetched_trial.batch_size != trial.batch_size\n        or self.fetched_trial.trial_number != trial.trial_number\n    ):\n        return None\n    return Trial.from_orm(self.fetched_trial)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/#zeus.optimizer.batch_size.server.batch_size_state.repository.BatchSizeStateRepository.create_trial","title":"create_trial","text":"<pre><code>create_trial(trial)\n</code></pre> <p>Create a trial in db.</p> <p>Refer to <code>CreateTrial</code>[zeus.optimizer.batch_size.server.batch_size_state.models.CreateTrial] for attributes.</p> <p>Parameters:</p> Name Type Description Default <code>trial</code> <code>CreateTrial</code> <p>The trial to add.</p> required Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/repository.py</code> <pre><code>def create_trial(self, trial: CreateTrial) -&gt; None:\n    \"\"\"Create a trial in db.\n\n    Refer to `CreateTrial`[zeus.optimizer.batch_size.server.batch_size_state.models.CreateTrial] for attributes.\n\n    Args:\n        trial (CreateTrial): The trial to add.\n    \"\"\"\n    self.session.add(trial.to_orm())\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/#zeus.optimizer.batch_size.server.batch_size_state.repository.BatchSizeStateRepository.updated_current_trial","title":"updated_current_trial","text":"<pre><code>updated_current_trial(updated_trial)\n</code></pre> <p>Update trial in the database (report the result of trial).</p> <p>Parameters:</p> Name Type Description Default <code>updated_trial</code> <code>UpdateTrial</code> <p>The updated trial. Refer to <code>UpdateTrial</code>[zeus.optimizer.batch_size.server.batch_size_state.models.UpdateTrial] for attributes.</p> required Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/repository.py</code> <pre><code>def updated_current_trial(self, updated_trial: UpdateTrial) -&gt; None:\n    \"\"\"Update trial in the database (report the result of trial).\n\n    Args:\n        updated_trial (UpdateTrial): The updated trial. Refer to `UpdateTrial`[zeus.optimizer.batch_size.server.batch_size_state.models.UpdateTrial] for attributes.\n    \"\"\"\n    if self.fetched_trial is None:\n        raise ZeusBSOValueError(\"No trial is fetched.\")\n\n    if (\n        self.fetched_trial.job_id != updated_trial.job_id\n        or self.fetched_trial.batch_size != updated_trial.batch_size\n        or self.fetched_trial.trial_number != updated_trial.trial_number\n    ):\n        raise ZeusBSOValueError(\"Trying to update invalid trial.\")\n\n    self.fetched_trial.end_timestamp = updated_trial.end_timestamp\n    self.fetched_trial.status = updated_trial.status\n    self.fetched_trial.time = updated_trial.time\n    self.fetched_trial.energy = updated_trial.energy\n    self.fetched_trial.converged = updated_trial.converged\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/#zeus.optimizer.batch_size.server.batch_size_state.repository.BatchSizeStateRepository.create_arms","title":"create_arms","text":"<pre><code>create_arms(new_arms)\n</code></pre> <p>Create Gaussian Thompson Sampling arms in the database.</p> <p>Parameters:</p> Name Type Description Default <code>new_arms</code> <code>List[GaussianTsArmStateModel]</code> <p>List of new arms to create. Refer to <code>GaussianTsArmStateModel</code>[zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmStateModel] for attributes.</p> required Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/repository.py</code> <pre><code>def create_arms(self, new_arms: list[GaussianTsArmState]) -&gt; None:\n    \"\"\"Create Gaussian Thompson Sampling arms in the database.\n\n    Args:\n        new_arms (List[GaussianTsArmStateModel]): List of new arms to create.\n            Refer to `GaussianTsArmStateModel`[zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmStateModel] for attributes.\n    \"\"\"\n    self.session.add_all([arm.to_orm() for arm in new_arms])\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/#zeus.optimizer.batch_size.server.batch_size_state.repository.BatchSizeStateRepository.update_arm_state","title":"update_arm_state","text":"<pre><code>update_arm_state(updated_mab_state)\n</code></pre> <p>Update Gaussian Thompson Sampling arm state in db.</p> <p>Parameters:</p> Name Type Description Default <code>updated_mab_state</code> <code>GaussianTsArmStateModel</code> <p>The updated arm state. Refer to <code>GaussianTsArmStateModel</code>[zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmStateModel] for attributes.</p> required Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/repository.py</code> <pre><code>def update_arm_state(self, updated_mab_state: GaussianTsArmState) -&gt; None:\n    \"\"\"Update Gaussian Thompson Sampling arm state in db.\n\n    Args:\n        updated_mab_state (GaussianTsArmStateModel): The updated arm state.\n            Refer to `GaussianTsArmStateModel`[zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmStateModel] for attributes.\n    \"\"\"\n    if self.fetched_arm is None:\n        raise ZeusBSOValueError(\"No arm is fetched.\")\n\n    if (\n        self.fetched_arm.job_id != updated_mab_state.job_id\n        or self.fetched_arm.batch_size != updated_mab_state.batch_size\n    ):\n        raise ZeusBSOValueError(\"Fetch arm does not correspond with the arm trying to update.\")\n\n    self.fetched_arm.param_mean = updated_mab_state.param_mean\n    self.fetched_arm.param_precision = updated_mab_state.param_precision\n    self.fetched_arm.reward_precision = updated_mab_state.reward_precision\n    self.fetched_arm.num_observations = updated_mab_state.num_observations\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/batch_size_state/repository/#zeus.optimizer.batch_size.server.batch_size_state.repository.BatchSizeStateRepository.get_explorations_of_job","title":"get_explorations_of_job  <code>async</code>","text":"<pre><code>get_explorations_of_job(job_id)\n</code></pre> <p>Retrieve succeeded or ongoing explorations for a given job.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>ID of the job</p> required <p>Returns:</p> Name Type Description <code>ExplorationsPerJob</code> <code>ExplorationsPerJob</code> <p>Explorations for the given batch size.</p> <code>ExplorationsPerJob</code> <p>Refer to <code>ExplorationsPerJob</code>[zeus.optimizer.batch_size.server.batch_size_state.models.ExplorationsPerJob] for attributes.</p> Source code in <code>zeus/optimizer/batch_size/server/batch_size_state/repository.py</code> <pre><code>async def get_explorations_of_job(self, job_id: str) -&gt; ExplorationsPerJob:\n    \"\"\"Retrieve succeeded or ongoing explorations for a given job.\n\n    Args:\n        job_id: ID of the job\n\n    Returns:\n        ExplorationsPerJob: Explorations for the given batch size.\n        Refer to `ExplorationsPerJob`[zeus.optimizer.batch_size.server.batch_size_state.models.ExplorationsPerJob] for attributes.\n    \"\"\"\n    stmt = (\n        select(TrialTable)\n        .where(\n            and_(\n                TrialTable.job_id == job_id,\n                TrialTable.type == TrialType.Exploration,\n                TrialTable.status != TrialStatus.Failed,\n            )\n        )\n        .order_by(TrialTable.trial_number.asc())\n    )\n\n    explorations = (await self.session.scalars(stmt)).all()\n    exps_per_bs: defaultdict[int, list[Trial]] = defaultdict(list)\n    for exp in explorations:\n        exps_per_bs[exp.batch_size].append(Trial.from_orm(exp))\n\n    return ExplorationsPerJob(job_id=job_id, explorations_per_bs=exps_per_bs)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/","title":"database","text":""},{"location":"reference/optimizer/batch_size/server/database/#zeus.optimizer.batch_size.server.database","title":"zeus.optimizer.batch_size.server.database","text":"<p>Manage database connection and define schema.</p>"},{"location":"reference/optimizer/batch_size/server/database/db_connection/","title":"db_connection","text":""},{"location":"reference/optimizer/batch_size/server/database/db_connection/#zeus.optimizer.batch_size.server.database.db_connection","title":"zeus.optimizer.batch_size.server.database.db_connection","text":"<p>Managing database connection.</p> <p>Heavily inspired by https://praciano.com.br/fastapi-and-async-sqlalchemy-20-with-pytest-done-right.html and https://medium.com/@tclaitken/setting-up-a-fastapi-app-with-async-sqlalchemy-2-0-pydantic-v2-e6c540be4308</p>"},{"location":"reference/optimizer/batch_size/server/database/db_connection/#zeus.optimizer.batch_size.server.database.db_connection.DatabaseSessionManager","title":"DatabaseSessionManager","text":"<p>Session manager class.</p> Source code in <code>zeus/optimizer/batch_size/server/database/db_connection.py</code> <pre><code>class DatabaseSessionManager:\n    \"\"\"Session manager class.\"\"\"\n\n    def __init__(self, host: str, engine_kwargs: dict[str, Any] | None = None):\n        \"\"\"Create async engine and session maker.\"\"\"\n        if engine_kwargs is None:\n            engine_kwargs = {}\n        self._engine = create_async_engine(host, **engine_kwargs)\n        self._sessionmaker = async_sessionmaker(autocommit=False, bind=self._engine)\n\n    async def close(self):\n        \"\"\"Close connection.\"\"\"\n        if self._engine is None:\n            raise ZeusBSOServerRuntimeError(\"DatabaseSessionManager is not initialized\")\n        await self._engine.dispose()\n\n        self._engine = None\n        self._sessionmaker = None\n\n    @contextlib.asynccontextmanager\n    async def connect(self) -&gt; AsyncIterator[AsyncConnection]:\n        \"\"\"Connect to db.\"\"\"\n        if self._engine is None:\n            raise ZeusBSOServerRuntimeError(\"DatabaseSessionManager is not initialized\")\n\n        async with self._engine.begin() as connection:\n            try:\n                yield connection\n            except Exception:\n                await connection.rollback()\n                raise\n\n    @contextlib.asynccontextmanager\n    async def session(self) -&gt; AsyncIterator[AsyncSession]:\n        \"\"\"Get session from session maker.\"\"\"\n        if self._sessionmaker is None:\n            raise ZeusBSOServerRuntimeError(\"DatabaseSessionManager is not initialized\")\n\n        session = self._sessionmaker()\n        try:\n            yield session\n        except Exception:\n            await session.rollback()\n            raise\n        finally:\n            await session.close()\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/db_connection/#zeus.optimizer.batch_size.server.database.db_connection.DatabaseSessionManager.__init__","title":"__init__","text":"<pre><code>__init__(host, engine_kwargs=None)\n</code></pre> Source code in <code>zeus/optimizer/batch_size/server/database/db_connection.py</code> <pre><code>def __init__(self, host: str, engine_kwargs: dict[str, Any] | None = None):\n    \"\"\"Create async engine and session maker.\"\"\"\n    if engine_kwargs is None:\n        engine_kwargs = {}\n    self._engine = create_async_engine(host, **engine_kwargs)\n    self._sessionmaker = async_sessionmaker(autocommit=False, bind=self._engine)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/db_connection/#zeus.optimizer.batch_size.server.database.db_connection.DatabaseSessionManager.close","title":"close  <code>async</code>","text":"<pre><code>close()\n</code></pre> <p>Close connection.</p> Source code in <code>zeus/optimizer/batch_size/server/database/db_connection.py</code> <pre><code>async def close(self):\n    \"\"\"Close connection.\"\"\"\n    if self._engine is None:\n        raise ZeusBSOServerRuntimeError(\"DatabaseSessionManager is not initialized\")\n    await self._engine.dispose()\n\n    self._engine = None\n    self._sessionmaker = None\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/db_connection/#zeus.optimizer.batch_size.server.database.db_connection.DatabaseSessionManager.connect","title":"connect  <code>async</code>","text":"<pre><code>connect()\n</code></pre> <p>Connect to db.</p> Source code in <code>zeus/optimizer/batch_size/server/database/db_connection.py</code> <pre><code>@contextlib.asynccontextmanager\nasync def connect(self) -&gt; AsyncIterator[AsyncConnection]:\n    \"\"\"Connect to db.\"\"\"\n    if self._engine is None:\n        raise ZeusBSOServerRuntimeError(\"DatabaseSessionManager is not initialized\")\n\n    async with self._engine.begin() as connection:\n        try:\n            yield connection\n        except Exception:\n            await connection.rollback()\n            raise\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/db_connection/#zeus.optimizer.batch_size.server.database.db_connection.DatabaseSessionManager.session","title":"session  <code>async</code>","text":"<pre><code>session()\n</code></pre> <p>Get session from session maker.</p> Source code in <code>zeus/optimizer/batch_size/server/database/db_connection.py</code> <pre><code>@contextlib.asynccontextmanager\nasync def session(self) -&gt; AsyncIterator[AsyncSession]:\n    \"\"\"Get session from session maker.\"\"\"\n    if self._sessionmaker is None:\n        raise ZeusBSOServerRuntimeError(\"DatabaseSessionManager is not initialized\")\n\n    session = self._sessionmaker()\n    try:\n        yield session\n    except Exception:\n        await session.rollback()\n        raise\n    finally:\n        await session.close()\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/db_connection/#zeus.optimizer.batch_size.server.database.db_connection.get_db_session","title":"get_db_session  <code>async</code>","text":"<pre><code>get_db_session()\n</code></pre> <p>Get db session from session manager. Used with fastapi dependency injection.</p> Source code in <code>zeus/optimizer/batch_size/server/database/db_connection.py</code> <pre><code>async def get_db_session() -&gt; AsyncIterator[AsyncSession]:\n    \"\"\"Get db session from session manager. Used with fastapi dependency injection.\"\"\"\n    async with sessionmanager.session() as session:\n        yield session\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/repository/","title":"repository","text":""},{"location":"reference/optimizer/batch_size/server/database/repository/#zeus.optimizer.batch_size.server.database.repository","title":"zeus.optimizer.batch_size.server.database.repository","text":"<p>Database repository (directly interacting with db) base class.</p>"},{"location":"reference/optimizer/batch_size/server/database/repository/#zeus.optimizer.batch_size.server.database.repository.DatabaseRepository","title":"DatabaseRepository","text":"<p>Base class for all repositories.</p> Source code in <code>zeus/optimizer/batch_size/server/database/repository.py</code> <pre><code>class DatabaseRepository:\n    \"\"\"Base class for all repositories.\"\"\"\n\n    def __init__(self, session: AsyncSession) -&gt; None:\n        \"\"\"Initizalize session.\"\"\"\n        self.session = session\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/repository/#zeus.optimizer.batch_size.server.database.repository.DatabaseRepository.__init__","title":"__init__","text":"<pre><code>__init__(session)\n</code></pre> Source code in <code>zeus/optimizer/batch_size/server/database/repository.py</code> <pre><code>def __init__(self, session: AsyncSession) -&gt; None:\n    \"\"\"Initizalize session.\"\"\"\n    self.session = session\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/schema/","title":"schema","text":""},{"location":"reference/optimizer/batch_size/server/database/schema/#zeus.optimizer.batch_size.server.database.schema","title":"zeus.optimizer.batch_size.server.database.schema","text":"<p>Database schema.</p>"},{"location":"reference/optimizer/batch_size/server/database/schema/#zeus.optimizer.batch_size.server.database.schema.Base","title":"Base","text":"<p>               Bases: <code>DeclarativeBase</code></p> <p>Base class for schemas.</p> Source code in <code>zeus/optimizer/batch_size/server/database/schema.py</code> <pre><code>class Base(DeclarativeBase):\n    \"\"\"Base class for schemas.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/schema/#zeus.optimizer.batch_size.server.database.schema.JobTable","title":"JobTable","text":"<p>               Bases: <code>Base</code></p> <p>Job table schema.</p> <p>Refer to <code>JobState</code> for attributes.</p> Source code in <code>zeus/optimizer/batch_size/server/database/schema.py</code> <pre><code>class JobTable(Base):\n    \"\"\"Job table schema.\n\n    Refer to [`JobState`][zeus.optimizer.batch_size.server.job.models.JobState] for attributes.\n    \"\"\"\n\n    __tablename__ = \"Job\"\n\n    job_id: Mapped[str] = mapped_column(VARCHAR(400), primary_key=True)\n    job_id_prefix: Mapped[str] = mapped_column(VARCHAR(300), nullable=False)\n    default_batch_size: Mapped[int] = mapped_column(Integer, nullable=False)\n    higher_is_better_metric: Mapped[bool] = mapped_column(Boolean, default=True)\n    eta_knob: Mapped[float] = mapped_column(Float, default=0.5)\n    beta_knob: Mapped[Optional[float]] = mapped_column(Float, nullable=True)\n    target_metric: Mapped[float] = mapped_column(Float, default=0.5)\n    max_epochs: Mapped[int] = mapped_column(Integer, default=100)\n    num_pruning_rounds: Mapped[int] = mapped_column(Integer, default=2)\n    window_size: Mapped[int] = mapped_column(Integer, default=10)\n\n    max_power: Mapped[float] = mapped_column(Float, nullable=False)\n    number_of_gpus: Mapped[int] = mapped_column(Integer, nullable=False)\n    gpu_model: Mapped[str] = mapped_column(VARCHAR(length=30), nullable=False)\n\n    mab_prior_mean: Mapped[float] = mapped_column(Float, default=0.0)\n    mab_prior_precision: Mapped[float] = mapped_column(Float, default=0.0)\n    mab_num_explorations: Mapped[int] = mapped_column(Integer, default=2)\n    mab_seed: Mapped[Optional[int]] = mapped_column(Integer, nullable=True)\n\n    mab_random_generator_state: Mapped[Optional[str]] = mapped_column(VARCHAR(length=10000), nullable=True)\n    exp_default_batch_size: Mapped[int] = mapped_column(Integer, nullable=False)\n\n    stage: Mapped[Stage] = mapped_column(Enum(Stage), default=Stage.Pruning)\n    min_cost: Mapped[Optional[float]] = mapped_column(Float, nullable=True)\n    min_cost_batch_size: Mapped[int] = mapped_column(Integer, nullable=False)\n\n    batch_sizes: Mapped[list[\"BatchSizeTable\"]] = relationship(\n        order_by=\"BatchSizeTable.batch_size.asc()\",\n        back_populates=\"job\",\n        # always fetch batch size(int) whenever we fetch the job.\n        # https://docs.sqlalchemy.org/en/14/orm/loading_relationships.html#relationship-loading-techniques\n        lazy=\"joined\",\n        # Delete all children if the job gets deleted.\n        # https://docs.sqlalchemy.org/en/20/orm/cascades.html\n        cascade=\"all, delete-orphan\",\n    )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/schema/#zeus.optimizer.batch_size.server.database.schema.BatchSizeTable","title":"BatchSizeTable","text":"<p>               Bases: <code>Base</code></p> <p>Batch size states table schema. Represents one batch size of a job.</p> <p>(job_id, batch_size) as a pk, and have three states(exploration, measurement, GaussianTs arm state) as fk. For explorations and measurements, one-to-many relationship. For arm_state, one-to-(zero or one) relationship.</p> Source code in <code>zeus/optimizer/batch_size/server/database/schema.py</code> <pre><code>class BatchSizeTable(Base):\n    \"\"\"Batch size states table schema. Represents one batch size of a job.\n\n    (job_id, batch_size) as a pk, and have three states(exploration, measurement, GaussianTs arm state) as fk.\n    For explorations and measurements, one-to-many relationship. For arm_state, one-to-(zero or one) relationship.\n    \"\"\"\n\n    __tablename__ = \"BatchSize\"\n\n    job_id: Mapped[str] = mapped_column(\n        ForeignKey(\n            \"Job.job_id\",\n            ondelete=\"CASCADE\",\n        ),\n        primary_key=True,\n    )\n    batch_size: Mapped[int] = mapped_column(Integer, primary_key=True)\n\n    trials: Mapped[list[\"TrialTable\"]] = relationship(back_populates=\"batch_size_state\", cascade=\"all, delete-orphan\")\n\n    arm_state: Mapped[Optional[\"GaussianTsArmStateTable\"]] = relationship(\n        back_populates=\"batch_size_state\",  # populates GaussianTsArmState-&gt;BatchSize\n        # https://stackoverflow.com/questions/39869793/when-do-i-need-to-use-sqlalchemy-back-populates\n        cascade=\"all, delete-orphan\",\n    )\n\n    job: Mapped[\"JobTable\"] = relationship(back_populates=\"batch_sizes\")\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/schema/#zeus.optimizer.batch_size.server.database.schema.GaussianTsArmStateTable","title":"GaussianTsArmStateTable","text":"<p>               Bases: <code>Base</code></p> <p>Gaussian arm state schema. Represents a gaussian thompson arm states of a batch size.</p> <p>Refer to <code>GaussianTsArmState</code> for attributes.</p> Source code in <code>zeus/optimizer/batch_size/server/database/schema.py</code> <pre><code>class GaussianTsArmStateTable(Base):\n    \"\"\"Gaussian arm state schema. Represents a gaussian thompson arm states of a batch size.\n\n    Refer to [`GaussianTsArmState`][zeus.optimizer.batch_size.server.batch_size_state.models.GaussianTsArmState] for attributes.\n    \"\"\"\n\n    __tablename__ = \"GaussianTsArmState\"\n\n    job_id: Mapped[str] = mapped_column(VARCHAR(300), primary_key=True)\n    batch_size: Mapped[int] = mapped_column(Integer, primary_key=True)  # arm\n\n    param_mean: Mapped[float] = mapped_column(Float, default=0.0)\n    param_precision: Mapped[float] = mapped_column(Float, default=0.0)\n    reward_precision: Mapped[float] = mapped_column(Float, default=0.0)\n    num_observations: Mapped[int] = mapped_column(Integer, default=0)\n\n    batch_size_state: Mapped[\"BatchSizeTable\"] = relationship(back_populates=\"arm_state\")\n\n    __table_args__ = (\n        ForeignKeyConstraint(\n            [job_id, batch_size],\n            [BatchSizeTable.job_id, BatchSizeTable.batch_size],\n            ondelete=\"CASCADE\",\n        ),\n    )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/schema/#zeus.optimizer.batch_size.server.database.schema.TrialType","title":"TrialType","text":"<p>               Bases: <code>Enum</code></p> <p>Type of trial.</p> <p>Exploration is a trial done during Pruning stage. Concurrent is a trial done as a concurrent job submission. MAB is a trial done during the MAB stage.</p> Source code in <code>zeus/optimizer/batch_size/server/database/schema.py</code> <pre><code>class TrialType(enum.Enum):\n    \"\"\"Type of trial.\n\n    Exploration is a trial done during Pruning stage.\n    Concurrent is a trial done as a concurrent job submission.\n    MAB is a trial done during the MAB stage.\n    \"\"\"\n\n    Exploration = \"Exploration\"\n    Concurrent = \"Concurrent\"\n    MAB = \"MAB\"\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/schema/#zeus.optimizer.batch_size.server.database.schema.TrialStatus","title":"TrialStatus","text":"<p>               Bases: <code>Enum</code></p> <p>Status of trial.</p> <p>Dispatched means this trial is issued. Succeded means trial ended without error. Failed means trial ended with error.</p> Source code in <code>zeus/optimizer/batch_size/server/database/schema.py</code> <pre><code>class TrialStatus(enum.Enum):\n    \"\"\"Status of trial.\n\n    Dispatched means this trial is issued.\n    Succeded means trial ended without error.\n    Failed means trial ended with error.\n    \"\"\"\n\n    Dispatched = \"Dispatched\"\n    Succeeded = \"Succeeded\"\n    Failed = \"Failed\"\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/database/schema/#zeus.optimizer.batch_size.server.database.schema.TrialTable","title":"TrialTable","text":"<p>               Bases: <code>Base</code></p> <p>Represents each trial of training.</p> <p>Refer to <code>Trial</code> for attributes.</p> Source code in <code>zeus/optimizer/batch_size/server/database/schema.py</code> <pre><code>class TrialTable(Base):\n    \"\"\"Represents each trial of training.\n\n    Refer to [`Trial`][zeus.optimizer.batch_size.server.batch_size_state.models.Trial] for attributes.\n    \"\"\"\n\n    __tablename__ = \"Trial\"\n\n    job_id: Mapped[str] = mapped_column(VARCHAR(300), primary_key=True, nullable=False)\n    batch_size: Mapped[int] = mapped_column(Integer, nullable=False)\n    trial_number: Mapped[int] = mapped_column(Integer, primary_key=True, nullable=False)\n    start_timestamp: Mapped[datetime] = mapped_column(DateTime, nullable=False)\n    type: Mapped[TrialType] = mapped_column(Enum(TrialType), nullable=False)\n    status: Mapped[TrialStatus] = mapped_column(Enum(TrialStatus), nullable=False)\n\n    end_timestamp: Mapped[Optional[datetime]] = mapped_column(DateTime, nullable=True)\n    time: Mapped[Optional[float]] = mapped_column(Float, nullable=True)\n    energy: Mapped[Optional[float]] = mapped_column(Float, nullable=True)\n    converged: Mapped[Optional[bool]] = mapped_column(Boolean, nullable=True)\n\n    batch_size_state: Mapped[\"BatchSizeTable\"] = relationship(back_populates=\"trials\")\n\n    __table_args__ = (\n        ForeignKeyConstraint(\n            [job_id, batch_size],\n            [BatchSizeTable.job_id, BatchSizeTable.batch_size],\n            ondelete=\"CASCADE\",\n        ),\n    )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/","title":"job","text":""},{"location":"reference/optimizer/batch_size/server/job/#zeus.optimizer.batch_size.server.job","title":"zeus.optimizer.batch_size.server.job","text":"<p>Models, commands, and repository for job states.</p>"},{"location":"reference/optimizer/batch_size/server/job/commands/","title":"commands","text":""},{"location":"reference/optimizer/batch_size/server/job/commands/#zeus.optimizer.batch_size.server.job.commands","title":"zeus.optimizer.batch_size.server.job.commands","text":"<p>Commands to use <code>JobStateRepository</code>.</p>"},{"location":"reference/optimizer/batch_size/server/job/commands/#zeus.optimizer.batch_size.server.job.commands.UpdateExpDefaultBs","title":"UpdateExpDefaultBs","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters to update the exploration default batch size.</p> <p>Attributes:</p> Name Type Description <code>job_id</code> <code>str</code> <p>Job Id.</p> <code>exp_default_batch_size</code> <code>int</code> <p>new default batch size to use.</p> Source code in <code>zeus/optimizer/batch_size/server/job/commands.py</code> <pre><code>class UpdateExpDefaultBs(BaseModel):\n    \"\"\"Parameters to update the exploration default batch size.\n\n    Attributes:\n        job_id: Job Id.\n        exp_default_batch_size: new default batch size to use.\n    \"\"\"\n\n    job_id: str\n    exp_default_batch_size: int = Field(gt=0)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/commands/#zeus.optimizer.batch_size.server.job.commands.UpdateJobStage","title":"UpdateJobStage","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters to update the job stage.</p> <p>Attributes:</p> Name Type Description <code>job_id</code> <code>str</code> <p>Job Id.</p> <code>stage</code> <code>Stage</code> <p>Set it to MAB since we only go from Pruning to MAB.</p> Source code in <code>zeus/optimizer/batch_size/server/job/commands.py</code> <pre><code>class UpdateJobStage(BaseModel):\n    \"\"\"Parameters to update the job stage.\n\n    Attributes:\n        job_id: Job Id.\n        stage: Set it to MAB since we only go from Pruning to MAB.\n    \"\"\"\n\n    job_id: str\n    stage: Stage = Field(Stage.MAB, const=True)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/commands/#zeus.optimizer.batch_size.server.job.commands.UpdateGeneratorState","title":"UpdateGeneratorState","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters to update the generator state.</p> <p>Attributes:</p> Name Type Description <code>job_id</code> <code>str</code> <p>Job Id.</p> <code>state</code> <code>str</code> <p>Generator state.</p> Source code in <code>zeus/optimizer/batch_size/server/job/commands.py</code> <pre><code>class UpdateGeneratorState(BaseModel):\n    \"\"\"Parameters to update the generator state.\n\n    Attributes:\n        job_id: Job Id.\n        state: Generator state.\n    \"\"\"\n\n    job_id: str\n    state: str\n\n    @validator(\"state\")\n    def _validate_state(cls, state: str) -&gt; str:\n        \"\"\"Validate the sanity of state.\"\"\"\n        try:\n            np.random.default_rng(1).__setstate__(json.loads(state))\n            return state\n        except (TypeError, ValueError) as err:\n            raise ValueError(f\"Invalid generator state ({state})\") from err\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/commands/#zeus.optimizer.batch_size.server.job.commands.UpdateGeneratorState._validate_state","title":"_validate_state","text":"<pre><code>_validate_state(state)\n</code></pre> <p>Validate the sanity of state.</p> Source code in <code>zeus/optimizer/batch_size/server/job/commands.py</code> <pre><code>@validator(\"state\")\ndef _validate_state(cls, state: str) -&gt; str:\n    \"\"\"Validate the sanity of state.\"\"\"\n    try:\n        np.random.default_rng(1).__setstate__(json.loads(state))\n        return state\n    except (TypeError, ValueError) as err:\n        raise ValueError(f\"Invalid generator state ({state})\") from err\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/commands/#zeus.optimizer.batch_size.server.job.commands.UpdateJobMinCost","title":"UpdateJobMinCost","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters to update the min training cost and corresponding batch size.</p> <p>Attributes:</p> Name Type Description <code>job_id</code> <code>str</code> <p>Job Id.</p> <code>min_cost</code> <code>float</code> <p>Min training cost.</p> <code>min_cost_batch_size</code> <code>int</code> <p>Corresponding batch size.</p> Source code in <code>zeus/optimizer/batch_size/server/job/commands.py</code> <pre><code>class UpdateJobMinCost(BaseModel):\n    \"\"\"Parameters to update the min training cost and corresponding batch size.\n\n    Attributes:\n        job_id: Job Id.\n        min_cost: Min training cost.\n        min_cost_batch_size: Corresponding batch size.\n    \"\"\"\n\n    job_id: str\n    min_cost: float = Field(ge=0)\n    min_cost_batch_size: int = Field(gt=0)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/commands/#zeus.optimizer.batch_size.server.job.commands.CreateJob","title":"CreateJob","text":"<p>               Bases: <code>GpuConfig</code>, <code>JobParams</code></p> <p>Parameters to create a new job.</p> <p>Attributes:</p> Name Type Description <code>exp_default_batch_size</code> <code>int</code> <p>Exploration default batch size that is used during Pruning stage.</p> <code>min_cost</code> <code>None</code> <p>Min training cost observed. Initially, None.</p> <code>min_cost_batch_size</code> <code>int</code> <p>Batch size that has minimum training cost observed.</p> <code>stage</code> <code>Stage</code> <p>Stage of the job.</p> <code>mab_random_generator_state</code> <code>Optional[str]</code> <p>Generator state if mab_seed is not None. Otherwise, None.</p> <p>For the rest of attributes, refer to <code>JobParams</code>[zeus.optimizer.batch_size.common.JobParams] and <code>GpuConfig</code>[zeus.optimizer.batch_size.common.GpuConfig]</p> Source code in <code>zeus/optimizer/batch_size/server/job/commands.py</code> <pre><code>class CreateJob(GpuConfig, JobParams):\n    \"\"\"Parameters to create a new job.\n\n    Attributes:\n        exp_default_batch_size: Exploration default batch size that is used during Pruning stage.\n        min_cost: Min training cost observed. Initially, None.\n        min_cost_batch_size: Batch size that has minimum training cost observed.\n        stage: Stage of the job.\n        mab_random_generator_state: Generator state if mab_seed is not None. Otherwise, None.\n\n    For the rest of attributes, refer to `JobParams`[zeus.optimizer.batch_size.common.JobParams] and `GpuConfig`[zeus.optimizer.batch_size.common.GpuConfig]\n    \"\"\"\n\n    exp_default_batch_size: int\n    min_cost: None = Field(None, const=True)\n    min_cost_batch_size: int\n    stage: Stage = Field(Stage.Pruning, const=True)\n    mab_random_generator_state: Optional[str] = None\n\n    class Config:\n        \"\"\"Model configuration.\n\n        Make it immutable after creation.\n        \"\"\"\n\n        frozen = True\n\n    @root_validator(skip_on_failure=True)\n    def _validate_states(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Validate Job states.\n\n        We are checking,\n            - If mab seed and generator state is matching.\n            - If default, exp_default, min batch sizes are correctly intialized.\n            - If default batch size is in the list of batch sizes.\n        \"\"\"\n        state: str | None = values[\"mab_random_generator_state\"]\n        mab_seed: int | None = values[\"mab_seed\"]\n        bss: list[int] = values[\"batch_sizes\"]\n        dbs: int = values[\"default_batch_size\"]\n        ebs: int = values[\"exp_default_batch_size\"]\n        mbs: int = values[\"min_cost_batch_size\"]\n\n        if mab_seed is not None:\n            if state is None:\n                raise ValueError(\"mab_seed is not none, but generator state is none\")\n            else:\n                try:\n                    np.random.default_rng(1).__setstate__(json.loads(state))\n                except (TypeError, ValueError) as err:\n                    raise ValueError(f\"Invalid generator state ({state})\") from err\n\n        if not (dbs == ebs == mbs):\n            raise ValueError(\n                f\"During initialization, default_batch_size({dbs}), exp_default_batch_size({ebs}), min_batch_size({mbs}) should be all the same\"\n            )\n        if dbs not in bss:\n            raise ValueError(f\"default_batch_size({dbs}) is not in the batch size list({bss})\")\n\n        return values\n\n    @classmethod\n    def from_job_config(cls, js: JobSpecFromClient) -&gt; \"CreateJob\":\n        \"\"\"From JobConfig, instantiate `CreateJob`.\n\n        Initialize generator state, exp_default_batch_size, and min_cost_batch_size.\n        \"\"\"\n        d = js.dict()\n        d[\"exp_default_batch_size\"] = js.default_batch_size\n        if js.mab_seed is not None:\n            rng = np.random.default_rng(js.mab_seed)\n            d[\"mab_random_generator_state\"] = json.dumps(rng.__getstate__())\n        d[\"min_cost_batch_size\"] = js.default_batch_size\n        return cls.parse_obj(d)\n\n    def to_orm(self) -&gt; JobTable:\n        \"\"\"Convert pydantic model `CreateJob` to ORM object Job.\"\"\"\n        d = self.dict()\n        job = JobTable()\n        for k, v in d.items():\n            if k != \"batch_sizes\":\n                setattr(job, k, v)\n        job.batch_sizes = [BatchSizeTable(job_id=self.job_id, batch_size=bs) for bs in self.batch_sizes]\n        return job\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/commands/#zeus.optimizer.batch_size.server.job.commands.CreateJob.Config","title":"Config","text":"<p>Model configuration.</p> <p>Make it immutable after creation.</p> Source code in <code>zeus/optimizer/batch_size/server/job/commands.py</code> <pre><code>class Config:\n    \"\"\"Model configuration.\n\n    Make it immutable after creation.\n    \"\"\"\n\n    frozen = True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/commands/#zeus.optimizer.batch_size.server.job.commands.CreateJob._validate_states","title":"_validate_states","text":"<pre><code>_validate_states(values)\n</code></pre> <p>Validate Job states.</p> <p>We are checking,     - If mab seed and generator state is matching.     - If default, exp_default, min batch sizes are correctly intialized.     - If default batch size is in the list of batch sizes.</p> Source code in <code>zeus/optimizer/batch_size/server/job/commands.py</code> <pre><code>@root_validator(skip_on_failure=True)\ndef _validate_states(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Validate Job states.\n\n    We are checking,\n        - If mab seed and generator state is matching.\n        - If default, exp_default, min batch sizes are correctly intialized.\n        - If default batch size is in the list of batch sizes.\n    \"\"\"\n    state: str | None = values[\"mab_random_generator_state\"]\n    mab_seed: int | None = values[\"mab_seed\"]\n    bss: list[int] = values[\"batch_sizes\"]\n    dbs: int = values[\"default_batch_size\"]\n    ebs: int = values[\"exp_default_batch_size\"]\n    mbs: int = values[\"min_cost_batch_size\"]\n\n    if mab_seed is not None:\n        if state is None:\n            raise ValueError(\"mab_seed is not none, but generator state is none\")\n        else:\n            try:\n                np.random.default_rng(1).__setstate__(json.loads(state))\n            except (TypeError, ValueError) as err:\n                raise ValueError(f\"Invalid generator state ({state})\") from err\n\n    if not (dbs == ebs == mbs):\n        raise ValueError(\n            f\"During initialization, default_batch_size({dbs}), exp_default_batch_size({ebs}), min_batch_size({mbs}) should be all the same\"\n        )\n    if dbs not in bss:\n        raise ValueError(f\"default_batch_size({dbs}) is not in the batch size list({bss})\")\n\n    return values\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/commands/#zeus.optimizer.batch_size.server.job.commands.CreateJob.from_job_config","title":"from_job_config  <code>classmethod</code>","text":"<pre><code>from_job_config(js)\n</code></pre> <p>From JobConfig, instantiate <code>CreateJob</code>.</p> <p>Initialize generator state, exp_default_batch_size, and min_cost_batch_size.</p> Source code in <code>zeus/optimizer/batch_size/server/job/commands.py</code> <pre><code>@classmethod\ndef from_job_config(cls, js: JobSpecFromClient) -&gt; \"CreateJob\":\n    \"\"\"From JobConfig, instantiate `CreateJob`.\n\n    Initialize generator state, exp_default_batch_size, and min_cost_batch_size.\n    \"\"\"\n    d = js.dict()\n    d[\"exp_default_batch_size\"] = js.default_batch_size\n    if js.mab_seed is not None:\n        rng = np.random.default_rng(js.mab_seed)\n        d[\"mab_random_generator_state\"] = json.dumps(rng.__getstate__())\n    d[\"min_cost_batch_size\"] = js.default_batch_size\n    return cls.parse_obj(d)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/commands/#zeus.optimizer.batch_size.server.job.commands.CreateJob.to_orm","title":"to_orm","text":"<pre><code>to_orm()\n</code></pre> <p>Convert pydantic model <code>CreateJob</code> to ORM object Job.</p> Source code in <code>zeus/optimizer/batch_size/server/job/commands.py</code> <pre><code>def to_orm(self) -&gt; JobTable:\n    \"\"\"Convert pydantic model `CreateJob` to ORM object Job.\"\"\"\n    d = self.dict()\n    job = JobTable()\n    for k, v in d.items():\n        if k != \"batch_sizes\":\n            setattr(job, k, v)\n    job.batch_sizes = [BatchSizeTable(job_id=self.job_id, batch_size=bs) for bs in self.batch_sizes]\n    return job\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/models/","title":"models","text":""},{"location":"reference/optimizer/batch_size/server/job/models/#zeus.optimizer.batch_size.server.job.models","title":"zeus.optimizer.batch_size.server.job.models","text":"<p>Pydantic models for Job.</p>"},{"location":"reference/optimizer/batch_size/server/job/models/#zeus.optimizer.batch_size.server.job.models.Stage","title":"Stage","text":"<p>               Bases: <code>Enum</code></p> <p>Job Stage.</p> Source code in <code>zeus/optimizer/batch_size/server/job/models.py</code> <pre><code>class Stage(Enum):\n    \"\"\"Job Stage.\"\"\"\n\n    Pruning = \"Pruning\"\n    MAB = \"MAB\"\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/models/#zeus.optimizer.batch_size.server.job.models.JobGetter","title":"JobGetter","text":"<p>               Bases: <code>GetterDict</code></p> <p>Getter for batch size to convert ORM batch size object to integer.</p> Source code in <code>zeus/optimizer/batch_size/server/job/models.py</code> <pre><code>class JobGetter(GetterDict):\n    \"\"\"Getter for batch size to convert ORM batch size object to integer.\"\"\"\n\n    def get(self, key: str, default: Any = None) -&gt; Any:\n        \"\"\"Get value from dict.\"\"\"\n        if key == \"batch_sizes\":\n            # If the key is batch_sizes, parse the integer from object.\n            return [bs.batch_size for bs in self._obj.batch_sizes]\n\n        return super().get(key, default)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/models/#zeus.optimizer.batch_size.server.job.models.JobGetter.get","title":"get","text":"<pre><code>get(key, default=None)\n</code></pre> <p>Get value from dict.</p> Source code in <code>zeus/optimizer/batch_size/server/job/models.py</code> <pre><code>def get(self, key: str, default: Any = None) -&gt; Any:\n    \"\"\"Get value from dict.\"\"\"\n    if key == \"batch_sizes\":\n        # If the key is batch_sizes, parse the integer from object.\n        return [bs.batch_size for bs in self._obj.batch_sizes]\n\n    return super().get(key, default)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/models/#zeus.optimizer.batch_size.server.job.models.JobState","title":"JobState","text":"<p>               Bases: <code>JobParams</code>, <code>GpuConfig</code></p> <p>Pydantic model for Job which includes job-level states.</p> <p>Attributes:</p> Name Type Description <code>exp_default_batch_size</code> <code>int</code> <p>Exploration default batch size that is used during Pruning stage.</p> <code>min_cost</code> <code>Optional[float]</code> <p>Min training cost observed. Initially, None.</p> <code>min_cost_batch_size</code> <code>int</code> <p>Batch size that has minimum training cost observed.</p> <code>stage</code> <code>Stage</code> <p>Stage of the job.</p> <code>mab_random_generator_state</code> <code>Optional[str]</code> <p>Generator state if mab_seed is not None. Otherwise, None.</p> <p>For the rest of attributes, refer to <code>JobParams</code> and <code>GpuConfig</code></p> Source code in <code>zeus/optimizer/batch_size/server/job/models.py</code> <pre><code>class JobState(JobParams, GpuConfig):\n    \"\"\"Pydantic model for Job which includes job-level states.\n\n    Attributes:\n        exp_default_batch_size: Exploration default batch size that is used during Pruning stage.\n        min_cost: Min training cost observed. Initially, None.\n        min_cost_batch_size: Batch size that has minimum training cost observed.\n        stage: Stage of the job.\n        mab_random_generator_state: Generator state if mab_seed is not None. Otherwise, None.\n\n    For the rest of attributes, refer to [`JobParams`][zeus.optimizer.batch_size.common.JobParams] and [`GpuConfig`][zeus.optimizer.batch_size.common.GpuConfig]\n    \"\"\"\n\n    exp_default_batch_size: int\n\n    min_cost: Optional[float] = None\n    min_cost_batch_size: int\n    stage: Stage = Stage.Pruning\n\n    mab_random_generator_state: Optional[str] = None\n\n    class Config:\n        \"\"\"Model configuration.\n\n        Allow instantiating the model from an ORM object.\n        \"\"\"\n\n        orm_mode = True\n        getter_dict = JobGetter\n\n    @root_validator(skip_on_failure=True)\n    def _validate_mab(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Validate generator state.\"\"\"\n        state: str | None = values[\"mab_random_generator_state\"]\n        mab_seed: int | None = values[\"mab_seed\"]\n\n        if mab_seed is not None:\n            if state is None:\n                raise ValueError(\"mab_seed is not none, but generator state is none\")\n            else:\n                try:\n                    # Check sanity of the generator state.\n                    np.random.default_rng(1).__setstate__(json.loads(state))\n                except (TypeError, ValueError) as err:\n                    raise ValueError(f\"Invalid generator state ({state})\") from err\n\n        return values\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/models/#zeus.optimizer.batch_size.server.job.models.JobState.Config","title":"Config","text":"<p>Model configuration.</p> <p>Allow instantiating the model from an ORM object.</p> Source code in <code>zeus/optimizer/batch_size/server/job/models.py</code> <pre><code>class Config:\n    \"\"\"Model configuration.\n\n    Allow instantiating the model from an ORM object.\n    \"\"\"\n\n    orm_mode = True\n    getter_dict = JobGetter\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/models/#zeus.optimizer.batch_size.server.job.models.JobState._validate_mab","title":"_validate_mab","text":"<pre><code>_validate_mab(values)\n</code></pre> <p>Validate generator state.</p> Source code in <code>zeus/optimizer/batch_size/server/job/models.py</code> <pre><code>@root_validator(skip_on_failure=True)\ndef _validate_mab(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Validate generator state.\"\"\"\n    state: str | None = values[\"mab_random_generator_state\"]\n    mab_seed: int | None = values[\"mab_seed\"]\n\n    if mab_seed is not None:\n        if state is None:\n            raise ValueError(\"mab_seed is not none, but generator state is none\")\n        else:\n            try:\n                # Check sanity of the generator state.\n                np.random.default_rng(1).__setstate__(json.loads(state))\n            except (TypeError, ValueError) as err:\n                raise ValueError(f\"Invalid generator state ({state})\") from err\n\n    return values\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/repository/","title":"repository","text":""},{"location":"reference/optimizer/batch_size/server/job/repository/#zeus.optimizer.batch_size.server.job.repository","title":"zeus.optimizer.batch_size.server.job.repository","text":"<p>Repository for manipulating Job table.</p>"},{"location":"reference/optimizer/batch_size/server/job/repository/#zeus.optimizer.batch_size.server.job.repository.JobStateRepository","title":"JobStateRepository","text":"<p>               Bases: <code>DatabaseRepository</code></p> <p>Repository that provides basic interfaces to interact with Job table.</p> Source code in <code>zeus/optimizer/batch_size/server/job/repository.py</code> <pre><code>class JobStateRepository(DatabaseRepository):\n    \"\"\"Repository that provides basic interfaces to interact with Job table.\"\"\"\n\n    def __init__(self, session: AsyncSession):\n        \"\"\"Set db session and intialize job. We are working with only one job per session.\"\"\"\n        super().__init__(session)\n        self.fetched_job: JobTable | None = None\n\n    async def get_job(self, job_id: str) -&gt; JobState | None:\n        \"\"\"Get job State, which includes jobSpec + batch_sizes(list[int]), without specific states of each batch_size.\n\n        Args:\n            job_id: Job id.\n\n        Returns:\n            set fetched_job and return `JobState` if we found a job, unless return None.\n        \"\"\"\n        stmt = select(JobTable).where(JobTable.job_id == job_id)\n        job = await self.session.scalar(stmt)\n\n        if job is None:\n            logger.info(\"get_job: NoResultFound\")\n            return None\n\n        self.fetched_job = job\n        return JobState.from_orm(job)\n\n    def get_job_from_session(self, job_id: str) -&gt; JobState | None:\n        \"\"\"Get a job that was fetched from this session.\n\n        Args:\n            job_id: Job id.\n\n        Returns:\n            Corresponding `JobState`. If none was found, return None.\n        \"\"\"\n        if self.fetched_job is None or self.fetched_job.job_id != job_id:\n            return None\n        return JobState.from_orm(self.fetched_job)\n\n    def update_exp_default_bs(self, updated_bs: UpdateExpDefaultBs) -&gt; None:\n        \"\"\"Update exploration default batch size on fetched job.\n\n        Args:\n            updated_bs: Job Id and new batch size.\n        \"\"\"\n        if self.fetched_job is None:\n            raise ZeusBSOServiceBadOperationError(\"No job is fetched.\")\n\n        if updated_bs.job_id == self.fetched_job.job_id:\n            self.fetched_job.exp_default_batch_size = updated_bs.exp_default_batch_size\n        else:\n            raise ZeusBSOValueError(f\"Unknown job_id ({updated_bs.job_id}). Expecting {self.fetched_job.job_id}\")\n\n    def update_stage(self, updated_stage: UpdateJobStage) -&gt; None:\n        \"\"\"Update stage on fetched job.\n\n        Args:\n            updated_stage: Job Id and new stage.\n        \"\"\"\n        if self.fetched_job is None:\n            raise ZeusBSOServiceBadOperationError(\"No job is fetched.\")\n\n        if self.fetched_job.job_id == updated_stage.job_id:\n            self.fetched_job.stage = updated_stage.stage\n        else:\n            raise ZeusBSOValueError(f\"Unknown job_id ({updated_stage.job_id}). Expecting {self.fetched_job.job_id}\")\n\n    def update_min(self, updated_min: UpdateJobMinCost) -&gt; None:\n        \"\"\"Update exploration min training cost and corresponding batch size on fetched job.\n\n        Args:\n            updated_min: Job Id, new min cost and batch size.\n        \"\"\"\n        if self.fetched_job is None:\n            raise ZeusBSOServiceBadOperationError(\"No job is fetched.\")\n\n        if self.fetched_job.job_id == updated_min.job_id:\n            self.fetched_job.min_cost = updated_min.min_cost\n            self.fetched_job.min_cost_batch_size = updated_min.min_cost_batch_size\n        else:\n            raise ZeusBSOValueError(f\"Unknown job_id ({updated_min.job_id}). Expecting {self.fetched_job.job_id}\")\n\n    def update_generator_state(self, updated_state: UpdateGeneratorState) -&gt; None:\n        \"\"\"Update generator state on fetched job.\n\n        Args:\n            updated_state: Job Id and new generator state.\n        \"\"\"\n        if self.fetched_job is None:\n            raise ZeusBSOServiceBadOperationError(\"No job is fetched.\")\n\n        if self.fetched_job.job_id == updated_state.job_id:\n            self.fetched_job.mab_random_generator_state = updated_state.state\n        else:\n            raise ZeusBSOValueError(f\"Unknown job_id ({updated_state.job_id}). Expecting {self.fetched_job.job_id}\")\n\n    def create_job(self, new_job: CreateJob) -&gt; None:\n        \"\"\"Create a new job by adding a new job to the session.\n\n        Args:\n            new_job: Job configuration for a new job.\n        \"\"\"\n        self.session.add(new_job.to_orm())\n\n    def check_job_fetched(self, job_id: str) -&gt; bool:\n        \"\"\"Check if this job is already fetched before.\n\n        Args:\n            job_id: Job id.\n\n        Returns:\n            True if this job was fetched and in session. Otherwise, return false.\n        \"\"\"\n        return not (self.fetched_job is None or self.fetched_job.job_id != job_id)\n\n    async def delete_job(self, job_id: str) -&gt; bool:\n        \"\"\"Delete the job of a given job_Id.\n\n        Args:\n            job_id: Job id.\n\n        Returns:\n            True if the job got deleted.\n        \"\"\"\n        stmt = select(JobTable).where(JobTable.job_id == job_id)\n        job = await self.session.scalar(stmt)\n\n        if job is None:\n            return False\n\n        # We can't straight delete using a query, since some db such as sqlite\n        # Foreign Key is default to OFF, so \"on delete = cascade\" will not be fired.\n        await self.session.delete(job)\n        return True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/repository/#zeus.optimizer.batch_size.server.job.repository.JobStateRepository.__init__","title":"__init__","text":"<pre><code>__init__(session)\n</code></pre> Source code in <code>zeus/optimizer/batch_size/server/job/repository.py</code> <pre><code>def __init__(self, session: AsyncSession):\n    \"\"\"Set db session and intialize job. We are working with only one job per session.\"\"\"\n    super().__init__(session)\n    self.fetched_job: JobTable | None = None\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/repository/#zeus.optimizer.batch_size.server.job.repository.JobStateRepository.get_job","title":"get_job  <code>async</code>","text":"<pre><code>get_job(job_id)\n</code></pre> <p>Get job State, which includes jobSpec + batch_sizes(list[int]), without specific states of each batch_size.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>Job id.</p> required <p>Returns:</p> Type Description <code>JobState | None</code> <p>set fetched_job and return <code>JobState</code> if we found a job, unless return None.</p> Source code in <code>zeus/optimizer/batch_size/server/job/repository.py</code> <pre><code>async def get_job(self, job_id: str) -&gt; JobState | None:\n    \"\"\"Get job State, which includes jobSpec + batch_sizes(list[int]), without specific states of each batch_size.\n\n    Args:\n        job_id: Job id.\n\n    Returns:\n        set fetched_job and return `JobState` if we found a job, unless return None.\n    \"\"\"\n    stmt = select(JobTable).where(JobTable.job_id == job_id)\n    job = await self.session.scalar(stmt)\n\n    if job is None:\n        logger.info(\"get_job: NoResultFound\")\n        return None\n\n    self.fetched_job = job\n    return JobState.from_orm(job)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/repository/#zeus.optimizer.batch_size.server.job.repository.JobStateRepository.get_job_from_session","title":"get_job_from_session","text":"<pre><code>get_job_from_session(job_id)\n</code></pre> <p>Get a job that was fetched from this session.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>Job id.</p> required <p>Returns:</p> Type Description <code>JobState | None</code> <p>Corresponding <code>JobState</code>. If none was found, return None.</p> Source code in <code>zeus/optimizer/batch_size/server/job/repository.py</code> <pre><code>def get_job_from_session(self, job_id: str) -&gt; JobState | None:\n    \"\"\"Get a job that was fetched from this session.\n\n    Args:\n        job_id: Job id.\n\n    Returns:\n        Corresponding `JobState`. If none was found, return None.\n    \"\"\"\n    if self.fetched_job is None or self.fetched_job.job_id != job_id:\n        return None\n    return JobState.from_orm(self.fetched_job)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/repository/#zeus.optimizer.batch_size.server.job.repository.JobStateRepository.update_exp_default_bs","title":"update_exp_default_bs","text":"<pre><code>update_exp_default_bs(updated_bs)\n</code></pre> <p>Update exploration default batch size on fetched job.</p> <p>Parameters:</p> Name Type Description Default <code>updated_bs</code> <code>UpdateExpDefaultBs</code> <p>Job Id and new batch size.</p> required Source code in <code>zeus/optimizer/batch_size/server/job/repository.py</code> <pre><code>def update_exp_default_bs(self, updated_bs: UpdateExpDefaultBs) -&gt; None:\n    \"\"\"Update exploration default batch size on fetched job.\n\n    Args:\n        updated_bs: Job Id and new batch size.\n    \"\"\"\n    if self.fetched_job is None:\n        raise ZeusBSOServiceBadOperationError(\"No job is fetched.\")\n\n    if updated_bs.job_id == self.fetched_job.job_id:\n        self.fetched_job.exp_default_batch_size = updated_bs.exp_default_batch_size\n    else:\n        raise ZeusBSOValueError(f\"Unknown job_id ({updated_bs.job_id}). Expecting {self.fetched_job.job_id}\")\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/repository/#zeus.optimizer.batch_size.server.job.repository.JobStateRepository.update_stage","title":"update_stage","text":"<pre><code>update_stage(updated_stage)\n</code></pre> <p>Update stage on fetched job.</p> <p>Parameters:</p> Name Type Description Default <code>updated_stage</code> <code>UpdateJobStage</code> <p>Job Id and new stage.</p> required Source code in <code>zeus/optimizer/batch_size/server/job/repository.py</code> <pre><code>def update_stage(self, updated_stage: UpdateJobStage) -&gt; None:\n    \"\"\"Update stage on fetched job.\n\n    Args:\n        updated_stage: Job Id and new stage.\n    \"\"\"\n    if self.fetched_job is None:\n        raise ZeusBSOServiceBadOperationError(\"No job is fetched.\")\n\n    if self.fetched_job.job_id == updated_stage.job_id:\n        self.fetched_job.stage = updated_stage.stage\n    else:\n        raise ZeusBSOValueError(f\"Unknown job_id ({updated_stage.job_id}). Expecting {self.fetched_job.job_id}\")\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/repository/#zeus.optimizer.batch_size.server.job.repository.JobStateRepository.update_min","title":"update_min","text":"<pre><code>update_min(updated_min)\n</code></pre> <p>Update exploration min training cost and corresponding batch size on fetched job.</p> <p>Parameters:</p> Name Type Description Default <code>updated_min</code> <code>UpdateJobMinCost</code> <p>Job Id, new min cost and batch size.</p> required Source code in <code>zeus/optimizer/batch_size/server/job/repository.py</code> <pre><code>def update_min(self, updated_min: UpdateJobMinCost) -&gt; None:\n    \"\"\"Update exploration min training cost and corresponding batch size on fetched job.\n\n    Args:\n        updated_min: Job Id, new min cost and batch size.\n    \"\"\"\n    if self.fetched_job is None:\n        raise ZeusBSOServiceBadOperationError(\"No job is fetched.\")\n\n    if self.fetched_job.job_id == updated_min.job_id:\n        self.fetched_job.min_cost = updated_min.min_cost\n        self.fetched_job.min_cost_batch_size = updated_min.min_cost_batch_size\n    else:\n        raise ZeusBSOValueError(f\"Unknown job_id ({updated_min.job_id}). Expecting {self.fetched_job.job_id}\")\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/repository/#zeus.optimizer.batch_size.server.job.repository.JobStateRepository.update_generator_state","title":"update_generator_state","text":"<pre><code>update_generator_state(updated_state)\n</code></pre> <p>Update generator state on fetched job.</p> <p>Parameters:</p> Name Type Description Default <code>updated_state</code> <code>UpdateGeneratorState</code> <p>Job Id and new generator state.</p> required Source code in <code>zeus/optimizer/batch_size/server/job/repository.py</code> <pre><code>def update_generator_state(self, updated_state: UpdateGeneratorState) -&gt; None:\n    \"\"\"Update generator state on fetched job.\n\n    Args:\n        updated_state: Job Id and new generator state.\n    \"\"\"\n    if self.fetched_job is None:\n        raise ZeusBSOServiceBadOperationError(\"No job is fetched.\")\n\n    if self.fetched_job.job_id == updated_state.job_id:\n        self.fetched_job.mab_random_generator_state = updated_state.state\n    else:\n        raise ZeusBSOValueError(f\"Unknown job_id ({updated_state.job_id}). Expecting {self.fetched_job.job_id}\")\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/repository/#zeus.optimizer.batch_size.server.job.repository.JobStateRepository.create_job","title":"create_job","text":"<pre><code>create_job(new_job)\n</code></pre> <p>Create a new job by adding a new job to the session.</p> <p>Parameters:</p> Name Type Description Default <code>new_job</code> <code>CreateJob</code> <p>Job configuration for a new job.</p> required Source code in <code>zeus/optimizer/batch_size/server/job/repository.py</code> <pre><code>def create_job(self, new_job: CreateJob) -&gt; None:\n    \"\"\"Create a new job by adding a new job to the session.\n\n    Args:\n        new_job: Job configuration for a new job.\n    \"\"\"\n    self.session.add(new_job.to_orm())\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/repository/#zeus.optimizer.batch_size.server.job.repository.JobStateRepository.check_job_fetched","title":"check_job_fetched","text":"<pre><code>check_job_fetched(job_id)\n</code></pre> <p>Check if this job is already fetched before.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>Job id.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if this job was fetched and in session. Otherwise, return false.</p> Source code in <code>zeus/optimizer/batch_size/server/job/repository.py</code> <pre><code>def check_job_fetched(self, job_id: str) -&gt; bool:\n    \"\"\"Check if this job is already fetched before.\n\n    Args:\n        job_id: Job id.\n\n    Returns:\n        True if this job was fetched and in session. Otherwise, return false.\n    \"\"\"\n    return not (self.fetched_job is None or self.fetched_job.job_id != job_id)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/job/repository/#zeus.optimizer.batch_size.server.job.repository.JobStateRepository.delete_job","title":"delete_job  <code>async</code>","text":"<pre><code>delete_job(job_id)\n</code></pre> <p>Delete the job of a given job_Id.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>Job id.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the job got deleted.</p> Source code in <code>zeus/optimizer/batch_size/server/job/repository.py</code> <pre><code>async def delete_job(self, job_id: str) -&gt; bool:\n    \"\"\"Delete the job of a given job_Id.\n\n    Args:\n        job_id: Job id.\n\n    Returns:\n        True if the job got deleted.\n    \"\"\"\n    stmt = select(JobTable).where(JobTable.job_id == job_id)\n    job = await self.session.scalar(stmt)\n\n    if job is None:\n        return False\n\n    # We can't straight delete using a query, since some db such as sqlite\n    # Foreign Key is default to OFF, so \"on delete = cascade\" will not be fired.\n    await self.session.delete(job)\n    return True\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/","title":"services","text":""},{"location":"reference/optimizer/batch_size/server/services/#zeus.optimizer.batch_size.server.services","title":"zeus.optimizer.batch_size.server.services","text":"<p>Service layer on top of repository layer. Provides core methods to interact with database.</p>"},{"location":"reference/optimizer/batch_size/server/services/commands/","title":"commands","text":""},{"location":"reference/optimizer/batch_size/server/services/commands/#zeus.optimizer.batch_size.server.services.commands","title":"zeus.optimizer.batch_size.server.services.commands","text":"<p>Commands on how to use some methods from the <code>ZeusService</code>.</p>"},{"location":"reference/optimizer/batch_size/server/services/commands/#zeus.optimizer.batch_size.server.services.commands.GetRandomChoices","title":"GetRandomChoices","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for getting a random choices.</p> <p>Attributes:</p> Name Type Description <code>job_id</code> <code>str</code> <p>Job Id</p> <code>choices</code> <code>list[int]</code> <p>List of choices</p> Source code in <code>zeus/optimizer/batch_size/server/services/commands.py</code> <pre><code>class GetRandomChoices(BaseModel):\n    \"\"\"Parameters for getting a random choices.\n\n    Attributes:\n        job_id: Job Id\n        choices: List of choices\n    \"\"\"\n\n    job_id: str\n    choices: list[int]\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/commands/#zeus.optimizer.batch_size.server.services.commands.GetNormal","title":"GetNormal","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters for getting a random sample from normal distribution.</p> <p>Attributes:</p> Name Type Description <code>job_id</code> <code>str</code> <p>Job id</p> <code>loc</code> <code>float</code> <p>Mean</p> <code>scale</code> <code>float</code> <p>Stdev</p> Source code in <code>zeus/optimizer/batch_size/server/services/commands.py</code> <pre><code>class GetNormal(BaseModel):\n    \"\"\"Parameters for getting a random sample from normal distribution.\n\n    Attributes:\n        job_id: Job id\n        loc: Mean\n        scale: Stdev\n    \"\"\"\n\n    job_id: str\n    loc: float\n    scale: float\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/commands/#zeus.optimizer.batch_size.server.services.commands.UpdateArm","title":"UpdateArm","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters to update an arm.</p> <p>Attributes:</p> Name Type Description <code>trial</code> <code>ReadTrial</code> <p>Identifier of trial</p> <code>updated_arm</code> <code>GaussianTsArmState</code> <p>Updated state of arm.</p> Source code in <code>zeus/optimizer/batch_size/server/services/commands.py</code> <pre><code>class UpdateArm(BaseModel):\n    \"\"\"Parameters to update an arm.\n\n    Attributes:\n        trial: Identifier of trial\n        updated_arm: Updated state of arm.\n    \"\"\"\n\n    trial: ReadTrial\n    updated_arm: GaussianTsArmState\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/","title":"service","text":""},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service","title":"zeus.optimizer.batch_size.server.services.service","text":"<p>Zeus batch size optimizer service layer.</p>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService","title":"ZeusService","text":"<p>Zeus Service that interacts with database using repository.</p> <p>Provides application layer methods to communicate with database. Each method is one or more number of db operations that have to be done at the same time.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>class ZeusService:\n    \"\"\"Zeus Service that interacts with database using repository.\n\n    Provides application layer methods to communicate with database.\n    Each method is one or more number of db operations that have to be done at the same time.\n    \"\"\"\n\n    def __init__(self, db_session: AsyncSession):\n        \"\"\"Set up repositories to use to talk to database.\"\"\"\n        self.bs_repo = BatchSizeStateRepository(db_session)\n        self.job_repo = JobStateRepository(db_session)\n\n    async def get_arms(self, job_id: str) -&gt; list[GaussianTsArmState]:\n        \"\"\"Get GaussianTs arm states for all arms(job_id, batch size).\n\n        Args:\n            job_id: Job id\n\n        Returns:\n            list of arms\n        \"\"\"\n        return await self.bs_repo.get_arms(job_id)\n\n    async def get_arm(self, bs: BatchSizeBase) -&gt; GaussianTsArmState | None:\n        \"\"\"Get arm state for one arm.\n\n        Args:\n            bs: (job_id, batch size) pair that represents one arm\n\n        Returns:\n            Result arm state or None if we cannot find that arm\n        \"\"\"\n        return await self.bs_repo.get_arm(bs)\n\n    async def get_explorations_of_job(self, job_id: str) -&gt; ExplorationsPerJob:\n        \"\"\"Get all explorations we have done for that job.\n\n        Args:\n            job_id: Job id\n\n        Returns:\n            list of explorations per each batch size\n        \"\"\"\n        return await self.bs_repo.get_explorations_of_job(job_id)\n\n    def update_trial(self, updated_trial: UpdateTrial) -&gt; None:\n        \"\"\"Update trial.\n\n        (1) update the corresponding trial.\n        (2) we update the min training cost observed so far if we have to.\n\n        Args:\n            updated_trial: Result of training that batch size\n\n        Raises:\n            [`ZeusBSOServiceBadOperationError`][zeus.optimizer.batch_size.server.exceptions.ZeusBSOServiceBadOperationError]: When we didn't fetch the job or trial during this session. This operation should have\n                    fetched the job and trial first. Also, check if trial type is matching with fetched trial's type.\n        \"\"\"\n        trial = self._get_trial(\n            ReadTrial(\n                job_id=updated_trial.job_id,\n                batch_size=updated_trial.batch_size,\n                trial_number=updated_trial.trial_number,\n            )\n        )\n        if trial.status != TrialStatus.Dispatched:\n            raise ZeusBSOServiceBadOperationError(\"Trial already has a result.\")\n\n        self.bs_repo.updated_current_trial(updated_trial)\n\n        # Update the corresponding batch size's min cost if needed.\n        if updated_trial.status != TrialStatus.Failed:\n            job = self._get_job(updated_trial.job_id)\n            if updated_trial.energy is None or updated_trial.time is None:\n                raise ZeusBSOValueError(\"Energy and time should be set if the trial is not failed.\")\n            cur_cost = zeus_cost(updated_trial.energy, updated_trial.time, job.eta_knob, job.max_power)\n            if job.min_cost is None or job.min_cost &gt; cur_cost:\n                self.job_repo.update_min(\n                    UpdateJobMinCost(\n                        job_id=job.job_id,\n                        min_cost=cur_cost,\n                        min_cost_batch_size=updated_trial.batch_size,\n                    )\n                )\n\n    def update_arm_state(\n        self,\n        arm: UpdateArm,\n    ) -&gt; None:\n        \"\"\"Update arm state.\n\n        Args:\n            arm: Updated arm state.\n\n        Raises:\n            `ZeusBSOServiceBadOperationError`: When we didn't fetch the job or trial during this session. This operation should have\n                    fetched the job and trial first. Also, check if trial type is matching with fetched trial's type.\n        \"\"\"\n        self._check_job_fetched(arm.trial.job_id)\n        trial = self._get_trial(\n            ReadTrial(\n                job_id=arm.trial.job_id,\n                batch_size=arm.trial.batch_size,\n                trial_number=arm.trial.trial_number,\n            )\n        )\n        if trial.type != TrialType.MAB:\n            raise ZeusBSOServiceBadOperationError(\"Cannot update an arm since this trial is not issued from MAB stage.\")\n        self.bs_repo.update_arm_state(arm.updated_arm)\n\n    def update_exp_default_bs(self, updated_default_bs: UpdateExpDefaultBs) -&gt; None:\n        \"\"\"Update the default batch size for exploration.\n\n        Args:\n            updated_default_bs: Job Id and new default batch size\n\n        Raises:\n            `ZeusBSOServiceBadOperationError`: When we didn't fetch the job during this session. This operation should have\n                    fetched the job first.\n        \"\"\"\n        self._check_job_fetched(updated_default_bs.job_id)\n        self.job_repo.update_exp_default_bs(updated_default_bs)\n\n    async def create_trial(self, trial: CreateExplorationTrial | CreateMabTrial | CreateConcurrentTrial) -&gt; ReadTrial:\n        \"\"\"Create a new trial.\n\n        Args:\n            trial: New trial to create.\n\n        Raises:\n            `ZeusBSOServiceBadOperationError`: When we didn't fetch the job during this session. This operation should have\n                    fetched the job first.\n        \"\"\"\n        self._check_job_fetched(trial.job_id)\n        trial_number = await self.bs_repo.get_next_trial_number(trial.job_id)\n        self.bs_repo.create_trial(CreateTrial(**trial.dict(), trial_number=trial_number))\n        return ReadTrial(job_id=trial.job_id, batch_size=trial.batch_size, trial_number=trial_number)\n\n    def get_random_choices(self, choice: GetRandomChoices) -&gt; np.ndarray[Any, Any]:\n        \"\"\"Get randome choices based on job's seed.\n\n        If seed is not None (set by the user) we get the random choices from the generator that is stored in the database.\n        Otherwise, we get random choices based on random seed.\n\n        Args:\n            choice: Job id and list of choices\n\n        Returns:\n            reuslt random choices\n\n        Raises:\n            `ZeusBSOServiceBadOperationError`: When we didn't fetch the job during this session. This operation should have\n                    fetched the job first.\n        \"\"\"\n        arr = np.array(choice.choices)\n        rng, should_update = self._get_generator(choice.job_id)\n        res = rng.choice(arr, len(arr), replace=False)\n\n        if should_update:\n            # If we used the generator from database, should update the generator state after using it\n            self.job_repo.update_generator_state(\n                UpdateGeneratorState(job_id=choice.job_id, state=json.dumps(rng.__getstate__()))\n            )\n\n        return res\n\n    def get_normal(self, arg: GetNormal) -&gt; float:\n        \"\"\"Sample from normal distribution and update the generator state if seed was set.\n\n        Args:\n            arg: args for `numpy.random.normal`, which is loc(mean of distribution) and scale(stdev of distribution)\n\n        Returns:\n            Drawn sample.\n\n        Raises:\n            `ZeusBSOServiceBadOperationError`: When we didn't fetch the job during this session. This operation should have\n                    fetched the job first.\n        \"\"\"\n        rng, should_update = self._get_generator(arg.job_id)\n        res = rng.normal(arg.loc, arg.scale)\n\n        if should_update:\n            # If we used the generator from database, should update the generator state after using it\n            self.job_repo.update_generator_state(\n                UpdateGeneratorState(job_id=arg.job_id, state=json.dumps(rng.__getstate__()))\n            )\n\n        return res\n\n    async def get_job(self, job_id: str) -&gt; JobState | None:\n        \"\"\"Get job from database.\n\n        Args:\n            job_id: Job Id\n\n        Returns:\n            JobState if we found one, None if we couldn't find a job matching the job id.\n        \"\"\"\n        return await self.job_repo.get_job(job_id)\n\n    async def get_trial(self, trial: ReadTrial) -&gt; Trial | None:\n        \"\"\"Get a trial from database.\n\n        Args:\n            trial: (Job Id, batch size, trial_number) triplet.\n\n        Returns:\n            Trial if we found one, None if we couldn't find a job matching trial.\n        \"\"\"\n        return await self.bs_repo.get_trial(trial)\n\n    def create_job(self, new_job: CreateJob) -&gt; None:\n        \"\"\"Create a new job.\n\n        Args:\n            new_job: Configuration of a new job\n        \"\"\"\n        return self.job_repo.create_job(new_job)\n\n    async def get_trial_results_of_bs(self, bs: BatchSizeBase) -&gt; TrialResultsPerBs:\n        \"\"\"Load window size amount of results for a given batch size. If window size &lt;= 0, load all of them.\n\n        Args:\n            bs: (job_id, batch size) pair.\n\n        Returns:\n            list of windowed measurements in descending order for that (job_id, batch size)\n\n        Raises:\n            `ZeusBSOServiceBadOperationError`: When we didn't fetch the job during this session. This operation should have\n                    fetched the job first.\n        \"\"\"\n        job = self._get_job(bs.job_id)\n        return await self.bs_repo.get_trial_results_of_bs(\n            BatchSizeBase(job_id=bs.job_id, batch_size=bs.batch_size),\n            job.window_size,\n        )\n\n    def create_arms(self, new_arms: list[GaussianTsArmState]) -&gt; None:\n        \"\"\"Create GuassianTs arms for the job.\n\n        Args:\n            new_arms: List of new arm states\n\n        Raises:\n            `ZeusBSOServiceBadOperationError`: When we didn't fetch the job during this session. This operation should have\n                    fetched the job first.\n        \"\"\"\n        if len(new_arms) != 0:\n            self._check_job_fetched(new_arms[0].job_id)\n            self.bs_repo.create_arms(new_arms)\n\n    def update_job_stage(self, updated_stage: UpdateJobStage) -&gt; None:\n        \"\"\"Update the job stage (Pruning -&gt; MAB).\n\n        Args:\n            updated_stage: Updated stage.\n\n        Raises:\n            `ZeusBSOServiceBadOperationError`: When we didn't fetch the job during this session. This operation should have\n                    fetched the job first.\n        \"\"\"\n        self._check_job_fetched(updated_stage.job_id)\n        self.job_repo.update_stage(updated_stage)\n\n    async def delete_job(self, job_id: str) -&gt; bool:\n        \"\"\"Delete the job.\n\n        Args:\n            job_id: ID of the job.\n\n        Returns:\n            True if the job is deleted. False if none was deleted\n        \"\"\"\n        return await self.job_repo.delete_job(job_id)\n\n    def _get_generator(self, job_id: str) -&gt; tuple[np_Generator, bool]:\n        \"\"\"Get generator based on job_id. If mab_seed is not none, we should update the state after using generator.\n\n        Returns:\n            Tuple of [Generator, if we should update state]\n        \"\"\"\n        job_state = self._get_job(job_id)\n\n        rng = np.random.default_rng(int(datetime.now().timestamp()))\n\n        should_update = job_state.mab_seed is not None\n        if job_state.mab_seed is not None:\n            if job_state.mab_random_generator_state is None:\n                raise ZeusBSOValueError(\"Seed is set but generator state is none. Should be impossible\")\n\n            state = json.loads(job_state.mab_random_generator_state)\n            rng.__setstate__(state)\n\n        return (rng, should_update)\n\n    def _get_job(self, job_id: str) -&gt; JobState:\n        \"\"\"Get the job from the session. If we couldn't find the job, raise a `ZeusBSOServiceBadOperationError`.\"\"\"\n        res = self.job_repo.get_job_from_session(job_id)\n        if res is None:\n            raise ZeusBSOServiceBadOperationError(\n                f\"Should have fetched the job first or job does not exist(job_id = {job_id})\"\n            )\n        return res\n\n    def _get_trial(self, trial: ReadTrial) -&gt; Trial:\n        \"\"\"Get the job from the session. If we couldn't find the trial, raise a `ZeusBSOServiceBadOperationError`.\"\"\"\n        res = self.bs_repo.get_trial_from_session(trial)\n        if res is None:\n            raise ZeusBSOServiceBadOperationError(\n                f\"Should have fetched the trial first or trial does not exist(trial = {trial})\"\n            )\n        return res\n\n    def _check_job_fetched(self, job_id: str) -&gt; None:\n        \"\"\"Check if we fetched the job in the current session. If we didn't raise a `ZeusBSOServiceBadOperationError`.\"\"\"\n        if not self.job_repo.check_job_fetched(job_id):\n            raise ZeusBSOServiceBadOperationError(f\"check_job_fetched: {job_id} is not currently in the session\")\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.__init__","title":"__init__","text":"<pre><code>__init__(db_session)\n</code></pre> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>def __init__(self, db_session: AsyncSession):\n    \"\"\"Set up repositories to use to talk to database.\"\"\"\n    self.bs_repo = BatchSizeStateRepository(db_session)\n    self.job_repo = JobStateRepository(db_session)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.get_arms","title":"get_arms  <code>async</code>","text":"<pre><code>get_arms(job_id)\n</code></pre> <p>Get GaussianTs arm states for all arms(job_id, batch size).</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>Job id</p> required <p>Returns:</p> Type Description <code>list[GaussianTsArmState]</code> <p>list of arms</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>async def get_arms(self, job_id: str) -&gt; list[GaussianTsArmState]:\n    \"\"\"Get GaussianTs arm states for all arms(job_id, batch size).\n\n    Args:\n        job_id: Job id\n\n    Returns:\n        list of arms\n    \"\"\"\n    return await self.bs_repo.get_arms(job_id)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.get_arm","title":"get_arm  <code>async</code>","text":"<pre><code>get_arm(bs)\n</code></pre> <p>Get arm state for one arm.</p> <p>Parameters:</p> Name Type Description Default <code>bs</code> <code>BatchSizeBase</code> <p>(job_id, batch size) pair that represents one arm</p> required <p>Returns:</p> Type Description <code>GaussianTsArmState | None</code> <p>Result arm state or None if we cannot find that arm</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>async def get_arm(self, bs: BatchSizeBase) -&gt; GaussianTsArmState | None:\n    \"\"\"Get arm state for one arm.\n\n    Args:\n        bs: (job_id, batch size) pair that represents one arm\n\n    Returns:\n        Result arm state or None if we cannot find that arm\n    \"\"\"\n    return await self.bs_repo.get_arm(bs)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.get_explorations_of_job","title":"get_explorations_of_job  <code>async</code>","text":"<pre><code>get_explorations_of_job(job_id)\n</code></pre> <p>Get all explorations we have done for that job.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>Job id</p> required <p>Returns:</p> Type Description <code>ExplorationsPerJob</code> <p>list of explorations per each batch size</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>async def get_explorations_of_job(self, job_id: str) -&gt; ExplorationsPerJob:\n    \"\"\"Get all explorations we have done for that job.\n\n    Args:\n        job_id: Job id\n\n    Returns:\n        list of explorations per each batch size\n    \"\"\"\n    return await self.bs_repo.get_explorations_of_job(job_id)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.update_trial","title":"update_trial","text":"<pre><code>update_trial(updated_trial)\n</code></pre> <p>Update trial.</p> <p>(1) update the corresponding trial. (2) we update the min training cost observed so far if we have to.</p> <p>Parameters:</p> Name Type Description Default <code>updated_trial</code> <code>UpdateTrial</code> <p>Result of training that batch size</p> required <p>Raises:</p> Type Description <code>[`ZeusBSOServiceBadOperationError`][zeus.optimizer.batch_size.server.exceptions.ZeusBSOServiceBadOperationError]</code> <p>When we didn't fetch the job or trial during this session. This operation should have     fetched the job and trial first. Also, check if trial type is matching with fetched trial's type.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>def update_trial(self, updated_trial: UpdateTrial) -&gt; None:\n    \"\"\"Update trial.\n\n    (1) update the corresponding trial.\n    (2) we update the min training cost observed so far if we have to.\n\n    Args:\n        updated_trial: Result of training that batch size\n\n    Raises:\n        [`ZeusBSOServiceBadOperationError`][zeus.optimizer.batch_size.server.exceptions.ZeusBSOServiceBadOperationError]: When we didn't fetch the job or trial during this session. This operation should have\n                fetched the job and trial first. Also, check if trial type is matching with fetched trial's type.\n    \"\"\"\n    trial = self._get_trial(\n        ReadTrial(\n            job_id=updated_trial.job_id,\n            batch_size=updated_trial.batch_size,\n            trial_number=updated_trial.trial_number,\n        )\n    )\n    if trial.status != TrialStatus.Dispatched:\n        raise ZeusBSOServiceBadOperationError(\"Trial already has a result.\")\n\n    self.bs_repo.updated_current_trial(updated_trial)\n\n    # Update the corresponding batch size's min cost if needed.\n    if updated_trial.status != TrialStatus.Failed:\n        job = self._get_job(updated_trial.job_id)\n        if updated_trial.energy is None or updated_trial.time is None:\n            raise ZeusBSOValueError(\"Energy and time should be set if the trial is not failed.\")\n        cur_cost = zeus_cost(updated_trial.energy, updated_trial.time, job.eta_knob, job.max_power)\n        if job.min_cost is None or job.min_cost &gt; cur_cost:\n            self.job_repo.update_min(\n                UpdateJobMinCost(\n                    job_id=job.job_id,\n                    min_cost=cur_cost,\n                    min_cost_batch_size=updated_trial.batch_size,\n                )\n            )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.update_arm_state","title":"update_arm_state","text":"<pre><code>update_arm_state(arm)\n</code></pre> <p>Update arm state.</p> <p>Parameters:</p> Name Type Description Default <code>arm</code> <code>UpdateArm</code> <p>Updated arm state.</p> required <p>Raises:</p> Type Description <code>`ZeusBSOServiceBadOperationError`</code> <p>When we didn't fetch the job or trial during this session. This operation should have     fetched the job and trial first. Also, check if trial type is matching with fetched trial's type.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>def update_arm_state(\n    self,\n    arm: UpdateArm,\n) -&gt; None:\n    \"\"\"Update arm state.\n\n    Args:\n        arm: Updated arm state.\n\n    Raises:\n        `ZeusBSOServiceBadOperationError`: When we didn't fetch the job or trial during this session. This operation should have\n                fetched the job and trial first. Also, check if trial type is matching with fetched trial's type.\n    \"\"\"\n    self._check_job_fetched(arm.trial.job_id)\n    trial = self._get_trial(\n        ReadTrial(\n            job_id=arm.trial.job_id,\n            batch_size=arm.trial.batch_size,\n            trial_number=arm.trial.trial_number,\n        )\n    )\n    if trial.type != TrialType.MAB:\n        raise ZeusBSOServiceBadOperationError(\"Cannot update an arm since this trial is not issued from MAB stage.\")\n    self.bs_repo.update_arm_state(arm.updated_arm)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.update_exp_default_bs","title":"update_exp_default_bs","text":"<pre><code>update_exp_default_bs(updated_default_bs)\n</code></pre> <p>Update the default batch size for exploration.</p> <p>Parameters:</p> Name Type Description Default <code>updated_default_bs</code> <code>UpdateExpDefaultBs</code> <p>Job Id and new default batch size</p> required <p>Raises:</p> Type Description <code>`ZeusBSOServiceBadOperationError`</code> <p>When we didn't fetch the job during this session. This operation should have     fetched the job first.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>def update_exp_default_bs(self, updated_default_bs: UpdateExpDefaultBs) -&gt; None:\n    \"\"\"Update the default batch size for exploration.\n\n    Args:\n        updated_default_bs: Job Id and new default batch size\n\n    Raises:\n        `ZeusBSOServiceBadOperationError`: When we didn't fetch the job during this session. This operation should have\n                fetched the job first.\n    \"\"\"\n    self._check_job_fetched(updated_default_bs.job_id)\n    self.job_repo.update_exp_default_bs(updated_default_bs)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.create_trial","title":"create_trial  <code>async</code>","text":"<pre><code>create_trial(trial)\n</code></pre> <p>Create a new trial.</p> <p>Parameters:</p> Name Type Description Default <code>trial</code> <code>CreateExplorationTrial | CreateMabTrial | CreateConcurrentTrial</code> <p>New trial to create.</p> required <p>Raises:</p> Type Description <code>`ZeusBSOServiceBadOperationError`</code> <p>When we didn't fetch the job during this session. This operation should have     fetched the job first.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>async def create_trial(self, trial: CreateExplorationTrial | CreateMabTrial | CreateConcurrentTrial) -&gt; ReadTrial:\n    \"\"\"Create a new trial.\n\n    Args:\n        trial: New trial to create.\n\n    Raises:\n        `ZeusBSOServiceBadOperationError`: When we didn't fetch the job during this session. This operation should have\n                fetched the job first.\n    \"\"\"\n    self._check_job_fetched(trial.job_id)\n    trial_number = await self.bs_repo.get_next_trial_number(trial.job_id)\n    self.bs_repo.create_trial(CreateTrial(**trial.dict(), trial_number=trial_number))\n    return ReadTrial(job_id=trial.job_id, batch_size=trial.batch_size, trial_number=trial_number)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.get_random_choices","title":"get_random_choices","text":"<pre><code>get_random_choices(choice)\n</code></pre> <p>Get randome choices based on job's seed.</p> <p>If seed is not None (set by the user) we get the random choices from the generator that is stored in the database. Otherwise, we get random choices based on random seed.</p> <p>Parameters:</p> Name Type Description Default <code>choice</code> <code>GetRandomChoices</code> <p>Job id and list of choices</p> required <p>Returns:</p> Type Description <code>ndarray[Any, Any]</code> <p>reuslt random choices</p> <p>Raises:</p> Type Description <code>`ZeusBSOServiceBadOperationError`</code> <p>When we didn't fetch the job during this session. This operation should have     fetched the job first.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>def get_random_choices(self, choice: GetRandomChoices) -&gt; np.ndarray[Any, Any]:\n    \"\"\"Get randome choices based on job's seed.\n\n    If seed is not None (set by the user) we get the random choices from the generator that is stored in the database.\n    Otherwise, we get random choices based on random seed.\n\n    Args:\n        choice: Job id and list of choices\n\n    Returns:\n        reuslt random choices\n\n    Raises:\n        `ZeusBSOServiceBadOperationError`: When we didn't fetch the job during this session. This operation should have\n                fetched the job first.\n    \"\"\"\n    arr = np.array(choice.choices)\n    rng, should_update = self._get_generator(choice.job_id)\n    res = rng.choice(arr, len(arr), replace=False)\n\n    if should_update:\n        # If we used the generator from database, should update the generator state after using it\n        self.job_repo.update_generator_state(\n            UpdateGeneratorState(job_id=choice.job_id, state=json.dumps(rng.__getstate__()))\n        )\n\n    return res\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.get_normal","title":"get_normal","text":"<pre><code>get_normal(arg)\n</code></pre> <p>Sample from normal distribution and update the generator state if seed was set.</p> <p>Parameters:</p> Name Type Description Default <code>arg</code> <code>GetNormal</code> <p>args for <code>numpy.random.normal</code>, which is loc(mean of distribution) and scale(stdev of distribution)</p> required <p>Returns:</p> Type Description <code>float</code> <p>Drawn sample.</p> <p>Raises:</p> Type Description <code>`ZeusBSOServiceBadOperationError`</code> <p>When we didn't fetch the job during this session. This operation should have     fetched the job first.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>def get_normal(self, arg: GetNormal) -&gt; float:\n    \"\"\"Sample from normal distribution and update the generator state if seed was set.\n\n    Args:\n        arg: args for `numpy.random.normal`, which is loc(mean of distribution) and scale(stdev of distribution)\n\n    Returns:\n        Drawn sample.\n\n    Raises:\n        `ZeusBSOServiceBadOperationError`: When we didn't fetch the job during this session. This operation should have\n                fetched the job first.\n    \"\"\"\n    rng, should_update = self._get_generator(arg.job_id)\n    res = rng.normal(arg.loc, arg.scale)\n\n    if should_update:\n        # If we used the generator from database, should update the generator state after using it\n        self.job_repo.update_generator_state(\n            UpdateGeneratorState(job_id=arg.job_id, state=json.dumps(rng.__getstate__()))\n        )\n\n    return res\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.get_job","title":"get_job  <code>async</code>","text":"<pre><code>get_job(job_id)\n</code></pre> <p>Get job from database.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>Job Id</p> required <p>Returns:</p> Type Description <code>JobState | None</code> <p>JobState if we found one, None if we couldn't find a job matching the job id.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>async def get_job(self, job_id: str) -&gt; JobState | None:\n    \"\"\"Get job from database.\n\n    Args:\n        job_id: Job Id\n\n    Returns:\n        JobState if we found one, None if we couldn't find a job matching the job id.\n    \"\"\"\n    return await self.job_repo.get_job(job_id)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.get_trial","title":"get_trial  <code>async</code>","text":"<pre><code>get_trial(trial)\n</code></pre> <p>Get a trial from database.</p> <p>Parameters:</p> Name Type Description Default <code>trial</code> <code>ReadTrial</code> <p>(Job Id, batch size, trial_number) triplet.</p> required <p>Returns:</p> Type Description <code>Trial | None</code> <p>Trial if we found one, None if we couldn't find a job matching trial.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>async def get_trial(self, trial: ReadTrial) -&gt; Trial | None:\n    \"\"\"Get a trial from database.\n\n    Args:\n        trial: (Job Id, batch size, trial_number) triplet.\n\n    Returns:\n        Trial if we found one, None if we couldn't find a job matching trial.\n    \"\"\"\n    return await self.bs_repo.get_trial(trial)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.create_job","title":"create_job","text":"<pre><code>create_job(new_job)\n</code></pre> <p>Create a new job.</p> <p>Parameters:</p> Name Type Description Default <code>new_job</code> <code>CreateJob</code> <p>Configuration of a new job</p> required Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>def create_job(self, new_job: CreateJob) -&gt; None:\n    \"\"\"Create a new job.\n\n    Args:\n        new_job: Configuration of a new job\n    \"\"\"\n    return self.job_repo.create_job(new_job)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.get_trial_results_of_bs","title":"get_trial_results_of_bs  <code>async</code>","text":"<pre><code>get_trial_results_of_bs(bs)\n</code></pre> <p>Load window size amount of results for a given batch size. If window size &lt;= 0, load all of them.</p> <p>Parameters:</p> Name Type Description Default <code>bs</code> <code>BatchSizeBase</code> <p>(job_id, batch size) pair.</p> required <p>Returns:</p> Type Description <code>TrialResultsPerBs</code> <p>list of windowed measurements in descending order for that (job_id, batch size)</p> <p>Raises:</p> Type Description <code>`ZeusBSOServiceBadOperationError`</code> <p>When we didn't fetch the job during this session. This operation should have     fetched the job first.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>async def get_trial_results_of_bs(self, bs: BatchSizeBase) -&gt; TrialResultsPerBs:\n    \"\"\"Load window size amount of results for a given batch size. If window size &lt;= 0, load all of them.\n\n    Args:\n        bs: (job_id, batch size) pair.\n\n    Returns:\n        list of windowed measurements in descending order for that (job_id, batch size)\n\n    Raises:\n        `ZeusBSOServiceBadOperationError`: When we didn't fetch the job during this session. This operation should have\n                fetched the job first.\n    \"\"\"\n    job = self._get_job(bs.job_id)\n    return await self.bs_repo.get_trial_results_of_bs(\n        BatchSizeBase(job_id=bs.job_id, batch_size=bs.batch_size),\n        job.window_size,\n    )\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.create_arms","title":"create_arms","text":"<pre><code>create_arms(new_arms)\n</code></pre> <p>Create GuassianTs arms for the job.</p> <p>Parameters:</p> Name Type Description Default <code>new_arms</code> <code>list[GaussianTsArmState]</code> <p>List of new arm states</p> required <p>Raises:</p> Type Description <code>`ZeusBSOServiceBadOperationError`</code> <p>When we didn't fetch the job during this session. This operation should have     fetched the job first.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>def create_arms(self, new_arms: list[GaussianTsArmState]) -&gt; None:\n    \"\"\"Create GuassianTs arms for the job.\n\n    Args:\n        new_arms: List of new arm states\n\n    Raises:\n        `ZeusBSOServiceBadOperationError`: When we didn't fetch the job during this session. This operation should have\n                fetched the job first.\n    \"\"\"\n    if len(new_arms) != 0:\n        self._check_job_fetched(new_arms[0].job_id)\n        self.bs_repo.create_arms(new_arms)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.update_job_stage","title":"update_job_stage","text":"<pre><code>update_job_stage(updated_stage)\n</code></pre> <p>Update the job stage (Pruning -&gt; MAB).</p> <p>Parameters:</p> Name Type Description Default <code>updated_stage</code> <code>UpdateJobStage</code> <p>Updated stage.</p> required <p>Raises:</p> Type Description <code>`ZeusBSOServiceBadOperationError`</code> <p>When we didn't fetch the job during this session. This operation should have     fetched the job first.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>def update_job_stage(self, updated_stage: UpdateJobStage) -&gt; None:\n    \"\"\"Update the job stage (Pruning -&gt; MAB).\n\n    Args:\n        updated_stage: Updated stage.\n\n    Raises:\n        `ZeusBSOServiceBadOperationError`: When we didn't fetch the job during this session. This operation should have\n                fetched the job first.\n    \"\"\"\n    self._check_job_fetched(updated_stage.job_id)\n    self.job_repo.update_stage(updated_stage)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService.delete_job","title":"delete_job  <code>async</code>","text":"<pre><code>delete_job(job_id)\n</code></pre> <p>Delete the job.</p> <p>Parameters:</p> Name Type Description Default <code>job_id</code> <code>str</code> <p>ID of the job.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the job is deleted. False if none was deleted</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>async def delete_job(self, job_id: str) -&gt; bool:\n    \"\"\"Delete the job.\n\n    Args:\n        job_id: ID of the job.\n\n    Returns:\n        True if the job is deleted. False if none was deleted\n    \"\"\"\n    return await self.job_repo.delete_job(job_id)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService._get_generator","title":"_get_generator","text":"<pre><code>_get_generator(job_id)\n</code></pre> <p>Get generator based on job_id. If mab_seed is not none, we should update the state after using generator.</p> <p>Returns:</p> Type Description <code>tuple[Generator, bool]</code> <p>Tuple of [Generator, if we should update state]</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>def _get_generator(self, job_id: str) -&gt; tuple[np_Generator, bool]:\n    \"\"\"Get generator based on job_id. If mab_seed is not none, we should update the state after using generator.\n\n    Returns:\n        Tuple of [Generator, if we should update state]\n    \"\"\"\n    job_state = self._get_job(job_id)\n\n    rng = np.random.default_rng(int(datetime.now().timestamp()))\n\n    should_update = job_state.mab_seed is not None\n    if job_state.mab_seed is not None:\n        if job_state.mab_random_generator_state is None:\n            raise ZeusBSOValueError(\"Seed is set but generator state is none. Should be impossible\")\n\n        state = json.loads(job_state.mab_random_generator_state)\n        rng.__setstate__(state)\n\n    return (rng, should_update)\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService._get_job","title":"_get_job","text":"<pre><code>_get_job(job_id)\n</code></pre> <p>Get the job from the session. If we couldn't find the job, raise a <code>ZeusBSOServiceBadOperationError</code>.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>def _get_job(self, job_id: str) -&gt; JobState:\n    \"\"\"Get the job from the session. If we couldn't find the job, raise a `ZeusBSOServiceBadOperationError`.\"\"\"\n    res = self.job_repo.get_job_from_session(job_id)\n    if res is None:\n        raise ZeusBSOServiceBadOperationError(\n            f\"Should have fetched the job first or job does not exist(job_id = {job_id})\"\n        )\n    return res\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService._get_trial","title":"_get_trial","text":"<pre><code>_get_trial(trial)\n</code></pre> <p>Get the job from the session. If we couldn't find the trial, raise a <code>ZeusBSOServiceBadOperationError</code>.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>def _get_trial(self, trial: ReadTrial) -&gt; Trial:\n    \"\"\"Get the job from the session. If we couldn't find the trial, raise a `ZeusBSOServiceBadOperationError`.\"\"\"\n    res = self.bs_repo.get_trial_from_session(trial)\n    if res is None:\n        raise ZeusBSOServiceBadOperationError(\n            f\"Should have fetched the trial first or trial does not exist(trial = {trial})\"\n        )\n    return res\n</code></pre>"},{"location":"reference/optimizer/batch_size/server/services/service/#zeus.optimizer.batch_size.server.services.service.ZeusService._check_job_fetched","title":"_check_job_fetched","text":"<pre><code>_check_job_fetched(job_id)\n</code></pre> <p>Check if we fetched the job in the current session. If we didn't raise a <code>ZeusBSOServiceBadOperationError</code>.</p> Source code in <code>zeus/optimizer/batch_size/server/services/service.py</code> <pre><code>def _check_job_fetched(self, job_id: str) -&gt; None:\n    \"\"\"Check if we fetched the job in the current session. If we didn't raise a `ZeusBSOServiceBadOperationError`.\"\"\"\n    if not self.job_repo.check_job_fetched(job_id):\n        raise ZeusBSOServiceBadOperationError(f\"check_job_fetched: {job_id} is not currently in the session\")\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/","title":"pipeline_frequency","text":""},{"location":"reference/optimizer/pipeline_frequency/#zeus.optimizer.pipeline_frequency","title":"zeus.optimizer.pipeline_frequency","text":"<p>Optimize the energy consumption of large model training with Perseus.</p> <p>A a high-level, this optimizer assigns each forward and backward computation in a pipeline parallel training iteration with a GPU frequency that leads to a Pareto-optimal training iteration time and energy consumption.</p> <p>Currently, this optimizer depends on PyTorch.</p>"},{"location":"reference/optimizer/pipeline_frequency/common/","title":"common","text":""},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common","title":"zeus.optimizer.pipeline_frequency.common","text":"<p>Shared constants and models between the server and the client (optimizer).</p>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.PFOServerSettings","title":"PFOServerSettings","text":"<p>               Bases: <code>BaseSettings</code></p> <p>PFO server settings, configurable via environment variables.</p> <p>For instance, setting <code>ZEUS_PFO_LOG_LEVEL=INFO</code> will automatically set the <code>log_level</code> variable to <code>\"INFO\"</code>.</p> <p>Attributes:</p> Name Type Description <code>scheduler</code> <code>PyObject</code> <p>Name of the <code>FrequencyScheduler</code> to use.</p> <code>scheduler_args</code> <code>dict[str, Any]</code> <p>Any extra arguments required by <code>scheduler.__init__</code>.</p> <code>log_level</code> <code>str</code> <p>Log level, e.g. \"debug\", \"info\".</p> <code>dump_data</code> <code>bool</code> <p>Whether the scheduler should dump internal state to the filesystem (for future inspection purposes).</p> <code>dump_dir</code> <code>str</code> <p>Directory to dump state in (if enabled)</p> <code>max_job_idle_time</code> <code>int</code> <p>Maximum time in seconds that a job can be idle for before its states are automatically deleted from the server.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>class PFOServerSettings(BaseSettings):\n    \"\"\"PFO server settings, configurable via environment variables.\n\n    For instance, setting `ZEUS_PFO_LOG_LEVEL=INFO` will automatically set\n    the `log_level` variable to `\"INFO\"`.\n\n    Attributes:\n        scheduler: Name of the `FrequencyScheduler` to use.\n        scheduler_args: Any extra arguments required by `scheduler.__init__`.\n        log_level: Log level, e.g. \"debug\", \"info\".\n        dump_data: Whether the scheduler should dump internal state to the filesystem\n            (for future inspection purposes).\n        dump_dir: Directory to dump state in (if enabled)\n        max_job_idle_time: Maximum time in seconds that a job can be idle for before\n            its states are automatically deleted from the server.\n    \"\"\"\n\n    scheduler: PyObject = \"PointSolution\"  # type: ignore\n    scheduler_args: dict[str, Any] = {}\n    log_level: str = \"DEBUG\"\n    dump_data: bool = True\n    dump_dir: str = \"./dump\"\n    max_job_idle_time: int = 60 * 60 * 24 * 7  # 1 week\n\n    @validator(\"scheduler\", pre=True)\n    def _fix_scheduler_import_path(cls, value):\n        \"\"\"Prepend `zeus.optimizer.pipeline_frequency.server.scheduler.` to the scheduler type name.\"\"\"\n        return f\"zeus.optimizer.pipeline_frequency.server.scheduler.{value}\"\n\n    @validator(\"scheduler_args\")\n    def _validate_scheduler_args(cls, args, values):\n        \"\"\"Check whether args are as expected by the scheduler's constructor.\"\"\"\n        scheduler = values[\"scheduler\"]\n        full_args = args | dict(job_info=None, rank_infos=None, pfo_settings=None)\n        constructor_args = inspect.signature(scheduler)\n        try:\n            constructor_args.bind(**full_args)\n        except TypeError as e:\n            raise ValueError(f\"Invalid scheduler args: {e}\") from None\n        return args\n\n    @validator(\"log_level\")\n    def _make_upper_case(cls, value):\n        return value.upper()\n\n    class Config:\n        \"\"\"Configuration class read by pydantic.\"\"\"\n\n        env_prefix = \"zeus_pfo_\"\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.PFOServerSettings.Config","title":"Config","text":"<p>Configuration class read by pydantic.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>class Config:\n    \"\"\"Configuration class read by pydantic.\"\"\"\n\n    env_prefix = \"zeus_pfo_\"\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.PFOServerSettings._fix_scheduler_import_path","title":"_fix_scheduler_import_path","text":"<pre><code>_fix_scheduler_import_path(value)\n</code></pre> <p>Prepend <code>zeus.optimizer.pipeline_frequency.server.scheduler.</code> to the scheduler type name.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>@validator(\"scheduler\", pre=True)\ndef _fix_scheduler_import_path(cls, value):\n    \"\"\"Prepend `zeus.optimizer.pipeline_frequency.server.scheduler.` to the scheduler type name.\"\"\"\n    return f\"zeus.optimizer.pipeline_frequency.server.scheduler.{value}\"\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.PFOServerSettings._validate_scheduler_args","title":"_validate_scheduler_args","text":"<pre><code>_validate_scheduler_args(args, values)\n</code></pre> <p>Check whether args are as expected by the scheduler's constructor.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>@validator(\"scheduler_args\")\ndef _validate_scheduler_args(cls, args, values):\n    \"\"\"Check whether args are as expected by the scheduler's constructor.\"\"\"\n    scheduler = values[\"scheduler\"]\n    full_args = args | dict(job_info=None, rank_infos=None, pfo_settings=None)\n    constructor_args = inspect.signature(scheduler)\n    try:\n        constructor_args.bind(**full_args)\n    except TypeError as e:\n        raise ValueError(f\"Invalid scheduler args: {e}\") from None\n    return args\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.JobInfo","title":"JobInfo","text":"<p>               Bases: <code>BaseModel</code></p> <p>Training job information reported to the server.</p> <p>Attributes:</p> Name Type Description <code>job_id</code> <code>str</code> <p>Globally unique ID of the training job, generated by the server. This field should be an empty string when sent to the server.</p> <code>pp_degree</code> <code>int</code> <p>Pipeline parallel degree.</p> <code>dp_degree</code> <code>int</code> <p>Data parallel degree.</p> <code>tp_degree</code> <code>int</code> <p>Tensor parallel degree.</p> <code>world_size</code> <code>int</code> <p>World size of the training job.</p> <code>job_metadata</code> <code>Optional[str]</code> <p>An optional arbitrary string that describes the job. This will be appended to the job ID if given. Typically for logging purposes.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>class JobInfo(BaseModel):\n    \"\"\"Training job information reported to the server.\n\n    Attributes:\n        job_id: Globally unique ID of the training job, generated by the server.\n            This field should be an empty string when sent to the server.\n        pp_degree: Pipeline parallel degree.\n        dp_degree: Data parallel degree.\n        tp_degree: Tensor parallel degree.\n        world_size: World size of the training job.\n        job_metadata: An optional arbitrary string that describes the job. This will\n            be appended to the job ID if given. Typically for logging purposes.\n    \"\"\"\n\n    job_id: str = \"\"\n    pp_degree: int = Field(ge=1)\n    dp_degree: int = Field(ge=1)\n    tp_degree: int = Field(ge=1)\n    world_size: int = Field(ge=1)\n    job_metadata: Optional[str] = None\n\n    @validator(\"job_id\")\n    def _check_empty_job_id(cls, job_id):\n        assert not job_id\n        return job_id\n\n    @validator(\"world_size\")\n    def _check_world_size(cls, world_size, values):\n        \"\"\"Product of PP, DP, and TP degree would be identical to the world size.\"\"\"\n        assert values[\"pp_degree\"] * values[\"dp_degree\"] * values[\"tp_degree\"] == world_size\n        return world_size\n\n    def set_job_id(self, scheduler_name: str):\n        \"\"\"Generate and set the job ID.\"\"\"\n        self.job_id = \"+\".join(\n            [\n                datetime.now().strftime(\"%F-%H-%M-%S\"),\n                f\"dp{self.dp_degree}\",\n                f\"pp{self.pp_degree}\",\n                f\"tp{self.tp_degree}\",\n                scheduler_name,\n            ]\n        )\n        if self.job_metadata:\n            self.job_id += f\"+{self.job_metadata}\"\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.JobInfo._check_world_size","title":"_check_world_size","text":"<pre><code>_check_world_size(world_size, values)\n</code></pre> <p>Product of PP, DP, and TP degree would be identical to the world size.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>@validator(\"world_size\")\ndef _check_world_size(cls, world_size, values):\n    \"\"\"Product of PP, DP, and TP degree would be identical to the world size.\"\"\"\n    assert values[\"pp_degree\"] * values[\"dp_degree\"] * values[\"tp_degree\"] == world_size\n    return world_size\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.JobInfo.set_job_id","title":"set_job_id","text":"<pre><code>set_job_id(scheduler_name)\n</code></pre> <p>Generate and set the job ID.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>def set_job_id(self, scheduler_name: str):\n    \"\"\"Generate and set the job ID.\"\"\"\n    self.job_id = \"+\".join(\n        [\n            datetime.now().strftime(\"%F-%H-%M-%S\"),\n            f\"dp{self.dp_degree}\",\n            f\"pp{self.pp_degree}\",\n            f\"tp{self.tp_degree}\",\n            scheduler_name,\n        ]\n    )\n    if self.job_metadata:\n        self.job_id += f\"+{self.job_metadata}\"\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.RankInfo","title":"RankInfo","text":"<p>               Bases: <code>BaseModel</code></p> <p>Information passed to the server from each rank.</p> <p>Attributes:</p> Name Type Description <code>rank</code> <code>int</code> <p>Global rank of the reporting process.</p> <code>dp_rank</code> <code>int</code> <p>Data parallel rank of the reporting procees.</p> <code>pp_rank</code> <code>int</code> <p>Pipeline parallel rank of the reporting procees.</p> <code>tp_rank</code> <code>int</code> <p>Tensor parallel rank of the reporting procees.</p> <code>available_frequencies</code> <code>list[int]</code> <p>List of available frequencies for the rank's GPU.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>class RankInfo(BaseModel):\n    \"\"\"Information passed to the server from each rank.\n\n    Attributes:\n        rank: Global rank of the reporting process.\n        dp_rank: Data parallel rank of the reporting procees.\n        pp_rank: Pipeline parallel rank of the reporting procees.\n        tp_rank: Tensor parallel rank of the reporting procees.\n        available_frequencies: List of available frequencies for the rank's GPU.\n    \"\"\"\n\n    rank: int = Field(ge=0)\n    dp_rank: int = Field(ge=0)\n    pp_rank: int = Field(ge=0)\n    tp_rank: int = Field(ge=0)\n    available_frequencies: list[int]\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.FrequencySchedule","title":"FrequencySchedule","text":"<p>               Bases: <code>BaseModel</code></p> <p>Frequency schedule for one iteration.</p> <p><code>frequencies</code> is a list of tuples, where the first element is the name of the instruction and the second element is the frequency to use for that instruction.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>class FrequencySchedule(BaseModel):\n    \"\"\"Frequency schedule for one iteration.\n\n    `frequencies` is a list of tuples, where the first element is the name of the\n    instruction and the second element is the frequency to use for that instruction.\n    \"\"\"\n\n    rank: int = Field(ge=0)\n    frequencies: list[tuple[str, int]]\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.ProfilingResult","title":"ProfilingResult","text":"<p>               Bases: <code>BaseModel</code></p> <p>Profiling results for a <code>FrequencySchedule</code> of a rank.</p> <p>Attributes:</p> Name Type Description <code>rank</code> <code>int</code> <p>Global rank of the reporting client.</p> <code>iter_time</code> <code>list[float]</code> <p>List of latency of all iterations within the profiling window in seconds.</p> <code>iter_energy</code> <code>list[float]</code> <p>List of energy consumption of all iterations within the profiling window in Joules.</p> <code>time_breakdown</code> <code>dict[str, list[list[float]]]</code> <p>Duration of each operation across multiple iterations. e.g. <code>time_breakdown[\"forward\"][i]</code> is the list of latencies of all forward computations in the <code>i</code>th iteration.</p> <code>energy_breakdown</code> <code>dict[str, list[list[float]]]</code> <p>Energy consumption of each operation across multple iterations. Value has the same structure as <code>time_breakdown</code>.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>class ProfilingResult(BaseModel):\n    \"\"\"Profiling results for a `FrequencySchedule` of a rank.\n\n    Attributes:\n        rank: Global rank of the reporting client.\n        iter_time: List of latency of all iterations within the profiling window in seconds.\n        iter_energy: List of energy consumption of all iterations within the profiling window in Joules.\n        time_breakdown: Duration of each operation across multiple iterations.\n            e.g. `time_breakdown[\"forward\"][i]` is the list of latencies of all forward computations\n            in the `i`th iteration.\n        energy_breakdown: Energy consumption of each operation across multple iterations.\n            Value has the same structure as `time_breakdown`.\n    \"\"\"\n\n    rank: int = Field(ge=0)\n    iter_time: list[float]\n    iter_energy: list[float]\n    time_breakdown: dict[str, list[list[float]]] = {}\n    energy_breakdown: dict[str, list[list[float]]] = {}\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.OfflineProfilingResult","title":"OfflineProfilingResult","text":"<p>               Bases: <code>BaseModel</code></p> <p>Profiling results generated from offline profiling each instruction.</p> <p>Attributes:</p> Name Type Description <code>rank</code> <code>int</code> <p>Global rank of the reporting client.</p> <code>dp_rank</code> <code>int</code> <p>Data parallel rank of the reporting procees.</p> <code>pp_rank</code> <code>int</code> <p>Pipeline parallel rank of the reporting procees.</p> <code>tp_rank</code> <code>int</code> <p>Tensor parallel rank of the reporting procees.</p> <code>forward_time</code> <code>dict[int, float]</code> <p>Dict that maps frequency to average forward computation time.</p> <code>forward_energy</code> <code>dict[int, float]</code> <p>Dict that maps frequency to average forward computation energy.</p> <code>backward_time</code> <code>dict[int, float]</code> <p>Dict that maps frequency to average backward computation time.</p> <code>backward_energy</code> <code>dict[int, float]</code> <p>Dict that maps frequency to average backward computation energy.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>class OfflineProfilingResult(BaseModel):\n    \"\"\"Profiling results generated from offline profiling each instruction.\n\n    Attributes:\n        rank: Global rank of the reporting client.\n        dp_rank: Data parallel rank of the reporting procees.\n        pp_rank: Pipeline parallel rank of the reporting procees.\n        tp_rank: Tensor parallel rank of the reporting procees.\n        forward_time: Dict that maps frequency to average forward computation time.\n        forward_energy: Dict that maps frequency to average forward computation energy.\n        backward_time: Dict that maps frequency to average backward computation time.\n        backward_energy: Dict that maps frequency to average backward computation energy.\n    \"\"\"\n\n    rank: int = Field(ge=0)\n    dp_rank: int = Field(ge=0)\n    pp_rank: int = Field(ge=0)\n    tp_rank: int = Field(ge=0)\n    forward_time: dict[int, float]\n    forward_energy: dict[int, float]\n    backward_time: dict[int, float]\n    backward_energy: dict[int, float]\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.InstructionProfilingResult","title":"InstructionProfilingResult","text":"<p>               Bases: <code>BaseModel</code></p> <p>Time and energy profiling results for each instruction in each stage.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>class InstructionProfilingResult(BaseModel):\n    \"\"\"Time and energy profiling results for each instruction in each stage.\"\"\"\n\n    __root__: list[OfflineProfilingResult]\n\n    def to_csv(self, filepath: str) -&gt; None:\n        \"\"\"Serialize and save this object into a CSV file.\n\n        Columns: rank, dp_rank, pp_rank, tp_rank, stage, instruction, frequency, time, energy\n        Notes\n            - `rank` is the global rank of the process.\n            - `pp_rank` and `stage` are always the same, for backwards compatibility.\n            - All ranks and `stage` are zero-indexed.\n            - `instruction` is either \"forward\" or \"backward\".\n            - `time` and `energy` are already averaged over profiling iterations.\n        \"\"\"\n        if not filepath.endswith(\".csv\"):\n            raise ValueError(\"Filepath does not end with '.csv'\")\n\n        # fmt: off\n        headers = [\"rank\", \"dp_rank\", \"pp_rank\", \"tp_rank\", \"stage\", \"instruction\", \"frequency\", \"time\", \"energy\"]\n        records: list[tuple[int, int, int, int, int, str, int, float, float]] = []\n        for res in self.__root__:\n            prefix = (res.rank, res.dp_rank, res.pp_rank, res.tp_rank, res.pp_rank)\n            for freq in res.forward_time:\n                records.append((*prefix, \"forward\", freq, res.forward_time[freq], res.forward_energy[freq]))\n            for freq in res.backward_time:\n                records.append((*prefix, \"backward\", freq, res.backward_time[freq], res.backward_energy[freq]))\n        # fmt: on\n\n        df = pd.DataFrame.from_records(records, columns=headers)\n        df.to_csv(filepath, index=False)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.InstructionProfilingResult.to_csv","title":"to_csv","text":"<pre><code>to_csv(filepath)\n</code></pre> <p>Serialize and save this object into a CSV file.</p> <p>Columns: rank, dp_rank, pp_rank, tp_rank, stage, instruction, frequency, time, energy Notes     - <code>rank</code> is the global rank of the process.     - <code>pp_rank</code> and <code>stage</code> are always the same, for backwards compatibility.     - All ranks and <code>stage</code> are zero-indexed.     - <code>instruction</code> is either \"forward\" or \"backward\".     - <code>time</code> and <code>energy</code> are already averaged over profiling iterations.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>def to_csv(self, filepath: str) -&gt; None:\n    \"\"\"Serialize and save this object into a CSV file.\n\n    Columns: rank, dp_rank, pp_rank, tp_rank, stage, instruction, frequency, time, energy\n    Notes\n        - `rank` is the global rank of the process.\n        - `pp_rank` and `stage` are always the same, for backwards compatibility.\n        - All ranks and `stage` are zero-indexed.\n        - `instruction` is either \"forward\" or \"backward\".\n        - `time` and `energy` are already averaged over profiling iterations.\n    \"\"\"\n    if not filepath.endswith(\".csv\"):\n        raise ValueError(\"Filepath does not end with '.csv'\")\n\n    # fmt: off\n    headers = [\"rank\", \"dp_rank\", \"pp_rank\", \"tp_rank\", \"stage\", \"instruction\", \"frequency\", \"time\", \"energy\"]\n    records: list[tuple[int, int, int, int, int, str, int, float, float]] = []\n    for res in self.__root__:\n        prefix = (res.rank, res.dp_rank, res.pp_rank, res.tp_rank, res.pp_rank)\n        for freq in res.forward_time:\n            records.append((*prefix, \"forward\", freq, res.forward_time[freq], res.forward_energy[freq]))\n        for freq in res.backward_time:\n            records.append((*prefix, \"backward\", freq, res.backward_time[freq], res.backward_energy[freq]))\n    # fmt: on\n\n    df = pd.DataFrame.from_records(records, columns=headers)\n    df.to_csv(filepath, index=False)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.save_prof","title":"save_prof  <code>async</code>","text":"<pre><code>save_prof(data, directory, schedule_num)\n</code></pre> <p>Save a list of <code>ProfilingResult</code>s in the designated directory.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>async def save_prof(\n    data: list[ProfilingResult],\n    directory: str,\n    schedule_num: int,\n) -&gt; None:\n    \"\"\"Save a list of `ProfilingResult`s in the designated directory.\"\"\"\n    os.makedirs(directory, exist_ok=True)\n    async with aiofiles.open(f\"{directory}/{schedule_num}.prof.json\", \"w\") as f:\n        obj = _ProfilingResultList(__root__=data).json()\n        await f.write(obj)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.load_prof","title":"load_prof","text":"<pre><code>load_prof(directory, schedule_num)\n</code></pre> <p>Load a list of <code>ProfilingResult</code>s saved in the designated directory.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>def load_prof(directory: str, schedule_num: int) -&gt; list[ProfilingResult]:\n    \"\"\"Load a list of `ProfilingResult`s saved in the designated directory.\"\"\"\n    filepath = f\"{directory}/{schedule_num}.prof.json\"\n    return _ProfilingResultList.parse_file(filepath).__root__\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.save_sched","title":"save_sched  <code>async</code>","text":"<pre><code>save_sched(data, directory, schedule_num)\n</code></pre> <p>Save a list of <code>FrequencySchedule</code>s in the designated directory.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>async def save_sched(\n    data: list[FrequencySchedule],\n    directory: str,\n    schedule_num: int,\n) -&gt; None:\n    \"\"\"Save a list of `FrequencySchedule`s in the designated directory.\"\"\"\n    os.makedirs(directory, exist_ok=True)\n    async with aiofiles.open(f\"{directory}/{schedule_num}.sched.json\", \"w\") as f:\n        obj = _FrequencyScheduleList(__root__=data).json()\n        await f.write(obj)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.load_sched","title":"load_sched","text":"<pre><code>load_sched(directory, schedule_num)\n</code></pre> <p>Load a list of <code>FrequencySchedule</code>s saved in the designated directory.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>def load_sched(directory: str, schedule_num: int) -&gt; list[FrequencySchedule]:\n    \"\"\"Load a list of `FrequencySchedule`s saved in the designated directory.\"\"\"\n    filepath = f\"{directory}/{schedule_num}.sched.json\"\n    return _FrequencyScheduleList.parse_file(filepath).__root__\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.save_ranks","title":"save_ranks  <code>async</code>","text":"<pre><code>save_ranks(data, directory)\n</code></pre> <p>Save a list of <code>RankInfo</code>s in the designated directory.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>async def save_ranks(data: list[RankInfo], directory: str) -&gt; None:\n    \"\"\"Save a list of `RankInfo`s in the designated directory.\"\"\"\n    os.makedirs(directory, exist_ok=True)\n    async with aiofiles.open(f\"{directory}/ranks.json\", \"w\") as f:\n        obj = _RankInfoList(__root__=data).json()\n        await f.write(obj)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/common/#zeus.optimizer.pipeline_frequency.common.load_ranks","title":"load_ranks","text":"<pre><code>load_ranks(directory)\n</code></pre> <p>Load a list of <code>RankInfo</code>s saved in the designated directory.</p> Source code in <code>zeus/optimizer/pipeline_frequency/common.py</code> <pre><code>def load_ranks(directory: str) -&gt; list[RankInfo]:\n    \"\"\"Load a list of `RankInfo`s saved in the designated directory.\"\"\"\n    filepath = f\"{directory}/ranks.json\"\n    return _RankInfoList.parse_file(filepath).__root__\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/frequency_controller/","title":"frequency_controller","text":""},{"location":"reference/optimizer/pipeline_frequency/frequency_controller/#zeus.optimizer.pipeline_frequency.frequency_controller","title":"zeus.optimizer.pipeline_frequency.frequency_controller","text":"<p>Controller that sets the GPU's frequency in a non-blocking fashion.</p>"},{"location":"reference/optimizer/pipeline_frequency/frequency_controller/#zeus.optimizer.pipeline_frequency.frequency_controller.FrequencyController","title":"FrequencyController","text":"<p>Spawns a separate process that sets the GPU frequency.</p> Source code in <code>zeus/optimizer/pipeline_frequency/frequency_controller.py</code> <pre><code>class FrequencyController:\n    \"\"\"Spawns a separate process that sets the GPU frequency.\"\"\"\n\n    def __init__(self, device_id: int = 0) -&gt; None:\n        \"\"\"Instantiate the frequency controller.\n\n        Args:\n            device_id: Device ID of the GPU to control.\n        \"\"\"\n        self._q: mp.Queue[int | None] = mp.Queue()\n        self._proc = mp.Process(target=self._controller_process, args=(device_id,))\n\n        atexit.register(self.end)\n        self._proc.start()\n\n    def set_frequency(self, frequency: int) -&gt; None:\n        \"\"\"Set the GPU's frequency asynchronously.\n\n        If `frequency` is zero, returns without doing anything.\n        \"\"\"\n        if frequency != 0:\n            self._q.put(frequency, block=False)\n\n    def end(self) -&gt; None:\n        \"\"\"Stop the controller process.\"\"\"\n        self._q.put(None, block=False)\n\n    def _controller_process(self, device_id: int) -&gt; None:\n        \"\"\"Receive frequency values through a queue and apply it.\"\"\"\n        gpus = get_gpus()\n        # Return the power limit to the default.\n        gpus.reset_power_management_limit(device_id)\n\n        # Set the memory frequency to be the highest.\n        max_mem_freq = max(gpus.get_supported_memory_clocks(device_id))\n        with contextlib.suppress(ZeusGPUNotSupportedError):\n            gpus.set_memory_locked_clocks(device_id, max_mem_freq, max_mem_freq)\n\n        # Set the SM frequency to be the highest.\n        max_freq = max(gpus.get_supported_graphics_clocks(device_id, max_mem_freq))\n        gpus.set_gpu_locked_clocks(device_id, max_freq, max_freq)\n        current_freq = max_freq\n\n        # Wait on the queue for the next frequency to set.\n        while True:\n            target_freq = self._q.get(block=True)\n            if target_freq is None:\n                break\n            if current_freq != target_freq:\n                gpus.set_gpu_locked_clocks(device_id, target_freq, target_freq)\n                current_freq = target_freq\n\n        # Reset everything.\n        with contextlib.suppress(ZeusGPUNotSupportedError):\n            gpus.reset_memory_locked_clocks(device_id)\n        gpus.reset_gpu_locked_clocks(device_id)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/frequency_controller/#zeus.optimizer.pipeline_frequency.frequency_controller.FrequencyController.__init__","title":"__init__","text":"<pre><code>__init__(device_id=0)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>device_id</code> <code>int</code> <p>Device ID of the GPU to control.</p> <code>0</code> Source code in <code>zeus/optimizer/pipeline_frequency/frequency_controller.py</code> <pre><code>def __init__(self, device_id: int = 0) -&gt; None:\n    \"\"\"Instantiate the frequency controller.\n\n    Args:\n        device_id: Device ID of the GPU to control.\n    \"\"\"\n    self._q: mp.Queue[int | None] = mp.Queue()\n    self._proc = mp.Process(target=self._controller_process, args=(device_id,))\n\n    atexit.register(self.end)\n    self._proc.start()\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/frequency_controller/#zeus.optimizer.pipeline_frequency.frequency_controller.FrequencyController.set_frequency","title":"set_frequency","text":"<pre><code>set_frequency(frequency)\n</code></pre> <p>Set the GPU's frequency asynchronously.</p> <p>If <code>frequency</code> is zero, returns without doing anything.</p> Source code in <code>zeus/optimizer/pipeline_frequency/frequency_controller.py</code> <pre><code>def set_frequency(self, frequency: int) -&gt; None:\n    \"\"\"Set the GPU's frequency asynchronously.\n\n    If `frequency` is zero, returns without doing anything.\n    \"\"\"\n    if frequency != 0:\n        self._q.put(frequency, block=False)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/frequency_controller/#zeus.optimizer.pipeline_frequency.frequency_controller.FrequencyController.end","title":"end","text":"<pre><code>end()\n</code></pre> <p>Stop the controller process.</p> Source code in <code>zeus/optimizer/pipeline_frequency/frequency_controller.py</code> <pre><code>def end(self) -&gt; None:\n    \"\"\"Stop the controller process.\"\"\"\n    self._q.put(None, block=False)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/frequency_controller/#zeus.optimizer.pipeline_frequency.frequency_controller.FrequencyController._controller_process","title":"_controller_process","text":"<pre><code>_controller_process(device_id)\n</code></pre> <p>Receive frequency values through a queue and apply it.</p> Source code in <code>zeus/optimizer/pipeline_frequency/frequency_controller.py</code> <pre><code>def _controller_process(self, device_id: int) -&gt; None:\n    \"\"\"Receive frequency values through a queue and apply it.\"\"\"\n    gpus = get_gpus()\n    # Return the power limit to the default.\n    gpus.reset_power_management_limit(device_id)\n\n    # Set the memory frequency to be the highest.\n    max_mem_freq = max(gpus.get_supported_memory_clocks(device_id))\n    with contextlib.suppress(ZeusGPUNotSupportedError):\n        gpus.set_memory_locked_clocks(device_id, max_mem_freq, max_mem_freq)\n\n    # Set the SM frequency to be the highest.\n    max_freq = max(gpus.get_supported_graphics_clocks(device_id, max_mem_freq))\n    gpus.set_gpu_locked_clocks(device_id, max_freq, max_freq)\n    current_freq = max_freq\n\n    # Wait on the queue for the next frequency to set.\n    while True:\n        target_freq = self._q.get(block=True)\n        if target_freq is None:\n            break\n        if current_freq != target_freq:\n            gpus.set_gpu_locked_clocks(device_id, target_freq, target_freq)\n            current_freq = target_freq\n\n    # Reset everything.\n    with contextlib.suppress(ZeusGPUNotSupportedError):\n        gpus.reset_memory_locked_clocks(device_id)\n    gpus.reset_gpu_locked_clocks(device_id)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/optimizer/","title":"optimizer","text":""},{"location":"reference/optimizer/pipeline_frequency/optimizer/#zeus.optimizer.pipeline_frequency.optimizer","title":"zeus.optimizer.pipeline_frequency.optimizer","text":"<p>Pipeline frequency optimizer implementation.</p> <p>The <code>PipelineFrequencyOptimizer</code> is to be integrated into the training framework. It is responsible for communicating with the PFO server and managing the <code>FrequencyController</code> instance, which is responsible for controlling the frequency of the CPU of the current process.</p>"},{"location":"reference/optimizer/pipeline_frequency/optimizer/#zeus.optimizer.pipeline_frequency.optimizer.PipelineFrequencyOptimizer","title":"PipelineFrequencyOptimizer","text":"<p>               Bases: <code>Callback</code></p> <p>Pipeline frequency optimizer.</p> Source code in <code>zeus/optimizer/pipeline_frequency/optimizer.py</code> <pre><code>class PipelineFrequencyOptimizer(Callback):\n    \"\"\"Pipeline frequency optimizer.\"\"\"\n\n    def __init__(\n        self,\n        rank: int,\n        dp_rank: int,\n        pp_rank: int,\n        tp_rank: int,\n        device_id: int,\n        dp_degree: int,\n        pp_degree: int,\n        tp_degree: int,\n        world_size: int,\n        server_url: str,\n        job_metadata: str | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize the Pipeline frequency optimizer.\n\n        Assumptions:\n            - `torch.distributed` has been initialized.\n            - `torch.cuda.set_device` has been called with `device_id`.\n                This is needed to broadcast the job ID to all ranks.\n\n        The master process (rank 0) will register the job with the Peresus\n        server and retrieve the job ID of this job. Then, each rank will\n        report itself to the PFO server with the job ID.\n\n        Args:\n            rank: Global rank of the current process.\n            dp_rank: Rank in the data parallel group.\n            pp_rank: Rank in the pipeline parallel group.\n            tp_rank: Rank in the tensor parallel group.\n            device_id: CUDA device ID that the current process manages.\n            dp_degree: Size of the data parallel group.\n            pp_degree: Size of the pipeline parallel group.\n            tp_degree: Size of the tensor parallel group.\n            world_size: Total number of ranks that participate in training.\n            server_url: URL of the PFO server.\n            job_metadata: An optional arbitrary string that describes the job. This will\n                be appended to the job ID if given. Typically for logging purposes.\n        \"\"\"\n        if not dist.is_initialized():  # ty: ignore[possibly-missing-attribute]\n            raise RuntimeError(\"Instantiate `PipelineFrequencyOptimizer` after `init_process_group`.\")\n\n        self.server_url = server_url\n        self.rank = rank\n        self.dp_rank = dp_rank\n        self.pp_rank = pp_rank\n        self.tp_rank = tp_rank\n        self.device_id = device_id\n\n        gpus = get_gpus()\n        torch.cuda.set_device(device_id)\n\n        # Rank 0 registers the job with the PFO server and retrieves the job ID.\n        job_id = None\n        if rank == 0:\n            job_info = JobInfo(\n                pp_degree=pp_degree,\n                dp_degree=dp_degree,\n                tp_degree=tp_degree,\n                world_size=world_size,\n                job_metadata=job_metadata,\n            )\n            response = httpx.post(self.server_url + REGISTER_JOB_URL, json=job_info.dict())\n            if (code := response.status_code) != 200:\n                raise RuntimeError(f\"PFO server returned status code {code}: {response.text}\")\n            job_id = response.json()\n            if not isinstance(job_id, str):\n                raise RuntimeError(f\"PFO server returned a strange job ID: {job_id=}\")\n\n        # Rank 0 broadcasts the job ID across all ranks.\n        objects = [job_id]\n        dist.broadcast_object_list(objects, src=0)  # ty: ignore[possibly-missing-attribute]\n        self.job_id = objects[0]\n        if self.job_id is None:\n            raise RuntimeError(\"Failed to broadcast job ID to all ranks\")\n\n        # Query the list of available frequencies of the GPU.\n        max_mem_freq = max(gpus.get_supported_memory_clocks(device_id))\n        freqs = sorted(\n            gpus.get_supported_graphics_clocks(device_id, max_mem_freq),\n            reverse=True,\n        )\n\n        # Each rank reports itself to the PFO server with the job ID.\n        rank_info = RankInfo(\n            rank=self.rank,\n            dp_rank=self.dp_rank,\n            pp_rank=self.pp_rank,\n            tp_rank=self.tp_rank,\n            available_frequencies=freqs,\n        )\n        response = httpx.post(\n            self.server_url + REGISTER_RANK_URL.format(job_id=self.job_id),\n            json=rank_info.dict(),\n        )\n        if (code := response.status_code) != 200:\n            raise RuntimeError(f\"PFO server returned status code {code}: {response.text}\")\n\n        # The frequency controller is responsible for controlling the frequency\n        # of the GPU (device_id) asynchronously.\n        self.frequency_controller = FrequencyController(device_id=device_id)\n\n        # Fetch the frequency schedule from the PFO server.\n        self.freq_schedule = self._get_frequency_schedule()\n        self.freq_schedule_iter = iter(self.freq_schedule)\n\n    def _get_frequency_schedule(self) -&gt; list[tuple[str, int]]:\n        \"\"\"Get the frequency schedule from the PFO server.\"\"\"\n        response = httpx.get(\n            self.server_url + GET_FREQUENCY_SCHEDULE_URL.format(job_id=self.job_id),\n            params={\"rank\": self.rank},\n            timeout=None,\n        )\n        if (code := response.status_code) != 200:\n            raise RuntimeError(f\"PFO server returned status code {code}: {response.text}\")\n        schedule = FrequencySchedule.parse_raw(response.text)\n        if schedule.rank != self.rank:\n            raise RuntimeError(f\"PFO server returned a schedule for rank {schedule.rank} to rank {self.rank}\")\n        return schedule.frequencies\n\n    def on_step_begin(self) -&gt; None:\n        \"\"\"Mark the beginning of a step.\n\n        TODO(jaywonchung): InstructionProfiler iteration start mark.\n        \"\"\"\n        pass\n\n    def on_step_end(self) -&gt; None:\n        \"\"\"Mark the end of a step.\n\n        TODO(jaywonchung): InstructionProfiler iteration end mark.\n        Also report the profiling result to the PFO server after N iterations.\n        \"\"\"\n        # Frequency schedule holds one iteration-worth of frequencies, so at\n        # the end of each iteration, the iterator should be exhausted.\n        item = next(self.freq_schedule_iter, None)\n        if item is not None:\n            raise RuntimeError(\n                f\"PFO server returned more frequencies than expected. Next expected instruction and frequency is {item}\"\n            )\n        self.freq_schedule_iter = iter(self.freq_schedule)\n\n    def on_instruction_begin(self, name: str) -&gt; None:\n        \"\"\"Mark the beginning of an instruction, like forward and backward.\n\n        Retrieve the next frequency from the schedule, check whether the next\n        expected instruction matches the name of the instruction, and set the\n        frequency accordingly.\n        \"\"\"\n        sync_execution([self.device_id], sync_with=\"torch\")\n\n        # Retrieve the next frequency from the schedule.\n        item = next(self.freq_schedule_iter, None)\n        if item is None:\n            raise RuntimeError(\"PFO server returned fewer frequencies than expected\")\n\n        # Check whether the next expected instruction matches the name of the instruction.\n        instruction, frequency = item\n        if instruction != name:\n            raise RuntimeError(f\"The next expected instruction is not forward: {instruction}\")\n\n        self.frequency_controller.set_frequency(frequency)\n\n    def on_instruction_end(self, name: str) -&gt; None:\n        \"\"\"Mark the end of an instruction, like forward and backward.\"\"\"\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/optimizer/#zeus.optimizer.pipeline_frequency.optimizer.PipelineFrequencyOptimizer.__init__","title":"__init__","text":"<pre><code>__init__(rank, dp_rank, pp_rank, tp_rank, device_id, dp_degree, pp_degree, tp_degree, world_size, server_url, job_metadata=None)\n</code></pre> Assumptions <ul> <li><code>torch.distributed</code> has been initialized.</li> <li><code>torch.cuda.set_device</code> has been called with <code>device_id</code>.     This is needed to broadcast the job ID to all ranks.</li> </ul> <p>The master process (rank 0) will register the job with the Peresus server and retrieve the job ID of this job. Then, each rank will report itself to the PFO server with the job ID.</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <code>int</code> <p>Global rank of the current process.</p> required <code>dp_rank</code> <code>int</code> <p>Rank in the data parallel group.</p> required <code>pp_rank</code> <code>int</code> <p>Rank in the pipeline parallel group.</p> required <code>tp_rank</code> <code>int</code> <p>Rank in the tensor parallel group.</p> required <code>device_id</code> <code>int</code> <p>CUDA device ID that the current process manages.</p> required <code>dp_degree</code> <code>int</code> <p>Size of the data parallel group.</p> required <code>pp_degree</code> <code>int</code> <p>Size of the pipeline parallel group.</p> required <code>tp_degree</code> <code>int</code> <p>Size of the tensor parallel group.</p> required <code>world_size</code> <code>int</code> <p>Total number of ranks that participate in training.</p> required <code>server_url</code> <code>str</code> <p>URL of the PFO server.</p> required <code>job_metadata</code> <code>str | None</code> <p>An optional arbitrary string that describes the job. This will be appended to the job ID if given. Typically for logging purposes.</p> <code>None</code> Source code in <code>zeus/optimizer/pipeline_frequency/optimizer.py</code> <pre><code>def __init__(\n    self,\n    rank: int,\n    dp_rank: int,\n    pp_rank: int,\n    tp_rank: int,\n    device_id: int,\n    dp_degree: int,\n    pp_degree: int,\n    tp_degree: int,\n    world_size: int,\n    server_url: str,\n    job_metadata: str | None = None,\n) -&gt; None:\n    \"\"\"Initialize the Pipeline frequency optimizer.\n\n    Assumptions:\n        - `torch.distributed` has been initialized.\n        - `torch.cuda.set_device` has been called with `device_id`.\n            This is needed to broadcast the job ID to all ranks.\n\n    The master process (rank 0) will register the job with the Peresus\n    server and retrieve the job ID of this job. Then, each rank will\n    report itself to the PFO server with the job ID.\n\n    Args:\n        rank: Global rank of the current process.\n        dp_rank: Rank in the data parallel group.\n        pp_rank: Rank in the pipeline parallel group.\n        tp_rank: Rank in the tensor parallel group.\n        device_id: CUDA device ID that the current process manages.\n        dp_degree: Size of the data parallel group.\n        pp_degree: Size of the pipeline parallel group.\n        tp_degree: Size of the tensor parallel group.\n        world_size: Total number of ranks that participate in training.\n        server_url: URL of the PFO server.\n        job_metadata: An optional arbitrary string that describes the job. This will\n            be appended to the job ID if given. Typically for logging purposes.\n    \"\"\"\n    if not dist.is_initialized():  # ty: ignore[possibly-missing-attribute]\n        raise RuntimeError(\"Instantiate `PipelineFrequencyOptimizer` after `init_process_group`.\")\n\n    self.server_url = server_url\n    self.rank = rank\n    self.dp_rank = dp_rank\n    self.pp_rank = pp_rank\n    self.tp_rank = tp_rank\n    self.device_id = device_id\n\n    gpus = get_gpus()\n    torch.cuda.set_device(device_id)\n\n    # Rank 0 registers the job with the PFO server and retrieves the job ID.\n    job_id = None\n    if rank == 0:\n        job_info = JobInfo(\n            pp_degree=pp_degree,\n            dp_degree=dp_degree,\n            tp_degree=tp_degree,\n            world_size=world_size,\n            job_metadata=job_metadata,\n        )\n        response = httpx.post(self.server_url + REGISTER_JOB_URL, json=job_info.dict())\n        if (code := response.status_code) != 200:\n            raise RuntimeError(f\"PFO server returned status code {code}: {response.text}\")\n        job_id = response.json()\n        if not isinstance(job_id, str):\n            raise RuntimeError(f\"PFO server returned a strange job ID: {job_id=}\")\n\n    # Rank 0 broadcasts the job ID across all ranks.\n    objects = [job_id]\n    dist.broadcast_object_list(objects, src=0)  # ty: ignore[possibly-missing-attribute]\n    self.job_id = objects[0]\n    if self.job_id is None:\n        raise RuntimeError(\"Failed to broadcast job ID to all ranks\")\n\n    # Query the list of available frequencies of the GPU.\n    max_mem_freq = max(gpus.get_supported_memory_clocks(device_id))\n    freqs = sorted(\n        gpus.get_supported_graphics_clocks(device_id, max_mem_freq),\n        reverse=True,\n    )\n\n    # Each rank reports itself to the PFO server with the job ID.\n    rank_info = RankInfo(\n        rank=self.rank,\n        dp_rank=self.dp_rank,\n        pp_rank=self.pp_rank,\n        tp_rank=self.tp_rank,\n        available_frequencies=freqs,\n    )\n    response = httpx.post(\n        self.server_url + REGISTER_RANK_URL.format(job_id=self.job_id),\n        json=rank_info.dict(),\n    )\n    if (code := response.status_code) != 200:\n        raise RuntimeError(f\"PFO server returned status code {code}: {response.text}\")\n\n    # The frequency controller is responsible for controlling the frequency\n    # of the GPU (device_id) asynchronously.\n    self.frequency_controller = FrequencyController(device_id=device_id)\n\n    # Fetch the frequency schedule from the PFO server.\n    self.freq_schedule = self._get_frequency_schedule()\n    self.freq_schedule_iter = iter(self.freq_schedule)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/optimizer/#zeus.optimizer.pipeline_frequency.optimizer.PipelineFrequencyOptimizer._get_frequency_schedule","title":"_get_frequency_schedule","text":"<pre><code>_get_frequency_schedule()\n</code></pre> <p>Get the frequency schedule from the PFO server.</p> Source code in <code>zeus/optimizer/pipeline_frequency/optimizer.py</code> <pre><code>def _get_frequency_schedule(self) -&gt; list[tuple[str, int]]:\n    \"\"\"Get the frequency schedule from the PFO server.\"\"\"\n    response = httpx.get(\n        self.server_url + GET_FREQUENCY_SCHEDULE_URL.format(job_id=self.job_id),\n        params={\"rank\": self.rank},\n        timeout=None,\n    )\n    if (code := response.status_code) != 200:\n        raise RuntimeError(f\"PFO server returned status code {code}: {response.text}\")\n    schedule = FrequencySchedule.parse_raw(response.text)\n    if schedule.rank != self.rank:\n        raise RuntimeError(f\"PFO server returned a schedule for rank {schedule.rank} to rank {self.rank}\")\n    return schedule.frequencies\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/optimizer/#zeus.optimizer.pipeline_frequency.optimizer.PipelineFrequencyOptimizer.on_step_begin","title":"on_step_begin","text":"<pre><code>on_step_begin()\n</code></pre> <p>Mark the beginning of a step.</p> <p>TODO(jaywonchung): InstructionProfiler iteration start mark.</p> Source code in <code>zeus/optimizer/pipeline_frequency/optimizer.py</code> <pre><code>def on_step_begin(self) -&gt; None:\n    \"\"\"Mark the beginning of a step.\n\n    TODO(jaywonchung): InstructionProfiler iteration start mark.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/optimizer/#zeus.optimizer.pipeline_frequency.optimizer.PipelineFrequencyOptimizer.on_step_end","title":"on_step_end","text":"<pre><code>on_step_end()\n</code></pre> <p>Mark the end of a step.</p> <p>TODO(jaywonchung): InstructionProfiler iteration end mark. Also report the profiling result to the PFO server after N iterations.</p> Source code in <code>zeus/optimizer/pipeline_frequency/optimizer.py</code> <pre><code>def on_step_end(self) -&gt; None:\n    \"\"\"Mark the end of a step.\n\n    TODO(jaywonchung): InstructionProfiler iteration end mark.\n    Also report the profiling result to the PFO server after N iterations.\n    \"\"\"\n    # Frequency schedule holds one iteration-worth of frequencies, so at\n    # the end of each iteration, the iterator should be exhausted.\n    item = next(self.freq_schedule_iter, None)\n    if item is not None:\n        raise RuntimeError(\n            f\"PFO server returned more frequencies than expected. Next expected instruction and frequency is {item}\"\n        )\n    self.freq_schedule_iter = iter(self.freq_schedule)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/optimizer/#zeus.optimizer.pipeline_frequency.optimizer.PipelineFrequencyOptimizer.on_instruction_begin","title":"on_instruction_begin","text":"<pre><code>on_instruction_begin(name)\n</code></pre> <p>Mark the beginning of an instruction, like forward and backward.</p> <p>Retrieve the next frequency from the schedule, check whether the next expected instruction matches the name of the instruction, and set the frequency accordingly.</p> Source code in <code>zeus/optimizer/pipeline_frequency/optimizer.py</code> <pre><code>def on_instruction_begin(self, name: str) -&gt; None:\n    \"\"\"Mark the beginning of an instruction, like forward and backward.\n\n    Retrieve the next frequency from the schedule, check whether the next\n    expected instruction matches the name of the instruction, and set the\n    frequency accordingly.\n    \"\"\"\n    sync_execution([self.device_id], sync_with=\"torch\")\n\n    # Retrieve the next frequency from the schedule.\n    item = next(self.freq_schedule_iter, None)\n    if item is None:\n        raise RuntimeError(\"PFO server returned fewer frequencies than expected\")\n\n    # Check whether the next expected instruction matches the name of the instruction.\n    instruction, frequency = item\n    if instruction != name:\n        raise RuntimeError(f\"The next expected instruction is not forward: {instruction}\")\n\n    self.frequency_controller.set_frequency(frequency)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/optimizer/#zeus.optimizer.pipeline_frequency.optimizer.PipelineFrequencyOptimizer.on_instruction_end","title":"on_instruction_end","text":"<pre><code>on_instruction_end(name)\n</code></pre> <p>Mark the end of an instruction, like forward and backward.</p> Source code in <code>zeus/optimizer/pipeline_frequency/optimizer.py</code> <pre><code>def on_instruction_end(self, name: str) -&gt; None:\n    \"\"\"Mark the end of an instruction, like forward and backward.\"\"\"\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/","title":"server","text":""},{"location":"reference/optimizer/pipeline_frequency/server/#zeus.optimizer.pipeline_frequency.server","title":"zeus.optimizer.pipeline_frequency.server","text":"<p>The server guides the <code>PipelineFrequencyOptimizer</code> with frequency plans.</p> <p>The server is agnostic to the training framework the <code>PipelineFrequencyOptimizer</code> is integrated with. A server is useful because large model training is typically distributed, and we still need one place to coordinate the frequency plans. Later, the server will be extended to support complete online profiling and optimization.</p>"},{"location":"reference/optimizer/pipeline_frequency/server/job_manager/","title":"job_manager","text":""},{"location":"reference/optimizer/pipeline_frequency/server/job_manager/#zeus.optimizer.pipeline_frequency.server.job_manager","title":"zeus.optimizer.pipeline_frequency.server.job_manager","text":"<p>The JobManager singleton class manages all job states.</p>"},{"location":"reference/optimizer/pipeline_frequency/server/job_manager/#zeus.optimizer.pipeline_frequency.server.job_manager.JobManager","title":"JobManager","text":"<p>A singleton class that manages all states.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/job_manager.py</code> <pre><code>class JobManager:\n    \"\"\"A singleton class that manages all states.\"\"\"\n\n    def __init__(self, pfo_settings: PFOServerSettings) -&gt; None:\n        \"\"\"Initialize the job manager.\"\"\"\n        self.pfo_settings = pfo_settings\n\n        self._job_infos: dict[str, JobInfo] = {}\n        self._job_rank_infos: dict[str, list[RankInfo]] = {}\n        self._job_tasks: dict[str, asyncio.Task] = {}\n        self._job_result_channels: dict[str, asyncio.Queue[ProfilingResult]] = {}\n        self._job_sched_request_channels: dict[str, asyncio.Queue] = {}\n        self._job_sched_response_channels: dict[str, list[asyncio.Queue]] = {}\n        self._job_last_active_time: dict[str, float] = {}\n\n        # Spawn cleanup task that evicts the state of jobs that have not been active\n        # for a long time.\n        create_task(\n            self._cleanup_task(\n                cleanup_period=60,\n                max_idle_time=pfo_settings.max_job_idle_time,\n            ),\n            logger=logger,\n        )\n\n    def register_job(self, job_info: JobInfo) -&gt; None:\n        \"\"\"Prepare internal state for a new job.\n\n        This method will be invoked exactly once by the global rank 0 (master) process.\n        \"\"\"\n        job_id = job_info.job_id\n        world_size = job_info.world_size\n        self._job_infos[job_id] = job_info\n        self._job_rank_infos[job_id] = []\n        self._job_result_channels[job_id] = asyncio.Queue(maxsize=world_size)\n        self._job_sched_request_channels[job_id] = asyncio.Queue(maxsize=world_size)\n        self._job_sched_response_channels[job_id] = [asyncio.Queue(maxsize=1) for _ in range(world_size)]\n        self._job_tasks[job_id] = create_task(\n            self._job_task(job_id, self.pfo_settings.dump_data),\n            logger=logger,\n        )\n        self._job_last_active_time[job_id] = time.monotonic()\n\n    def register_rank(self, job_id: str, rank_info: RankInfo) -&gt; None:\n        \"\"\"Register rank-specific information for an already registered job.\n\n        This method will be invoked `world_size` number of times (once per rank).\n        \"\"\"\n        self._job_rank_infos[job_id].append(rank_info)\n        self._job_last_active_time[job_id] = time.monotonic()\n\n    async def get_frequency_schedule(self, job_id: str, rank: int) -&gt; FrequencySchedule:\n        \"\"\"Get the next frequency schedule for a rank.\n\n        This method will be called `world_size` number of times (once per rank).\n        All ranks will block on this method untill everyone reports their\n        profiling results and calls this method.\n\n        When an internal scheduler error happened at any point of servicing the\n        job, clients will be notified through this API with a 500 Internal Error.\n        \"\"\"\n        await self._job_sched_request_channels[job_id].put(rank)\n        res = await self._job_sched_response_channels[job_id][rank].get()\n        if isinstance(res, Exception):\n            code = 400 if isinstance(res, ValueError) else 500\n            raise HTTPException(\n                status_code=code,\n                detail=\"\".join(traceback.format_exception(type(res), res, res.__traceback__)),\n            )\n        self._job_last_active_time[job_id] = time.monotonic()\n        return res\n\n    def report_profiling_result(self, job_id: str, result: ProfilingResult) -&gt; None:\n        \"\"\"Send the profiling result to the job task and immediately return.\n\n        This method will be called `world_size` number of times - one for each rank.\n        \"\"\"\n        self._job_result_channels[job_id].put_nowait(result)\n        self._job_last_active_time[job_id] = time.monotonic()\n\n    async def _cleanup_task(\n        self,\n        cleanup_period: int,\n        max_idle_time: int,\n    ) -&gt; None:\n        \"\"\"Periodically evict job states.\n\n        Args:\n            cleanup_period: How often to run the cleanup task, in seconds.\n            max_idle_time: Maximum amount of time a job can be idle for, in seconds.\n        \"\"\"\n        while True:\n            await asyncio.sleep(cleanup_period)\n            for job_id in list(self._job_last_active_time.keys()):\n                if time.monotonic() - self._job_last_active_time[job_id] &gt; max_idle_time:\n                    self._job_tasks[job_id].cancel()\n                    del self._job_infos[job_id]\n                    del self._job_rank_infos[job_id]\n                    del self._job_result_channels[job_id]\n                    del self._job_sched_request_channels[job_id]\n                    del self._job_sched_response_channels[job_id]\n                    del self._job_tasks[job_id]\n                    del self._job_last_active_time[job_id]\n\n    async def _job_task(self, job_id: str, dump_data: bool) -&gt; None:\n        \"\"\"Coalese requests and responses of each rank and interface with the scheduler.\"\"\"\n        result_chan = self._job_result_channels[job_id]\n        sched_req_chan = self._job_sched_request_channels[job_id]\n        sched_resp_chan = self._job_sched_response_channels[job_id]\n\n        job_info = self._job_infos[job_id]\n\n        try:\n            # Wait until all ranks have reported their `RankInfo`s.\n            rank_infos = self._job_rank_infos[job_id]\n            while True:\n                await asyncio.sleep(0.1)\n                # Indexing the first element is always safe because this task is\n                # created after putting the `RankInfo` of the first-connected rank\n                # in `self.job_rank_infos[job_id]`.\n                if len(rank_infos) == job_info.world_size:\n                    break\n\n            # Sort `RankInfo`s in rank order.\n            rank_infos.sort(key=lambda r: r.rank)\n\n            # Create directory to dump PFO server states.\n            dump_dir = f\"{self.pfo_settings.dump_dir}/{job_id}\"\n            if dump_data:\n                await save_ranks(rank_infos, dump_dir)\n\n            # Instantiate the frequency scheduler.\n            scheduler = self.pfo_settings.scheduler(\n                job_info,\n                rank_infos,\n                self.pfo_settings,\n                **self.pfo_settings.scheduler_args,\n            )\n\n            # Provide next schedules, observe profiling results, and repeat.\n            schedule_num = 0\n            while True:\n                # Compute the next `FrequencySchedule`s.\n                schedules = scheduler.next_schedule()\n\n                # Wait until all the ranks ask for the next schedule.\n                await asyncio.gather(*[sched_req_chan.get() for _ in rank_infos])\n\n                # Send out `FrequencySchedule`s.\n                await asyncio.gather(*[sched_resp_chan[s.rank].put(s) for s in schedules])\n\n                # Gather profiling results from all ranks.\n                results = await asyncio.gather(*[result_chan.get() for _ in rank_infos])\n                results.sort(key=lambda r: r.rank)\n\n                # Dump profiling results and schedules.\n                if dump_data:\n                    schedules.sort(key=lambda s: s.rank)\n                    await save_prof(results, dump_dir, schedule_num)\n                    await save_sched(schedules, dump_dir, schedule_num)\n\n                # Send `ProfilingResult`s to the scheduler.\n                scheduler.observe(results)\n\n                # Increment schedule number.\n                schedule_num += 1\n\n        except asyncio.CancelledError:\n            # This task gets cancelled when it's idle for too long and evicted.\n            pass\n\n        except Exception as exc:\n            # In case the scheduler errored, send out the exception to the clients.\n            # The clients will receive the error when they ask for the next schedule.\n            for chan in sched_resp_chan:\n                chan.put_nowait(exc)\n            raise\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/job_manager/#zeus.optimizer.pipeline_frequency.server.job_manager.JobManager.__init__","title":"__init__","text":"<pre><code>__init__(pfo_settings)\n</code></pre> Source code in <code>zeus/optimizer/pipeline_frequency/server/job_manager.py</code> <pre><code>def __init__(self, pfo_settings: PFOServerSettings) -&gt; None:\n    \"\"\"Initialize the job manager.\"\"\"\n    self.pfo_settings = pfo_settings\n\n    self._job_infos: dict[str, JobInfo] = {}\n    self._job_rank_infos: dict[str, list[RankInfo]] = {}\n    self._job_tasks: dict[str, asyncio.Task] = {}\n    self._job_result_channels: dict[str, asyncio.Queue[ProfilingResult]] = {}\n    self._job_sched_request_channels: dict[str, asyncio.Queue] = {}\n    self._job_sched_response_channels: dict[str, list[asyncio.Queue]] = {}\n    self._job_last_active_time: dict[str, float] = {}\n\n    # Spawn cleanup task that evicts the state of jobs that have not been active\n    # for a long time.\n    create_task(\n        self._cleanup_task(\n            cleanup_period=60,\n            max_idle_time=pfo_settings.max_job_idle_time,\n        ),\n        logger=logger,\n    )\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/job_manager/#zeus.optimizer.pipeline_frequency.server.job_manager.JobManager.register_job","title":"register_job","text":"<pre><code>register_job(job_info)\n</code></pre> <p>Prepare internal state for a new job.</p> <p>This method will be invoked exactly once by the global rank 0 (master) process.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/job_manager.py</code> <pre><code>def register_job(self, job_info: JobInfo) -&gt; None:\n    \"\"\"Prepare internal state for a new job.\n\n    This method will be invoked exactly once by the global rank 0 (master) process.\n    \"\"\"\n    job_id = job_info.job_id\n    world_size = job_info.world_size\n    self._job_infos[job_id] = job_info\n    self._job_rank_infos[job_id] = []\n    self._job_result_channels[job_id] = asyncio.Queue(maxsize=world_size)\n    self._job_sched_request_channels[job_id] = asyncio.Queue(maxsize=world_size)\n    self._job_sched_response_channels[job_id] = [asyncio.Queue(maxsize=1) for _ in range(world_size)]\n    self._job_tasks[job_id] = create_task(\n        self._job_task(job_id, self.pfo_settings.dump_data),\n        logger=logger,\n    )\n    self._job_last_active_time[job_id] = time.monotonic()\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/job_manager/#zeus.optimizer.pipeline_frequency.server.job_manager.JobManager.register_rank","title":"register_rank","text":"<pre><code>register_rank(job_id, rank_info)\n</code></pre> <p>Register rank-specific information for an already registered job.</p> <p>This method will be invoked <code>world_size</code> number of times (once per rank).</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/job_manager.py</code> <pre><code>def register_rank(self, job_id: str, rank_info: RankInfo) -&gt; None:\n    \"\"\"Register rank-specific information for an already registered job.\n\n    This method will be invoked `world_size` number of times (once per rank).\n    \"\"\"\n    self._job_rank_infos[job_id].append(rank_info)\n    self._job_last_active_time[job_id] = time.monotonic()\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/job_manager/#zeus.optimizer.pipeline_frequency.server.job_manager.JobManager.get_frequency_schedule","title":"get_frequency_schedule  <code>async</code>","text":"<pre><code>get_frequency_schedule(job_id, rank)\n</code></pre> <p>Get the next frequency schedule for a rank.</p> <p>This method will be called <code>world_size</code> number of times (once per rank). All ranks will block on this method untill everyone reports their profiling results and calls this method.</p> <p>When an internal scheduler error happened at any point of servicing the job, clients will be notified through this API with a 500 Internal Error.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/job_manager.py</code> <pre><code>async def get_frequency_schedule(self, job_id: str, rank: int) -&gt; FrequencySchedule:\n    \"\"\"Get the next frequency schedule for a rank.\n\n    This method will be called `world_size` number of times (once per rank).\n    All ranks will block on this method untill everyone reports their\n    profiling results and calls this method.\n\n    When an internal scheduler error happened at any point of servicing the\n    job, clients will be notified through this API with a 500 Internal Error.\n    \"\"\"\n    await self._job_sched_request_channels[job_id].put(rank)\n    res = await self._job_sched_response_channels[job_id][rank].get()\n    if isinstance(res, Exception):\n        code = 400 if isinstance(res, ValueError) else 500\n        raise HTTPException(\n            status_code=code,\n            detail=\"\".join(traceback.format_exception(type(res), res, res.__traceback__)),\n        )\n    self._job_last_active_time[job_id] = time.monotonic()\n    return res\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/job_manager/#zeus.optimizer.pipeline_frequency.server.job_manager.JobManager.report_profiling_result","title":"report_profiling_result","text":"<pre><code>report_profiling_result(job_id, result)\n</code></pre> <p>Send the profiling result to the job task and immediately return.</p> <p>This method will be called <code>world_size</code> number of times - one for each rank.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/job_manager.py</code> <pre><code>def report_profiling_result(self, job_id: str, result: ProfilingResult) -&gt; None:\n    \"\"\"Send the profiling result to the job task and immediately return.\n\n    This method will be called `world_size` number of times - one for each rank.\n    \"\"\"\n    self._job_result_channels[job_id].put_nowait(result)\n    self._job_last_active_time[job_id] = time.monotonic()\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/job_manager/#zeus.optimizer.pipeline_frequency.server.job_manager.JobManager._cleanup_task","title":"_cleanup_task  <code>async</code>","text":"<pre><code>_cleanup_task(cleanup_period, max_idle_time)\n</code></pre> <p>Periodically evict job states.</p> <p>Parameters:</p> Name Type Description Default <code>cleanup_period</code> <code>int</code> <p>How often to run the cleanup task, in seconds.</p> required <code>max_idle_time</code> <code>int</code> <p>Maximum amount of time a job can be idle for, in seconds.</p> required Source code in <code>zeus/optimizer/pipeline_frequency/server/job_manager.py</code> <pre><code>async def _cleanup_task(\n    self,\n    cleanup_period: int,\n    max_idle_time: int,\n) -&gt; None:\n    \"\"\"Periodically evict job states.\n\n    Args:\n        cleanup_period: How often to run the cleanup task, in seconds.\n        max_idle_time: Maximum amount of time a job can be idle for, in seconds.\n    \"\"\"\n    while True:\n        await asyncio.sleep(cleanup_period)\n        for job_id in list(self._job_last_active_time.keys()):\n            if time.monotonic() - self._job_last_active_time[job_id] &gt; max_idle_time:\n                self._job_tasks[job_id].cancel()\n                del self._job_infos[job_id]\n                del self._job_rank_infos[job_id]\n                del self._job_result_channels[job_id]\n                del self._job_sched_request_channels[job_id]\n                del self._job_sched_response_channels[job_id]\n                del self._job_tasks[job_id]\n                del self._job_last_active_time[job_id]\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/job_manager/#zeus.optimizer.pipeline_frequency.server.job_manager.JobManager._job_task","title":"_job_task  <code>async</code>","text":"<pre><code>_job_task(job_id, dump_data)\n</code></pre> <p>Coalese requests and responses of each rank and interface with the scheduler.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/job_manager.py</code> <pre><code>async def _job_task(self, job_id: str, dump_data: bool) -&gt; None:\n    \"\"\"Coalese requests and responses of each rank and interface with the scheduler.\"\"\"\n    result_chan = self._job_result_channels[job_id]\n    sched_req_chan = self._job_sched_request_channels[job_id]\n    sched_resp_chan = self._job_sched_response_channels[job_id]\n\n    job_info = self._job_infos[job_id]\n\n    try:\n        # Wait until all ranks have reported their `RankInfo`s.\n        rank_infos = self._job_rank_infos[job_id]\n        while True:\n            await asyncio.sleep(0.1)\n            # Indexing the first element is always safe because this task is\n            # created after putting the `RankInfo` of the first-connected rank\n            # in `self.job_rank_infos[job_id]`.\n            if len(rank_infos) == job_info.world_size:\n                break\n\n        # Sort `RankInfo`s in rank order.\n        rank_infos.sort(key=lambda r: r.rank)\n\n        # Create directory to dump PFO server states.\n        dump_dir = f\"{self.pfo_settings.dump_dir}/{job_id}\"\n        if dump_data:\n            await save_ranks(rank_infos, dump_dir)\n\n        # Instantiate the frequency scheduler.\n        scheduler = self.pfo_settings.scheduler(\n            job_info,\n            rank_infos,\n            self.pfo_settings,\n            **self.pfo_settings.scheduler_args,\n        )\n\n        # Provide next schedules, observe profiling results, and repeat.\n        schedule_num = 0\n        while True:\n            # Compute the next `FrequencySchedule`s.\n            schedules = scheduler.next_schedule()\n\n            # Wait until all the ranks ask for the next schedule.\n            await asyncio.gather(*[sched_req_chan.get() for _ in rank_infos])\n\n            # Send out `FrequencySchedule`s.\n            await asyncio.gather(*[sched_resp_chan[s.rank].put(s) for s in schedules])\n\n            # Gather profiling results from all ranks.\n            results = await asyncio.gather(*[result_chan.get() for _ in rank_infos])\n            results.sort(key=lambda r: r.rank)\n\n            # Dump profiling results and schedules.\n            if dump_data:\n                schedules.sort(key=lambda s: s.rank)\n                await save_prof(results, dump_dir, schedule_num)\n                await save_sched(schedules, dump_dir, schedule_num)\n\n            # Send `ProfilingResult`s to the scheduler.\n            scheduler.observe(results)\n\n            # Increment schedule number.\n            schedule_num += 1\n\n    except asyncio.CancelledError:\n        # This task gets cancelled when it's idle for too long and evicted.\n        pass\n\n    except Exception as exc:\n        # In case the scheduler errored, send out the exception to the clients.\n        # The clients will receive the error when they ask for the next schedule.\n        for chan in sched_resp_chan:\n            chan.put_nowait(exc)\n        raise\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/job_manager/#zeus.optimizer.pipeline_frequency.server.job_manager.init_global_job_manager","title":"init_global_job_manager","text":"<pre><code>init_global_job_manager(pfo_settings)\n</code></pre> <p>Instantiate the global singleton <code>JobManager</code>.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/job_manager.py</code> <pre><code>def init_global_job_manager(pfo_settings: PFOServerSettings) -&gt; None:\n    \"\"\"Instantiate the global singleton `JobManager`.\"\"\"\n    global GLOBAL_JOB_MANAGER\n    GLOBAL_JOB_MANAGER = JobManager(pfo_settings=pfo_settings)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/job_manager/#zeus.optimizer.pipeline_frequency.server.job_manager.get_global_job_manager","title":"get_global_job_manager","text":"<pre><code>get_global_job_manager()\n</code></pre> <p>Fetch the global singleton <code>JobManager</code>.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/job_manager.py</code> <pre><code>def get_global_job_manager() -&gt; JobManager:\n    \"\"\"Fetch the global singleton `JobManager`.\"\"\"\n    assert GLOBAL_JOB_MANAGER is not None, \"`init_global_job_manager` was not called.\"\n    return GLOBAL_JOB_MANAGER\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/router/","title":"router","text":""},{"location":"reference/optimizer/pipeline_frequency/server/router/#zeus.optimizer.pipeline_frequency.server.router","title":"zeus.optimizer.pipeline_frequency.server.router","text":"<p>Pipeline frequency optimizer server FastAPI router.</p>"},{"location":"reference/optimizer/pipeline_frequency/server/router/#zeus.optimizer.pipeline_frequency.server.router.LoggingRoute","title":"LoggingRoute","text":"<p>               Bases: <code>APIRoute</code></p> <p>Route handler that logs out all requests and responses in DEBUG level.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/router.py</code> <pre><code>class LoggingRoute(APIRoute):\n    \"\"\"Route handler that logs out all requests and responses in DEBUG level.\"\"\"\n\n    def get_route_handler(self) -&gt; Callable:\n        \"\"\"Wrap the original handler with debug messages.\"\"\"\n        original_route_handler = super().get_route_handler()\n\n        async def custom_route_handler(request: Request) -&gt; Response:\n            response: Response = await original_route_handler(request)\n            logger.debug(\n                \"%s %s: %s -&gt; %s\",\n                request.method,\n                request.url,\n                await request.json() if await request.body() else \"None\",\n                bytes(response.body).decode(response.charset),\n            )\n            return response\n\n        return custom_route_handler\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/router/#zeus.optimizer.pipeline_frequency.server.router.LoggingRoute.get_route_handler","title":"get_route_handler","text":"<pre><code>get_route_handler()\n</code></pre> <p>Wrap the original handler with debug messages.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/router.py</code> <pre><code>def get_route_handler(self) -&gt; Callable:\n    \"\"\"Wrap the original handler with debug messages.\"\"\"\n    original_route_handler = super().get_route_handler()\n\n    async def custom_route_handler(request: Request) -&gt; Response:\n        response: Response = await original_route_handler(request)\n        logger.debug(\n            \"%s %s: %s -&gt; %s\",\n            request.method,\n            request.url,\n            await request.json() if await request.body() else \"None\",\n            bytes(response.body).decode(response.charset),\n        )\n        return response\n\n    return custom_route_handler\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/router/#zeus.optimizer.pipeline_frequency.server.router.startup_hook","title":"startup_hook  <code>async</code>","text":"<pre><code>startup_hook()\n</code></pre> <p>Startup hook.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/router.py</code> <pre><code>@app.on_event(\"startup\")  # ty: ignore[deprecated]\nasync def startup_hook():\n    \"\"\"Startup hook.\"\"\"\n    logger.info(\"Using scheduler `%s`\", settings.scheduler.__name__)  # ty: ignore[unresolved-attribute]\n    init_global_job_manager(settings)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/router/#zeus.optimizer.pipeline_frequency.server.router.register_job","title":"register_job  <code>async</code>","text":"<pre><code>register_job(job_info, job_manager=Depends(get_global_job_manager))\n</code></pre> <p>Register the training job's information in the server.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/router.py</code> <pre><code>@app.post(REGISTER_JOB_URL, response_model=str)\nasync def register_job(job_info: JobInfo, job_manager: JobManager = Depends(get_global_job_manager)) -&gt; str:\n    \"\"\"Register the training job's information in the server.\"\"\"\n    job_info.set_job_id(scheduler_name=settings.scheduler.__name__)  # ty: ignore[unresolved-attribute]\n    job_manager.register_job(job_info)\n    return job_info.job_id\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/router/#zeus.optimizer.pipeline_frequency.server.router.register_rank","title":"register_rank  <code>async</code>","text":"<pre><code>register_rank(job_id, rank_info, job_manager=Depends(get_global_job_manager))\n</code></pre> <p>Register each rank's information in the server.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/router.py</code> <pre><code>@app.post(REGISTER_RANK_URL)\nasync def register_rank(\n    job_id: str,\n    rank_info: RankInfo,\n    job_manager: JobManager = Depends(get_global_job_manager),\n) -&gt; None:\n    \"\"\"Register each rank's information in the server.\"\"\"\n    job_manager.register_rank(job_id, rank_info)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/router/#zeus.optimizer.pipeline_frequency.server.router.get_frequency_schedule","title":"get_frequency_schedule  <code>async</code>","text":"<pre><code>get_frequency_schedule(job_id, rank, job_manager=Depends(get_global_job_manager))\n</code></pre> <p>Return the next frequency schedule for the rank.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/router.py</code> <pre><code>@app.get(GET_FREQUENCY_SCHEDULE_URL, response_model=FrequencySchedule)\nasync def get_frequency_schedule(\n    job_id: str,\n    rank: int,\n    job_manager: JobManager = Depends(get_global_job_manager),\n) -&gt; FrequencySchedule:\n    \"\"\"Return the next frequency schedule for the rank.\"\"\"\n    return await job_manager.get_frequency_schedule(job_id, rank)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/router/#zeus.optimizer.pipeline_frequency.server.router.report_profiling_result","title":"report_profiling_result  <code>async</code>","text":"<pre><code>report_profiling_result(job_id, profiling_result, job_manager=Depends(get_global_job_manager))\n</code></pre> <p>Report the profiling result for the most recent frequency schedule.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/router.py</code> <pre><code>@app.post(REPORT_PROFILING_RESULT_URL)\nasync def report_profiling_result(\n    job_id: str,\n    profiling_result: ProfilingResult,\n    job_manager: JobManager = Depends(get_global_job_manager),\n) -&gt; None:\n    \"\"\"Report the profiling result for the most recent frequency schedule.\"\"\"\n    job_manager.report_profiling_result(job_id, profiling_result)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/scheduler/","title":"scheduler","text":""},{"location":"reference/optimizer/pipeline_frequency/server/scheduler/#zeus.optimizer.pipeline_frequency.server.scheduler","title":"zeus.optimizer.pipeline_frequency.server.scheduler","text":"<p>Interfaces for defining frequency schedulers.</p>"},{"location":"reference/optimizer/pipeline_frequency/server/scheduler/#zeus.optimizer.pipeline_frequency.server.scheduler.FrequencyScheduler","title":"FrequencyScheduler","text":"<p>               Bases: <code>ABC</code></p> <p>Interface for classes that enclose frequency scheduling policies.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/scheduler.py</code> <pre><code>class FrequencyScheduler(ABC):\n    \"\"\"Interface for classes that enclose frequency scheduling policies.\"\"\"\n\n    def __init__(\n        self,\n        job_info: JobInfo,\n        rank_infos: list[RankInfo],\n        pfo_settings: PFOServerSettings,\n    ) -&gt; None:\n        \"\"\"Initialize the scheduler.\n\n        Args:\n            job_info: Info about the training job.\n            rank_infos: Info about all ranks. May not be sorted in rank order.\n            pfo_settings: PFOServerSettings object.\n        \"\"\"\n        self.job_info = job_info\n        self.rank_infos = sorted(rank_infos, key=lambda info: info.rank)\n        self.world_size = self.job_info.world_size\n        self.pfo_settings = pfo_settings\n\n        self._generator = self._run()\n        self._next_schedule: list[FrequencySchedule] | None = None\n\n    def observe(self, profiling_results: list[ProfilingResult]) -&gt; None:\n        \"\"\"Ingest the profiling results for the previous schedule.\n\n        Args:\n            profiling_results: Doesn't have to be sorted in rank order.\n        \"\"\"\n        # When there are no more schedules left to yield, the generator will\n        # raise `StopIteration`. We just ignore this, and later invocations of\n        # `next_schedule()` will return the last schedule returned forever.\n        with suppress(StopIteration):\n            self._next_schedule = self._generator.send(profiling_results)\n\n    def next_schedule(self) -&gt; list[FrequencySchedule]:\n        \"\"\"Return the schedules for the next round of iterations.\n\n        Returns:\n            A list of `FrequencySchedule`s. May not be sorted in rank order.\n        \"\"\"\n        if self._next_schedule is None:\n            try:\n                self._next_schedule = next(self._generator)\n            except StopIteration as exc:\n                raise RuntimeError(\n                    \"The _run generator raised StopIteration on its first next call.\",\n                ) from exc\n        return self._next_schedule\n\n    @abstractmethod\n    def _run(self) -&gt; Generator[list[FrequencySchedule], list[ProfilingResult], None]:\n        \"\"\"Yield next schedules and receives profiling results in one place.\n\n        This is an alternative way to write a frequency scheduler. The advantage is\n        that everything is enclosed inside this method. The downside is that you'll\n        have to read this and understand how this generator works.\n\n        The following implementation is a simple example of writing a scheduler using\n        this class. `yield` the next frequency schedule, and receive the profiling\n        results corresponding to that schedule from the `yield`. `observe` and\n        `next_schedule` will run the generator for you.\n\n        In general, this generator should be designed to `yield` schedules infinitely.\n        However, if this was written to write a finite number of next schedules and\n        raise `StopIteration`, the last schedule cached inside `self._next_schedule`\n        will infinitely be returned from the call to `next_schedule`. This can be\n        useful when you converge to the optimal schedule and stop the generator, and\n        the rest of training will run with the final optimal schedule indefinitely.\n        \"\"\"\n        # This is an example implementation.\n        while True:\n            # Generate the next frequency schedule\n            next_schedule: list[FrequencySchedule] = []\n            # Send the next schedule to client and receive the profiling result from client\n            profiling_results = yield next_schedule\n            # Ingest the profiling result\n            logger.debug(\"%s\", profiling_results)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/scheduler/#zeus.optimizer.pipeline_frequency.server.scheduler.FrequencyScheduler.__init__","title":"__init__","text":"<pre><code>__init__(job_info, rank_infos, pfo_settings)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>job_info</code> <code>JobInfo</code> <p>Info about the training job.</p> required <code>rank_infos</code> <code>list[RankInfo]</code> <p>Info about all ranks. May not be sorted in rank order.</p> required <code>pfo_settings</code> <code>PFOServerSettings</code> <p>PFOServerSettings object.</p> required Source code in <code>zeus/optimizer/pipeline_frequency/server/scheduler.py</code> <pre><code>def __init__(\n    self,\n    job_info: JobInfo,\n    rank_infos: list[RankInfo],\n    pfo_settings: PFOServerSettings,\n) -&gt; None:\n    \"\"\"Initialize the scheduler.\n\n    Args:\n        job_info: Info about the training job.\n        rank_infos: Info about all ranks. May not be sorted in rank order.\n        pfo_settings: PFOServerSettings object.\n    \"\"\"\n    self.job_info = job_info\n    self.rank_infos = sorted(rank_infos, key=lambda info: info.rank)\n    self.world_size = self.job_info.world_size\n    self.pfo_settings = pfo_settings\n\n    self._generator = self._run()\n    self._next_schedule: list[FrequencySchedule] | None = None\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/scheduler/#zeus.optimizer.pipeline_frequency.server.scheduler.FrequencyScheduler.observe","title":"observe","text":"<pre><code>observe(profiling_results)\n</code></pre> <p>Ingest the profiling results for the previous schedule.</p> <p>Parameters:</p> Name Type Description Default <code>profiling_results</code> <code>list[ProfilingResult]</code> <p>Doesn't have to be sorted in rank order.</p> required Source code in <code>zeus/optimizer/pipeline_frequency/server/scheduler.py</code> <pre><code>def observe(self, profiling_results: list[ProfilingResult]) -&gt; None:\n    \"\"\"Ingest the profiling results for the previous schedule.\n\n    Args:\n        profiling_results: Doesn't have to be sorted in rank order.\n    \"\"\"\n    # When there are no more schedules left to yield, the generator will\n    # raise `StopIteration`. We just ignore this, and later invocations of\n    # `next_schedule()` will return the last schedule returned forever.\n    with suppress(StopIteration):\n        self._next_schedule = self._generator.send(profiling_results)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/scheduler/#zeus.optimizer.pipeline_frequency.server.scheduler.FrequencyScheduler.next_schedule","title":"next_schedule","text":"<pre><code>next_schedule()\n</code></pre> <p>Return the schedules for the next round of iterations.</p> <p>Returns:</p> Type Description <code>list[FrequencySchedule]</code> <p>A list of <code>FrequencySchedule</code>s. May not be sorted in rank order.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/scheduler.py</code> <pre><code>def next_schedule(self) -&gt; list[FrequencySchedule]:\n    \"\"\"Return the schedules for the next round of iterations.\n\n    Returns:\n        A list of `FrequencySchedule`s. May not be sorted in rank order.\n    \"\"\"\n    if self._next_schedule is None:\n        try:\n            self._next_schedule = next(self._generator)\n        except StopIteration as exc:\n            raise RuntimeError(\n                \"The _run generator raised StopIteration on its first next call.\",\n            ) from exc\n    return self._next_schedule\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/scheduler/#zeus.optimizer.pipeline_frequency.server.scheduler.FrequencyScheduler._run","title":"_run  <code>abstractmethod</code>","text":"<pre><code>_run()\n</code></pre> <p>Yield next schedules and receives profiling results in one place.</p> <p>This is an alternative way to write a frequency scheduler. The advantage is that everything is enclosed inside this method. The downside is that you'll have to read this and understand how this generator works.</p> <p>The following implementation is a simple example of writing a scheduler using this class. <code>yield</code> the next frequency schedule, and receive the profiling results corresponding to that schedule from the <code>yield</code>. <code>observe</code> and <code>next_schedule</code> will run the generator for you.</p> <p>In general, this generator should be designed to <code>yield</code> schedules infinitely. However, if this was written to write a finite number of next schedules and raise <code>StopIteration</code>, the last schedule cached inside <code>self._next_schedule</code> will infinitely be returned from the call to <code>next_schedule</code>. This can be useful when you converge to the optimal schedule and stop the generator, and the rest of training will run with the final optimal schedule indefinitely.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/scheduler.py</code> <pre><code>@abstractmethod\ndef _run(self) -&gt; Generator[list[FrequencySchedule], list[ProfilingResult], None]:\n    \"\"\"Yield next schedules and receives profiling results in one place.\n\n    This is an alternative way to write a frequency scheduler. The advantage is\n    that everything is enclosed inside this method. The downside is that you'll\n    have to read this and understand how this generator works.\n\n    The following implementation is a simple example of writing a scheduler using\n    this class. `yield` the next frequency schedule, and receive the profiling\n    results corresponding to that schedule from the `yield`. `observe` and\n    `next_schedule` will run the generator for you.\n\n    In general, this generator should be designed to `yield` schedules infinitely.\n    However, if this was written to write a finite number of next schedules and\n    raise `StopIteration`, the last schedule cached inside `self._next_schedule`\n    will infinitely be returned from the call to `next_schedule`. This can be\n    useful when you converge to the optimal schedule and stop the generator, and\n    the rest of training will run with the final optimal schedule indefinitely.\n    \"\"\"\n    # This is an example implementation.\n    while True:\n        # Generate the next frequency schedule\n        next_schedule: list[FrequencySchedule] = []\n        # Send the next schedule to client and receive the profiling result from client\n        profiling_results = yield next_schedule\n        # Ingest the profiling result\n        logger.debug(\"%s\", profiling_results)\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/scheduler/#zeus.optimizer.pipeline_frequency.server.scheduler.PointSolution","title":"PointSolution","text":"<p>               Bases: <code>FrequencyScheduler</code></p> <p>Runs the given frequency schedule.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/scheduler.py</code> <pre><code>class PointSolution(FrequencyScheduler):\n    \"\"\"Runs the given frequency schedule.\"\"\"\n\n    def __init__(\n        self,\n        job_info: JobInfo,\n        rank_infos: list[RankInfo],\n        pfo_settings: PFOServerSettings,\n        solution_path: str,\n    ) -&gt; None:\n        \"\"\"Initialize the scheduler.\n\n        Args:\n            job_info: Info about the training job.\n            rank_infos: Info about all ranks. May not be sorted in rank order.\n            pfo_settings: PFOServerSettings object.\n            solution_path: Path to the frequency Python file generated by lowtime.\n        \"\"\"\n        super().__init__(job_info, rank_infos, pfo_settings)\n\n        self.solution_path = Path(solution_path)\n        if not self.solution_path.is_file():\n            raise RuntimeError(f\"Solution file not found: {solution_path}\")\n        if self.solution_path.suffix != \".py\":\n            raise RuntimeError(f\"Solution file is not a Python file: {solution_path}\")\n\n        with open(self.solution_path, encoding=\"utf-8\") as f:\n            schedule: list[list[tuple[str, int]]] = eval(f.read())\n            if len(schedule) != self.world_size:\n                raise RuntimeError(\n                    f\"Solution file assumes {len(schedule)} ranks, but the job has {self.world_size} ranks.\"\n                )\n\n            self.schedule = []\n            for rank, freqs in enumerate(schedule):\n                self.schedule.append(FrequencySchedule(rank=rank, frequencies=freqs))\n\n    def _run(self) -&gt; Generator[list[FrequencySchedule], list[ProfilingResult], None]:\n        \"\"\"Yield the schedule given by the solution path.\"\"\"\n        yield self.schedule\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/scheduler/#zeus.optimizer.pipeline_frequency.server.scheduler.PointSolution.__init__","title":"__init__","text":"<pre><code>__init__(job_info, rank_infos, pfo_settings, solution_path)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>job_info</code> <code>JobInfo</code> <p>Info about the training job.</p> required <code>rank_infos</code> <code>list[RankInfo]</code> <p>Info about all ranks. May not be sorted in rank order.</p> required <code>pfo_settings</code> <code>PFOServerSettings</code> <p>PFOServerSettings object.</p> required <code>solution_path</code> <code>str</code> <p>Path to the frequency Python file generated by lowtime.</p> required Source code in <code>zeus/optimizer/pipeline_frequency/server/scheduler.py</code> <pre><code>def __init__(\n    self,\n    job_info: JobInfo,\n    rank_infos: list[RankInfo],\n    pfo_settings: PFOServerSettings,\n    solution_path: str,\n) -&gt; None:\n    \"\"\"Initialize the scheduler.\n\n    Args:\n        job_info: Info about the training job.\n        rank_infos: Info about all ranks. May not be sorted in rank order.\n        pfo_settings: PFOServerSettings object.\n        solution_path: Path to the frequency Python file generated by lowtime.\n    \"\"\"\n    super().__init__(job_info, rank_infos, pfo_settings)\n\n    self.solution_path = Path(solution_path)\n    if not self.solution_path.is_file():\n        raise RuntimeError(f\"Solution file not found: {solution_path}\")\n    if self.solution_path.suffix != \".py\":\n        raise RuntimeError(f\"Solution file is not a Python file: {solution_path}\")\n\n    with open(self.solution_path, encoding=\"utf-8\") as f:\n        schedule: list[list[tuple[str, int]]] = eval(f.read())\n        if len(schedule) != self.world_size:\n            raise RuntimeError(\n                f\"Solution file assumes {len(schedule)} ranks, but the job has {self.world_size} ranks.\"\n            )\n\n        self.schedule = []\n        for rank, freqs in enumerate(schedule):\n            self.schedule.append(FrequencySchedule(rank=rank, frequencies=freqs))\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/scheduler/#zeus.optimizer.pipeline_frequency.server.scheduler.PointSolution._run","title":"_run","text":"<pre><code>_run()\n</code></pre> <p>Yield the schedule given by the solution path.</p> Source code in <code>zeus/optimizer/pipeline_frequency/server/scheduler.py</code> <pre><code>def _run(self) -&gt; Generator[list[FrequencySchedule], list[ProfilingResult], None]:\n    \"\"\"Yield the schedule given by the solution path.\"\"\"\n    yield self.schedule\n</code></pre>"},{"location":"reference/optimizer/pipeline_frequency/server/scheduler/#zeus.optimizer.pipeline_frequency.server.scheduler.make_3d_parallel","title":"make_3d_parallel","text":"<pre><code>make_3d_parallel(sched_cls, name=None)\n</code></pre> <p>Wrap <code>sched_cls</code> so that it is aware of 3D parallelism.</p> <p>Internally, this function subclasses <code>sched_cls</code> and overrides <code>observe</code> and <code>next_schedule</code>. <code>observe</code> will aggregate the profiling results from all ranks that share the same pp_rank and feed it to <code>super().observe</code>, while <code>next_schedule</code> will first retrieve the per-stage schedule from <code>super().next_schedule</code> and then copy-paste it to all ranks that share the same pp_rank. With this, the wrapped scheduler can operate under the illusion that it's only deadling with pure pipeline parallelism.</p> <p>Parameters:</p> Name Type Description Default <code>sched_cls</code> <code>Type[FrequencyScheduler]</code> <p>The scheduler class to wrap.</p> required <code>name</code> <code>str | None</code> <p>Name of the scheduler. If None, use <code>sched_cls.__name__ + \"3D\"</code>.</p> <code>None</code> Source code in <code>zeus/optimizer/pipeline_frequency/server/scheduler.py</code> <pre><code>def make_3d_parallel(sched_cls: Type[FrequencyScheduler], name: str | None = None) -&gt; Type[FrequencyScheduler]:\n    \"\"\"Wrap `sched_cls` so that it is aware of 3D parallelism.\n\n    Internally, this function subclasses `sched_cls` and overrides `observe` and\n    `next_schedule`. `observe` will aggregate the profiling results from all ranks\n    that share the same pp_rank and feed it to `super().observe`, while `next_schedule`\n    will first retrieve the per-stage schedule from `super().next_schedule` and then\n    copy-paste it to all ranks that share the same pp_rank. With this, the wrapped\n    scheduler can operate under the illusion that it's only deadling with pure pipeline\n    parallelism.\n\n    Args:\n        sched_cls: The scheduler class to wrap.\n        name: Name of the scheduler. If None, use `sched_cls.__name__ + \"3D\"`.\n    \"\"\"\n\n    class Wrapper(sched_cls):  # type: ignore[valid-type,misc]\n        def __init__(\n            self,\n            job_info: JobInfo,\n            rank_infos: list[RankInfo],\n            pfo_settings: PFOServerSettings,\n            *args,\n            **kwargs,\n        ) -&gt; None:\n            self._orig_job_info = job_info\n            self._orig_rank_infos = rank_infos\n\n            # Give the wrapped scheduler a perfect illusion of pure pipeline parallelism\n            # and no data or tensor parallelism. New rank is given by pp_rank.\n            job_info = copy.deepcopy(job_info)\n            job_info.dp_degree = 1\n            job_info.tp_degree = 1\n            job_info.world_size = job_info.pp_degree\n\n            new_rank_infos = []\n            for rank_info in rank_infos:\n                if rank_info.dp_rank == 0 and rank_info.tp_rank == 0:\n                    new_rank_info = copy.deepcopy(rank_info)\n                    new_rank_info.rank = rank_info.pp_rank\n                    new_rank_infos.append(new_rank_info)\n\n            super().__init__(job_info, rank_infos, pfo_settings, *args, **kwargs)\n\n        def observe(self, profiling_results: list[ProfilingResult]) -&gt; None:\n            \"\"\"Aggregate results so that each pipeline stage has one result.\"\"\"\n            # Aggregate results from ranks that share the same pp_rank.\n            rank_to_pp_rank = {rank_info.rank: rank_info.pp_rank for rank_info in self._orig_rank_infos}\n            pp_results: list[list[ProfilingResult]] = [[] for _ in range(self._orig_job_info.pp_degree)]\n            for result in profiling_results:\n                pp_results[rank_to_pp_rank[result.rank]].append(result)\n\n            # For each stage, construct a new ProfilingResult that aggregates all ranks.\n            # For iter_time and values in time_breakdown, take the max.\n            # For iter_energy and values in energy_breakdown, take the sum.\n            def agg_list(values: Sequence[list[float]], fun: Callable) -&gt; list[float]:\n                return [fun(vals) for vals in zip(*values)]\n\n            def agg_list_of_list(values: Sequence[list[list[float]]], fun: Callable) -&gt; list[list[float]]:\n                return [agg_list(vals, fun) for vals in zip(*values)]\n\n            agg_results = []\n            for pp_rank, results in enumerate(pp_results):\n                agg_result = ProfilingResult(\n                    rank=pp_rank,\n                    iter_time=agg_list([result.iter_time for result in results], max),\n                    iter_energy=agg_list([result.iter_energy for result in results], sum),\n                    time_breakdown={\n                        key: agg_list_of_list([result.time_breakdown[key] for result in results], max)\n                        for key in results[0].time_breakdown\n                    },\n                    energy_breakdown={\n                        key: agg_list_of_list([result.energy_breakdown[key] for result in results], sum)\n                        for key in results[0].energy_breakdown\n                    },\n                )\n                agg_results.append(agg_result)\n                logger.debug(\n                    \"Aggregated rank %s results for pp_rank %d: %s\",\n                    \", \".join([str(r.rank) for r in results]),\n                    pp_rank,\n                    agg_result,\n                )\n\n            # Finally, let the wrapped scheduler observe the aggregated results.\n            super().observe(agg_results)\n\n        def next_schedule(self) -&gt; list[FrequencySchedule]:\n            \"\"\"Copy and paste the schedule for each stage to all ranks in that stage.\"\"\"\n            # Retrive the next schedule for each stage.\n            schedules = super().next_schedule()\n\n            # Copy and paste the schedule for each stage to all ranks in that stage.\n            rank_to_pp_rank = {rank_info.rank: rank_info.pp_rank for rank_info in self._orig_rank_infos}\n            next_schedule = []\n            for rank in range(self._orig_job_info.world_size):\n                pp_rank = rank_to_pp_rank[rank]\n                sched = copy.deepcopy(schedules[pp_rank])\n                sched.rank = rank\n                next_schedule.append(sched)\n                logger.debug(\n                    \"Copied schedule for pp_rank %d to rank %d: %s\",\n                    pp_rank,\n                    rank,\n                    sched,\n                )\n            return next_schedule\n\n    Wrapper.__name__ = name or (sched_cls.__name__ + \"3D\")\n    if sched_cls.__doc__ is not None:\n        Wrapper.__doc__ = \"[Wrapped for 3D parallelism] \" + sched_cls.__doc__\n\n    return Wrapper\n</code></pre>"},{"location":"reference/utils/","title":"utils","text":""},{"location":"reference/utils/#zeus.utils","title":"zeus.utils","text":"<p>Utility functions and classes.</p>"},{"location":"reference/utils/async_utils/","title":"async_utils","text":""},{"location":"reference/utils/async_utils/#zeus.utils.async_utils","title":"zeus.utils.async_utils","text":"<p>Utilities for asyncio.</p>"},{"location":"reference/utils/async_utils/#zeus.utils.async_utils.create_task","title":"create_task","text":"<pre><code>create_task(coroutine, logger=None)\n</code></pre> <p>Create an <code>asyncio.Task</code> but ensure that exceptions are logged.</p> <p>Reference: https://quantlane.com/blog/ensure-asyncio-task-exceptions-get-logged/</p> <p>Parameters:</p> Name Type Description Default <code>coroutine</code> <code>Coroutine[Any, Any, T]</code> <p>The coroutine to be wrapped.</p> required <code>logger</code> <code>Logger | None</code> <p>The logger to be used for logging exceptions. If <code>None</code>, the the logger with the name <code>zeus.utils.async_utils</code> is used.</p> <code>None</code> Source code in <code>zeus/utils/async_utils.py</code> <pre><code>def create_task(\n    coroutine: Coroutine[Any, Any, T],\n    logger: logging.Logger | None = None,\n) -&gt; asyncio.Task[T]:\n    \"\"\"Create an `asyncio.Task` but ensure that exceptions are logged.\n\n    Reference: https://quantlane.com/blog/ensure-asyncio-task-exceptions-get-logged/\n\n    Args:\n        coroutine: The coroutine to be wrapped.\n        logger: The logger to be used for logging exceptions. If `None`, the\n            the logger with the name `zeus.utils.async_utils` is used.\n    \"\"\"\n    loop = asyncio.get_running_loop()\n    task = loop.create_task(coroutine)\n    task.add_done_callback(functools.partial(_handle_task_exception, logger=logger or default_logger))\n    return task\n</code></pre>"},{"location":"reference/utils/async_utils/#zeus.utils.async_utils._handle_task_exception","title":"_handle_task_exception","text":"<pre><code>_handle_task_exception(task, logger)\n</code></pre> <p>Print out exception and tracebook when a task dies with an exception.</p> Source code in <code>zeus/utils/async_utils.py</code> <pre><code>def _handle_task_exception(task: asyncio.Task, logger: logging.Logger) -&gt; None:\n    \"\"\"Print out exception and tracebook when a task dies with an exception.\"\"\"\n    try:\n        task.result()\n    except asyncio.CancelledError:\n        # Cancellation should not be logged as an error.\n        pass\n    except Exception:\n        # `logger.exception` automatically handles exception and traceback info.\n        logger.exception(\"Job task died with an exception!\")\n</code></pre>"},{"location":"reference/utils/env/","title":"env","text":""},{"location":"reference/utils/env/#zeus.utils.env","title":"zeus.utils.env","text":"<p>Tools related to environment variables.</p>"},{"location":"reference/utils/env/#zeus.utils.env.get_env","title":"get_env","text":"<pre><code>get_env(name, valtype, default=None)\n</code></pre> <p>Fetch an environment variable and cast it to the given type.</p> Source code in <code>zeus/utils/env.py</code> <pre><code>def get_env(name: str, valtype: Type[T], default: T | None = None) -&gt; T:\n    \"\"\"Fetch an environment variable and cast it to the given type.\"\"\"\n    try:\n        if valtype is bool:\n            val = os.environ[name].lower()\n            if val not in [\"true\", \"false\"]:\n                raise ValueError(f\"Strange boolean environment variable value '{val}'\")\n            return cast(T, val == \"true\")\n        return valtype(os.environ[name])\n    except KeyError:\n        if default is not None:\n            return default\n        raise ValueError(f\"Missing environment variable '{name}'\") from None\n</code></pre>"},{"location":"reference/utils/framework/","title":"framework","text":""},{"location":"reference/utils/framework/#zeus.utils.framework","title":"zeus.utils.framework","text":"<p>Utilities for framework-specific code.</p>"},{"location":"reference/utils/framework/#zeus.utils.framework.torch_is_available","title":"torch_is_available  <code>cached</code>","text":"<pre><code>torch_is_available(ensure_available=False, ensure_cuda=True)\n</code></pre> <p>Check if PyTorch is available.</p> Source code in <code>zeus/utils/framework.py</code> <pre><code>@lru_cache(maxsize=1)\ndef torch_is_available(ensure_available: bool = False, ensure_cuda: bool = True) -&gt; bool:\n    \"\"\"Check if PyTorch is available.\"\"\"\n    try:\n        import torch\n\n        cuda_available = torch.cuda.is_available()\n        if ensure_cuda and not cuda_available:\n            raise RuntimeError(\"PyTorch is available but does not have CUDA support.\")\n        MODULE_CACHE[\"torch\"] = torch\n        logger.info(\n            \"PyTorch %s CUDA support is available.\",\n            \"with\" if cuda_available else \"without\",\n        )\n        return True\n    except ImportError as e:\n        logger.info(\"PyTorch is not available.\")\n        if ensure_available:\n            raise RuntimeError(\"Failed to import Pytorch\") from e\n        return False\n</code></pre>"},{"location":"reference/utils/framework/#zeus.utils.framework.jax_is_available","title":"jax_is_available  <code>cached</code>","text":"<pre><code>jax_is_available(ensure_available=False, ensure_cuda=True)\n</code></pre> <p>Check if JAX is available.</p> Source code in <code>zeus/utils/framework.py</code> <pre><code>@lru_cache(maxsize=1)\ndef jax_is_available(ensure_available: bool = False, ensure_cuda: bool = True) -&gt; bool:\n    \"\"\"Check if JAX is available.\"\"\"\n    try:\n        import jax  # type: ignore\n\n        cuda_available = jax.devices(\"gpu\")\n        if ensure_cuda and not cuda_available:\n            raise RuntimeError(\"JAX is available but does not have CUDA support.\")\n        MODULE_CACHE[\"jax\"] = jax\n        logger.info(\"JAX %s CUDA support is available.\", \"with\" if cuda_available else \"without\")\n        return True\n    except ImportError as e:\n        logger.info(\"JAX is not available\")\n        if ensure_available:\n            raise RuntimeError(\"Failed to import JAX\") from e\n        return False\n</code></pre>"},{"location":"reference/utils/framework/#zeus.utils.framework.cupy_is_available","title":"cupy_is_available  <code>cached</code>","text":"<pre><code>cupy_is_available(ensure_available=False)\n</code></pre> <p>Check if CuPy is available.</p> Source code in <code>zeus/utils/framework.py</code> <pre><code>@lru_cache(maxsize=1)\ndef cupy_is_available(ensure_available: bool = False) -&gt; bool:\n    \"\"\"Check if CuPy is available.\"\"\"\n    try:\n        import cupy  # type: ignore\n\n        MODULE_CACHE[\"cupy\"] = cupy\n        logger.info(\"CuPy is available.\")\n        return True\n    except ImportError as e:\n        logger.info(\"CuPy is not available.\")\n        if ensure_available:\n            raise RuntimeError(\"Failed to import CuPy\") from e\n        return False\n</code></pre>"},{"location":"reference/utils/framework/#zeus.utils.framework.sync_execution","title":"sync_execution","text":"<pre><code>sync_execution(gpu_devices, sync_with='torch')\n</code></pre> <p>Block until all computations on the specified devices are finished.</p> <p>PyTorch only runs GPU computations asynchronously, so synchronizing computations for the given GPU devices is done by calling <code>torch.cuda.synchronize</code> on each device. On the other hand, JAX runs both CPU and GPU computations asynchronously, but by default it only has a single CPU device (id=0). Therefore for JAX, all GPU devices passed in and the CPU device (id=0) are synchronized.</p> <p>CuPy is independent of the specific ML framework and calls the CUDA API directly, but you'll need to install it separately and ensure that it is compatible with the ML framework you are using.</p> <p>Note</p> <p><code>jax.device_put</code> with <code>block_until_ready</code> is used to synchronize computations on JAX devices. This is a workaround to the lack of a direct API for synchronizing computations on JAX devices. Tracking issue: https://github.com/google/jax/issues/4335</p> <p>Note</p> <p>Across the Zeus library, an integer device index corresponds to a single whole physical device. This is usually what you want, except when using more advanced device partitioning (e.g., using <code>--xla_force_host_platform_device_count</code> in JAX to partition CPUs into more pieces). In such cases, you probably want to opt out from using this function and handle synchronization manually at the appropriate granularity.</p> <p>Parameters:</p> Name Type Description Default <code>gpu_devices</code> <code>list[int]</code> <p>GPU device indices to synchronize.</p> required <code>sync_with</code> <code>Literal['torch', 'jax', 'cupy']</code> <p>Deep learning framework to use to synchronize computations. Defaults to <code>\"torch\"</code>, in which case <code>torch.cuda.synchronize</code> will be used.</p> <code>'torch'</code> Source code in <code>zeus/utils/framework.py</code> <pre><code>def sync_execution(gpu_devices: list[int], sync_with: Literal[\"torch\", \"jax\", \"cupy\"] = \"torch\") -&gt; None:\n    \"\"\"Block until all computations on the specified devices are finished.\n\n    PyTorch only runs GPU computations asynchronously, so synchronizing computations\n    for the given GPU devices is done by calling `torch.cuda.synchronize` on each\n    device. On the other hand, JAX runs both CPU and GPU computations asynchronously,\n    but by default it only has a single CPU device (id=0). Therefore for JAX, all GPU\n    devices passed in and the CPU device (id=0) are synchronized.\n\n    CuPy is independent of the specific ML framework and calls the CUDA API directly,\n    but you'll need to install it separately and ensure that it is compatible with\n    the ML framework you are using.\n\n    !!! Note\n        `jax.device_put` with `block_until_ready` is used to synchronize computations\n        on JAX devices. This is a workaround to the lack of a direct API for\n        synchronizing computations on JAX devices. Tracking issue:\n        https://github.com/google/jax/issues/4335\n\n    !!! Note\n        Across the Zeus library, an integer device index corresponds to a single whole\n        physical device. This is usually what you want, except when using more advanced\n        device partitioning (e.g., using `--xla_force_host_platform_device_count` in JAX\n        to partition CPUs into more pieces). In such cases, you probably want to opt out\n        from using this function and handle synchronization manually at the appropriate\n        granularity.\n\n    Args:\n        gpu_devices: GPU device indices to synchronize.\n        sync_with: Deep learning framework to use to synchronize computations.\n            Defaults to `\"torch\"`, in which case `torch.cuda.synchronize` will be used.\n    \"\"\"\n    if sync_with == \"torch\" and torch_is_available(ensure_available=True):\n        torch = MODULE_CACHE[\"torch\"]\n        for device in gpu_devices:\n            torch.cuda.synchronize(device)\n        return\n\n    if sync_with == \"jax\" and jax_is_available(ensure_available=True):\n        jax = MODULE_CACHE[\"jax\"]\n        futures = [jax.device_put(0.0, device=jax.devices(\"gpu\")[device]) + 0 for device in gpu_devices]\n        futures.append(jax.device_put(0.0, device=jax.devices(\"cpu\")[0]) + 0)\n        jax.block_until_ready(futures)\n        return\n\n    if sync_with == \"cupy\" and cupy_is_available(ensure_available=True):\n        cupy = MODULE_CACHE[\"cupy\"]\n        for device in gpu_devices:\n            cupy.cuda.Device(device).synchronize()\n        return\n\n    raise RuntimeError(\"No framework is available.\")\n</code></pre>"},{"location":"reference/utils/framework/#zeus.utils.framework.all_reduce","title":"all_reduce","text":"<pre><code>all_reduce(object, operation)\n</code></pre> <p>Reduce objects from all replicas through the specified operation.</p> <p>If the current execution is not distributed, the object is returned as is.</p> Source code in <code>zeus/utils/framework.py</code> <pre><code>def all_reduce(object: list[int] | list[float], operation: Literal[\"sum\", \"max\"]) -&gt; list[int] | list[float]:\n    \"\"\"Reduce objects from all replicas through the specified operation.\n\n    If the current execution is not distributed, the object is returned as is.\n    \"\"\"\n    if torch_is_available(ensure_cuda=False):\n        torch = MODULE_CACHE[\"torch\"]\n\n        # if torch.distributed is not available or not initialized, return the object as is\n        if not torch.distributed.is_available() or not torch.distributed.is_initialized():\n            return object\n\n        # wrap object in a tensor\n        tensor = torch.Tensor(object).cuda()\n\n        # determine operation\n        if operation == \"sum\":\n            torch_op = torch.distributed.ReduceOp.SUM\n        elif operation == \"max\":\n            torch_op = torch.distributed.ReduceOp.MAX\n        else:\n            raise ValueError(f\"all_reduce unsupported operation: {operation}\")\n\n        torch.distributed.all_reduce(tensor, op=torch_op)\n        return tensor.cpu().tolist()\n\n    if jax_is_available():\n        # Check if not distributed\n        jax = MODULE_CACHE[\"jax\"]\n        # if jax is not distributed, return the object as is\n        if jax.device_count() == 1:\n            return object\n\n        # TODO: Implement JAX distributed all-reduce logic.\n        raise NotImplementedError(\"JAX distributed is not supported yet.\")\n\n    raise RuntimeError(\"No framework is available.\")\n</code></pre>"},{"location":"reference/utils/framework/#zeus.utils.framework.is_distributed","title":"is_distributed","text":"<pre><code>is_distributed()\n</code></pre> <p>Check if the current execution is distributed across multiple devices.</p> Source code in <code>zeus/utils/framework.py</code> <pre><code>def is_distributed() -&gt; bool:\n    \"\"\"Check if the current execution is distributed across multiple devices.\"\"\"\n    if torch_is_available(ensure_cuda=False):\n        torch = MODULE_CACHE[\"torch\"]\n        return torch.distributed.is_available() and torch.distributed.is_initialized()\n    if jax_is_available():\n        jax = MODULE_CACHE[\"jax\"]\n        return jax.device_count() &gt; 1\n    raise RuntimeError(\"No framework is available.\")\n</code></pre>"},{"location":"reference/utils/lat_lon/","title":"lat_lon","text":""},{"location":"reference/utils/lat_lon/#zeus.utils.lat_lon","title":"zeus.utils.lat_lon","text":"<p>Function for getting latitude and longitude.</p>"},{"location":"reference/utils/lat_lon/#zeus.utils.lat_lon.get_ip_lat_long","title":"get_ip_lat_long","text":"<pre><code>get_ip_lat_long()\n</code></pre> <p>Retrieve the latitude and longitude of the current IP position.</p> Source code in <code>zeus/utils/lat_lon.py</code> <pre><code>def get_ip_lat_long() -&gt; tuple[float, float]:\n    \"\"\"Retrieve the latitude and longitude of the current IP position.\"\"\"\n    try:\n        ip_url = \"http://ipinfo.io/json\"\n        resp = requests.get(ip_url)\n        loc = resp.json()[\"loc\"]\n        lat, long = map(float, loc.split(\",\"))\n        logger.info(\"Retrieved latitude and longitude: %s, %s\", lat, long)\n        return lat, long\n    except requests.exceptions.RequestException as e:\n        logger.exception(\"Failed to retrieve current latitude and longitude of IP: %s\", e)\n        raise\n</code></pre>"},{"location":"reference/utils/logging/","title":"logging","text":""},{"location":"reference/utils/logging/#zeus.utils.logging","title":"zeus.utils.logging","text":"<p>Utilities for logging.</p>"},{"location":"reference/utils/logging/#zeus.utils.logging.FileAndConsole","title":"FileAndConsole","text":"<p>Like tee, but for Python prints.</p> Source code in <code>zeus/utils/logging.py</code> <pre><code>class FileAndConsole:\n    \"\"\"Like tee, but for Python prints.\"\"\"\n\n    def __init__(self, filepath: Path) -&gt; None:\n        \"\"\"Initialize the object.\"\"\"\n        self.file = open(filepath, \"w\")\n        self.stdout = sys.stdout\n\n    def write(self, message):\n        \"\"\"Write message.\"\"\"\n        self.file.write(message)\n        self.stdout.write(message)\n        self.file.flush()\n        self.stdout.flush()\n\n    def flush(self):\n        \"\"\"Flush both log file and stdout.\"\"\"\n        self.file.flush()\n        self.stdout.flush()\n</code></pre>"},{"location":"reference/utils/logging/#zeus.utils.logging.FileAndConsole.__init__","title":"__init__","text":"<pre><code>__init__(filepath)\n</code></pre> Source code in <code>zeus/utils/logging.py</code> <pre><code>def __init__(self, filepath: Path) -&gt; None:\n    \"\"\"Initialize the object.\"\"\"\n    self.file = open(filepath, \"w\")\n    self.stdout = sys.stdout\n</code></pre>"},{"location":"reference/utils/logging/#zeus.utils.logging.FileAndConsole.write","title":"write","text":"<pre><code>write(message)\n</code></pre> <p>Write message.</p> Source code in <code>zeus/utils/logging.py</code> <pre><code>def write(self, message):\n    \"\"\"Write message.\"\"\"\n    self.file.write(message)\n    self.stdout.write(message)\n    self.file.flush()\n    self.stdout.flush()\n</code></pre>"},{"location":"reference/utils/logging/#zeus.utils.logging.FileAndConsole.flush","title":"flush","text":"<pre><code>flush()\n</code></pre> <p>Flush both log file and stdout.</p> Source code in <code>zeus/utils/logging.py</code> <pre><code>def flush(self):\n    \"\"\"Flush both log file and stdout.\"\"\"\n    self.file.flush()\n    self.stdout.flush()\n</code></pre>"},{"location":"reference/utils/lr_scaler/","title":"lr_scaler","text":""},{"location":"reference/utils/lr_scaler/#zeus.utils.lr_scaler","title":"zeus.utils.lr_scaler","text":"<p>Classes that enclose learning rate scaling rules.</p>"},{"location":"reference/utils/lr_scaler/#zeus.utils.lr_scaler.SquareRootScaler","title":"SquareRootScaler  <code>dataclass</code>","text":"<p>Square root scaling.</p> <p>Parameters:</p> Name Type Description Default <code>bs</code> <code>int</code> <p>The initial batch size</p> required <code>lr</code> <code>float</code> <p>The initial learning rate</p> required Source code in <code>zeus/utils/lr_scaler.py</code> <pre><code>@dataclass\nclass SquareRootScaler:\n    \"\"\"Square root scaling.\n\n    Args:\n        bs: The initial batch size\n        lr: The initial learning rate\n    \"\"\"\n\n    bs: int\n    lr: float\n\n    def compute_lr(self, new_bs: int) -&gt; float:\n        \"\"\"Compute the scaled learning rate given the new batch size.\"\"\"\n        return self.lr * math.sqrt(new_bs / self.bs)\n</code></pre>"},{"location":"reference/utils/lr_scaler/#zeus.utils.lr_scaler.SquareRootScaler.compute_lr","title":"compute_lr","text":"<pre><code>compute_lr(new_bs)\n</code></pre> <p>Compute the scaled learning rate given the new batch size.</p> Source code in <code>zeus/utils/lr_scaler.py</code> <pre><code>def compute_lr(self, new_bs: int) -&gt; float:\n    \"\"\"Compute the scaled learning rate given the new batch size.\"\"\"\n    return self.lr * math.sqrt(new_bs / self.bs)\n</code></pre>"},{"location":"reference/utils/lr_scaler/#zeus.utils.lr_scaler.LinearScaler","title":"LinearScaler  <code>dataclass</code>","text":"<p>Linear scaling.</p> <p>Parameters:</p> Name Type Description Default <code>bs</code> <code>int</code> <p>The initial batch size</p> required <code>lr</code> <code>float</code> <p>The initial learning rate</p> required Source code in <code>zeus/utils/lr_scaler.py</code> <pre><code>@dataclass\nclass LinearScaler:\n    \"\"\"Linear scaling.\n\n    Args:\n        bs: The initial batch size\n        lr: The initial learning rate\n    \"\"\"\n\n    bs: int\n    lr: float\n\n    def compute_lr(self, new_bs: int) -&gt; float:\n        \"\"\"Compute the scaled learning rate given the new batch size.\"\"\"\n        return self.lr * new_bs / self.bs\n</code></pre>"},{"location":"reference/utils/lr_scaler/#zeus.utils.lr_scaler.LinearScaler.compute_lr","title":"compute_lr","text":"<pre><code>compute_lr(new_bs)\n</code></pre> <p>Compute the scaled learning rate given the new batch size.</p> Source code in <code>zeus/utils/lr_scaler.py</code> <pre><code>def compute_lr(self, new_bs: int) -&gt; float:\n    \"\"\"Compute the scaled learning rate given the new batch size.\"\"\"\n    return self.lr * new_bs / self.bs\n</code></pre>"},{"location":"reference/utils/metric/","title":"metric","text":""},{"location":"reference/utils/metric/#zeus.utils.metric","title":"zeus.utils.metric","text":"<p>Defines the energy-time cost metric function.</p>"},{"location":"reference/utils/metric/#zeus.utils.metric.zeus_cost","title":"zeus_cost","text":"<pre><code>zeus_cost(energy, time, eta_knob, max_power)\n</code></pre> <p>Compute Zeus's energy-time cost metric.</p> <p>Trades off ETA and TTA based on the value of <code>eta_knob</code>. The caller is expected to do bound checking for <code>eta_knob</code>, because <code>eta_knob</code> does not change frequently.</p> <p>Parameters:</p> Name Type Description Default <code>energy</code> <code>float</code> <p>Joules</p> required <code>time</code> <code>float</code> <p>seconds</p> required <code>eta_knob</code> <code>float</code> <p>Real number in [0, 1].</p> required <code>max_power</code> <code>int | float</code> <p>The maximum power limit of the GPU.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The cost of the DL training job.</p> Source code in <code>zeus/utils/metric.py</code> <pre><code>def zeus_cost(energy: float, time: float, eta_knob: float, max_power: int | float) -&gt; float:\n    \"\"\"Compute Zeus's energy-time cost metric.\n\n    Trades off ETA and TTA based on the value of `eta_knob`.\n    The caller is expected to do bound checking for `eta_knob`,\n    because `eta_knob` does not change frequently.\n\n    Args:\n        energy: Joules\n        time: seconds\n        eta_knob: Real number in [0, 1].\n        max_power: The maximum power limit of the GPU.\n\n    Returns:\n        The cost of the DL training job.\n    \"\"\"\n    return eta_knob * energy + (1 - eta_knob) * max_power * time\n</code></pre>"},{"location":"reference/utils/metric/#zeus.utils.metric.energy","title":"energy","text":"<pre><code>energy(logfile, start=None, end=None)\n</code></pre> <p>Compute the energy consumption from the Zeus monitor power log file.</p> <p><code>start</code> and <code>end</code> are in units of seconds, relative to the beginning of the time window captured by the log file. Only the time window between <code>start</code> and <code>end</code> will be considered when computing energy.</p> <p><code>start</code> and <code>end</code> can be negative, in which case the pointers wrap around and effectively the absolute value is subtracted from the end of the window.</p> <p>Parameters:</p> Name Type Description Default <code>logfile</code> <code>Path | str</code> <p>Path to the power log file produced by the Zeus monitor.</p> required <code>start</code> <code>float | None</code> <p>Start time of the window to consider.</p> <code>None</code> <code>end</code> <code>float | None</code> <p>End time of the window to consider.</p> <code>None</code> Source code in <code>zeus/utils/metric.py</code> <pre><code>def energy(\n    logfile: Path | str,\n    start: float | None = None,\n    end: float | None = None,\n) -&gt; float:\n    \"\"\"Compute the energy consumption from the Zeus monitor power log file.\n\n    `start` and `end` are in units of seconds, relative to the beginning of\n    the time window captured by the log file. Only the time window between\n    `start` and `end` will be considered when computing energy.\n\n    `start` and `end` can be negative, in which case the pointers wrap around\n    and effectively the absolute value is subtracted from the end of the window.\n\n    Args:\n        logfile: Path to the power log file produced by the Zeus monitor.\n        start: Start time of the window to consider.\n        end: End time of the window to consider.\n    \"\"\"\n    df = pd.read_csv(logfile, engine=\"python\", skipfooter=1)\n    df[\"Time\"] = pd.to_datetime(df[\"Time\"])\n    start_timestamp = df.iloc[0][\"Time\"]\n    end_timestamp = df.iloc[-1][\"Time\"]\n    if start is not None:\n        origin = start_timestamp if start &gt;= 0.0 else end_timestamp\n        df = df.loc[df[\"Time\"] &gt;= origin + timedelta(seconds=start)]\n    if end is not None:\n        origin = start_timestamp if end &gt;= 0.0 else end_timestamp\n        df = df.loc[df[\"Time\"] &lt;= origin + timedelta(seconds=end)]\n    seconds = _get_seconds(df)\n    watts = _get_watts(df)\n    return auc(seconds, watts)\n</code></pre>"},{"location":"reference/utils/metric/#zeus.utils.metric.avg_power","title":"avg_power","text":"<pre><code>avg_power(logfile, start=None, end=None)\n</code></pre> <p>Compute the average power consumption from the Zeus monitor power log file.</p> <p><code>start</code> and <code>end</code> are in units of seconds, relative to the beginning of the time window captured by the log file. Only the time window between <code>start</code> and <code>end</code> will be considered when computing average power.</p> <p><code>start</code> and <code>end</code> can be negative, in which case the pointers wrap around and effectively the absolute value is subtracted from the end of the window.</p> <p>Parameters:</p> Name Type Description Default <code>logfile</code> <code>Path | str</code> <p>Path to the power log file produced by the Zeus monitor.</p> required <code>start</code> <code>float | None</code> <p>Start time of the window to consider.</p> <code>None</code> <code>end</code> <code>float | None</code> <p>End time of the window to consider.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>From <code>sklearn.metrics.auc</code>, when the duration of the profiling window is too small.</p> Source code in <code>zeus/utils/metric.py</code> <pre><code>def avg_power(\n    logfile: Path | str,\n    start: float | None = None,\n    end: float | None = None,\n) -&gt; float:\n    \"\"\"Compute the average power consumption from the Zeus monitor power log file.\n\n    `start` and `end` are in units of seconds, relative to the beginning of\n    the time window captured by the log file. Only the time window between\n    `start` and `end` will be considered when computing average power.\n\n    `start` and `end` can be negative, in which case the pointers wrap around\n    and effectively the absolute value is subtracted from the end of the window.\n\n    Args:\n        logfile: Path to the power log file produced by the Zeus monitor.\n        start: Start time of the window to consider.\n        end: End time of the window to consider.\n\n    Raises:\n        ValueError: From `sklearn.metrics.auc`, when the duration of the\n            profiling window is too small.\n    \"\"\"\n    df = pd.read_csv(logfile, engine=\"python\", skipfooter=1)\n    df[\"Time\"] = pd.to_datetime(df[\"Time\"])\n    if start is not None:\n        df = df.loc[df[\"Time\"] &gt;= df.iloc[0][\"Time\"] + timedelta(seconds=start)]\n    if end is not None:\n        df = df.loc[df[\"Time\"] &lt;= df.iloc[0][\"Time\"] + timedelta(seconds=end)]\n    seconds = _get_seconds(df)\n    watts = _get_watts(df)\n    area = auc(seconds, watts)\n    return area / (max(seconds) - min(seconds))\n</code></pre>"},{"location":"reference/utils/multiprocessing/","title":"multiprocessing","text":""},{"location":"reference/utils/multiprocessing/#zeus.utils.multiprocessing","title":"zeus.utils.multiprocessing","text":"<p>Multiprocessing-related utilities.</p>"},{"location":"reference/utils/multiprocessing/#zeus.utils.multiprocessing._is_global_in_spawned_child","title":"_is_global_in_spawned_child","text":"<pre><code>_is_global_in_spawned_child()\n</code></pre> <p>Return True if called from module-level code in a spawned child's main script.</p> <p>In a spawned child process (using the \"spawn\" start method), the main script is re-imported with <code>__name__ = \"__mp_main__\"</code> instead of <code>\"__main__\"</code>. This function detects if we're currently executing module-level code (like global variable initialization) in such a script.</p> <p>This walks the call stack looking for any  frame from a real Python file (not multiprocessing infrastructure like  or ) where the module's <code>__name__</code> is <code>\"__mp_main__\"</code>. Source code in <code>zeus/utils/multiprocessing.py</code> <pre><code>def _is_global_in_spawned_child() -&gt; bool:\n    \"\"\"Return True if called from module-level code in a spawned child's main script.\n\n    In a spawned child process (using the \"spawn\" start method), the main script\n    is re-imported with `__name__ = \"__mp_main__\"` instead of `\"__main__\"`. This\n    function detects if we're currently executing module-level code (like global\n    variable initialization) in such a script.\n\n    This walks the call stack looking for any &lt;module&gt; frame from a real Python\n    file (not multiprocessing infrastructure like &lt;string&gt; or &lt;frozen ...&gt;) where\n    the module's `__name__` is `\"__mp_main__\"`.\n    \"\"\"\n    frame = inspect.currentframe()\n    if frame is None:\n        return False\n    frame = frame.f_back\n    while frame is not None:\n        if frame.f_code.co_name == \"&lt;module&gt;\":\n            filename = frame.f_code.co_filename\n            # Skip multiprocessing infrastructure frames\n            if not (filename == \"&lt;string&gt;\" or filename.startswith(\"&lt;frozen\")):\n                module_name = frame.f_globals.get(\"__name__\", \"\")\n                if module_name == \"__mp_main__\":\n                    return True\n        frame = frame.f_back\n    return False\n</code></pre>"},{"location":"reference/utils/multiprocessing/#zeus.utils.multiprocessing.warn_if_global_in_subprocess","title":"warn_if_global_in_subprocess","text":"<pre><code>warn_if_global_in_subprocess(self)\n</code></pre> <p>Warn when a monitor is created at import time in a spawned subprocess.</p> <p>This detects a common pitfall where ZeusMonitor (or related classes) is instantiated as a global variable or called from module-level code. When the script spawns subprocesses using the \"spawn\" method, the subprocess re-imports the main module, causing global initialization code to run again (e.g., loading DNN models), leading to OOM errors.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>Any</code> <p>The instance being constructed (used to derive class name).</p> required Source code in <code>zeus/utils/multiprocessing.py</code> <pre><code>def warn_if_global_in_subprocess(self: Any) -&gt; None:\n    \"\"\"Warn when a monitor is created at import time in a spawned subprocess.\n\n    This detects a common pitfall where ZeusMonitor (or related classes) is\n    instantiated as a global variable or called from module-level code.\n    When the script spawns subprocesses using the \"spawn\" method, the\n    subprocess re-imports the main module, causing global initialization\n    code to run again (e.g., loading DNN models), leading to OOM errors.\n\n    Args:\n        self: The instance being constructed (used to derive class name).\n    \"\"\"\n    if not _is_global_in_spawned_child():\n        return\n    class_name = type(self).__name__\n    warnings.warn(\n        f\"{class_name} was instantiated during module import in a spawned subprocess. \"\n        \"This usually means the monitor was created as a global variable, so the child \"\n        \"process re-imported your main module and executed global code again. Move monitor \"\n        'construction under `if __name__ == \"__main__\":` or inside a function to avoid '\n        \"repeated imports and memory issues.\",\n        stacklevel=4,\n    )\n</code></pre>"},{"location":"reference/utils/pydantic_v1/","title":"pydantic_v1","text":""},{"location":"reference/utils/pydantic_v1/#zeus.utils.pydantic_v1","title":"zeus.utils.pydantic_v1","text":"<p>Compatibility layer for Pydantic v1 and v2.</p> <p>We don't want to pin any specific version of Pydantic. With this, we can import things from <code>zeus.utils.pydantic_v1</code> and always use the V1 API regardless of the installed version of Pydantic.</p> <p>Inspired by Deepspeed: https://github.com/microsoft/DeepSpeed/blob/5d754606/deepspeed/pydantic_v1.py</p>"},{"location":"reference/utils/testing/","title":"testing","text":""},{"location":"reference/utils/testing/#zeus.utils.testing","title":"zeus.utils.testing","text":"<p>Utilities for testing.</p>"},{"location":"reference/utils/testing/#zeus.utils.testing.ReplayZeusMonitor","title":"ReplayZeusMonitor","text":"<p>               Bases: <code>ZeusMonitor</code></p> <p>A mock ZeusMonitor that replays windows recorded by a real monitor.</p> <p>This class is for testing only. Based on a CSV log file that records the time and energy measurements of <code>ZeusMonitor</code> measurement windows, users can drop-in replace <code>ZeusMonitor</code> with this class to replay the measurement windows and fast forward training and time/energy measurement.</p> <p>The methods exposed is identical to or a superset of <code>ZeusMonitor</code>, but behaves differently. Instead of monitoring the GPU, it replays events from a log file. The log file generated by <code>ZeusMonitor</code> (<code>log_file</code>) is guaranteed to be compatible and will replay time and energy measurements just like how the real monitor experienced them. Note that in the case of concurrent ongoing measurement windows, the log rows file should record windows in the order of <code>end_window</code> calls.</p> <p>Attributes:</p> Name Type Description <code>gpu_indices</code> <code>`list[int]`</code> <p>Indices of all the CUDA devices to monitor.</p> Source code in <code>zeus/utils/testing.py</code> <pre><code>class ReplayZeusMonitor(ZeusMonitor):\n    \"\"\"A mock ZeusMonitor that replays windows recorded by a real monitor.\n\n    This class is for testing only. Based on a CSV log file that records the time\n    and energy measurements of `ZeusMonitor` measurement windows, users can drop-in\n    replace `ZeusMonitor` with this class to replay the measurement windows and\n    *fast forward* training and time/energy measurement.\n\n    The methods exposed is identical to or a superset of `ZeusMonitor`, but behaves\n    differently. Instead of monitoring the GPU, it replays events from a log file.\n    The log file generated by `ZeusMonitor` (`log_file`) is guaranteed to be compatible\n    and will replay time and energy measurements just like how the real monitor\n    experienced them. Note that in the case of concurrent ongoing measurement windows,\n    the log rows file should record windows in the order of `end_window` calls.\n\n    Attributes:\n        gpu_indices (`list[int]`): Indices of all the CUDA devices to monitor.\n    \"\"\"\n\n    def __init__(\n        self,\n        gpu_indices: list[int] | None = None,\n        approx_instant_energy: bool = False,\n        log_file: str | Path | None = None,\n        ignore_sync_execution: bool = False,\n        match_window_name: bool = True,\n    ) -&gt; None:\n        \"\"\"Initialize the replay monitor.\n\n        The log file should be a CSV file with the following header (e.g. gpu_indices=[0, 2]):\n        `start_time,window_name,elapsed_time,gpu0_energy,gpu2_energy`\n\n        Args:\n            gpu_indices: Indices of all the CUDA devices to monitor. This should be consistent\n                with the indices used in the log file. If `None`, GPU indices will be inferred\n                from the log file header. Does not respect `CUDA_VISIBLE_DEVICES`.\n                (Default: `None`)\n            approx_instant_energy: Whether to approximate the instant energy consumption. Not used.\n            log_file: Path to the log CSV file to replay events from. `None` is not allowed.\n            ignore_sync_execution: Whether to ignore `sync_execution` calls. (Default: `False`)\n            match_window_name: Whether to make sure window names match. (Default: `True`)\n        \"\"\"\n        if log_file is None:\n            raise ValueError(\"`log_file` cannot be `None` for `ReplayZeusMonitor`.\")\n\n        self.approx_instant_energy = approx_instant_energy\n        self.log_file = open(log_file)\n        self.ignore_sync_execution = ignore_sync_execution\n        self.match_window_name = match_window_name\n\n        # Infer GPU indices from the log file if not provided.\n        header = self.log_file.readline()\n        if gpu_indices is None:\n            gpu_indices = [int(gpu.split(\"_\")[0][3:]) for gpu in header.split(\",\")[3:] if gpu]\n        self.nvml_gpu_indices = self.gpu_indices = gpu_indices\n\n        logger.info(\"Replaying from '%s' with GPU indices %s\", log_file, gpu_indices)\n\n        # Keep track of ongoing measurement windows.\n        self.ongoing_windows = []\n\n    def begin_window(self, key: str, sync_execution: bool = True) -&gt; None:\n        \"\"\"Begin a new window.\n\n        This method just pushes the key into a list of ongoing measurement windows,\n        and just makes sure it's unique.\n\n        Args:\n            key: Name of the measurement window.\n            sync_execution: Whether to synchronize CUDA before starting the measurement window.\n                (Default: `True`)\n        \"\"\"\n        if key in self.ongoing_windows:\n            raise RuntimeError(f\"Window {key} is already ongoing.\")\n        self.ongoing_windows.append(key)\n\n        if not self.ignore_sync_execution and sync_execution:\n            sync_execution_fn(self.gpu_indices)\n\n        logger.info(\"Measurement window '%s' started.\", key)\n\n    def end_window(self, key: str, sync_execution: bool = True, cancel: bool = False) -&gt; Measurement:\n        \"\"\"End an ongoing window.\n\n        This method pops the key from a list of ongoing measurement windows and\n        constructs a `Measurement` object corresponding to the name of the window\n        from the log file. If the name of the window does not match the expected\n        one, a `RuntimeError` is raised.\n\n        Args:\n            key: Name of the measurement window.\n            sync_execution: Whether to synchronize CUDA before ending the measurement window.\n                (Default: `True`)\n            cancel: Whether to cancel the measurement window. This will not consume a\n                line from the log file. (Default: `False`)\n        \"\"\"\n        try:\n            self.ongoing_windows.remove(key)\n        except ValueError:\n            raise RuntimeError(f\"Window {key} is not ongoing.\") from None\n\n        if not self.ignore_sync_execution and sync_execution:\n            sync_execution_fn(self.gpu_indices)\n\n        if cancel:\n            logger.info(\"Measurement window '%s' cancelled.\", key)\n            return Measurement(\n                time=0.0,\n                gpu_energy={gpu_index: 0.0 for gpu_index in self.gpu_indices},\n            )\n\n        # Read the next line from the log file.\n        assert self.log_file is not None\n        line = self.log_file.readline()\n        if not line:\n            raise RuntimeError(\"No more lines in the log file.\")\n        _, window_name, *nums = line.split(\",\")\n        if self.match_window_name and window_name != key:\n            raise RuntimeError(f\"Was expecting {window_name}, not {key}.\")\n        if len(nums) != len(self.gpu_indices) + 1:\n            raise RuntimeError(f\"Line has unexpected number of energy measurements: {line}\")\n        time_consumption, *energy_consumptions = map(float, nums)\n        energy = dict(zip(self.gpu_indices, energy_consumptions))\n        measurement = Measurement(time=time_consumption, gpu_energy=energy)\n\n        logger.info(\"Measurement window '%s' ended (%s).\", key, measurement)\n\n        return measurement\n</code></pre>"},{"location":"reference/utils/testing/#zeus.utils.testing.ReplayZeusMonitor.__init__","title":"__init__","text":"<pre><code>__init__(gpu_indices=None, approx_instant_energy=False, log_file=None, ignore_sync_execution=False, match_window_name=True)\n</code></pre> <p>The log file should be a CSV file with the following header (e.g. gpu_indices=[0, 2]): <code>start_time,window_name,elapsed_time,gpu0_energy,gpu2_energy</code></p> <p>Parameters:</p> Name Type Description Default <code>gpu_indices</code> <code>list[int] | None</code> <p>Indices of all the CUDA devices to monitor. This should be consistent with the indices used in the log file. If <code>None</code>, GPU indices will be inferred from the log file header. Does not respect <code>CUDA_VISIBLE_DEVICES</code>. (Default: <code>None</code>)</p> <code>None</code> <code>approx_instant_energy</code> <code>bool</code> <p>Whether to approximate the instant energy consumption. Not used.</p> <code>False</code> <code>log_file</code> <code>str | Path | None</code> <p>Path to the log CSV file to replay events from. <code>None</code> is not allowed.</p> <code>None</code> <code>ignore_sync_execution</code> <code>bool</code> <p>Whether to ignore <code>sync_execution</code> calls. (Default: <code>False</code>)</p> <code>False</code> <code>match_window_name</code> <code>bool</code> <p>Whether to make sure window names match. (Default: <code>True</code>)</p> <code>True</code> Source code in <code>zeus/utils/testing.py</code> <pre><code>def __init__(\n    self,\n    gpu_indices: list[int] | None = None,\n    approx_instant_energy: bool = False,\n    log_file: str | Path | None = None,\n    ignore_sync_execution: bool = False,\n    match_window_name: bool = True,\n) -&gt; None:\n    \"\"\"Initialize the replay monitor.\n\n    The log file should be a CSV file with the following header (e.g. gpu_indices=[0, 2]):\n    `start_time,window_name,elapsed_time,gpu0_energy,gpu2_energy`\n\n    Args:\n        gpu_indices: Indices of all the CUDA devices to monitor. This should be consistent\n            with the indices used in the log file. If `None`, GPU indices will be inferred\n            from the log file header. Does not respect `CUDA_VISIBLE_DEVICES`.\n            (Default: `None`)\n        approx_instant_energy: Whether to approximate the instant energy consumption. Not used.\n        log_file: Path to the log CSV file to replay events from. `None` is not allowed.\n        ignore_sync_execution: Whether to ignore `sync_execution` calls. (Default: `False`)\n        match_window_name: Whether to make sure window names match. (Default: `True`)\n    \"\"\"\n    if log_file is None:\n        raise ValueError(\"`log_file` cannot be `None` for `ReplayZeusMonitor`.\")\n\n    self.approx_instant_energy = approx_instant_energy\n    self.log_file = open(log_file)\n    self.ignore_sync_execution = ignore_sync_execution\n    self.match_window_name = match_window_name\n\n    # Infer GPU indices from the log file if not provided.\n    header = self.log_file.readline()\n    if gpu_indices is None:\n        gpu_indices = [int(gpu.split(\"_\")[0][3:]) for gpu in header.split(\",\")[3:] if gpu]\n    self.nvml_gpu_indices = self.gpu_indices = gpu_indices\n\n    logger.info(\"Replaying from '%s' with GPU indices %s\", log_file, gpu_indices)\n\n    # Keep track of ongoing measurement windows.\n    self.ongoing_windows = []\n</code></pre>"},{"location":"reference/utils/testing/#zeus.utils.testing.ReplayZeusMonitor.begin_window","title":"begin_window","text":"<pre><code>begin_window(key, sync_execution=True)\n</code></pre> <p>Begin a new window.</p> <p>This method just pushes the key into a list of ongoing measurement windows, and just makes sure it's unique.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the measurement window.</p> required <code>sync_execution</code> <code>bool</code> <p>Whether to synchronize CUDA before starting the measurement window. (Default: <code>True</code>)</p> <code>True</code> Source code in <code>zeus/utils/testing.py</code> <pre><code>def begin_window(self, key: str, sync_execution: bool = True) -&gt; None:\n    \"\"\"Begin a new window.\n\n    This method just pushes the key into a list of ongoing measurement windows,\n    and just makes sure it's unique.\n\n    Args:\n        key: Name of the measurement window.\n        sync_execution: Whether to synchronize CUDA before starting the measurement window.\n            (Default: `True`)\n    \"\"\"\n    if key in self.ongoing_windows:\n        raise RuntimeError(f\"Window {key} is already ongoing.\")\n    self.ongoing_windows.append(key)\n\n    if not self.ignore_sync_execution and sync_execution:\n        sync_execution_fn(self.gpu_indices)\n\n    logger.info(\"Measurement window '%s' started.\", key)\n</code></pre>"},{"location":"reference/utils/testing/#zeus.utils.testing.ReplayZeusMonitor.end_window","title":"end_window","text":"<pre><code>end_window(key, sync_execution=True, cancel=False)\n</code></pre> <p>End an ongoing window.</p> <p>This method pops the key from a list of ongoing measurement windows and constructs a <code>Measurement</code> object corresponding to the name of the window from the log file. If the name of the window does not match the expected one, a <code>RuntimeError</code> is raised.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the measurement window.</p> required <code>sync_execution</code> <code>bool</code> <p>Whether to synchronize CUDA before ending the measurement window. (Default: <code>True</code>)</p> <code>True</code> <code>cancel</code> <code>bool</code> <p>Whether to cancel the measurement window. This will not consume a line from the log file. (Default: <code>False</code>)</p> <code>False</code> Source code in <code>zeus/utils/testing.py</code> <pre><code>def end_window(self, key: str, sync_execution: bool = True, cancel: bool = False) -&gt; Measurement:\n    \"\"\"End an ongoing window.\n\n    This method pops the key from a list of ongoing measurement windows and\n    constructs a `Measurement` object corresponding to the name of the window\n    from the log file. If the name of the window does not match the expected\n    one, a `RuntimeError` is raised.\n\n    Args:\n        key: Name of the measurement window.\n        sync_execution: Whether to synchronize CUDA before ending the measurement window.\n            (Default: `True`)\n        cancel: Whether to cancel the measurement window. This will not consume a\n            line from the log file. (Default: `False`)\n    \"\"\"\n    try:\n        self.ongoing_windows.remove(key)\n    except ValueError:\n        raise RuntimeError(f\"Window {key} is not ongoing.\") from None\n\n    if not self.ignore_sync_execution and sync_execution:\n        sync_execution_fn(self.gpu_indices)\n\n    if cancel:\n        logger.info(\"Measurement window '%s' cancelled.\", key)\n        return Measurement(\n            time=0.0,\n            gpu_energy={gpu_index: 0.0 for gpu_index in self.gpu_indices},\n        )\n\n    # Read the next line from the log file.\n    assert self.log_file is not None\n    line = self.log_file.readline()\n    if not line:\n        raise RuntimeError(\"No more lines in the log file.\")\n    _, window_name, *nums = line.split(\",\")\n    if self.match_window_name and window_name != key:\n        raise RuntimeError(f\"Was expecting {window_name}, not {key}.\")\n    if len(nums) != len(self.gpu_indices) + 1:\n        raise RuntimeError(f\"Line has unexpected number of energy measurements: {line}\")\n    time_consumption, *energy_consumptions = map(float, nums)\n    energy = dict(zip(self.gpu_indices, energy_consumptions))\n    measurement = Measurement(time=time_consumption, gpu_energy=energy)\n\n    logger.info(\"Measurement window '%s' ended (%s).\", key, measurement)\n\n    return measurement\n</code></pre>"},{"location":"reference/utils/zeusd/","title":"zeusd","text":""},{"location":"reference/utils/zeusd/#zeus.utils.zeusd","title":"zeus.utils.zeusd","text":"<p>Zeusd client library.</p> <p>Provides <code>ZeusdConfig</code> and <code>ZeusdClient</code>, the entry points for communicating with a Zeusd daemon.  Handles connection, discovery, authentication, and exposes typed methods for every Zeusd endpoint.</p> <p>Typical usage:</p> <pre><code>from zeus.utils.zeusd import ZeusdConfig, ZeusdClient\n\nclient = ZeusdClient(ZeusdConfig.uds(socket_path=\"/var/run/zeusd.sock\"))\nprint(client.gpu_ids)       # [0, 1, 2, 3]\nprint(client.can_read_gpu)  # True\n\nsnapshot = client.get_gpu_power()\nprint(snapshot.power_mw)    # {0: 75000, 1: 120000, ...}\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdConnectionError","title":"ZeusdConnectionError","text":"<p>               Bases: <code>ZeusBaseError</code></p> <p>Cannot reach the Zeusd daemon.</p> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>class ZeusdConnectionError(ZeusBaseError):\n    \"\"\"Cannot reach the Zeusd daemon.\"\"\"\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdAuthError","title":"ZeusdAuthError","text":"<p>               Bases: <code>ZeusBaseError</code></p> <p>Authentication or authorization failure.</p> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>class ZeusdAuthError(ZeusBaseError):\n    \"\"\"Authentication or authorization failure.\"\"\"\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdCapabilityError","title":"ZeusdCapabilityError","text":"<p>               Bases: <code>ZeusBaseError</code></p> <p>Requested capabilities exceed what the daemon offers.</p> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>class ZeusdCapabilityError(ZeusBaseError):\n    \"\"\"Requested capabilities exceed what the daemon offers.\"\"\"\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.GpuPowerSnapshot","title":"GpuPowerSnapshot  <code>dataclass</code>","text":"<p>Instantaneous GPU power readings from the daemon.</p> <p>Attributes:</p> Name Type Description <code>timestamp_ms</code> <code>int</code> <p>Daemon-side Unix timestamp in milliseconds.</p> <code>power_mw</code> <code>dict[int, int]</code> <p>Mapping of GPU index to power draw in milliwatts.</p> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>@dataclass(frozen=True)\nclass GpuPowerSnapshot:\n    \"\"\"Instantaneous GPU power readings from the daemon.\n\n    Attributes:\n        timestamp_ms: Daemon-side Unix timestamp in milliseconds.\n        power_mw: Mapping of GPU index to power draw in milliwatts.\n    \"\"\"\n\n    timestamp_ms: int\n    power_mw: dict[int, int]\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.CpuDramPower","title":"CpuDramPower  <code>dataclass</code>","text":"<p>Power reading for a single CPU package.</p> <p>Attributes:</p> Name Type Description <code>cpu_mw</code> <code>int</code> <p>CPU package power in milliwatts.</p> <code>dram_mw</code> <code>int | None</code> <p>DRAM power in milliwatts, or None if unavailable.</p> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>@dataclass(frozen=True)\nclass CpuDramPower:\n    \"\"\"Power reading for a single CPU package.\n\n    Attributes:\n        cpu_mw: CPU package power in milliwatts.\n        dram_mw: DRAM power in milliwatts, or None if unavailable.\n    \"\"\"\n\n    cpu_mw: int\n    dram_mw: int | None\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.CpuPowerSnapshot","title":"CpuPowerSnapshot  <code>dataclass</code>","text":"<p>Instantaneous CPU power readings from the daemon.</p> <p>Attributes:</p> Name Type Description <code>timestamp_ms</code> <code>int</code> <p>Daemon-side Unix timestamp in milliseconds.</p> <code>power_mw</code> <code>dict[int, CpuDramPower]</code> <p>Mapping of CPU index to power readings.</p> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>@dataclass(frozen=True)\nclass CpuPowerSnapshot:\n    \"\"\"Instantaneous CPU power readings from the daemon.\n\n    Attributes:\n        timestamp_ms: Daemon-side Unix timestamp in milliseconds.\n        power_mw: Mapping of CPU index to power readings.\n    \"\"\"\n\n    timestamp_ms: int\n    power_mw: dict[int, CpuDramPower]\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.CpuEnergyResult","title":"CpuEnergyResult  <code>dataclass</code>","text":"<p>Cumulative energy for a single CPU package.</p> <p>Attributes:</p> Name Type Description <code>cpu_energy_uj</code> <code>int | None</code> <p>CPU package energy in microjoules, or None.</p> <code>dram_energy_uj</code> <code>int | None</code> <p>DRAM energy in microjoules, or None.</p> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>@dataclass(frozen=True)\nclass CpuEnergyResult:\n    \"\"\"Cumulative energy for a single CPU package.\n\n    Attributes:\n        cpu_energy_uj: CPU package energy in microjoules, or None.\n        dram_energy_uj: DRAM energy in microjoules, or None.\n    \"\"\"\n\n    cpu_energy_uj: int | None\n    dram_energy_uj: int | None\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdConfig","title":"ZeusdConfig  <code>dataclass</code>","text":"<p>Connection configuration for a Zeusd daemon.</p> <p>Use the classmethods <code>tcp</code>, <code>uds</code>, or <code>from_env</code> to construct.</p> <p>Attributes:</p> Name Type Description <code>host_port</code> <code>str | None</code> <p><code>host:port</code> string (TCP mode). None for UDS.</p> <code>socket_path</code> <code>str | None</code> <p>Unix domain socket path (UDS mode). None for TCP.</p> <code>token</code> <code>str | None</code> <p>JWT token. Falls back to <code>ZEUSD_TOKEN</code> env var.</p> <code>gpu_indices</code> <code>list[int] | None</code> <p>GPU indices to stream (for <code>PowerStreamingClient</code>). None means all, empty list means skip. Ignored by <code>ZeusdClient</code>.</p> <code>cpu_indices</code> <code>list[int] | None</code> <p>CPU indices to stream (for <code>PowerStreamingClient</code>). None means all, empty list means skip. Ignored by <code>ZeusdClient</code>.</p> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>@dataclass(frozen=True)\nclass ZeusdConfig:\n    \"\"\"Connection configuration for a Zeusd daemon.\n\n    Use the classmethods `tcp`, `uds`, or `from_env` to construct.\n\n    Attributes:\n        host_port: `host:port` string (TCP mode). None for UDS.\n        socket_path: Unix domain socket path (UDS mode). None for TCP.\n        token: JWT token. Falls back to `ZEUSD_TOKEN` env var.\n        gpu_indices: GPU indices to stream (for `PowerStreamingClient`).\n            None means all, empty list means skip. Ignored by `ZeusdClient`.\n        cpu_indices: CPU indices to stream (for `PowerStreamingClient`).\n            None means all, empty list means skip. Ignored by `ZeusdClient`.\n    \"\"\"\n\n    host_port: str | None = None\n    socket_path: str | None = None\n    token: str | None = None\n    gpu_indices: list[int] | None = None\n    cpu_indices: list[int] | None = None\n\n    @classmethod\n    def tcp(\n        cls,\n        host: str,\n        port: int,\n        *,\n        token: str | None = None,\n        gpu_indices: list[int] | None = None,\n        cpu_indices: list[int] | None = None,\n    ) -&gt; ZeusdConfig:\n        \"\"\"Create a TCP connection config.\n\n        Args:\n            host: Hostname or IP of the Zeusd instance.\n            port: TCP port.\n            token: JWT token. Falls back to `ZEUSD_TOKEN` env var.\n            gpu_indices: GPU indices to stream (for `PowerStreamingClient`).\n            cpu_indices: CPU indices to stream (for `PowerStreamingClient`).\n        \"\"\"\n        return cls(\n            host_port=f\"{host}:{port}\",\n            token=token,\n            gpu_indices=gpu_indices,\n            cpu_indices=cpu_indices,\n        )\n\n    @classmethod\n    def uds(\n        cls,\n        socket_path: str,\n        *,\n        token: str | None = None,\n        gpu_indices: list[int] | None = None,\n        cpu_indices: list[int] | None = None,\n    ) -&gt; ZeusdConfig:\n        \"\"\"Create a Unix domain socket connection config.\n\n        Args:\n            socket_path: Path to the Zeusd Unix domain socket.\n            token: JWT token. Falls back to `ZEUSD_TOKEN` env var.\n            gpu_indices: GPU indices to stream (for `PowerStreamingClient`).\n            cpu_indices: CPU indices to stream (for `PowerStreamingClient`).\n        \"\"\"\n        return cls(\n            socket_path=socket_path,\n            token=token,\n            gpu_indices=gpu_indices,\n            cpu_indices=cpu_indices,\n        )\n\n    @classmethod\n    def from_env(cls) -&gt; ZeusdConfig | None:\n        \"\"\"Create from environment variables.\n\n        Tries `ZEUSD_SOCK_PATH` (UDS) first, then `ZEUSD_HOST_PORT` (TCP).\n        `ZEUSD_HOST_PORT` should be `host:port`. `ZEUSD_TOKEN` is read for\n        JWT authentication.\n\n        Returns None if neither env var is set.\n        \"\"\"\n        token = os.environ.get(\"ZEUSD_TOKEN\")\n        sock = os.environ.get(\"ZEUSD_SOCK_PATH\")\n        if sock is not None:\n            return cls.uds(socket_path=sock, token=token)\n        host_port = os.environ.get(\"ZEUSD_HOST_PORT\")\n        if host_port is not None:\n            return cls(host_port=host_port, token=token)\n        return None\n\n    @property\n    def _is_uds(self) -&gt; bool:\n        return self.socket_path is not None\n\n    def make_client(self) -&gt; httpx.Client:\n        \"\"\"Create an httpx.Client with the appropriate transport and auth.\"\"\"\n        headers = self._auth_headers()\n        if self._is_uds:\n            transport = httpx.HTTPTransport(uds=self.socket_path)\n            return httpx.Client(transport=transport, headers=headers)\n        return httpx.Client(headers=headers)\n\n    def url(self, path: str) -&gt; str:\n        \"\"\"Build the full URL for the given path.\"\"\"\n        if self._is_uds:\n            return f\"http://localhost{path}\"\n        return f\"http://{self.host_port}{path}\"\n\n    @property\n    def endpoint(self) -&gt; str:\n        \"\"\"Human-readable identifier for this connection.\"\"\"\n        if self._is_uds:\n            return self.socket_path  # type: ignore[return-value]\n        return self.host_port  # type: ignore[return-value]\n\n    def _auth_headers(self) -&gt; dict[str, str]:\n        if self.token:\n            return {\"Authorization\": f\"Bearer {self.token}\"}\n        return {}\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdConfig.endpoint","title":"endpoint  <code>property</code>","text":"<pre><code>endpoint\n</code></pre> <p>Human-readable identifier for this connection.</p>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdConfig.tcp","title":"tcp  <code>classmethod</code>","text":"<pre><code>tcp(host, port, *, token=None, gpu_indices=None, cpu_indices=None)\n</code></pre> <p>Create a TCP connection config.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>Hostname or IP of the Zeusd instance.</p> required <code>port</code> <code>int</code> <p>TCP port.</p> required <code>token</code> <code>str | None</code> <p>JWT token. Falls back to <code>ZEUSD_TOKEN</code> env var.</p> <code>None</code> <code>gpu_indices</code> <code>list[int] | None</code> <p>GPU indices to stream (for <code>PowerStreamingClient</code>).</p> <code>None</code> <code>cpu_indices</code> <code>list[int] | None</code> <p>CPU indices to stream (for <code>PowerStreamingClient</code>).</p> <code>None</code> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>@classmethod\ndef tcp(\n    cls,\n    host: str,\n    port: int,\n    *,\n    token: str | None = None,\n    gpu_indices: list[int] | None = None,\n    cpu_indices: list[int] | None = None,\n) -&gt; ZeusdConfig:\n    \"\"\"Create a TCP connection config.\n\n    Args:\n        host: Hostname or IP of the Zeusd instance.\n        port: TCP port.\n        token: JWT token. Falls back to `ZEUSD_TOKEN` env var.\n        gpu_indices: GPU indices to stream (for `PowerStreamingClient`).\n        cpu_indices: CPU indices to stream (for `PowerStreamingClient`).\n    \"\"\"\n    return cls(\n        host_port=f\"{host}:{port}\",\n        token=token,\n        gpu_indices=gpu_indices,\n        cpu_indices=cpu_indices,\n    )\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdConfig.uds","title":"uds  <code>classmethod</code>","text":"<pre><code>uds(socket_path, *, token=None, gpu_indices=None, cpu_indices=None)\n</code></pre> <p>Create a Unix domain socket connection config.</p> <p>Parameters:</p> Name Type Description Default <code>socket_path</code> <code>str</code> <p>Path to the Zeusd Unix domain socket.</p> required <code>token</code> <code>str | None</code> <p>JWT token. Falls back to <code>ZEUSD_TOKEN</code> env var.</p> <code>None</code> <code>gpu_indices</code> <code>list[int] | None</code> <p>GPU indices to stream (for <code>PowerStreamingClient</code>).</p> <code>None</code> <code>cpu_indices</code> <code>list[int] | None</code> <p>CPU indices to stream (for <code>PowerStreamingClient</code>).</p> <code>None</code> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>@classmethod\ndef uds(\n    cls,\n    socket_path: str,\n    *,\n    token: str | None = None,\n    gpu_indices: list[int] | None = None,\n    cpu_indices: list[int] | None = None,\n) -&gt; ZeusdConfig:\n    \"\"\"Create a Unix domain socket connection config.\n\n    Args:\n        socket_path: Path to the Zeusd Unix domain socket.\n        token: JWT token. Falls back to `ZEUSD_TOKEN` env var.\n        gpu_indices: GPU indices to stream (for `PowerStreamingClient`).\n        cpu_indices: CPU indices to stream (for `PowerStreamingClient`).\n    \"\"\"\n    return cls(\n        socket_path=socket_path,\n        token=token,\n        gpu_indices=gpu_indices,\n        cpu_indices=cpu_indices,\n    )\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdConfig.from_env","title":"from_env  <code>classmethod</code>","text":"<pre><code>from_env()\n</code></pre> <p>Create from environment variables.</p> <p>Tries <code>ZEUSD_SOCK_PATH</code> (UDS) first, then <code>ZEUSD_HOST_PORT</code> (TCP). <code>ZEUSD_HOST_PORT</code> should be <code>host:port</code>. <code>ZEUSD_TOKEN</code> is read for JWT authentication.</p> <p>Returns None if neither env var is set.</p> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>@classmethod\ndef from_env(cls) -&gt; ZeusdConfig | None:\n    \"\"\"Create from environment variables.\n\n    Tries `ZEUSD_SOCK_PATH` (UDS) first, then `ZEUSD_HOST_PORT` (TCP).\n    `ZEUSD_HOST_PORT` should be `host:port`. `ZEUSD_TOKEN` is read for\n    JWT authentication.\n\n    Returns None if neither env var is set.\n    \"\"\"\n    token = os.environ.get(\"ZEUSD_TOKEN\")\n    sock = os.environ.get(\"ZEUSD_SOCK_PATH\")\n    if sock is not None:\n        return cls.uds(socket_path=sock, token=token)\n    host_port = os.environ.get(\"ZEUSD_HOST_PORT\")\n    if host_port is not None:\n        return cls(host_port=host_port, token=token)\n    return None\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdConfig.make_client","title":"make_client","text":"<pre><code>make_client()\n</code></pre> <p>Create an httpx.Client with the appropriate transport and auth.</p> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>def make_client(self) -&gt; httpx.Client:\n    \"\"\"Create an httpx.Client with the appropriate transport and auth.\"\"\"\n    headers = self._auth_headers()\n    if self._is_uds:\n        transport = httpx.HTTPTransport(uds=self.socket_path)\n        return httpx.Client(transport=transport, headers=headers)\n    return httpx.Client(headers=headers)\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdConfig.url","title":"url","text":"<pre><code>url(path)\n</code></pre> <p>Build the full URL for the given path.</p> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>def url(self, path: str) -&gt; str:\n    \"\"\"Build the full URL for the given path.\"\"\"\n    if self._is_uds:\n        return f\"http://localhost{path}\"\n    return f\"http://{self.host_port}{path}\"\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdClient","title":"ZeusdClient","text":"<p>Authenticated client for a Zeusd daemon.</p> <p>Handles connection, service discovery, and JWT authentication in one place.  Provides typed methods for every Zeusd endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ZeusdConfig | None</code> <p>Connection configuration.  If None, tries environment variables: <code>ZEUSD_SOCK_PATH</code> (UDS) first, then <code>ZEUSD_HOST_PORT</code> (TCP).</p> <code>None</code> <p>Raises:</p> Type Description <code>ZeusdConnectionError</code> <p>If the daemon is unreachable.</p> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>class ZeusdClient:\n    \"\"\"Authenticated client for a Zeusd daemon.\n\n    Handles connection, service discovery, and JWT authentication in\n    one place.  Provides typed methods for every Zeusd endpoint.\n\n    Args:\n        config: Connection configuration.  If None, tries environment\n            variables: `ZEUSD_SOCK_PATH` (UDS) first, then\n            `ZEUSD_HOST_PORT` (TCP).\n\n    Raises:\n        ZeusdConnectionError: If the daemon is unreachable.\n    \"\"\"\n\n    def __init__(self, config: ZeusdConfig | None = None) -&gt; None:\n        \"\"\"Initialize the client, run discovery, and attempt authentication.\"\"\"\n        if config is None:\n            config = ZeusdConfig.from_env()\n            if config is None:\n                raise ZeusdConnectionError(\n                    \"No Zeusd connection configured. Set ZEUSD_SOCK_PATH or ZEUSD_HOST_PORT, or pass a ZeusdConfig.\"\n                )\n        self._config = config\n        self._client = config.make_client()\n\n        try:\n            resp = self._client.get(config.url(\"/discover\"))\n        except httpx.RequestError as exc:\n            raise ZeusdConnectionError(f\"Cannot reach Zeusd at {config.endpoint}: {exc}\") from exc\n        if resp.status_code != 200:\n            raise ZeusdConnectionError(\n                f\"Zeusd at {config.endpoint} returned HTTP {resp.status_code} on /discover: {resp.text}\"\n            )\n        data = resp.json()\n        self._gpu_ids: list[int] = data.get(\"gpu_ids\", [])\n        self._cpu_ids: list[int] = data.get(\"cpu_ids\", [])\n        self._dram_available: list[bool] = data.get(\"dram_available\", [])\n        self._enabled_api_groups: set[str] = set(data.get(\"enabled_api_groups\", []))\n        self._auth_required: bool = data.get(\"auth_required\", False)\n\n        self._auth_error: str | None = None\n        self._granted_scopes: frozenset[str] = frozenset()\n        self._whoami_sub: str | None = None\n        self._whoami_exp: int | None = None\n        if self._auth_required:\n            if not config.token:\n                self._auth_error = (\n                    f\"Zeusd at {config.endpoint} requires authentication but \"\n                    \"no token was provided. Set the ZEUSD_TOKEN environment \"\n                    \"variable or pass token= in the config.\"\n                )\n            else:\n                whoami_resp = self._client.get(config.url(\"/auth/whoami\"))\n                if whoami_resp.status_code == 401:\n                    self._auth_error = f\"Token rejected by Zeusd at {config.endpoint}: {whoami_resp.text}\"\n                elif whoami_resp.status_code != 200:\n                    self._auth_error = (\n                        f\"Unexpected response from /auth/whoami at \"\n                        f\"{config.endpoint} (HTTP {whoami_resp.status_code}): \"\n                        f\"{whoami_resp.text}\"\n                    )\n                else:\n                    whoami = whoami_resp.json()\n                    self._granted_scopes = frozenset(whoami.get(\"scopes\", []))\n                    self._whoami_sub = whoami.get(\"sub\")\n                    self._whoami_exp = whoami.get(\"exp\")\n                    logger.info(\n                        \"Authenticated with Zeusd at %s as user '%s' (scopes: %s)\",\n                        config.endpoint,\n                        self._whoami_sub,\n                        sorted(self._granted_scopes),\n                    )\n            if self._auth_error:\n                logger.warning(\"Auth issue with Zeusd at %s: %s\", config.endpoint, self._auth_error)\n\n    @property\n    def endpoint(self) -&gt; str:\n        \"\"\"Human-readable identifier for this connection.\"\"\"\n        return self._config.endpoint\n\n    @property\n    def gpu_ids(self) -&gt; list[int]:\n        \"\"\"GPU device indices available on this daemon.\"\"\"\n        return list(self._gpu_ids)\n\n    @property\n    def cpu_ids(self) -&gt; list[int]:\n        \"\"\"CPU device indices available on this daemon.\"\"\"\n        return list(self._cpu_ids)\n\n    @property\n    def dram_available(self) -&gt; list[bool]:\n        \"\"\"Per-CPU DRAM energy availability, aligned with `cpu_ids`.\"\"\"\n        return list(self._dram_available)\n\n    @property\n    def auth_required(self) -&gt; bool:\n        \"\"\"Whether this daemon requires JWT authentication.\"\"\"\n        return self._auth_required\n\n    @property\n    def auth_error(self) -&gt; str | None:\n        \"\"\"Auth error message, or None if auth succeeded or is not required.\"\"\"\n        return self._auth_error\n\n    @property\n    def granted_scopes(self) -&gt; frozenset[str]:\n        \"\"\"Scopes granted by the current token (empty if auth is off or failed).\"\"\"\n        return self._granted_scopes\n\n    def _can(self, api_group: str, scope: str) -&gt; bool:\n        if api_group not in self._enabled_api_groups:\n            return False\n        return not (self._auth_required and scope not in self._granted_scopes)\n\n    @property\n    def can_read_gpu(self) -&gt; bool:\n        \"\"\"Whether GPU read endpoints are accessible.\"\"\"\n        return self._can(\"gpu-read\", \"gpu-read\")\n\n    @property\n    def can_control_gpu(self) -&gt; bool:\n        \"\"\"Whether GPU control endpoints are accessible.\"\"\"\n        return self._can(\"gpu-control\", \"gpu-control\")\n\n    @property\n    def can_read_cpu(self) -&gt; bool:\n        \"\"\"Whether CPU read endpoints are accessible.\"\"\"\n        return self._can(\"cpu-read\", \"cpu-read\")\n\n    def get_gpu_energy(self, gpu_ids: list[int]) -&gt; dict[int, int]:\n        \"\"\"Get cumulative energy consumption per GPU.\n\n        Args:\n            gpu_ids: GPU indices to query.\n\n        Returns:\n            Mapping of GPU index to cumulative energy in millijoules.\n        \"\"\"\n        resp = self._client.get(\n            self._config.url(\"/gpu/get_cumulative_energy\"),\n            params={\"gpu_ids\": \",\".join(str(i) for i in gpu_ids)},\n        )\n        self._check(resp, \"get_gpu_energy\")\n        data = resp.json()\n        return {int(k): v[\"energy_mj\"] for k, v in data.items()}\n\n    def get_gpu_power(self, gpu_ids: list[int] | None = None) -&gt; GpuPowerSnapshot:\n        \"\"\"Get instantaneous GPU power readings.\n\n        Args:\n            gpu_ids: GPU indices to query.  None means all.\n\n        Returns:\n            Snapshot with timestamp and per-GPU power in milliwatts.\n        \"\"\"\n        params: dict[str, str] = {}\n        if gpu_ids is not None:\n            params[\"gpu_ids\"] = \",\".join(str(i) for i in gpu_ids)\n        resp = self._client.get(self._config.url(\"/gpu/get_power\"), params=params)\n        self._check(resp, \"get_gpu_power\")\n        data = resp.json()\n        return GpuPowerSnapshot(\n            timestamp_ms=data[\"timestamp_ms\"],\n            power_mw={int(k): v for k, v in data[\"power_mw\"].items()},\n        )\n\n    def set_power_limit(self, gpu_ids: list[int], power_limit_mw: int, block: bool = True) -&gt; None:\n        \"\"\"Set the power management limit for the given GPUs.\"\"\"\n        resp = self._client.post(\n            self._config.url(\"/gpu/set_power_limit\"),\n            params={\n                \"gpu_ids\": \",\".join(str(i) for i in gpu_ids),\n                \"power_limit_mw\": str(power_limit_mw),\n                \"block\": \"true\" if block else \"false\",\n            },\n        )\n        self._check(resp, \"set_power_limit\")\n\n    def set_persistence_mode(self, gpu_ids: list[int], enabled: bool, block: bool = True) -&gt; None:\n        \"\"\"Set persistence mode for the given GPUs.\"\"\"\n        resp = self._client.post(\n            self._config.url(\"/gpu/set_persistence_mode\"),\n            params={\n                \"gpu_ids\": \",\".join(str(i) for i in gpu_ids),\n                \"enabled\": \"true\" if enabled else \"false\",\n                \"block\": \"true\" if block else \"false\",\n            },\n        )\n        self._check(resp, \"set_persistence_mode\")\n\n    def set_gpu_locked_clocks(\n        self,\n        gpu_ids: list[int],\n        min_clock_mhz: int,\n        max_clock_mhz: int,\n        block: bool = True,\n    ) -&gt; None:\n        \"\"\"Lock the GPU clock to a specified range (MHz).\"\"\"\n        resp = self._client.post(\n            self._config.url(\"/gpu/set_gpu_locked_clocks\"),\n            params={\n                \"gpu_ids\": \",\".join(str(i) for i in gpu_ids),\n                \"min_clock_mhz\": str(min_clock_mhz),\n                \"max_clock_mhz\": str(max_clock_mhz),\n                \"block\": \"true\" if block else \"false\",\n            },\n        )\n        self._check(resp, \"set_gpu_locked_clocks\")\n\n    def reset_gpu_locked_clocks(self, gpu_ids: list[int], block: bool = True) -&gt; None:\n        \"\"\"Reset locked GPU clocks to the default.\"\"\"\n        resp = self._client.post(\n            self._config.url(\"/gpu/reset_gpu_locked_clocks\"),\n            params={\n                \"gpu_ids\": \",\".join(str(i) for i in gpu_ids),\n                \"block\": \"true\" if block else \"false\",\n            },\n        )\n        self._check(resp, \"reset_gpu_locked_clocks\")\n\n    def set_mem_locked_clocks(\n        self,\n        gpu_ids: list[int],\n        min_clock_mhz: int,\n        max_clock_mhz: int,\n        block: bool = True,\n    ) -&gt; None:\n        \"\"\"Lock the memory clock to a specified range (MHz).\"\"\"\n        resp = self._client.post(\n            self._config.url(\"/gpu/set_mem_locked_clocks\"),\n            params={\n                \"gpu_ids\": \",\".join(str(i) for i in gpu_ids),\n                \"min_clock_mhz\": str(min_clock_mhz),\n                \"max_clock_mhz\": str(max_clock_mhz),\n                \"block\": \"true\" if block else \"false\",\n            },\n        )\n        self._check(resp, \"set_mem_locked_clocks\")\n\n    def reset_mem_locked_clocks(self, gpu_ids: list[int], block: bool = True) -&gt; None:\n        \"\"\"Reset locked memory clocks to the default.\"\"\"\n        resp = self._client.post(\n            self._config.url(\"/gpu/reset_mem_locked_clocks\"),\n            params={\n                \"gpu_ids\": \",\".join(str(i) for i in gpu_ids),\n                \"block\": \"true\" if block else \"false\",\n            },\n        )\n        self._check(resp, \"reset_mem_locked_clocks\")\n\n    def get_cpu_energy(\n        self,\n        cpu_ids: list[int],\n        cpu: bool = True,\n        dram: bool = True,\n    ) -&gt; dict[int, CpuEnergyResult]:\n        \"\"\"Get cumulative energy consumption per CPU.\n\n        Args:\n            cpu_ids: CPU indices to query.\n            cpu: Whether to include CPU package energy.\n            dram: Whether to include DRAM energy.\n\n        Returns:\n            Mapping of CPU index to energy results.\n        \"\"\"\n        resp = self._client.get(\n            self._config.url(\"/cpu/get_cumulative_energy\"),\n            params={\n                \"cpu_ids\": \",\".join(str(i) for i in cpu_ids),\n                \"cpu\": \"true\" if cpu else \"false\",\n                \"dram\": \"true\" if dram else \"false\",\n            },\n        )\n        self._check(resp, \"get_cpu_energy\")\n        data = resp.json()\n        return {\n            int(k): CpuEnergyResult(\n                cpu_energy_uj=v.get(\"cpu_energy_uj\"),\n                dram_energy_uj=v.get(\"dram_energy_uj\"),\n            )\n            for k, v in data.items()\n        }\n\n    def get_cpu_power(self, cpu_ids: list[int] | None = None) -&gt; CpuPowerSnapshot:\n        \"\"\"Get instantaneous CPU power readings.\n\n        Args:\n            cpu_ids: CPU indices to query.  None means all.\n\n        Returns:\n            Snapshot with timestamp and per-CPU power in milliwatts.\n        \"\"\"\n        params: dict[str, str] = {}\n        if cpu_ids is not None:\n            params[\"cpu_ids\"] = \",\".join(str(i) for i in cpu_ids)\n        resp = self._client.get(self._config.url(\"/cpu/get_power\"), params=params)\n        self._check(resp, \"get_cpu_power\")\n        data = resp.json()\n        return CpuPowerSnapshot(\n            timestamp_ms=data[\"timestamp_ms\"],\n            power_mw={\n                int(k): CpuDramPower(cpu_mw=v[\"cpu_mw\"], dram_mw=v.get(\"dram_mw\")) for k, v in data[\"power_mw\"].items()\n            },\n        )\n\n    def get_time(self) -&gt; float:\n        \"\"\"Get daemon timestamp in seconds.\"\"\"\n        resp = self._client.get(self._config.url(\"/time\"))\n        self._check(resp, \"get_time\")\n        return resp.json()[\"timestamp_ms\"] / 1000.0\n\n    def make_client(self) -&gt; httpx.Client:\n        \"\"\"Create a new httpx.Client with this client's transport and auth.\n\n        Used by `PowerStreamingClient` for SSE streaming connections\n        where a dedicated, long-lived httpx.Client is needed.\n        \"\"\"\n        return self._config.make_client()\n\n    def url(self, path: str) -&gt; str:\n        \"\"\"Build the full URL for the given path.\n\n        Used together with `make_client()` for streaming URLs.\n        \"\"\"\n        return self._config.url(path)\n\n    @staticmethod\n    def _check(resp: httpx.Response, operation: str) -&gt; None:\n        \"\"\"Raise ZeusdError if the response is not 200.\"\"\"\n        if resp.status_code != 200:\n            # Import here to avoid circular import at module level.\n            from zeus.device.exception import ZeusdError\n\n            raise ZeusdError(f\"Failed to {operation}: {resp.text}\")\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdClient.endpoint","title":"endpoint  <code>property</code>","text":"<pre><code>endpoint\n</code></pre> <p>Human-readable identifier for this connection.</p>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdClient.gpu_ids","title":"gpu_ids  <code>property</code>","text":"<pre><code>gpu_ids\n</code></pre> <p>GPU device indices available on this daemon.</p>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdClient.cpu_ids","title":"cpu_ids  <code>property</code>","text":"<pre><code>cpu_ids\n</code></pre> <p>CPU device indices available on this daemon.</p>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdClient.dram_available","title":"dram_available  <code>property</code>","text":"<pre><code>dram_available\n</code></pre> <p>Per-CPU DRAM energy availability, aligned with <code>cpu_ids</code>.</p>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdClient.auth_required","title":"auth_required  <code>property</code>","text":"<pre><code>auth_required\n</code></pre> <p>Whether this daemon requires JWT authentication.</p>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdClient.auth_error","title":"auth_error  <code>property</code>","text":"<pre><code>auth_error\n</code></pre> <p>Auth error message, or None if auth succeeded or is not required.</p>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdClient.granted_scopes","title":"granted_scopes  <code>property</code>","text":"<pre><code>granted_scopes\n</code></pre> <p>Scopes granted by the current token (empty if auth is off or failed).</p>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdClient.can_read_gpu","title":"can_read_gpu  <code>property</code>","text":"<pre><code>can_read_gpu\n</code></pre> <p>Whether GPU read endpoints are accessible.</p>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdClient.can_control_gpu","title":"can_control_gpu  <code>property</code>","text":"<pre><code>can_control_gpu\n</code></pre> <p>Whether GPU control endpoints are accessible.</p>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdClient.can_read_cpu","title":"can_read_cpu  <code>property</code>","text":"<pre><code>can_read_cpu\n</code></pre> <p>Whether CPU read endpoints are accessible.</p>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdClient.__init__","title":"__init__","text":"<pre><code>__init__(config=None)\n</code></pre> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>def __init__(self, config: ZeusdConfig | None = None) -&gt; None:\n    \"\"\"Initialize the client, run discovery, and attempt authentication.\"\"\"\n    if config is None:\n        config = ZeusdConfig.from_env()\n        if config is None:\n            raise ZeusdConnectionError(\n                \"No Zeusd connection configured. Set ZEUSD_SOCK_PATH or ZEUSD_HOST_PORT, or pass a ZeusdConfig.\"\n            )\n    self._config = config\n    self._client = config.make_client()\n\n    try:\n        resp = self._client.get(config.url(\"/discover\"))\n    except httpx.RequestError as exc:\n        raise ZeusdConnectionError(f\"Cannot reach Zeusd at {config.endpoint}: {exc}\") from exc\n    if resp.status_code != 200:\n        raise ZeusdConnectionError(\n            f\"Zeusd at {config.endpoint} returned HTTP {resp.status_code} on /discover: {resp.text}\"\n        )\n    data = resp.json()\n    self._gpu_ids: list[int] = data.get(\"gpu_ids\", [])\n    self._cpu_ids: list[int] = data.get(\"cpu_ids\", [])\n    self._dram_available: list[bool] = data.get(\"dram_available\", [])\n    self._enabled_api_groups: set[str] = set(data.get(\"enabled_api_groups\", []))\n    self._auth_required: bool = data.get(\"auth_required\", False)\n\n    self._auth_error: str | None = None\n    self._granted_scopes: frozenset[str] = frozenset()\n    self._whoami_sub: str | None = None\n    self._whoami_exp: int | None = None\n    if self._auth_required:\n        if not config.token:\n            self._auth_error = (\n                f\"Zeusd at {config.endpoint} requires authentication but \"\n                \"no token was provided. Set the ZEUSD_TOKEN environment \"\n                \"variable or pass token= in the config.\"\n            )\n        else:\n            whoami_resp = self._client.get(config.url(\"/auth/whoami\"))\n            if whoami_resp.status_code == 401:\n                self._auth_error = f\"Token rejected by Zeusd at {config.endpoint}: {whoami_resp.text}\"\n            elif whoami_resp.status_code != 200:\n                self._auth_error = (\n                    f\"Unexpected response from /auth/whoami at \"\n                    f\"{config.endpoint} (HTTP {whoami_resp.status_code}): \"\n                    f\"{whoami_resp.text}\"\n                )\n            else:\n                whoami = whoami_resp.json()\n                self._granted_scopes = frozenset(whoami.get(\"scopes\", []))\n                self._whoami_sub = whoami.get(\"sub\")\n                self._whoami_exp = whoami.get(\"exp\")\n                logger.info(\n                    \"Authenticated with Zeusd at %s as user '%s' (scopes: %s)\",\n                    config.endpoint,\n                    self._whoami_sub,\n                    sorted(self._granted_scopes),\n                )\n        if self._auth_error:\n            logger.warning(\"Auth issue with Zeusd at %s: %s\", config.endpoint, self._auth_error)\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdClient.get_gpu_energy","title":"get_gpu_energy","text":"<pre><code>get_gpu_energy(gpu_ids)\n</code></pre> <p>Get cumulative energy consumption per GPU.</p> <p>Parameters:</p> Name Type Description Default <code>gpu_ids</code> <code>list[int]</code> <p>GPU indices to query.</p> required <p>Returns:</p> Type Description <code>dict[int, int]</code> <p>Mapping of GPU index to cumulative energy in millijoules.</p> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>def get_gpu_energy(self, gpu_ids: list[int]) -&gt; dict[int, int]:\n    \"\"\"Get cumulative energy consumption per GPU.\n\n    Args:\n        gpu_ids: GPU indices to query.\n\n    Returns:\n        Mapping of GPU index to cumulative energy in millijoules.\n    \"\"\"\n    resp = self._client.get(\n        self._config.url(\"/gpu/get_cumulative_energy\"),\n        params={\"gpu_ids\": \",\".join(str(i) for i in gpu_ids)},\n    )\n    self._check(resp, \"get_gpu_energy\")\n    data = resp.json()\n    return {int(k): v[\"energy_mj\"] for k, v in data.items()}\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdClient.get_gpu_power","title":"get_gpu_power","text":"<pre><code>get_gpu_power(gpu_ids=None)\n</code></pre> <p>Get instantaneous GPU power readings.</p> <p>Parameters:</p> Name Type Description Default <code>gpu_ids</code> <code>list[int] | None</code> <p>GPU indices to query.  None means all.</p> <code>None</code> <p>Returns:</p> Type Description <code>GpuPowerSnapshot</code> <p>Snapshot with timestamp and per-GPU power in milliwatts.</p> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>def get_gpu_power(self, gpu_ids: list[int] | None = None) -&gt; GpuPowerSnapshot:\n    \"\"\"Get instantaneous GPU power readings.\n\n    Args:\n        gpu_ids: GPU indices to query.  None means all.\n\n    Returns:\n        Snapshot with timestamp and per-GPU power in milliwatts.\n    \"\"\"\n    params: dict[str, str] = {}\n    if gpu_ids is not None:\n        params[\"gpu_ids\"] = \",\".join(str(i) for i in gpu_ids)\n    resp = self._client.get(self._config.url(\"/gpu/get_power\"), params=params)\n    self._check(resp, \"get_gpu_power\")\n    data = resp.json()\n    return GpuPowerSnapshot(\n        timestamp_ms=data[\"timestamp_ms\"],\n        power_mw={int(k): v for k, v in data[\"power_mw\"].items()},\n    )\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdClient.set_power_limit","title":"set_power_limit","text":"<pre><code>set_power_limit(gpu_ids, power_limit_mw, block=True)\n</code></pre> <p>Set the power management limit for the given GPUs.</p> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>def set_power_limit(self, gpu_ids: list[int], power_limit_mw: int, block: bool = True) -&gt; None:\n    \"\"\"Set the power management limit for the given GPUs.\"\"\"\n    resp = self._client.post(\n        self._config.url(\"/gpu/set_power_limit\"),\n        params={\n            \"gpu_ids\": \",\".join(str(i) for i in gpu_ids),\n            \"power_limit_mw\": str(power_limit_mw),\n            \"block\": \"true\" if block else \"false\",\n        },\n    )\n    self._check(resp, \"set_power_limit\")\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdClient.set_persistence_mode","title":"set_persistence_mode","text":"<pre><code>set_persistence_mode(gpu_ids, enabled, block=True)\n</code></pre> <p>Set persistence mode for the given GPUs.</p> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>def set_persistence_mode(self, gpu_ids: list[int], enabled: bool, block: bool = True) -&gt; None:\n    \"\"\"Set persistence mode for the given GPUs.\"\"\"\n    resp = self._client.post(\n        self._config.url(\"/gpu/set_persistence_mode\"),\n        params={\n            \"gpu_ids\": \",\".join(str(i) for i in gpu_ids),\n            \"enabled\": \"true\" if enabled else \"false\",\n            \"block\": \"true\" if block else \"false\",\n        },\n    )\n    self._check(resp, \"set_persistence_mode\")\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdClient.set_gpu_locked_clocks","title":"set_gpu_locked_clocks","text":"<pre><code>set_gpu_locked_clocks(gpu_ids, min_clock_mhz, max_clock_mhz, block=True)\n</code></pre> <p>Lock the GPU clock to a specified range (MHz).</p> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>def set_gpu_locked_clocks(\n    self,\n    gpu_ids: list[int],\n    min_clock_mhz: int,\n    max_clock_mhz: int,\n    block: bool = True,\n) -&gt; None:\n    \"\"\"Lock the GPU clock to a specified range (MHz).\"\"\"\n    resp = self._client.post(\n        self._config.url(\"/gpu/set_gpu_locked_clocks\"),\n        params={\n            \"gpu_ids\": \",\".join(str(i) for i in gpu_ids),\n            \"min_clock_mhz\": str(min_clock_mhz),\n            \"max_clock_mhz\": str(max_clock_mhz),\n            \"block\": \"true\" if block else \"false\",\n        },\n    )\n    self._check(resp, \"set_gpu_locked_clocks\")\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdClient.reset_gpu_locked_clocks","title":"reset_gpu_locked_clocks","text":"<pre><code>reset_gpu_locked_clocks(gpu_ids, block=True)\n</code></pre> <p>Reset locked GPU clocks to the default.</p> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>def reset_gpu_locked_clocks(self, gpu_ids: list[int], block: bool = True) -&gt; None:\n    \"\"\"Reset locked GPU clocks to the default.\"\"\"\n    resp = self._client.post(\n        self._config.url(\"/gpu/reset_gpu_locked_clocks\"),\n        params={\n            \"gpu_ids\": \",\".join(str(i) for i in gpu_ids),\n            \"block\": \"true\" if block else \"false\",\n        },\n    )\n    self._check(resp, \"reset_gpu_locked_clocks\")\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdClient.set_mem_locked_clocks","title":"set_mem_locked_clocks","text":"<pre><code>set_mem_locked_clocks(gpu_ids, min_clock_mhz, max_clock_mhz, block=True)\n</code></pre> <p>Lock the memory clock to a specified range (MHz).</p> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>def set_mem_locked_clocks(\n    self,\n    gpu_ids: list[int],\n    min_clock_mhz: int,\n    max_clock_mhz: int,\n    block: bool = True,\n) -&gt; None:\n    \"\"\"Lock the memory clock to a specified range (MHz).\"\"\"\n    resp = self._client.post(\n        self._config.url(\"/gpu/set_mem_locked_clocks\"),\n        params={\n            \"gpu_ids\": \",\".join(str(i) for i in gpu_ids),\n            \"min_clock_mhz\": str(min_clock_mhz),\n            \"max_clock_mhz\": str(max_clock_mhz),\n            \"block\": \"true\" if block else \"false\",\n        },\n    )\n    self._check(resp, \"set_mem_locked_clocks\")\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdClient.reset_mem_locked_clocks","title":"reset_mem_locked_clocks","text":"<pre><code>reset_mem_locked_clocks(gpu_ids, block=True)\n</code></pre> <p>Reset locked memory clocks to the default.</p> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>def reset_mem_locked_clocks(self, gpu_ids: list[int], block: bool = True) -&gt; None:\n    \"\"\"Reset locked memory clocks to the default.\"\"\"\n    resp = self._client.post(\n        self._config.url(\"/gpu/reset_mem_locked_clocks\"),\n        params={\n            \"gpu_ids\": \",\".join(str(i) for i in gpu_ids),\n            \"block\": \"true\" if block else \"false\",\n        },\n    )\n    self._check(resp, \"reset_mem_locked_clocks\")\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdClient.get_cpu_energy","title":"get_cpu_energy","text":"<pre><code>get_cpu_energy(cpu_ids, cpu=True, dram=True)\n</code></pre> <p>Get cumulative energy consumption per CPU.</p> <p>Parameters:</p> Name Type Description Default <code>cpu_ids</code> <code>list[int]</code> <p>CPU indices to query.</p> required <code>cpu</code> <code>bool</code> <p>Whether to include CPU package energy.</p> <code>True</code> <code>dram</code> <code>bool</code> <p>Whether to include DRAM energy.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[int, CpuEnergyResult]</code> <p>Mapping of CPU index to energy results.</p> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>def get_cpu_energy(\n    self,\n    cpu_ids: list[int],\n    cpu: bool = True,\n    dram: bool = True,\n) -&gt; dict[int, CpuEnergyResult]:\n    \"\"\"Get cumulative energy consumption per CPU.\n\n    Args:\n        cpu_ids: CPU indices to query.\n        cpu: Whether to include CPU package energy.\n        dram: Whether to include DRAM energy.\n\n    Returns:\n        Mapping of CPU index to energy results.\n    \"\"\"\n    resp = self._client.get(\n        self._config.url(\"/cpu/get_cumulative_energy\"),\n        params={\n            \"cpu_ids\": \",\".join(str(i) for i in cpu_ids),\n            \"cpu\": \"true\" if cpu else \"false\",\n            \"dram\": \"true\" if dram else \"false\",\n        },\n    )\n    self._check(resp, \"get_cpu_energy\")\n    data = resp.json()\n    return {\n        int(k): CpuEnergyResult(\n            cpu_energy_uj=v.get(\"cpu_energy_uj\"),\n            dram_energy_uj=v.get(\"dram_energy_uj\"),\n        )\n        for k, v in data.items()\n    }\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdClient.get_cpu_power","title":"get_cpu_power","text":"<pre><code>get_cpu_power(cpu_ids=None)\n</code></pre> <p>Get instantaneous CPU power readings.</p> <p>Parameters:</p> Name Type Description Default <code>cpu_ids</code> <code>list[int] | None</code> <p>CPU indices to query.  None means all.</p> <code>None</code> <p>Returns:</p> Type Description <code>CpuPowerSnapshot</code> <p>Snapshot with timestamp and per-CPU power in milliwatts.</p> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>def get_cpu_power(self, cpu_ids: list[int] | None = None) -&gt; CpuPowerSnapshot:\n    \"\"\"Get instantaneous CPU power readings.\n\n    Args:\n        cpu_ids: CPU indices to query.  None means all.\n\n    Returns:\n        Snapshot with timestamp and per-CPU power in milliwatts.\n    \"\"\"\n    params: dict[str, str] = {}\n    if cpu_ids is not None:\n        params[\"cpu_ids\"] = \",\".join(str(i) for i in cpu_ids)\n    resp = self._client.get(self._config.url(\"/cpu/get_power\"), params=params)\n    self._check(resp, \"get_cpu_power\")\n    data = resp.json()\n    return CpuPowerSnapshot(\n        timestamp_ms=data[\"timestamp_ms\"],\n        power_mw={\n            int(k): CpuDramPower(cpu_mw=v[\"cpu_mw\"], dram_mw=v.get(\"dram_mw\")) for k, v in data[\"power_mw\"].items()\n        },\n    )\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdClient.get_time","title":"get_time","text":"<pre><code>get_time()\n</code></pre> <p>Get daemon timestamp in seconds.</p> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>def get_time(self) -&gt; float:\n    \"\"\"Get daemon timestamp in seconds.\"\"\"\n    resp = self._client.get(self._config.url(\"/time\"))\n    self._check(resp, \"get_time\")\n    return resp.json()[\"timestamp_ms\"] / 1000.0\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdClient.make_client","title":"make_client","text":"<pre><code>make_client()\n</code></pre> <p>Create a new httpx.Client with this client's transport and auth.</p> <p>Used by <code>PowerStreamingClient</code> for SSE streaming connections where a dedicated, long-lived httpx.Client is needed.</p> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>def make_client(self) -&gt; httpx.Client:\n    \"\"\"Create a new httpx.Client with this client's transport and auth.\n\n    Used by `PowerStreamingClient` for SSE streaming connections\n    where a dedicated, long-lived httpx.Client is needed.\n    \"\"\"\n    return self._config.make_client()\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdClient.url","title":"url","text":"<pre><code>url(path)\n</code></pre> <p>Build the full URL for the given path.</p> <p>Used together with <code>make_client()</code> for streaming URLs.</p> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>def url(self, path: str) -&gt; str:\n    \"\"\"Build the full URL for the given path.\n\n    Used together with `make_client()` for streaming URLs.\n    \"\"\"\n    return self._config.url(path)\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.ZeusdClient._check","title":"_check  <code>staticmethod</code>","text":"<pre><code>_check(resp, operation)\n</code></pre> <p>Raise ZeusdError if the response is not 200.</p> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>@staticmethod\ndef _check(resp: httpx.Response, operation: str) -&gt; None:\n    \"\"\"Raise ZeusdError if the response is not 200.\"\"\"\n    if resp.status_code != 200:\n        # Import here to avoid circular import at module level.\n        from zeus.device.exception import ZeusdError\n\n        raise ZeusdError(f\"Failed to {operation}: {resp.text}\")\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd.require_capabilities","title":"require_capabilities","text":"<pre><code>require_capabilities(client, *, read_gpu=False, control_gpu=False, read_cpu=False, gpu_ids=None, cpu_ids=None)\n</code></pre> <p>Fail-fast validation that the daemon supports what the caller needs.</p> <p>Checks that the required API groups are enabled, the required scopes are granted by the token, and that the requested device IDs are available on the daemon.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>ZeusdClient</code> <p>The ZeusdClient to validate against.</p> required <code>read_gpu</code> <code>bool</code> <p>Require the gpu-read capability.</p> <code>False</code> <code>control_gpu</code> <code>bool</code> <p>Require the gpu-control capability.</p> <code>False</code> <code>read_cpu</code> <code>bool</code> <p>Require the cpu-read capability.</p> <code>False</code> <code>gpu_ids</code> <code>list[int] | None</code> <p>GPU indices that must be available.</p> <code>None</code> <code>cpu_ids</code> <code>list[int] | None</code> <p>CPU indices that must be available.</p> <code>None</code> <p>Raises:</p> Type Description <code>ZeusdCapabilityError</code> <p>If any requirement is not met.</p> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>def require_capabilities(\n    client: ZeusdClient,\n    *,\n    read_gpu: bool = False,\n    control_gpu: bool = False,\n    read_cpu: bool = False,\n    gpu_ids: list[int] | None = None,\n    cpu_ids: list[int] | None = None,\n) -&gt; None:\n    \"\"\"Fail-fast validation that the daemon supports what the caller needs.\n\n    Checks that the required API groups are enabled, the required scopes\n    are granted by the token, and that the requested device IDs are\n    available on the daemon.\n\n    Args:\n        client: The ZeusdClient to validate against.\n        read_gpu: Require the gpu-read capability.\n        control_gpu: Require the gpu-control capability.\n        read_cpu: Require the cpu-read capability.\n        gpu_ids: GPU indices that must be available.\n        cpu_ids: CPU indices that must be available.\n\n    Raises:\n        ZeusdCapabilityError: If any requirement is not met.\n    \"\"\"\n    if client.auth_error:\n        raise ZeusdAuthError(client.auth_error)\n\n    errors: list[str] = []\n\n    if read_gpu and not client.can_read_gpu:\n        errors.append(_capability_reason(client, \"gpu-read\"))\n    if control_gpu and not client.can_control_gpu:\n        errors.append(_capability_reason(client, \"gpu-control\"))\n    if read_cpu and not client.can_read_cpu:\n        errors.append(_capability_reason(client, \"cpu-read\"))\n\n    if gpu_ids is not None:\n        available = set(client.gpu_ids)\n        missing = set(gpu_ids) - available\n        if missing:\n            errors.append(f\"GPU indices {sorted(missing)} not available (available: {sorted(available)})\")\n\n    if cpu_ids is not None:\n        available = set(client.cpu_ids)\n        missing = set(cpu_ids) - available\n        if missing:\n            errors.append(f\"CPU indices {sorted(missing)} not available (available: {sorted(available)})\")\n\n    if errors:\n        raise ZeusdCapabilityError(f\"Zeusd at {client.endpoint}: \" + \"; \".join(errors))\n</code></pre>"},{"location":"reference/utils/zeusd/#zeus.utils.zeusd._capability_reason","title":"_capability_reason","text":"<pre><code>_capability_reason(client, scope)\n</code></pre> <p>Build a human-readable reason why a capability is unavailable.</p> Source code in <code>zeus/utils/zeusd.py</code> <pre><code>def _capability_reason(client: ZeusdClient, scope: str) -&gt; str:\n    \"\"\"Build a human-readable reason why a capability is unavailable.\"\"\"\n    if scope not in client._enabled_api_groups:\n        return f\"API group '{scope}' is not enabled on this server\"\n    if client.auth_required and scope not in client.granted_scopes:\n        return f\"Token lacks required scope '{scope}' (granted: {sorted(client.granted_scopes)})\"\n    return f\"'{scope}' is not available\"\n</code></pre>"},{"location":"research_overview/","title":"Research Overview","text":""},{"location":"research_overview/#research-overview","title":"Research Overview","text":"<p>Zeus is rooted on multiple research papers. Even more research is ongoing, and Zeus will continue to expand and get better at what it's doing.</p> <ol> <li>Zeus (NSDI 23): Paper | Blog | Slides</li> <li>Chase (ICLR Workshop 23): Paper</li> <li>Perseus (SOSP 24): Paper | Blog | Slides</li> <li>The ML.ENERGY Benchmark (NeurIPS 25 D&amp;B Spotlight): Paper | Repository | Leaderboard</li> <li>Where Do the Joules Go? Diagnosing Inference Energy Consumption: ArXiv | Blog</li> </ol> <p>Please cite the NSDI 2023 paper to reference the open-source Zeus library in general:</p> <pre><code>@inproceedings{zeus-nsdi23,\n    title     = {Zeus: Understanding and Optimizing {GPU} Energy Consumption of {DNN} Training},\n    author    = {Jie You and Jae-Won Chung and Mosharaf Chowdhury},\n    booktitle = {USENIX NSDI},\n    year      = {2023}\n}\n</code></pre>"},{"location":"research_overview/perseus/","title":"Perseus (SOSP '24)","text":""},{"location":"research_overview/perseus/#perseusreducing-energy-bloat-in-large-model-training","title":"Perseus:Reducing Energy Bloat in Large Model Training","text":"<p>SOSP '24</p> <p>Paper | ArXiv</p>"},{"location":"research_overview/perseus/#abstract","title":"Abstract","text":"<p>With the rise of GenAI, power delivery became one of the largest limiting factors in building and operating datacenters for AI workloads. However, we observe that not all energy consumed during training directly contributes to end-to-end throughput, and a significant portion can be removed without slowing down training, which we call energy bloat.</p> <p>In this work, we identify two independent sources of energy bloat in large model training and propose Perseus, a training system that mitigates both. To do this, Perseus obtains the \u201citeration time\u2013energy\u201d Pareto frontier of any large model training job using an efficient graph cut-based algorithm and schedules the energy consumption of computations across time to reduce both types of energy bloat. Evaluation on large models like GPT-3 shows that Perseus reduces the energy consumption of large model training by up to 30% with little throughput loss and no hardware modification.</p>"},{"location":"research_overview/perseus/#the-energy-bottleneck","title":"The Energy Bottleneck","text":"<p>\"We would probably build out bigger clusters if we could get the energy to do it.\"</p> <p>\"No one has built a 1 GW datacenter yet. I think it will happen. This is only a matter of time.\"</p> <p>-- Mark Zuckerberg's interview with Dwarkesh Patel</p> <p>Exponentially growing things only do so until they hit a bottleneck, which becomes the next big challenge to solve. Today, energy is one of such bottlenecks for GenAI. People need more compute (usually from GPUs) to train and serve large models, but it's very difficult to get access to electricity which ultimately powers those hardware.<sup>1</sup><sup>2</sup></p> <p>The goal of Perseus is to reduce the total energy consumption of large model training without slowing down training by finding and removing energy wastage during training, which we call energy bloat. This leads to both less total energy consumption and lower average power draw.</p>"},{"location":"research_overview/perseus/#energy-bloat","title":"Energy Bloat","text":"<p>The core idea of energy bloat is that if some computation is running at an unnecessarily fast speed, it may be wasting energy. Perseus identifies two independent sources of energy bloat for training pipelines (pipeline as in pipeline parallel training), and proposes an optimization method that reduces both.</p>"},{"location":"research_overview/perseus/#intrinsic-bloat","title":"Intrinsic Bloat","text":"<p>Large model training requires the distribution of work across multiple GPUs using a combination of multiple parallelization methods. The core observation of Perseus is that especially for pipeline parallelism, work cannot be perfectly split and balanced across every GPU; some GPUs have more work to do and some less. GPUs with smaller amounts of work finish before GPUs with more amounts of work, but ultimately training throughput is bound by GPUs with the most amount of work. In other words, GPUs with lighter load are running unnecessarily fast and generating energy bloat.</p>"},{"location":"research_overview/perseus/#extrinsic-bloat","title":"Extrinsic Bloat","text":"<p>In large scale training that involves tens of thousands of GPUs, stragglers (or slowdowns) become a reality -- hardware and software faults, slowdowns due to thermal and power throttling, data pipeline stalls, and more. When GPU stragglers emerge, the training pipeline (among multiple data parallel pipelines) that contains the straggler GPU will slow down, and every other pipeline must wait for the straggler pipeline to finish before they can synchronize gradients and move on to the next iteration. This means that when a straggler pipeline emerges, running other pipelines at their full speed is wasting energy -- they can slow down and reduce energy bloat.</p>"},{"location":"research_overview/perseus/#reducing-energy-bloat","title":"Reducing Energy Bloat","text":"<p>To reduce intrinsic bloat, we need to precisely slow down select computations in the pipeline without affecting its end-to-end iteration time. On the other hand, to reduce extrinsic bloat, we need to figure out how to make the whole pipeline slower, while keeping intrinsic bloat low.</p> Perseus discovers the entire iteration time--energy Pareto frontier. <p>To do so, Perseus pre-characterizes every GPU frequency plan on the Iteration time--energy Pareto frontier upfront. Every frequency plan on this Pareto frontier has low intrinsic bloat, and when a straggler pipeline emerges, Perseus can simply look up the frequency plan that leads to the right pipeline iteration time on the frontier and deploy it to all non-straggler pipelines.</p> <p>You can also see Perseus's optimizer in action:</p> Perseus optimizer in action <p>As you can see, Perseus controls the GPU frequency of each forward and backward computation in one training pipeline. One training pipeline is actually a DAG of computations. Assigning the right GPU frequency to each computation while controlling the end-to-end execution time of the DAG and minimizing its total energy consumption happens to be an NP-Hard problem, but Perseus introduces a cool graph cut-based algorithm that produces high-quality approximate solutions. Check out the algorithm in our paper!</p>"},{"location":"research_overview/perseus/#using-perseus","title":"Using Perseus","text":"<p>Perseus is open-sourced as the Pipeline Frequency Optimizer. It's still in early-stage development and we have a lot of sharp edges to cut, but we're hoping to talk more with the community to drive its development. Let's chat!</p> <ol> <li> <p>CBRE, Global Data Center Trends 2023, https://www.cbre.com/insights/reports/global-data-center-trends-2023 \u21a9</p> </li> <li> <p>CBRE, Global Data Center Trends 2024, https://www.cbre.com/insights/reports/global-data-center-trends-2024 \u21a9</p> </li> </ol>"},{"location":"research_overview/zeus/","title":"Zeus (NSDI '23)","text":""},{"location":"research_overview/zeus/#zeus-understanding-and-optimizinggpu-energy-consumption-of-dnn-training","title":"Zeus: Understanding and OptimizingGPU Energy Consumption of DNN Training","text":"<p>NSDI '23</p> <p>Paper | Slides | YouTube</p>"},{"location":"research_overview/zeus/#abstract","title":"Abstract","text":"<p>Training Deep Neural Networks (DNNs) is becoming more and more resource- and energy-intensive every year. Unfortunately, existing works primarily focus on optimizing DNN training for faster completion, often without considering the impact on energy efficiency.</p> <p>In this paper, we observe that common practices of DNN training can lead to inefficient energy usage. More importantly, we demonstrate that there is a tradeoff between energy consumption and performance optimization. To this end, we propose an optimization framework, Zeus, to navigate this tradeoff by automatically finding optimal job- and GPU-level configurations for recurring DNN training jobs. Zeus does not require any offline profiling and can adapt to data drifts.</p>"},{"location":"research_overview/zeus/#why-care-about-gpu-energy","title":"Why care about GPU energy?","text":"<p>Recent years have seen an increasing adoption of DNNs for intelligent applications. Large clusters of GPUs were created to support such growth, and the surge continues.</p> <p>GPUs are power-hungry hardware; GPUs consume ~ 70% of the power of the entire server when training DNNs.<sup>1</sup> At extreme scales, training the GPT-3 model just once consumes 1,287 MWh,<sup>2</sup> which is enough to supply an average US household for 120 years.<sup>3</sup></p> <p>However, latency and throughput have been the primary targets of existing optimization techniques, devoid of any careful consideration of how such optimizations might impact energy efficiency. We argue that energy should be considered as the third dimension.</p>"},{"location":"research_overview/zeus/#opportunity-for-energy-savings","title":"Opportunity for energy savings","text":"<p>We observe that common practices of DNN training can often lead to energy inefficiency.</p> <p>To see this, we trained<sup>4</sup> the same DNN multiple times using a sweep of possible batch sizes and GPU power limits.<sup>5</sup></p> Potential energy savings on an NVIDIA V100 GPU. <p>The baseline dotted line uses the default batch size from the model's publication and the default (maximum) GPU power limit. It can be seen that choosing the best batch size and power limit can lead to large energy savings.</p>"},{"location":"research_overview/zeus/#tradeoff-between-time-energy","title":"Tradeoff between time &amp; energy","text":"<p>Is energy reduction free?</p> <p>We discover that there is a tradeoff between DNN training time and energy consumption.</p> All (batch size, power limit) configurations and their time/energy consumption. The energy-time Pareto frontier zoomed in. <p>These results are from training DeepSpeech2 on LibriSpeech with an NVIDIA V100 GPU. Notice the yellow Pareto frontier of efficient (time, energy) pairs, resulting from a set of efficient (batch size, power limit) knobs.</p>"},{"location":"research_overview/zeus/#navigating-the-tradeoff","title":"Navigating the tradeoff","text":"<p>All points on the Pareto frontier are efficient, but which one is the best?</p> <p>Different users will have different answers, because they have different preferences of how they would like to trade off time and energy.<sup>6</sup></p> <p>To allow users to express their tradeoff preference, we define a simple cost metric<sup>7</sup></p> \\[ \\textrm{Cost} = \\eta \\cdot \\textrm{Energy} + (1 - \\eta) \\cdot \\textrm{MaxPower} \\cdot \\textrm{Time,} \\] <p>where the user picks the value of \\(\\eta\\) between 0 and 1. Smaller \\(\\eta\\) values will reduce more time, while larger ones will prefer to reduce more energy.</p>"},{"location":"research_overview/zeus/#finding-the-optimal-knob","title":"Finding the optimal knob","text":"<p>Given the user's preference via the value of \\(\\eta\\), how do we find the best (batch size, power limit) knob on the Pareto frontier?</p> <p>This is no easy problem. We only have the Pareto frontier in the previous plot because we trained all possible combinations of batch size and power limit until completion to characterize the tradeoff.<sup>8</sup></p> <p>Fortunately, DNN training jobs often recur in production GPU clusters,<sup>9</sup> allowing us to explore, observe, and optimize across job recurrences.</p> <p>This results in two main components in Zeus:</p> <ul> <li>Just-In-Time energy profiler: Finds the optimal power limit via online profiling.</li> <li>Multi-Armed Bandit + Thompson Sampling: Finds the optimal batch size across recurring training runs.</li> </ul>"},{"location":"research_overview/zeus/#research-reproducibility","title":"Research reproducibility","text":"<p>We have our trace-driven simulator open-sourced here with instructions.</p>"},{"location":"research_overview/zeus/#extending-the-zeus-simulator","title":"Extending the Zeus simulator","text":"<p>Users can implement custom policies that optimize batch size and power limit, and plug it into the Zeus simulator. We have training and energy traces for 6 different DNNs and 4 different NVIDIA GPU microarchitectures here, which the simulator runs with.</p> <p>Zeus defines two abstract classes <code>BatchSizeOptimizer</code> and <code>PowerLimitOptimizer</code> in <code>zeus._legacy.policy.interface</code>. Each class optimizes the batch size and power limit of a recurring training job respectively. As in our paper, the batch size optimizer is first invoked to decide which batch size to use, and then the power limit optimizer is invoked with both the job and the batch size chosen to decide which power limit to use. You can find examples of policy implementations in <code>zeus._legacy.policy.optimizer</code>.</p> <p>The Zeus simulator (<code>Simulator</code>) accepts one <code>BatchSizeOptimizer</code> and <code>PowerLimitOptimizer</code> in its constructor. A full-example can be found here.</p> <ol> <li> <p>Jesse Dodge, Taylor Prewitt, Remi Tachet des Combes, Erika Odmark, Roy Schwartz, Emma Strubell, Alexandra Sasha Luccioni, Noah A. Smith, Nicole DeCario, and Will Buchanan. Measuring the carbon intensity of ai in cloud instances. In 2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201922, page 1877\u20131894, New York, NY, USA, 2022. Association for Computing Machinery.\u00a0\u21a9</p> </li> <li> <p>David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350, 2021.\u00a0\u21a9</p> </li> <li> <p>How much electricity does an American home use? https://www.eia.gov/tools/faqs/faq.php?id=97&amp;t=3.\u00a0\u21a9</p> </li> <li> <p>In all cases of training, we train until the DNN reaches a specific target validation metric. Thus, when we say time, it's TTA (Time To Accuracy). Likewise for energy, it's ETA (Enerty To Accuracy). Please refer to our paper for the complete workload table.\u00a0\u21a9</p> </li> <li> <p>It is possible to cap the maximum power draw of a GPU using NVML.\u00a0\u21a9</p> </li> <li> <p>For instance, some production training jobs might have tight deadlines; they probably don't want to trade time for energy savings. On the other hand, exploratory training jobs may have more leeway; it might make sense for them to reduce energy consumption at the cost of longer training time.\u00a0\u21a9</p> </li> <li> <p>\\(\\textrm{MaxPower}\\) is the maximum possible power limit of the GPU. It's just a constant number introduced to equalize the units of the left and right terms to Joules.\u00a0\u21a9</p> </li> <li> <p>Since doing this will consume so much time and energy, it may even offset or exceed the energy savings from choosing the optimal knobs if we decide to do it for every future incoming job!\u00a0\u21a9</p> </li> <li> <p>Kim Hazelwood, Sarah Bird, David Brooks, Soumith Chintala, Utku Diril, Dmytro Dzhulgakov, Mohamed Fawzy, Bill Jia, Yangqing Jia, Aditya Kalro, et al. Applied machine learning at facebook: A datacenter infrastructure perspective. In 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA), pages 620\u2013629. IEEE, 2018.\u00a0\u21a9</p> </li> </ol>"}]}